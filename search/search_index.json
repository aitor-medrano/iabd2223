{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . Este curso se ha dise\u00f1ado mediante un proceso de flexibilizaci\u00f3n del curriculum, mediante el cual hemos repartido los diferentes resultados de aprendizaje a lo largo de tres unidades formativas. Cada una de las unidades formativas contiene varias unidades de trabajo centradas en un \u00e1rea de trabajo concreta. En cada unidad formativa vamos a realizar como hilo conductor un Proyecto de Innovaci\u00f3n Aplicado (PIAFP) que dirija el aprendizaje del alumnado. En este sitio web podr\u00e1s consultar los apuntes y ejercicios que he trabajado directamente con el alumnado durante el curso 22/23: Unidad Formativa I - Toma de decisiones UT2 - Sistemas de almacenamiento UT5 - Ecosistema Hadoop UT6 - Reto: cloud UT7 - PIA FP Lara](https://aitor-medrano.github.io/pia2223/) Unidad Formativa II - Industria IoT Unidad Formativa III - Sistemas Expertos Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . Este curso se ha dise\u00f1ado mediante un proceso de flexibilizaci\u00f3n del curriculum, mediante el cual hemos repartido los diferentes resultados de aprendizaje a lo largo de tres unidades formativas. Cada una de las unidades formativas contiene varias unidades de trabajo centradas en un \u00e1rea de trabajo concreta. En cada unidad formativa vamos a realizar como hilo conductor un Proyecto de Innovaci\u00f3n Aplicado (PIAFP) que dirija el aprendizaje del alumnado. En este sitio web podr\u00e1s consultar los apuntes y ejercicios que he trabajado directamente con el alumnado durante el curso 22/23: Unidad Formativa I - Toma de decisiones UT2 - Sistemas de almacenamiento UT5 - Ecosistema Hadoop UT6 - Reto: cloud UT7 - PIA FP Lara](https://aitor-medrano.github.io/pia2223/) Unidad Formativa II - Industria IoT Unidad Formativa III - Sistemas Expertos Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"cloud/index.html","text":"Unidad de Trabajo 6.- Reto: Datos en el cloud \u00b6 Resultados de aprendizaje \u00b6 RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Ingenier\u00eda de datos / NoSQL Mi\u00e9rcoles 26 Oct 2p + 2o 24.- MongoDB Lunes 7 Nov 2p + 2o 26.- Modelado documental. Agregaciones en MongoDB Mi\u00e9rcoles 9 Nov 2p + 2o 28.- Formatos de datos Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Unidad de Trabajo 6.- Reto: *Datos en el cloud*"},{"location":"cloud/index.html#unidad-de-trabajo-6-reto-datos-en-el-cloud","text":"","title":"Unidad de Trabajo 6.- Reto: Datos en el cloud"},{"location":"cloud/index.html#resultados-de-aprendizaje","text":"RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.","title":"Resultados de aprendizaje"},{"location":"cloud/index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Ingenier\u00eda de datos / NoSQL Mi\u00e9rcoles 26 Oct 2p + 2o 24.- MongoDB Lunes 7 Nov 2p + 2o 26.- Modelado documental. Agregaciones en MongoDB Mi\u00e9rcoles 9 Nov 2p + 2o 28.- Formatos de datos Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Planificaci\u00f3n"},{"location":"cloud/01cloud.html","text":"Cloud \u00b6","title":"Cloud"},{"location":"cloud/01cloud.html#cloud","text":"","title":"Cloud"},{"location":"hadoop/index.html","text":"Unidad de Trabajo 5.- Ecosistema Hadoop \u00b6 Resultados de aprendizaje \u00b6 RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 34.- Hadoop Lunes 21 Nov 2p + 2o 35.- ETL/ELT Mi\u00e9rcoles 23 Nov 2p + 2o 36.- HDFS Mi\u00e9rcoles 23 Nov 2p + 2o 37.- Sqoop / Flume Lunes 28 Nov 2p + 2o 38.- Hive Lunes 28 Nov 2p + 2o","title":"Unidad de Trabajo 5.- Ecosistema Hadoop"},{"location":"hadoop/index.html#unidad-de-trabajo-5-ecosistema-hadoop","text":"","title":"Unidad de Trabajo 5.- Ecosistema Hadoop"},{"location":"hadoop/index.html#resultados-de-aprendizaje","text":"RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.","title":"Resultados de aprendizaje"},{"location":"hadoop/index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 34.- Hadoop Lunes 21 Nov 2p + 2o 35.- ETL/ELT Mi\u00e9rcoles 23 Nov 2p + 2o 36.- HDFS Mi\u00e9rcoles 23 Nov 2p + 2o 37.- Sqoop / Flume Lunes 28 Nov 2p + 2o 38.- Hive Lunes 28 Nov 2p + 2o","title":"Planificaci\u00f3n"},{"location":"hadoop/01hadoop.html","text":"Hadoop \u00b6 Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). La filosof\u00eda de Hadoop es almacenar todos los datos en un lugar y procesar los datos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos/ workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Cada vez que a\u00f1adimos un nuevo nodo esclavo, aumentamos tanto la capacidad como el rendimiento de nuestro sistema. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2. Componentes y Ecosistema \u00b6 El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop, como ficheros de logs, bloques de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft HDFS \u00b6 Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google explicando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos. Bloques \u00b6 Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS Namenode \u00b6 Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento. Datanode \u00b6 De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS MapReduce \u00b6 Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos al final del curso), que mejora el rendimiento por una orden de magnitud. YARN \u00b6 Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados. Componentes en YARN Resource Manager \u00b6 El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Node Manager \u00b6 Contenedores en NodeManager Arranca el Application Masters tras petici\u00f3n del Resource Manager , iniciando las tareas/jobs que le indique el Application Master . Gestiona los trabajos en contenedores proporcionado los recursos computacionales necesarios para las aplicaciones. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Tambi\u00e9n implementa heartbeats para mantener informado del estado al Resource Manager . Finalmente, almacena los logs de aplicaci\u00f3n en HDFS. Application Master \u00b6 El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ). Instalaci\u00f3n \u00b6 Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos. Configuraci\u00f3n \u00b6 Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> Puesta en marcha \u00b6 Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN Hola Mundo \u00b6 El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -01-15 12 :59:49,015 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -01-15 12 :59:49,844 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1642247847632_0001 2022 -01-15 12 :59:51,042 INFO input.FileInputFormat: Total input files to process : 1 2022 -01-15 12 :59:51,669 INFO mapreduce.JobSubmitter: number of splits:1 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642247847632_0001 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -01-15 12 :59:52,351 INFO conf.Configuration: resource-types.xml not found 2022 -01-15 12 :59:52,355 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -01-15 12 :59:53,142 INFO impl.YarnClientImpl: Submitted application application_1642247847632_0001 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1642247847632_0001/ 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: Running job: job_1642247847632_0001 2022 -01-15 13 :00:08,894 INFO mapreduce.Job: Job job_1642247847632_0001 running in uber mode : false 2022 -01-15 13 :00:08,932 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -01-15 13 :00:32,985 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -01-15 13 :00:47,344 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -01-15 13 :00:48,373 INFO mapreduce.Job: Job job_1642247847632_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -01-15 13 :00:48,679 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241519 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 20353 Total time spent by all reduces in occupied slots ( ms )= 12093 Total time spent by all map tasks ( ms )= 20353 Total time spent by all reduce tasks ( ms )= 12093 Total vcore-milliseconds taken by all map tasks = 20353 Total vcore-milliseconds taken by all reduce tasks = 12093 Total megabyte-milliseconds taken by all map tasks = 20841472 Total megabyte-milliseconds taken by all reduce tasks = 12383232 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 394 CPU time spent ( ms )= 7470 Physical memory ( bytes ) snapshot = 384565248 Virtual memory ( bytes ) snapshot = 5007564800 Total committed heap usage ( bytes )= 295571456 Peak Map Physical memory ( bytes )= 247332864 Peak Map Virtual memory ( bytes )= 2500415488 Peak Reduce Physical memory ( bytes )= 137232384 Peak Reduce Virtual memory ( bytes )= 2507149312 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/quijote/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el j ob de MapReduce* se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC MapReduce en Python \u00b6 El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python . Mapper \u00b6 Primero creamos el mapeador , el cual se encarga de parsear l\u00ednea a l\u00ednea el fragmento de documento que reciba, y va a generar una nueva salida con todas las palabras de manera que cada nueva l\u00ednea la compongan una tupla formada por la palabra, un tabulador y el n\u00famero 1 (hay una ocurrencia de dicha palabra) mapper.py #!/usr/bin/python3 import sys for linea in sys . stdin : # eliminamos los espacios de delante y de detr\u00e1s linea = linea . strip () # dividimos la l\u00ednea en palabras palabras = linea . split () # creamos tuplas de (palabra, 1) for palabra in palabras : print ( palabra , \" \\t 1\" ) Si queremos probar el mapper, podr\u00edamos ejecutar el siguiente comando: cat el_quijote.txt | python3 mapper.py Obteniendo un resultado similar a: ... gritos 1 al 1 cielo 1 alli\u0301 1 se 1 renovaron 1 las 1 maldiciones 1 ... Reducer \u00b6 A continuaci\u00f3n, en el reducer , vamos a recibir la salida del mapper y parseamos la cadena para separar la palabra del contador. Para llevar la cuenta de las palabras, vamos a meterlas dentro de un diccionario para incrementar las ocurrencias encontradas. Cuidado con la memoria En un caso real, hemos de evitar almacenar todos los datos que recibimos en memoria, ya que es posible que al trabajar con big data no quepa en la RAM de cada datanode . Para ello, se recomienda el uso de la librer\u00eda itertools , por ejemplo, utilizando la funci\u00f3n groupby() . Finalmente, volvemos a crear tuplas de palabra, tabulador y cantidad de ocurrencias. reducer.py #!/usr/bin/python3 import sys # inicializamos el diccionario dictPalabras = {} for linea in sys . stdin : # quitamos espacios de sobra linea = linea . strip () # parseamos la entrada de mapper.py palabra , cuenta = linea . split ( ' \\t ' , 1 ) # convertimos cuenta de string a int try : cuenta = int ( cuenta ) except ValueError : # cuenta no era un numero, descartamos la linea continue try : dictPalabras [ palabra ] += cuenta except : dictPalabras [ palabra ] = cuenta for palabra in dictPalabras . keys (): print ( palabra , \" \\t \" , dictPalabras [ palabra ]) Para probar el proceso completo, ejecutaremos el siguiente comando: cat el_quijote.txt | python3 mapper.py | python3 reducer.py > salida.tsv Si abrimos el fichero, podemos ver el resultado: salida.tsv don 1072 quijote 812 de 9035 la 5014 mancha 50 miguel 3 cervantes 3 ... Hadoop Streaming \u00b6 Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende, codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming envia los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <parametros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-r-00000 Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint . Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Realizar el ejemplo de MapReduce con el fichero de El Quijote utilizando el proceso que ofrece Hadoop . Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabr\u00e1s de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. (opcional) Desarrolla el ejemplo del art\u00edculo Creaci\u00f3n y ejecuci\u00f3n de un programa Python para Hadoop Map Reduce en Linux . Adjunta los scripts, el fichero de datos, y el fichero de resultado. Versi\u00f3n de Python En el art\u00edculo, en el encabezado del mapper y del reducer , utilizan como interprete de Python la ruta #!/usr/bin/python cuando en nuestra m\u00e1quina virtual ser\u00eda #!/usr/bin/python3 FIXME: corregir con info/apuntes del MEC, tema 1 BDA sobre especificaciones del hw de Hadoop","title":"Hadoop"},{"location":"hadoop/01hadoop.html#hadoop","text":"Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). La filosof\u00eda de Hadoop es almacenar todos los datos en un lugar y procesar los datos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos/ workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Cada vez que a\u00f1adimos un nuevo nodo esclavo, aumentamos tanto la capacidad como el rendimiento de nuestro sistema. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2.","title":"Hadoop"},{"location":"hadoop/01hadoop.html#componentes-y-ecosistema","text":"El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop, como ficheros de logs, bloques de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft","title":"Componentes y Ecosistema"},{"location":"hadoop/01hadoop.html#hdfs","text":"Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google explicando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos.","title":"HDFS"},{"location":"hadoop/01hadoop.html#bloques","text":"Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS","title":"Bloques"},{"location":"hadoop/01hadoop.html#namenode","text":"Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento.","title":"Namenode"},{"location":"hadoop/01hadoop.html#datanode","text":"De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS","title":"Datanode"},{"location":"hadoop/01hadoop.html#mapreduce","text":"Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos al final del curso), que mejora el rendimiento por una orden de magnitud.","title":"MapReduce"},{"location":"hadoop/01hadoop.html#yarn","text":"Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados. Componentes en YARN","title":"YARN"},{"location":"hadoop/01hadoop.html#resource-manager","text":"El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles.","title":"Resource Manager"},{"location":"hadoop/01hadoop.html#node-manager","text":"Contenedores en NodeManager Arranca el Application Masters tras petici\u00f3n del Resource Manager , iniciando las tareas/jobs que le indique el Application Master . Gestiona los trabajos en contenedores proporcionado los recursos computacionales necesarios para las aplicaciones. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Tambi\u00e9n implementa heartbeats para mantener informado del estado al Resource Manager . Finalmente, almacena los logs de aplicaci\u00f3n en HDFS.","title":"Node Manager"},{"location":"hadoop/01hadoop.html#application-master","text":"El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ).","title":"Application Master"},{"location":"hadoop/01hadoop.html#instalacion","text":"Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.","title":"Instalaci\u00f3n"},{"location":"hadoop/01hadoop.html#configuracion","text":"Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration>","title":"Configuraci\u00f3n"},{"location":"hadoop/01hadoop.html#puesta-en-marcha","text":"Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN","title":"Puesta en marcha"},{"location":"hadoop/01hadoop.html#hola-mundo","text":"El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -01-15 12 :59:49,015 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -01-15 12 :59:49,844 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1642247847632_0001 2022 -01-15 12 :59:51,042 INFO input.FileInputFormat: Total input files to process : 1 2022 -01-15 12 :59:51,669 INFO mapreduce.JobSubmitter: number of splits:1 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642247847632_0001 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -01-15 12 :59:52,351 INFO conf.Configuration: resource-types.xml not found 2022 -01-15 12 :59:52,355 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -01-15 12 :59:53,142 INFO impl.YarnClientImpl: Submitted application application_1642247847632_0001 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1642247847632_0001/ 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: Running job: job_1642247847632_0001 2022 -01-15 13 :00:08,894 INFO mapreduce.Job: Job job_1642247847632_0001 running in uber mode : false 2022 -01-15 13 :00:08,932 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -01-15 13 :00:32,985 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -01-15 13 :00:47,344 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -01-15 13 :00:48,373 INFO mapreduce.Job: Job job_1642247847632_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -01-15 13 :00:48,679 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241519 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 20353 Total time spent by all reduces in occupied slots ( ms )= 12093 Total time spent by all map tasks ( ms )= 20353 Total time spent by all reduce tasks ( ms )= 12093 Total vcore-milliseconds taken by all map tasks = 20353 Total vcore-milliseconds taken by all reduce tasks = 12093 Total megabyte-milliseconds taken by all map tasks = 20841472 Total megabyte-milliseconds taken by all reduce tasks = 12383232 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 394 CPU time spent ( ms )= 7470 Physical memory ( bytes ) snapshot = 384565248 Virtual memory ( bytes ) snapshot = 5007564800 Total committed heap usage ( bytes )= 295571456 Peak Map Physical memory ( bytes )= 247332864 Peak Map Virtual memory ( bytes )= 2500415488 Peak Reduce Physical memory ( bytes )= 137232384 Peak Reduce Virtual memory ( bytes )= 2507149312 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/quijote/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el j ob de MapReduce* se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC","title":"Hola Mundo"},{"location":"hadoop/01hadoop.html#mapreduce-en-python","text":"El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python .","title":"MapReduce en Python"},{"location":"hadoop/01hadoop.html#hadoop-streaming","text":"Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende, codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming envia los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <parametros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-r-00000","title":"Hadoop Streaming"},{"location":"hadoop/01hadoop.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint .","title":"Referencias"},{"location":"hadoop/01hadoop.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Realizar el ejemplo de MapReduce con el fichero de El Quijote utilizando el proceso que ofrece Hadoop . Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabr\u00e1s de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. (opcional) Desarrolla el ejemplo del art\u00edculo Creaci\u00f3n y ejecuci\u00f3n de un programa Python para Hadoop Map Reduce en Linux . Adjunta los scripts, el fichero de datos, y el fichero de resultado. Versi\u00f3n de Python En el art\u00edculo, en el encabezado del mapper y del reducer , utilizan como interprete de Python la ruta #!/usr/bin/python cuando en nuestra m\u00e1quina virtual ser\u00eda #!/usr/bin/python3 FIXME: corregir con info/apuntes del MEC, tema 1 BDA sobre especificaciones del hw de Hadoop","title":"Actividades"},{"location":"hadoop/03hdfs.html","text":"HDFS \u00b6 Funcionamiento de HDFS \u00b6 En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet . Procesos de lectura \u00b6 Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster. Proceso de escritura \u00b6 El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura). HDFS por dentro \u00b6 HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log . Trabajando con HDFS \u00b6 Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp Bloques \u00b6 A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451 Administraci\u00f3n \u00b6 Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos. Snapshots \u00b6 Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos HDFS UI \u00b6 En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd . HDFS y Python \u00b6 Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas . Hue \u00b6 Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Explica paso a paso el proceso de lectura (indicando qu\u00e9 bloques y los datanodes empleados) que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. (opcional) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/instantaneas . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/instantaneas as\u00ed como de sus snapshots . (opcional) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales. FIXME: completar HDFS con documento 2 del MEC de SBD sobre teor\u00eda de discos RAID","title":"HDFS, acceso y gesti\u00f3n. HDFS y Python. Formatos de datos Avro y Parquet."},{"location":"hadoop/03hdfs.html#hdfs","text":"","title":"HDFS"},{"location":"hadoop/03hdfs.html#funcionamiento-de-hdfs","text":"En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet .","title":"Funcionamiento de HDFS"},{"location":"hadoop/03hdfs.html#procesos-de-lectura","text":"Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster.","title":"Procesos de lectura"},{"location":"hadoop/03hdfs.html#proceso-de-escritura","text":"El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura).","title":"Proceso de escritura"},{"location":"hadoop/03hdfs.html#hdfs-por-dentro","text":"HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log .","title":"HDFS por dentro"},{"location":"hadoop/03hdfs.html#trabajando-con-hdfs","text":"Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp","title":"Trabajando con HDFS"},{"location":"hadoop/03hdfs.html#bloques","text":"A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451","title":"Bloques"},{"location":"hadoop/03hdfs.html#administracion","text":"Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.","title":"Administraci\u00f3n"},{"location":"hadoop/03hdfs.html#snapshots","text":"Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos","title":"Snapshots"},{"location":"hadoop/03hdfs.html#hdfs-ui","text":"En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd .","title":"HDFS UI"},{"location":"hadoop/03hdfs.html#hdfs-y-python","text":"Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas .","title":"HDFS y Python"},{"location":"hadoop/03hdfs.html#hue","text":"Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue","title":"Hue"},{"location":"hadoop/03hdfs.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python","title":"Referencias"},{"location":"hadoop/03hdfs.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Explica paso a paso el proceso de lectura (indicando qu\u00e9 bloques y los datanodes empleados) que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. (opcional) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/instantaneas . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/instantaneas as\u00ed como de sus snapshots . (opcional) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales. FIXME: completar HDFS con documento 2 del MEC de SBD sobre teor\u00eda de discos RAID","title":"Actividades"},{"location":"sa/index.html","text":"Unidad de Trabajo 2.- Sistemas de almacenamiento \u00b6 Resultados de aprendizaje \u00b6 RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Planificaci\u00f3n (20h) \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Ingenier\u00eda de datos / NoSQL Mi\u00e9rcoles 26 Oct 1p + 1o 24.- MongoDB Mi\u00e9rcoles 26 Oct / Lunes 7 Nov 3p + 3o 26.- Modelado documental Mi\u00e9rcoles 9 Nov 1p + 1o 26.- Formatos de datos Mi\u00e9rcoles 9 Nov 1p + 1o 28.- Agregaciones en MongoDB. Replicaci\u00f3n y Particionado Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Unidad de Trabajo 2.- Sistemas de almacenamiento"},{"location":"sa/index.html#unidad-de-trabajo-2-sistemas-de-almacenamiento","text":"","title":"Unidad de Trabajo 2.- Sistemas de almacenamiento"},{"location":"sa/index.html#resultados-de-aprendizaje","text":"RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.","title":"Resultados de aprendizaje"},{"location":"sa/index.html#planificacion-20h","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Ingenier\u00eda de datos / NoSQL Mi\u00e9rcoles 26 Oct 1p + 1o 24.- MongoDB Mi\u00e9rcoles 26 Oct / Lunes 7 Nov 3p + 3o 26.- Modelado documental Mi\u00e9rcoles 9 Nov 1p + 1o 26.- Formatos de datos Mi\u00e9rcoles 9 Nov 1p + 1o 28.- Agregaciones en MongoDB. Replicaci\u00f3n y Particionado Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Planificaci\u00f3n (20h)"},{"location":"sa/01nosql.html","text":"Almacenamiento de Datos \u00b6 Se puede decir que estamos en la [tercera plataforma] del almacenamiento de datos. La primera lleg\u00f3 con los primeros computadores y se materializ\u00f3 en las bases de datos jer\u00e1rquicas y en red, as\u00ed como en el almacenamiento ISAM. La segunda vino de la mano de internet y las arquitecturas cliente-servidor, lo que di\u00f3 lugar a las bases de datos relacionales. La tercera se ve motivada por el big data, los dispositivos m\u00f3viles, las arquitecturas cloud, las redes de IoT y las tecnolog\u00edas/redes sociales. Es tal el volumen de datos que se genera que aparecen nuevos paradigmas como NoSQL, NewSQL y las plataformas de Big Data. En esta sesi\u00f3n nos vamos a centrar en NoSQL. NoSQL aparece como una necesidad debida al creciente volumen de datos sobre usuarios, objetos y productos que las empresas tienen que almacenar, as\u00ed como la frecuencia con la que se accede a los datos. Los SGDB relacionales existentes no fueron dise\u00f1ados teniendo en cuenta la escalabilidad ni la flexibilidad necesaria por las frecuentes modificaciones que necesitan las aplicaciones modernas; tampoco aprovechan que el almacenamiento a d\u00eda de hoy es muy barato, ni el nivel de procesamiento que alcanzan las m\u00e1quinas actuales. Motivaci\u00f3n de NoSQL La soluci\u00f3n es el despliegue de las aplicaciones y sus datos en cl\u00fasteres de servidores, distribuyendo el procesamiento en m\u00faltiples m\u00e1quinas. No Solo SQL \u00b6 Si definimos NoSQL formalmente, podemos decir que se trata de un conjunto de tecnolog\u00edas que permiten el procesamiento r\u00e1pido y eficiente de conjuntos de datos dando la mayor importancia al rendimiento, la fiabilidad y la agilidad. Si nos basamos en el acr\u00f3nimo, el t\u00e9rmino se refiere a cualquier almac\u00e9n de datos que no sigue un modelo relacional, los datos no son relacionales y por tanto no utilizan SQL como lenguaje de consulta. As\u00ed pues, los sistemas NoSQL se centran en sistemas complementarios a los SGBD relacionales, que fijan sus prioridades en la escalabilidad y la disponibilidad en contra de la atomicidad y consistencia de los datos. ACID Las bases de datos relacionales cumplen las caracter\u00edsticas ACID para ofrecer transaccionalidad sobre los datos: A tomicidad: las transacciones implican que se realizan todas las operaciones o no se realiza ninguna. C onsistencia: la base de datos asegura que los datos pasan de un estado v\u00e1lido o otro tambi\u00e9n. I solation (Aislamiento): Una transacci\u00f3n no afecta a otras transacciones, de manera que la modificaci\u00f3n de un registro / documento no es visible por otras lecturas. Du rabilidad: La escritura de los datos asegura que una vez finalizada una operaci\u00f3n, los datos no se perder\u00e1n. Los diferentes tipos de bases de datos NoSQL existentes se pueden agrupar en cuatro categor\u00edas: Clave-Valor : Los almacenes clave-valor son las bases de datos NoSQL m\u00e1s simples. Cada elemento de la base de datos se almacena con un nombre de atributo (o clave) junto a su valor. Los almacenes m\u00e1s conocidos son Redis , Riak y AWS DynamoDB . Algunos almacenes, como es el caso de Redis, permiten que cada valor tenga un tipo (por ejemplo, integer) lo cual a\u00f1ade funcionalidad extra. Documentales : Cada clave se asocia a una estructura compleja que se conoce como documento. Este puede contener diferentes pares clave-valor, o pares de clave-array o incluso documentos anidados, como en un documento JSON. Los ejemplos m\u00e1s conocidos son MongoDB y CouchDB . Grafos : Los almacenes de grafos se usan para almacenar informaci\u00f3n sobre redes, como pueden ser conexiones sociales. Los ejemplos m\u00e1s conocidos son Neo4J , AWS Neptune y ArangoDB . Basados en columnas : Los almacenes basados en columnas como BigTable , Cassandra y HBase est\u00e1n optimizados para consultas sobre grandes conjuntos de datos, y almacenan los datos como columnas en vez de como filas. Sistemas NoSQL Caracter\u00edsticas \u00b6 Si nos centramos en sus beneficios y los comparamos con las base de datos relacionales, las bases de datos NoSQL son m\u00e1s escalables, ofrecen un rendimiento mayor y sus modelos de datos resuelven varios problemas que no se plantearon al definir el modelo relacional: Grandes vol\u00famenes de datos estructurados, semi-estructurados y sin estructurar. Casi todas las implementaciones NoSQL ofrecen alg\u00fan tipo de representaci\u00f3n para datos sin esquema, lo que permite comenzar con una estructura y con el paso del tiempo, a\u00f1adir nuevos campos, ya sean sencillos o anidados a datos ya existentes. Sprints \u00e1giles, iteraciones r\u00e1pidas y frecuentes commits / pushes de c\u00f3digo, al emplear una sintaxis sencilla para la realizaci\u00f3n de consultas y la posibilidad de tener un modelo que vaya creciendo al mismo ritmo que el desarrollo. Arquitectura eficiente y escalable dise\u00f1ada para trabajar con clusters en vez de una arquitectura monol\u00edtica y costosa. Las soluciones NoSQL soportan la escalabilidad de un modo transparente para el desarrollador. Una caracter\u00edstica adicional que comparten los sistemas NoSQL es que ofrecen un mecanismo de cach\u00e9 de datos integrado (en los sistemas relacionales se pueden configurar de manera externa), de manera que se pueden configurar los sistemas para que los datos se mantengan en memoria y se persistan de manera peri\u00f3dica. El uso de una cach\u00e9 conlleva que la consistencia de los datos no sea completa y podamos tener una consistencia eventual. Esquema din\u00e1micos \u00b6 Las bases de datos relacionales requieren definir los esquemas antes de a\u00f1adir los datos. Una base de datos SQL necesita saber de antemano los datos que vamos a almacenar; por ejemplo, si nos centramos en los datos de un cliente, ser\u00edan el nombre, apellidos, n\u00famero de tel\u00e9fono, etc\u2026\u200b Esto casa bastante mal con los enfoques de desarrollo \u00e1gil, ya que cada vez que a\u00f1adimos nuevas funcionalidades, el esquema de la base de datos suele cambiar. De modo que si a mitad de desarrollo decidimos almacenar los productos favoritos de un cliente del cual guard\u00e1bamos su direcci\u00f3n y n\u00fameros de tel\u00e9fono, tendr\u00edamos que a\u00f1adir una nueva columna a la base de datos y migrar la base de datos entera a un nuevo esquema. Si la base de datos es grande, conlleva un proceso lento que implica parar el sistema durante un tiempo considerable. Si frecuentemente cambiamos los datos que la aplicaci\u00f3n almacena (al usar un desarrollo iterativo), tambi\u00e9n tendremos per\u00edodos frecuentes de inactividad del sistema. As\u00ed pues, no hay un modo efectivo mediante una base de datos relacional, de almacenar los datos que est\u00e1n desestructurados o que no se conocen de antemano. Las bases de datos NoSQL se construyen para permitir la inserci\u00f3n de datos sin un esquema predefinido. Esto facilita la modificaci\u00f3n de la aplicaci\u00f3n en tiempo real, sin preocuparse por interrupciones de servicio. Aunque no tengamos un esquema al guardar la informaci\u00f3n, s\u00ed que podemos definir esquemas de lectura ( schema-on-read ) para comprobar que la informaci\u00f3n almacenada tiene el formato que espera cargar cada aplicaci\u00f3n. De este modo se consigue un desarrollo m\u00e1s r\u00e1pido, integraci\u00f3n de c\u00f3digo m\u00e1s robusto y menos tiempo empleado en la administraci\u00f3n de la base de datos. Particionado \u00b6 Dado el modo en el que se estructuran las bases de datos relacionales, normalmente escalan verticalmente - un \u00fanico servidor que almacena toda la base de datos para asegurar la disponibilidad continua de los datos. Esto se traduce en costes que se incrementan r\u00e1pidamente, con un l\u00edmites definidos por el propio hardware, y en un peque\u00f1o n\u00famero de puntos cr\u00edticos de fallo dentro de la infraestructura de datos. La soluci\u00f3n es escalar horizontalmente, a\u00f1adiendo nuevos servidores en vez de concentrarse en incrementar la capacidad de un \u00fanico servidor. Este escalado horizontal se conoce como Sharding o Particionado. Particionado de los datos El particionado no es \u00fanico de las bases de datos NoSQL. Las bases de datos relacionales tambi\u00e9n lo soportan. Si en un sistema relacional queremos particionar los datos, podemos distinguir entre particionado: Horizontal : diferentes filas en diferentes particiones. Vertical : diferentes columnas en particiones distintas. En el caso de las bases de datos NoSQL, el particionado depende del modelo de la base de datos: Los almacenes clave-valor y las bases de datos documentales normalmente se particionan horizontalmente. Las bases de datos basados en columnas se pueden particionar horizontal o verticalmente. Escalar horizontalmente una base de datos relacional entre muchas instancias de servidores se puede conseguir pero normalmente conlleva el uso de SANs ( Storage Area Networks ) y otras triqui\u00f1uelas para hacer que el hardware act\u00fae como un \u00fanico servidor. Como los sistemas SQL no ofrecen esta prestaci\u00f3n de forma nativa, los equipos de desarrollo se las tienen que ingeniar para conseguir desplegar m\u00faltiples bases de datos relacionales en varias m\u00e1quinas. Para ello: Los datos se almacenan en cada instancia de base de datos de manera aut\u00f3noma El c\u00f3digo de aplicaci\u00f3n se desarrolla para distribuir los datos y las consultas y agregar los resultados de los datos a trav\u00e9s de todas las instancias de bases de datos Se debe desarrollar c\u00f3digo adicional para gestionar los fallos sobre los recursos, para realizar joins entre diferentes bases de datos, balancear los datos y/o replicarlos, etc\u2026\u200b Adem\u00e1s, muchos beneficios de las bases de datos como la integridad transaccional se ven comprometidos o incluso eliminados al emplear un escalado horizontal. Auto-sharding \u00b6 Por contra, las bases de datos NoSQL normalmente soportan auto-sharding , lo que implica que de manera nativa y autom\u00e1ticamente se dividen los datos entre un n\u00famero arbitrario de servidores, sin que la aplicaci\u00f3n sea consciente de la composici\u00f3n del pool de servidores. Los datos y las consultas se balancean entre los servidores. El particionado se realiza mediante un m\u00e9todo consistente, como puede ser: Por rangos de su id: por ejemplo \"los usuarios del 1 al mill\u00f3n est\u00e1n en la partici\u00f3n 1\" o \"los usuarios cuyo nombre va de la A a la E\" en una partici\u00f3n, en otra de la M a la Q, y de la R a la Z en la tercera. Por listas : dividiendo los datos por la categor\u00eda del dato, es decir, en el caso de datos sobre libros, las novelas en una partici\u00f3n, las recetas de cocina en otra, etc.. Mediante un funci\u00f3n hash , la cual devuelve un valor para un elemento que determine a que partici\u00f3n pertenece. Cuando particionar \u00b6 El motivo para particionar los datos se debe a: limitaciones de almacenamiento: los datos no caben en un \u00fanico servidor, tanto a nivel de disco como de memoria RAM. rendimiento: al balancear la carga entre particiones las escrituras ser\u00e1n m\u00e1s r\u00e1pidas que al centrarlas en un \u00fanico servidor. disponibilidad: si un servidor esta ocupado, otro servidor puede devolver los datos. La carga de los servidores se reduce. No particionaremos los datos cuando la cantidad sea peque\u00f1a, ya que el hecho de distribuir los datos conlleva unos costes que pueden no compensar con un volumen de datos insuficiente. Tampoco esperaremos a particionar cuando tengamos much\u00edsimos datos, ya que el proceso de particionado puede provocar sobrecarga del sistema. La nube facilita de manera considerable este escalado, mediante proveedores como Amazon Web Services el cual ofrece virtualmente una capacidad ilimitada bajo demanda, y preocup\u00e1ndose de todas las tareas necesarias para la administraci\u00f3n de la base de datos. Los desarrolladores ya no necesitamos construir plataformas complejas para nuestras aplicaciones, de modo que nos podemos centrar en escribir c\u00f3digo de aplicaci\u00f3n. Una granja de servidores con commodity hardware puede ofrecer el mismo procesamiento y capacidad de almacenamiento que un \u00fanico servidor de alto rendimiento por mucho menos coste. Replicaci\u00f3n \u00b6 La replicaci\u00f3n mantiene copias id\u00e9nticas de los datos en m\u00faltiples servidores, lo que facilita que las aplicaciones siempre funcionen y los datos se mantengan seguros, incluso si alguno de los servidores sufre alg\u00fan problema. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n soportan la replicaci\u00f3n autom\u00e1tica, lo que implica una alta disponibilidad y recuperaci\u00f3n frente a desastres sin la necesidad de aplicaciones de terceros encargadas de ello. Desde el punto de vista del desarrollador, el entorno de almacenamiento es virtual y ajeno al c\u00f3digo de aplicaci\u00f3n. Replicaci\u00f3n de los datos La replicaci\u00f3n de los datos se utiliza para alcanzar: escalabilidad , incrementando el rendimiento al poder distribuir las consultas en diferentes nodos, y mejorar la redundancia al permitir que cada nodo tenga una copia de los datos. disponibilidad , ofreciendo tolerancia a fallos de hardware o corrupci\u00f3n de la base de datos. Al replicar los datos vamos a poder tener una copia de la base de datos, dar soporte a un servidor de datos agregados, o tener nodos a modo de copias de seguridad que pueden tomar el control en caso de fallo. aislamiento (la i en ACID - isolation ), entendido como la propiedad que define cuando y c\u00f3mo al realizar cambios en un nodo se propagan al resto de nodos. Si replicamos los datos podemos crear copias sincronizadas para separar procesos de la base de datos de producci\u00f3n, pudiendo ejecutar informes, anal\u00edtica de datos o copias de seguridad en nodos secundarios de modo que no tenga un impacto negativo en el nodo principal, as\u00ed como ofrecer un sistema sencillo para separar el entorno de producci\u00f3n del de preproducci\u00f3n. Replicaci\u00f3n vs particionado No hay que confundir la replicaci\u00f3n (copia de los datos en varias m\u00e1quinas) con el particionado (cada m\u00e1quina tiene un subconjunto de los datos). El entorno m\u00e1s seguro y con mejor rendimiento es aquel que tiene los datos particionados y replicados (cada m\u00e1quina que tiene un subconjunto de los datos est\u00e1 replicada en 2 o m\u00e1s). Implantando NoSQL \u00b6 Normalmente, las empresas empezar\u00e1n con una prueba de baja escalabilidad de una base de datos NoSQL, de modo que les permita comprender la tecnolog\u00eda asumiendo muy poco riesgo. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n son open-source, y por tanto se pueden probar sin ning\u00fan coste extra. Al tener unos ciclos de desarrollo m\u00e1s r\u00e1pidos, las empresas pueden innovar con mayor velocidad y mejorar la experiencia de sus cliente a un menor coste. Elegir la base de datos correcta para el proyecto es un tema importante. Se deben considerar las diferentes alternativas a las infraestructuras legacy teniendo en cuenta varios factores: la escalabilidad o el rendimiento m\u00e1s all\u00e1 de las capacidades del sistema existente identificar alternativas viables respecto al software propietario incrementar la velocidad y agilidad del proceso de desarrollo As\u00ed pues, al elegir un base de datos hemos de tener en cuenta las siguientes dimensiones: Modelo de Datos: A elegir entre un modelo documental, basado en columnas, de grafos o mediante clave-valor. Modelo de Consultas: Dependiendo de la aplicaci\u00f3n, puede ser aceptable un modelo de consultas que s\u00f3lo accede a los registros por su clave primaria. En cambio, otras aplicaciones pueden necesitar consultar por diferentes valores de cada registro. Adem\u00e1s, si la aplicaci\u00f3n necesita modificar los registros, la base de datos necesita consultar los datos por un \u00edndice secundario. Modelo de Consistencia: Los sistemas NoSQL normalmente mantienen m\u00faltiples copias de los datos para ofrecer disponibilidad y escalabilidad al sistema, lo que define la consistencia del mismo. Los sistemas NoSQL tienden a ser consistentes o eventualmente consistentes. APIs: No existe un est\u00e1ndar para interactuar con los sistemas NoSQL. Cada sistema presenta diferentes dise\u00f1os y capacidades para los equipos de desarrollo. La madurez de un API puede suponer una inversi\u00f3n en tiempo y dinero a la hora de desarrollar y mantener el sistema NoSQL. Soporte Comercial y de la Comunidad: Los usuarios deben considerar la salud de la compa\u00f1ia o de los proyectos al evaluar una base de datos. El producto debe evolucionar y se mantenga para introducir nuevas prestaciones y corregir fallos. Una base de datos con una comunidad fuerte de usuarios: permite encontrar y contratar desarrolladores con destrezas en el producto. facilita encontrar informaci\u00f3n, documentaci\u00f3n y ejemplos de c\u00f3digo. ayuda a las empresas a retener el talento. favorece que otras empresas de software integren sus productos y participen en el ecosistema de la base de datos. Casos de uso \u00b6 Una vez conocemos los diferentes sistemas y qu\u00e9 elementos puede hacer que nos decidamos por una soluci\u00f3n u otra, conviene repasar los casos de uso m\u00e1s comunes: Si vamos a crear una aplicaci\u00f3n web cuyo campos sean personalizables, usaremos una soluci\u00f3n documental. Como una capa de cach\u00e9, mediante un almac\u00e9n clave-valor. Para almacenar archivos binarios sin preocuparse de la gesti\u00f3n de permisos del sistema de archivos, y poder realizar consultas sobre sus metadatos, ya sea mediante una soluci\u00f3n documental o un almac\u00e9n clave-valor. Para almacenar un enorme volumen de datos, donde la consistencia no es lo m\u00e1s importante, pero si la disponibilidad y su capacidad de ser distribuida, mediante una soluci\u00f3n documental o basada en columnas. Modelos de Datos \u00b6 La principal clasificaci\u00f3n de los sistemas de bases de datos NoSQL se realiza respecto a los diferentes modelos de datos: Documental \u00b6 Mientras las bases de datos relacionales almacenan los datos en filas y columnas, las bases de datos documentales emplean documentos. Estos documentos utilizan una estructura JSON, ofreciendo un modo natural e intuitivo para modelar datos de manera similar a la orientaci\u00f3n a objetos, donde cada documento es un objeto. Representaci\u00f3n de un documento Los documentos se agrupan en colecciones o bases de datos, dependiendo del sistema, lo que permite agrupar documentos. Los documentos contienen uno o m\u00e1s campos, donde cada campo contiene un valor con un tipo, ya sea cadena, fecha, binario o array u otro documento. En vez de extender los datos entre m\u00faltiples columnas y tablas, cada registro y sus datos asociados se almacenan de manera unida en un \u00fanico documento. Esto simplifica el acceso a los datos y reduce (y en ocasiones elimina) la necesidad de joins y transacciones complejas. Dicho de otra manera, en las bases de datos documentales, los datos que van juntos y se emplean juntos, se almacenan juntos. Caracter\u00edsticas \u00b6 En una base de datos documental, la noci\u00f3n de esquema es din\u00e1mico: cada documento puede contener diferentes campos. Esta flexibilidad puede ser \u00fatil para modelar datos desestructurados y polim\u00f3rficos, lo que facilita la evoluci\u00f3n del desarrollo al permitir a\u00f1adir nuevos campos de manera din\u00e1mica. Perfectamente podemos tener dos documentos que pertenecen a la misma colecci\u00f3n, pero con atributos diferentes. Por ejemplo, un primer documento puede ser el siguiente: { \"_id\" : \"BW001\" , \"nombre\" : \"Bruce\" , \"apellido\" : \"Wayne\" , \"edad\" : 35 , \"salario\" : 10000000 } Mientras que un segundo documento dentro de la misma colecci\u00f3n podr\u00eda ser: { \"_id\" : \"JK1\" , \"nombre\" : \"Joker\" , \"edad\" : 34 , \"salario\" : 5000000 , \"direccion\" : { // (1)! \"calle\" : \"Asilo Arkham\" , \"ciudad\" : \"Gotham\" }, \"proyectos\" : [ // (2)! \"desintoxicacion-virus\" , \"top-secret-007\" ] } Un objeto o subdocumento permite agrupar informaci\u00f3n similar a una relaci\u00f3n 1:1 de un modelo relacional. De esta manera, no necesitamos una tabla Direccion . Un array puede contener valores o documentos, de manera que podr\u00edamos tener un array de documentos, permitiendo agrupar informaci\u00f3n similar a una relaci\u00f3n 1:N de un modelo relacional. De esta manera, no necesitamos una tabla Proyectos . Normalmente, cada documento contiene un elemento clave, sobre el cual se puede obtener un documento de manera un\u00edvoca. De todos modos, las bases de datos documentales ofrecen un completo mecanismo de consultas, posibilitando obtener informaci\u00f3n por cualquier campo del documento. Algunos productos ofrecen opciones de indexado para optimizar las consultas, como pueden ser \u00edndices compuestos, dispersos, con tiempo de vida (TTL), \u00fanicos, de texto o geoespaciales. Adem\u00e1s, estos sistemas ofrecen productos que permiten analizar los datos, mediante funciones de agregaci\u00f3n o implementaci\u00f3n de MapReduce. Respecto a la modificaciones, los documentos se pueden actualizar en una \u00fanica sentencia, sin necesidad de dar rodeos para elegir los datos a modificar. Casos de uso \u00b6 Las bases de datos documentales sirven para prop\u00f3sito general, v\u00e1lidos para un amplio abanico de aplicaciones gracias a la flexibilidad que ofrece el modelo de datos, lo que permite consultar cualquier campo y modelar de manera natural de manera similar a la programaci\u00f3n orientada a objetos. Entre los casos de \u00e9xito de estos sistemas cabe destacar: Sistemas de flujo de eventos: entre diferentes aplicaciones dentro de una empresa Gestores de Contenido, plataformas de Blogging: al almacenar los documentos mediante JSON, facilita la estructura de datos para guardar los comentarios, registros de usuarios, etc\u2026\u200b Anal\u00edticas Web, datos en Tiempo Real: al permitir modificar partes de un documento, e insertar nuevos atributos a un documento cuando se necesita una nueva m\u00e9trica Aplicaciones eCommerce: conforme las aplicaciones crecen, el esquema tambi\u00e9n lo hace Si nos centramos en aquellos casos donde no conviene este tipo de sistemas podemos destacar: Sistemas operacionales con transacciones complejas. Sistemas con consultas agregadas que modifican su estructura. Si los criterios de las consultas no paran de cambiar, acabaremos normalizando los datos. Los productos m\u00e1s destacados son: MongoDB: http://www.mongodb.com . Esta base de datos la vamos a estudiar en profundidad en esta unidad de trabajo. CouchDB: http://couchdb.apache.org Clave-Valor \u00b6 Un almac\u00e9n clave-valor es una simple tabla hash donde todos los accesos a la base de datos se realizan a trav\u00e9s de la clave primaria. Desde una perspectiva de modelo de datos, los almacenes de clave-valor son los m\u00e1s b\u00e1sicos. Su funcionamiento es similar a tener una tabla relacional con dos columnas, por ejemplo id y nombre , siendo id la columna utilizada como clave y nombre como valor. Mientras que en una base de datos en el campo nombre s\u00f3lo podemos almacenar datos de tipo cadena o num\u00e9rico, en un almac\u00e9n clave-valor, el valor puede ser de un dato simple o un objeto. Cuando una aplicaci\u00f3n accede mediante la clave y el valor, se almacenan el par de elementos. Si la clave ya existe, el valor se modifica. Representaci\u00f3n de un almac\u00e9n clave-valor El cliente puede tanto obtener el valor por la clave, asignar un valor a una clave o eliminar una clave del almac\u00e9n. El valor, sin embargo, es opaco al sistema, el cual no sabe que hay dentro de \u00e9l, ya que los datos s\u00f3lo se pueden consultar por la clave, lo cual puede ser un inconveniente. As\u00ed pues, la aplicaci\u00f3n es responsable de saber qu\u00e9 hay almacenado en cada valor. Por ejemplo, Riak utiliza el concepto de bucket (cubo) como una manera de agrupar claves, de manera similar a una tabla. Por ejemplo, Riak permite interactuar con la base de datos mediante peticiones HTTP: curl -v -X PUT <http://localhost:8091/riak/heroes/ace> -H \"Content-Type: application/json\" -d { \"nombre\" : \"Batman\" , \"color\" : \"Negro\" } Algunos almacenes clave-valor, como puede ser Redis , permiten almacenar datos con cualquier estructura, como por ejemplos listas, conjuntos, hashes y pueden realizar operaciones como intersecci\u00f3n, uni\u00f3n, diferencia y rango. Comandos Redis Python SET nombre \"Bruce Wayne\" // String HSET heroe nombre \"Batman\" // Hash \u2013 set HSET heroe color \"Negro\" SADD \"heroe:amigos\" \"Robin\" \"Alfred\" // Set \u2013 create/update import redis r = redis . Redis () r . mset ({ \"Croatia\" : \"Zagreb\" , \"Bahamas\" : \"Nassau\" }) r . get ( \"Bahamas\" ) # b'Nassau' Estas prestaciones hacen que Redis se extrapole a \u00e1mbitos ajenos a un almac\u00e9n clave-valor. Otra caracter\u00edstica que ofrecen algunos almacenes es que permiten crear un segundo nivel de consulta o incluso definir m\u00e1s de una clave para un mismo objeto. Como los almacenes clave-valor siempre utilizan accesos por clave primaria, de manera general tienen un gran rendimiento y son f\u00e1cilmente escalables. Si queremos que su rendimiento sea m\u00e1ximo, pueden configurarse para que mantengan la informaci\u00f3n en memoria y que se serialice de manera peri\u00f3dica, a costa de tener una consistencia eventual de los datos. Casos de uso \u00b6 Este modelo es muy \u00fatil para representar datos desestructurados o polim\u00f3rficos, ya que no fuerzan ning\u00fan esquema m\u00e1s all\u00e1 de los pares de clave-valor. Entre los casos de uso de estos almacenes podemos destacar el almacenaje de: Informaci\u00f3n sobre la sesi\u00f3n de navegaci\u00f3n ( sessionid ) Perfiles de usuario, preferencias Datos del carrito de la compra Cachear datos Todas estas operaciones van a asociada a operaciones de recuperaci\u00f3n, modificaci\u00f3n o inserci\u00f3n de los datos de una sola vez, de ah\u00ed su elecci\u00f3n. En cambio, no conviene utilizar estos almacenes cuando queremos realizar: Relaciones entre datos Transacciones entre varias operaciones Consultas por los datos del valor Operaciones con conjuntos de claves Los almacenes m\u00e1s empleados son: Riak: https://riak.com Redis: http://redis.io AWS DynamoDB: http://aws.amazon.com/dynamodb Voldemort: http://www.project-voldemort.com/voldemort implementaci\u00f3n open-source de Amazon DynamoDB Basado en columnas \u00b6 Las bases de datos relacionales utilizan la fila como unidad de almacenamiento, lo que permite un buen rendimiento de escritura. Sin embargo, cuando las escrituras son ocasionales y es m\u00e1s comun tener que leer unas pocas columnas de muchas filas a la vez, es mejor utilizar como unidad de almacenamiento un grupos de columnas. Es decir, lo que hacemos es girar el modelo 90 grados, de manera que los registros se almacenan en columnas en vez de hacerlo por filas. Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que permite codificarlos/comprimirlos, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas Autoevaluaci\u00f3n Si tenemos que a\u00f1adir un nuevo registro \u00bfQu\u00e9 modelo ser\u00e1 m\u00e1s eficiente? Sin embargo, a medida que se incrementa la utilizaci\u00f3n de an\u00e1lisis de datos en memoria, con soluciones como Spark , los beneficios relativos de la base de datos columnares comparados con los de las bases de datos orientadas a filas pueden llegar a ser menos importantes. Representaci\u00f3n \u00b6 Un modelo basado en columnas se representa como una estructura agregada de dos niveles. El primer nivel formado por un almac\u00e9n clave-valor, siendo la clave el identificador de la fila, y el valor un nuevo mapa con los datos agregados de la fila (familias de columnas). Los valores de este segundo nivel son las columnas. De este modo, podemos acceder a los datos de un fila, o a una determinada columna: Representaci\u00f3n de un almac\u00e9n basado en columnas BigTable Los modelos de datos basados en columnas se basan en la implementaci\u00f3n de Google de la tecnolog\u00eda BigTable ( http://research.google.com/archive/bigtable.html ), la cual consiste en columnas separadas y sin esquema, a modo de mapa de dos niveles. As\u00ed pues, los almacenes basados en columnas utilizan un mapa ordenado multi-dimensional y distribuido para almacenar los datos. Est\u00e1n pensados para que cada fila tenga una gran n\u00famero de columnas (del orden del mill\u00f3n), almacenando las diferentes versiones que tenga una fila (pudiendo almacenar del orden de miles de millones de filas). Familias de columnas \u00b6 Una columna consiste en un pareja name - value , donde el nombre hace de clave. Adem\u00e1s, contiene un atributo timestamp para poder expirar datos y resolver conflictos de escritura. Un ejemplo de columna podr\u00eda ser: { na me : \"nombre\" , value : \"Bruce\" , t imes ta mp : 12345667890 } Una fila es una colecci\u00f3n de columnas agrupadas a una clave. Si agrupamos filas similares tendremos una familia de columnas : // familia de columnas { // fila \"tim-gordon\" : { n ombre : \"Tim\" , apellido : \"Gordon\" , ul t imaVisi ta : \"2015/12/12\" } // fila \"bruce-wayne\" : { n ombre : \"Bruce\" , apellido : \"Wayne\" , lugar : \"Gotham\" } } Con este ejemplo, podemos ver como las diferentes filas de la misma tabla (familia de columnas) no tienen por que compartir el mismo conjunto de columnas. Adem\u00e1s, las columnas se pueden anidar dentro de otras formando super-columnas , donde el valor es un nuevo mapa de columnas. { na me : \"libro:978-84-16152-08-7\" , value : { au t or : \"Grant Morrison\" , t i tul o : \"Batman - Asilo Arkham\" , isb n : \"978-84-16152-08-7\" } } Cuando se utilizan super columnas para crear familias de columnas tendremos una familia de super columnas. En resumen, las bases de datos basadas en columnas, almacenan los datos en familias de columnas como filas, las cuales tienen muchas columnas asociadas al identificador de una fila. Las familias de columnas son grupos de datos relacionados, a las cuales normalmente se accede de manera conjunta. Operaciones \u00b6 A la hora de consultar los datos, \u00e9stos se pueden obtener por la clave primaria de la familia. As\u00ed pues, podemos obtener toda una familia, o la columna de una familia: // Mediante Cassandra GET Clientes [ 'bruce-wayne' ]; // familia GET Clientes [ 'bruce-wayne' ][ 'lugar' ]; // columna Algunos productos ofrecen un soporte limitado para \u00edndices secundarios, pero con restricciones. Por ejemplo, Cassandra ofrece el lenguaje CQL similar a SQL pero sin joins, ni subconsultas donde las restricciones de where son sencillas: SELECT * FROM Clientes SELECT nombre , email FROM Clientes SELECT nombre , email FROM Clientes WHERE lugar = 'Gotham' Las actualizaciones se realizan en dos pasos: primero encontrar el registro y segundo modificarlo. En estos sistemas, una modificaci\u00f3n puede suponer una reescritura completa del registro independientemente que hayan cambiado unos pocos bytes del mismo. Casos de uso \u00b6 Las bases de datos columnares se han empleado durante d\u00e9cadas ofreciendo beneficios a las aplicaciones de negocio modernas, como la anal\u00edtica de datos, business intelligence y data warehousing . Son multiprop\u00f3sito, aunque su uso se centra en el mercado del big data, la anal\u00edtica de datos, cubos multidimensionales OLAP y/o almacenar metadatos y realizar anal\u00edtica en tiempo real. Adem\u00e1s de poder comprimir los datos, los datos est\u00e1n auto-indexados, lo que implica que utiliza menos espacio en disco, y acelera la ejecuci\u00f3n de consultas agregadas entre m\u00faltiples tablas que implica el uso de joins. En cambio, no se recomienda su uso en aplicaciones de procesamiento transaccional (OLTP), ya que las bases de datos relaciones gestionan mejor el procesamiento concurrente y el aislamiento de las operaciones. Los productos m\u00e1s destacados son: HBase : http://hbase.apache.org , el cual se basa en Hadoop - http://hadoop.apache.org Cassandra : http://cassandra.apache.org Amazon Redshift: https://aws.amazon.com/es/redshift/ Grafos \u00b6 Las bases de datos de grafos almacenan entidades y las relaciones entre estas entidades. Las entidades se conocen como nodos, los cuales tienen propiedades. Cada nodo es similar a una instancia de un objeto. Las relaciones, tambi\u00e9n conocidas como v\u00e9rtices, a su vez tienen propiedades, y su sentido es importante. Representaci\u00f3n de un grafo Los nodos se organizan mediante relaciones que facilitan encontrar patrones de informaci\u00f3n existente entre los nodos. Este tipo de organizaci\u00f3n permite almacenar los datos una vez e interpretar los datos de diferentes maneras dependiendo de sus relaciones. Los nodos son entidades que tienen propiedades, tales como el nombre. Por ejemplo, en el gr\u00e1fico cada nodo tiene una propiedad name . Tambi\u00e9n podemos ver que las relaciones tienen tipos, como label , since , etc\u2026\u200b Estas propiedades permiten organizar los nodos. Las relaciones pueden tener m\u00faltiples propiedades, y adem\u00e1s tienen direcci\u00f3n, con lo cual si queremos incluir bidireccionalidad tenemos que a\u00f1adir dos relaciones en sentidos opuestos. Tanto los nodos como las relaciones tienen un atributo id que los identifica. Por ejemplo, podemos comenzar a crear el grafo anterior mediante Neo4J de la siguiente manera: Node alice = graphDb . createNode (); alice . setProperty ( \"name\" , \"Alice\" ); Node bob = graphDb . createNode (); bob . setProperty ( \"name\" , \"Bob\" ); alice . createRelationshipTo ( bob , FRIEND ); bob . createRelationshipTo ( alice , FRIEND ); Los nodos permiten tener diferentes tipos de relaciones entre ellos y as\u00ed representar relaciones entre las entidades del dominio, y tener relaciones secundarias para caracter\u00edsticas como categor\u00eda, camino, \u00e1rboles de tiempo, listas enlazas para acceso ordenado, etc\u2026\u200b Al no existir un l\u00edmite en el n\u00famero ni en el tipo de relaciones que puede tener un nodo, todas se pueden representar en la misma base de datos. Traversing \u00b6 Una vez tenemos creado un grafo de nodos y relaciones, podemos consultar el grafo de muchas maneras; por ejemplo \"obtener todos los nodos que son miembros del grupo de ajedrez y que tienen m\u00e1s de 20 a\u00f1os\". Realizar una consulta se conoce como hacer un traversing (recorrido) del mismo. Ejemplo de Traversing mediante Neo4J: Node ajedrez = nodeIndex . get ( \"name\" , \"chess\" ). getSingle (); allRelationships = ajedrez . getRelationships ( Direction . INCOMING ); Una ventaja a destacar de las bases de datos basadas en grafos es que podemos cambiar los requisitos de traversing sin tener que cambiar los nodos o sus relaciones. En las bases de datos de grafos, recorrer las relaciones es muy r\u00e1pido, ya que no se calculan en tiempo de consulta, sino que se persisten como una relaci\u00f3n, y por tanto no hay que hacer ning\u00fan c\u00e1lculo. En cambio, en una base de datos relacional, para crear una estructura de grafo se realiza para una relaci\u00f3n sencilla ( \u00bfQuien es mi jefe?\" ). Para poder a\u00f1adir otras relaciones necesitamos muchos cambios en el esquema y trasladar datos entre tablas. Adem\u00e1s, necesitamos de antemano saber que consultas queremos realizar para modelar las tablas y las relaciones acorde a las consultas. As\u00ed pues, estos sistemas ofrecen modelos ricos de consultas donde se pueden investigar las relaciones simples y complejas entre los nodos para obtener informaci\u00f3n directa e indirecta de los datos del sistemas. Los tipos de an\u00e1lisis que se realizan sobre estos sistema se ci\u00f1en a los tipos de relaci\u00f3n existente entre los datos. Casos de uso \u00b6 Mientras que el modelo de grafos no es muy intuitivo y tiene una importante curva de aprendizaje, se puede usar en un gran n\u00famero de aplicaciones. Su principal atractivo es que facilitan almacenar las relaciones entre entidades de una aplicaci\u00f3n, como por ejemplo en una red social, o las intersecciones existentes entre carreteras. Es decir, se emplean para almacenar datos que se representan como nodos interconectados. Por lo tanto, los casos de uso son: Datos conectados: redes sociales con diferentes tipos de conexiones entre los usuarios. Enrutamiento, entrega o servicios basados en la posici\u00f3n: si las relaciones almacenan la distancia entre los nodos, podemos realizar consultas sobre lugares cercanos, trayecto m\u00e1s corto, etc\u2026\u200b Motores de recomendaciones: de compras, de lugares visitados, etc\u2026\u200b En cambio, no se recomienda su uso cuando necesitemos modificar todos o un subconjunto de entidades, ya que modificar una propiedad en todos los nodos es una operaci\u00f3n compleja. Los productos m\u00e1s destacados son: Neo4j: http://neo4j.com ArangoDB: https://www.arangodb.com/ Apache Giraph: https://giraph.apache.org/ Amazon Neptune: https://aws.amazon.com/es/neptune/ Consistencia \u00b6 En un sistema consistente, las escrituras de una aplicaci\u00f3n son visibles en siguientes consultas. Con una consistencia eventual, las escrituras no son visibles inmediatamente. Por ejemplo, en un sistema de control de stock, si el sistema es consistente, cada consulta obtendr\u00e1 el estado real del inventario, mientras que si tiene consistencia eventual, puede que no sea el estado real en un momento concreto pero terminar\u00e1 si\u00e9ndolo en breve. Sistemas consistentes \u00b6 Cada aplicaci\u00f3n tiene diferentes requisitos para la consistencia de los datos. Para muchas aplicaciones, es imprescindible que los datos sean consistentes en todo momento. Como los equipos de desarrollo han estado trabajo con un modelo de datos relacional durante d\u00e9cadas, este enfoque parece natural. Sin embargo, en otras ocasiones, la consistencia eventual es un traspi\u00e9s aceptable si conlleva una mayor flexibilidad en la disponibilidad del sistema. Las bases de datos documentales y de grafos pueden ser consistentes o eventualmente consistentes. Por ejemplo, MongoDB ofrece un consistencia configurable. De manera predeterminada, los datos son consistentes, de modo que todas las escrituras y lecturas se realizan sobre la copia principal de los datos. Pero como opci\u00f3n, las consultas de lectura, se pueden realizar con las copias secundarias donde los datos tendr\u00e1n consistencia eventual. La elecci\u00f3n de la consistencia se realiza a nivel de consulta. Sistemas de consistencia eventual \u00b6 Con los sistemas eventualmente consistentes, hay un per\u00edodo de tiempo en el que todas las copias de los datos no est\u00e1n sincronizados. Esto puede ser aceptable para aplicaciones de s\u00f3lo-lectura y almacenes de datos que no cambian frecuentemente, como los archivos hist\u00f3ricos. Dentro del mismo saco podemos meter las aplicaciones con alta tasa de escritura donde las lecturas sean poco frecuentes, como un archivo de log. Un claro ejemplo de sistema eventualmente consistente es el servicio DNS, donde tras registrar un dominio, puede tardar varios d\u00edas en propagar los datos a trav\u00e9s de Internet, pero siempre est\u00e1n disponibles aunque contenga una versi\u00f3n antigua de los datos. Respecto a las bases de datos NoSQL, los almacenes de clave-valor y los basados en columnas son sistemas eventualmente consistentes. Estos tienen que soportar conflictos en las actualizaciones de registros individuales. Como las escrituras se pueden aplicar a cualquier copia de los datos, puede ocurrir, y no ser\u00eda muy extra\u00f1o, que hubiese un conflicto de escritura. Algunos sistemas como Riak utilizan vectores de reloj para determinar el orden de los eventos y asegurar que la operaci\u00f3n m\u00e1s reciente gana en caso de un conflicto. Otros sistemas como CouchDB , retienen todos los valores conflictivos y permiten al usuario resolver el conflicto. Otro enfoque seguido por Cassandra sencillamente asume que el valor m\u00e1s grande es el correcto. Por estos motivos, las escrituras tienden a comportarse bien en sistemas eventualmente consistentes, pero las actualizaciones pueden conllevar sacrificios que complican la aplicaci\u00f3n. Teorema de CAP \u00b6 Propuesto por Eric Brewer en el a\u00f1o 2000, prueba que podemos crear una base de datos distribuida que elija dos de las siguientes tres caracter\u00edsticas: C onsistencia: las escrituras son at\u00f3micas y todas las peticiones posteriores obtienen el nuevo valor, independientemente del lugar de la petici\u00f3n. Disponibilidad ( A vailable ): la base de datos devolver\u00e1 siempre un valor. En la pr\u00e1ctica significa que no hay downtime . Tolerancia a P articiones: el sistema funcionar\u00e1 incluso si la comunicaci\u00f3n con un servidor se interrumpe de manera temporal (para ello, ha de dividir los datos entre diferentes nodos). Es decir, implica que se pueden recibir lecturas desde unos nodos que no contienen informaci\u00f3n escrita en otros. En otras palabras, podemos crear un sistema de base de datos que sea consistente y tolerante a particiones (CP), un sistema que sea disponible y tolerante a particiones (AP), o un sistema que sea consistente y disponible (CA). Pero no es posible crear una base de datos distribuida que sea consistente, disponible y tolerante a particiones al mismo tiempo. Teorema de CAP El teorema CAP es \u00fatil cuando consideramos el sistema de base de datos que necesitamos, ya que nos permite decidir cual de las tres caracter\u00edsticas vamos a descartar. La elecci\u00f3n realmente se centra entre la disponibilidad y la consistencia, ya que la tolerancia a particiones es una decisi\u00f3n de arquitectura (sea o no distribuida). Aunque el teorema dicte que si en un sistema distribuido elegimos disponibilidad no podemos tener consistencia, todav\u00eda podemos obtener consistencia eventual. Es decir, cada nodo siempre estar\u00e1 disponible para servir peticiones, aunque estos nodos no puedan asegurar que la informaci\u00f3n que contienen sea consistente (pero si bastante precisa), en alg\u00fan momento lo ser\u00e1. Algunas bases de datos tolerantes a particiones se pueden ajustar para ser m\u00e1s o menos consistentes o disponible a nivel de petici\u00f3n. Por ejemplo, Riak trabaja de esta manera, permitiendo a los clientes decidir en tiempo de petici\u00f3n que nivel de consistencia necesitan. Clasificaci\u00f3n seg\u00fan CAP \u00b6 El siguiente gr\u00e1fico muestra como dependiendo de estos atributos podemos clasificar los sistemas NoSQL: Clasificaci\u00f3n seg\u00fan CAP As\u00ed pues, las bases de datos NoSQL se clasifican en: CP : Consistente y tolerantes a particiones. Tanto MongoDB como HBase son CP, ya que dentro de una partici\u00f3n pueden no estar disponibles para responder una determinada consulta (por ejemplo, evitando lecturas en los nodos secundarios), aunque son tolerantes a fallos porque cualquier nodo secundario se puede convertir en principal y asumir el rol del nodo ca\u00eddo. AP : Disponibles y tolerantes a particiones. DynamoDB permite replicar los datos entre sus nodos aunque no garantiza la consistencia en ninguno de los sus servidores. CA : Consistentes y disponible. Aqu\u00ed es donde situar\u00edamos a los SGDB relacionales. Por ejemplo, PostreSQL es CA (aunque ambas soluciones ofrecen productos complementarios para dar soporte al particionado, como Redis Cluster y PgCluster), ya que no distribuyen los datos y por tanto la partici\u00f3n no es una restricci\u00f3n. Lo bueno es que la gran mayor\u00eda de sistemas permiten configurarse para cambiar su tipo CAP, lo que permite que MongoDB pase de CP a AP, o CouchDB de AP a CP. BASE \u00b6 Partiendo del teorema de CAP, de forma an\u00e1loga al modelo transaccional ACID para las bases de datos relacionales que dan soporte a la transaccionalidad ofreciendo en todo momento un sistema consistente, las bases de datos distribuidas siguen el modelo transaccional BASE, el cual se centra en la alta disponibilidad y significa: B\u00e1sicamente disponible ( B asically A vailable ): la base de datos siempre responde a las solicitudes recibidas, ya sea con una respuesta exitosa o con un error, a\u00fan en el caso de que el sistema soporte la tolerancia a particiones (de manera que caiga alg\u00fan nodo o no est\u00e9 accesible por problemas de la red). Esto puedo implicar lecturas desde nodos que no han recibido la \u00faltima escritura, por lo que el resultado puede no ser consistente. Estado blando ( S oft State ): la base de datos puede encontrarse en un estado inconsistente cuando se produce una lectura, de modo que es posible realizar dos veces la misma lectura y obtener dos resultados distintos a pesar de que no haya habido ninguna escritura entre ambas operaciones, sino que la escritura se hab\u00eda realizado antes en el tiempo y no se hab\u00eda persistido hasta ese momento. Consistencia eventual ( E ventual consistency ): tras cada escritura, la consistencia de la base de datos s\u00f3lo se alcanza una vez el cambio ha sido propagado a todos los nodos. Durante el tiempo que tarda en producirse la consistencia, observamos un estado blando de la base de datos. Una base de datos que sigue el modelo transaccional BASE prefiere la disponibilidad antes que la consistencia (es decir, desde el punto de vista del teorema CAP es AP). Referencias \u00b6 Next Generation Databases : NoSQL, NewSQL, and Big Data NoSQL Distilled : A Brief Guide to the Emerging World of Polyglot Persistence Row vs Column Oriented Databases Actividades \u00b6 ( RA5075.2 / CE5.2a / 2p) Contesta a las siguientes preguntas: \u00bfQu\u00e9 significa el prefijo No del acr\u00f3nimo NoSQL ? \u00bfUn sistema puede soportar al mismo tiempo replicaci\u00f3n y particionado? Para los siguientes supuestos, indica qu\u00e9 modelo de datos emplear\u00edas y justifica tu respuesta: Wiki sobre de personajes de c\u00f3mics. Informaci\u00f3n acad\u00e9mica de un pa\u00eds (centros, alumnos, profesores, asignaturas, calificaciones, \u2026\u200b) Investiga en qu\u00e9 consiste la persistencia pol\u00edglota . Clasifica las siguientes bases de datos seg\u00fan el teorema de CAP en CA, CP o AP: BigTable, Cassandra, CouchDB, DynamoDB, HBase, MongoDB, Redis, Riak, Voldemort . ( RA5075.2 / CE5.2a / 1p) Crea una presentaci\u00f3n de 5-6 diapositivas donde expliques en qu\u00e9 consiste el movimiento NewSQL , su relaci\u00f3n con NoSQL y qu\u00e9 ofrecen bases de datos como CockroachDB y/o VoltDB .","title":"S18.- Almacenamiento de datos. NoSQL"},{"location":"sa/01nosql.html#almacenamiento-de-datos","text":"Se puede decir que estamos en la [tercera plataforma] del almacenamiento de datos. La primera lleg\u00f3 con los primeros computadores y se materializ\u00f3 en las bases de datos jer\u00e1rquicas y en red, as\u00ed como en el almacenamiento ISAM. La segunda vino de la mano de internet y las arquitecturas cliente-servidor, lo que di\u00f3 lugar a las bases de datos relacionales. La tercera se ve motivada por el big data, los dispositivos m\u00f3viles, las arquitecturas cloud, las redes de IoT y las tecnolog\u00edas/redes sociales. Es tal el volumen de datos que se genera que aparecen nuevos paradigmas como NoSQL, NewSQL y las plataformas de Big Data. En esta sesi\u00f3n nos vamos a centrar en NoSQL. NoSQL aparece como una necesidad debida al creciente volumen de datos sobre usuarios, objetos y productos que las empresas tienen que almacenar, as\u00ed como la frecuencia con la que se accede a los datos. Los SGDB relacionales existentes no fueron dise\u00f1ados teniendo en cuenta la escalabilidad ni la flexibilidad necesaria por las frecuentes modificaciones que necesitan las aplicaciones modernas; tampoco aprovechan que el almacenamiento a d\u00eda de hoy es muy barato, ni el nivel de procesamiento que alcanzan las m\u00e1quinas actuales. Motivaci\u00f3n de NoSQL La soluci\u00f3n es el despliegue de las aplicaciones y sus datos en cl\u00fasteres de servidores, distribuyendo el procesamiento en m\u00faltiples m\u00e1quinas.","title":"Almacenamiento de Datos"},{"location":"sa/01nosql.html#no-solo-sql","text":"Si definimos NoSQL formalmente, podemos decir que se trata de un conjunto de tecnolog\u00edas que permiten el procesamiento r\u00e1pido y eficiente de conjuntos de datos dando la mayor importancia al rendimiento, la fiabilidad y la agilidad. Si nos basamos en el acr\u00f3nimo, el t\u00e9rmino se refiere a cualquier almac\u00e9n de datos que no sigue un modelo relacional, los datos no son relacionales y por tanto no utilizan SQL como lenguaje de consulta. As\u00ed pues, los sistemas NoSQL se centran en sistemas complementarios a los SGBD relacionales, que fijan sus prioridades en la escalabilidad y la disponibilidad en contra de la atomicidad y consistencia de los datos. ACID Las bases de datos relacionales cumplen las caracter\u00edsticas ACID para ofrecer transaccionalidad sobre los datos: A tomicidad: las transacciones implican que se realizan todas las operaciones o no se realiza ninguna. C onsistencia: la base de datos asegura que los datos pasan de un estado v\u00e1lido o otro tambi\u00e9n. I solation (Aislamiento): Una transacci\u00f3n no afecta a otras transacciones, de manera que la modificaci\u00f3n de un registro / documento no es visible por otras lecturas. Du rabilidad: La escritura de los datos asegura que una vez finalizada una operaci\u00f3n, los datos no se perder\u00e1n. Los diferentes tipos de bases de datos NoSQL existentes se pueden agrupar en cuatro categor\u00edas: Clave-Valor : Los almacenes clave-valor son las bases de datos NoSQL m\u00e1s simples. Cada elemento de la base de datos se almacena con un nombre de atributo (o clave) junto a su valor. Los almacenes m\u00e1s conocidos son Redis , Riak y AWS DynamoDB . Algunos almacenes, como es el caso de Redis, permiten que cada valor tenga un tipo (por ejemplo, integer) lo cual a\u00f1ade funcionalidad extra. Documentales : Cada clave se asocia a una estructura compleja que se conoce como documento. Este puede contener diferentes pares clave-valor, o pares de clave-array o incluso documentos anidados, como en un documento JSON. Los ejemplos m\u00e1s conocidos son MongoDB y CouchDB . Grafos : Los almacenes de grafos se usan para almacenar informaci\u00f3n sobre redes, como pueden ser conexiones sociales. Los ejemplos m\u00e1s conocidos son Neo4J , AWS Neptune y ArangoDB . Basados en columnas : Los almacenes basados en columnas como BigTable , Cassandra y HBase est\u00e1n optimizados para consultas sobre grandes conjuntos de datos, y almacenan los datos como columnas en vez de como filas. Sistemas NoSQL","title":"No Solo SQL"},{"location":"sa/01nosql.html#caracteristicas","text":"Si nos centramos en sus beneficios y los comparamos con las base de datos relacionales, las bases de datos NoSQL son m\u00e1s escalables, ofrecen un rendimiento mayor y sus modelos de datos resuelven varios problemas que no se plantearon al definir el modelo relacional: Grandes vol\u00famenes de datos estructurados, semi-estructurados y sin estructurar. Casi todas las implementaciones NoSQL ofrecen alg\u00fan tipo de representaci\u00f3n para datos sin esquema, lo que permite comenzar con una estructura y con el paso del tiempo, a\u00f1adir nuevos campos, ya sean sencillos o anidados a datos ya existentes. Sprints \u00e1giles, iteraciones r\u00e1pidas y frecuentes commits / pushes de c\u00f3digo, al emplear una sintaxis sencilla para la realizaci\u00f3n de consultas y la posibilidad de tener un modelo que vaya creciendo al mismo ritmo que el desarrollo. Arquitectura eficiente y escalable dise\u00f1ada para trabajar con clusters en vez de una arquitectura monol\u00edtica y costosa. Las soluciones NoSQL soportan la escalabilidad de un modo transparente para el desarrollador. Una caracter\u00edstica adicional que comparten los sistemas NoSQL es que ofrecen un mecanismo de cach\u00e9 de datos integrado (en los sistemas relacionales se pueden configurar de manera externa), de manera que se pueden configurar los sistemas para que los datos se mantengan en memoria y se persistan de manera peri\u00f3dica. El uso de una cach\u00e9 conlleva que la consistencia de los datos no sea completa y podamos tener una consistencia eventual.","title":"Caracter\u00edsticas"},{"location":"sa/01nosql.html#esquema-dinamicos","text":"Las bases de datos relacionales requieren definir los esquemas antes de a\u00f1adir los datos. Una base de datos SQL necesita saber de antemano los datos que vamos a almacenar; por ejemplo, si nos centramos en los datos de un cliente, ser\u00edan el nombre, apellidos, n\u00famero de tel\u00e9fono, etc\u2026\u200b Esto casa bastante mal con los enfoques de desarrollo \u00e1gil, ya que cada vez que a\u00f1adimos nuevas funcionalidades, el esquema de la base de datos suele cambiar. De modo que si a mitad de desarrollo decidimos almacenar los productos favoritos de un cliente del cual guard\u00e1bamos su direcci\u00f3n y n\u00fameros de tel\u00e9fono, tendr\u00edamos que a\u00f1adir una nueva columna a la base de datos y migrar la base de datos entera a un nuevo esquema. Si la base de datos es grande, conlleva un proceso lento que implica parar el sistema durante un tiempo considerable. Si frecuentemente cambiamos los datos que la aplicaci\u00f3n almacena (al usar un desarrollo iterativo), tambi\u00e9n tendremos per\u00edodos frecuentes de inactividad del sistema. As\u00ed pues, no hay un modo efectivo mediante una base de datos relacional, de almacenar los datos que est\u00e1n desestructurados o que no se conocen de antemano. Las bases de datos NoSQL se construyen para permitir la inserci\u00f3n de datos sin un esquema predefinido. Esto facilita la modificaci\u00f3n de la aplicaci\u00f3n en tiempo real, sin preocuparse por interrupciones de servicio. Aunque no tengamos un esquema al guardar la informaci\u00f3n, s\u00ed que podemos definir esquemas de lectura ( schema-on-read ) para comprobar que la informaci\u00f3n almacenada tiene el formato que espera cargar cada aplicaci\u00f3n. De este modo se consigue un desarrollo m\u00e1s r\u00e1pido, integraci\u00f3n de c\u00f3digo m\u00e1s robusto y menos tiempo empleado en la administraci\u00f3n de la base de datos.","title":"Esquema din\u00e1micos"},{"location":"sa/01nosql.html#particionado","text":"Dado el modo en el que se estructuran las bases de datos relacionales, normalmente escalan verticalmente - un \u00fanico servidor que almacena toda la base de datos para asegurar la disponibilidad continua de los datos. Esto se traduce en costes que se incrementan r\u00e1pidamente, con un l\u00edmites definidos por el propio hardware, y en un peque\u00f1o n\u00famero de puntos cr\u00edticos de fallo dentro de la infraestructura de datos. La soluci\u00f3n es escalar horizontalmente, a\u00f1adiendo nuevos servidores en vez de concentrarse en incrementar la capacidad de un \u00fanico servidor. Este escalado horizontal se conoce como Sharding o Particionado. Particionado de los datos El particionado no es \u00fanico de las bases de datos NoSQL. Las bases de datos relacionales tambi\u00e9n lo soportan. Si en un sistema relacional queremos particionar los datos, podemos distinguir entre particionado: Horizontal : diferentes filas en diferentes particiones. Vertical : diferentes columnas en particiones distintas. En el caso de las bases de datos NoSQL, el particionado depende del modelo de la base de datos: Los almacenes clave-valor y las bases de datos documentales normalmente se particionan horizontalmente. Las bases de datos basados en columnas se pueden particionar horizontal o verticalmente. Escalar horizontalmente una base de datos relacional entre muchas instancias de servidores se puede conseguir pero normalmente conlleva el uso de SANs ( Storage Area Networks ) y otras triqui\u00f1uelas para hacer que el hardware act\u00fae como un \u00fanico servidor. Como los sistemas SQL no ofrecen esta prestaci\u00f3n de forma nativa, los equipos de desarrollo se las tienen que ingeniar para conseguir desplegar m\u00faltiples bases de datos relacionales en varias m\u00e1quinas. Para ello: Los datos se almacenan en cada instancia de base de datos de manera aut\u00f3noma El c\u00f3digo de aplicaci\u00f3n se desarrolla para distribuir los datos y las consultas y agregar los resultados de los datos a trav\u00e9s de todas las instancias de bases de datos Se debe desarrollar c\u00f3digo adicional para gestionar los fallos sobre los recursos, para realizar joins entre diferentes bases de datos, balancear los datos y/o replicarlos, etc\u2026\u200b Adem\u00e1s, muchos beneficios de las bases de datos como la integridad transaccional se ven comprometidos o incluso eliminados al emplear un escalado horizontal.","title":"Particionado"},{"location":"sa/01nosql.html#replicacion","text":"La replicaci\u00f3n mantiene copias id\u00e9nticas de los datos en m\u00faltiples servidores, lo que facilita que las aplicaciones siempre funcionen y los datos se mantengan seguros, incluso si alguno de los servidores sufre alg\u00fan problema. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n soportan la replicaci\u00f3n autom\u00e1tica, lo que implica una alta disponibilidad y recuperaci\u00f3n frente a desastres sin la necesidad de aplicaciones de terceros encargadas de ello. Desde el punto de vista del desarrollador, el entorno de almacenamiento es virtual y ajeno al c\u00f3digo de aplicaci\u00f3n. Replicaci\u00f3n de los datos La replicaci\u00f3n de los datos se utiliza para alcanzar: escalabilidad , incrementando el rendimiento al poder distribuir las consultas en diferentes nodos, y mejorar la redundancia al permitir que cada nodo tenga una copia de los datos. disponibilidad , ofreciendo tolerancia a fallos de hardware o corrupci\u00f3n de la base de datos. Al replicar los datos vamos a poder tener una copia de la base de datos, dar soporte a un servidor de datos agregados, o tener nodos a modo de copias de seguridad que pueden tomar el control en caso de fallo. aislamiento (la i en ACID - isolation ), entendido como la propiedad que define cuando y c\u00f3mo al realizar cambios en un nodo se propagan al resto de nodos. Si replicamos los datos podemos crear copias sincronizadas para separar procesos de la base de datos de producci\u00f3n, pudiendo ejecutar informes, anal\u00edtica de datos o copias de seguridad en nodos secundarios de modo que no tenga un impacto negativo en el nodo principal, as\u00ed como ofrecer un sistema sencillo para separar el entorno de producci\u00f3n del de preproducci\u00f3n. Replicaci\u00f3n vs particionado No hay que confundir la replicaci\u00f3n (copia de los datos en varias m\u00e1quinas) con el particionado (cada m\u00e1quina tiene un subconjunto de los datos). El entorno m\u00e1s seguro y con mejor rendimiento es aquel que tiene los datos particionados y replicados (cada m\u00e1quina que tiene un subconjunto de los datos est\u00e1 replicada en 2 o m\u00e1s).","title":"Replicaci\u00f3n"},{"location":"sa/01nosql.html#implantando-nosql","text":"Normalmente, las empresas empezar\u00e1n con una prueba de baja escalabilidad de una base de datos NoSQL, de modo que les permita comprender la tecnolog\u00eda asumiendo muy poco riesgo. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n son open-source, y por tanto se pueden probar sin ning\u00fan coste extra. Al tener unos ciclos de desarrollo m\u00e1s r\u00e1pidos, las empresas pueden innovar con mayor velocidad y mejorar la experiencia de sus cliente a un menor coste. Elegir la base de datos correcta para el proyecto es un tema importante. Se deben considerar las diferentes alternativas a las infraestructuras legacy teniendo en cuenta varios factores: la escalabilidad o el rendimiento m\u00e1s all\u00e1 de las capacidades del sistema existente identificar alternativas viables respecto al software propietario incrementar la velocidad y agilidad del proceso de desarrollo As\u00ed pues, al elegir un base de datos hemos de tener en cuenta las siguientes dimensiones: Modelo de Datos: A elegir entre un modelo documental, basado en columnas, de grafos o mediante clave-valor. Modelo de Consultas: Dependiendo de la aplicaci\u00f3n, puede ser aceptable un modelo de consultas que s\u00f3lo accede a los registros por su clave primaria. En cambio, otras aplicaciones pueden necesitar consultar por diferentes valores de cada registro. Adem\u00e1s, si la aplicaci\u00f3n necesita modificar los registros, la base de datos necesita consultar los datos por un \u00edndice secundario. Modelo de Consistencia: Los sistemas NoSQL normalmente mantienen m\u00faltiples copias de los datos para ofrecer disponibilidad y escalabilidad al sistema, lo que define la consistencia del mismo. Los sistemas NoSQL tienden a ser consistentes o eventualmente consistentes. APIs: No existe un est\u00e1ndar para interactuar con los sistemas NoSQL. Cada sistema presenta diferentes dise\u00f1os y capacidades para los equipos de desarrollo. La madurez de un API puede suponer una inversi\u00f3n en tiempo y dinero a la hora de desarrollar y mantener el sistema NoSQL. Soporte Comercial y de la Comunidad: Los usuarios deben considerar la salud de la compa\u00f1ia o de los proyectos al evaluar una base de datos. El producto debe evolucionar y se mantenga para introducir nuevas prestaciones y corregir fallos. Una base de datos con una comunidad fuerte de usuarios: permite encontrar y contratar desarrolladores con destrezas en el producto. facilita encontrar informaci\u00f3n, documentaci\u00f3n y ejemplos de c\u00f3digo. ayuda a las empresas a retener el talento. favorece que otras empresas de software integren sus productos y participen en el ecosistema de la base de datos.","title":"Implantando NoSQL"},{"location":"sa/01nosql.html#modelos-de-datos","text":"La principal clasificaci\u00f3n de los sistemas de bases de datos NoSQL se realiza respecto a los diferentes modelos de datos:","title":"Modelos de Datos"},{"location":"sa/01nosql.html#documental","text":"Mientras las bases de datos relacionales almacenan los datos en filas y columnas, las bases de datos documentales emplean documentos. Estos documentos utilizan una estructura JSON, ofreciendo un modo natural e intuitivo para modelar datos de manera similar a la orientaci\u00f3n a objetos, donde cada documento es un objeto. Representaci\u00f3n de un documento Los documentos se agrupan en colecciones o bases de datos, dependiendo del sistema, lo que permite agrupar documentos. Los documentos contienen uno o m\u00e1s campos, donde cada campo contiene un valor con un tipo, ya sea cadena, fecha, binario o array u otro documento. En vez de extender los datos entre m\u00faltiples columnas y tablas, cada registro y sus datos asociados se almacenan de manera unida en un \u00fanico documento. Esto simplifica el acceso a los datos y reduce (y en ocasiones elimina) la necesidad de joins y transacciones complejas. Dicho de otra manera, en las bases de datos documentales, los datos que van juntos y se emplean juntos, se almacenan juntos.","title":"Documental"},{"location":"sa/01nosql.html#clave-valor","text":"Un almac\u00e9n clave-valor es una simple tabla hash donde todos los accesos a la base de datos se realizan a trav\u00e9s de la clave primaria. Desde una perspectiva de modelo de datos, los almacenes de clave-valor son los m\u00e1s b\u00e1sicos. Su funcionamiento es similar a tener una tabla relacional con dos columnas, por ejemplo id y nombre , siendo id la columna utilizada como clave y nombre como valor. Mientras que en una base de datos en el campo nombre s\u00f3lo podemos almacenar datos de tipo cadena o num\u00e9rico, en un almac\u00e9n clave-valor, el valor puede ser de un dato simple o un objeto. Cuando una aplicaci\u00f3n accede mediante la clave y el valor, se almacenan el par de elementos. Si la clave ya existe, el valor se modifica. Representaci\u00f3n de un almac\u00e9n clave-valor El cliente puede tanto obtener el valor por la clave, asignar un valor a una clave o eliminar una clave del almac\u00e9n. El valor, sin embargo, es opaco al sistema, el cual no sabe que hay dentro de \u00e9l, ya que los datos s\u00f3lo se pueden consultar por la clave, lo cual puede ser un inconveniente. As\u00ed pues, la aplicaci\u00f3n es responsable de saber qu\u00e9 hay almacenado en cada valor. Por ejemplo, Riak utiliza el concepto de bucket (cubo) como una manera de agrupar claves, de manera similar a una tabla. Por ejemplo, Riak permite interactuar con la base de datos mediante peticiones HTTP: curl -v -X PUT <http://localhost:8091/riak/heroes/ace> -H \"Content-Type: application/json\" -d { \"nombre\" : \"Batman\" , \"color\" : \"Negro\" } Algunos almacenes clave-valor, como puede ser Redis , permiten almacenar datos con cualquier estructura, como por ejemplos listas, conjuntos, hashes y pueden realizar operaciones como intersecci\u00f3n, uni\u00f3n, diferencia y rango. Comandos Redis Python SET nombre \"Bruce Wayne\" // String HSET heroe nombre \"Batman\" // Hash \u2013 set HSET heroe color \"Negro\" SADD \"heroe:amigos\" \"Robin\" \"Alfred\" // Set \u2013 create/update import redis r = redis . Redis () r . mset ({ \"Croatia\" : \"Zagreb\" , \"Bahamas\" : \"Nassau\" }) r . get ( \"Bahamas\" ) # b'Nassau' Estas prestaciones hacen que Redis se extrapole a \u00e1mbitos ajenos a un almac\u00e9n clave-valor. Otra caracter\u00edstica que ofrecen algunos almacenes es que permiten crear un segundo nivel de consulta o incluso definir m\u00e1s de una clave para un mismo objeto. Como los almacenes clave-valor siempre utilizan accesos por clave primaria, de manera general tienen un gran rendimiento y son f\u00e1cilmente escalables. Si queremos que su rendimiento sea m\u00e1ximo, pueden configurarse para que mantengan la informaci\u00f3n en memoria y que se serialice de manera peri\u00f3dica, a costa de tener una consistencia eventual de los datos.","title":"Clave-Valor"},{"location":"sa/01nosql.html#basado-en-columnas","text":"Las bases de datos relacionales utilizan la fila como unidad de almacenamiento, lo que permite un buen rendimiento de escritura. Sin embargo, cuando las escrituras son ocasionales y es m\u00e1s comun tener que leer unas pocas columnas de muchas filas a la vez, es mejor utilizar como unidad de almacenamiento un grupos de columnas. Es decir, lo que hacemos es girar el modelo 90 grados, de manera que los registros se almacenan en columnas en vez de hacerlo por filas. Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que permite codificarlos/comprimirlos, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas Autoevaluaci\u00f3n Si tenemos que a\u00f1adir un nuevo registro \u00bfQu\u00e9 modelo ser\u00e1 m\u00e1s eficiente? Sin embargo, a medida que se incrementa la utilizaci\u00f3n de an\u00e1lisis de datos en memoria, con soluciones como Spark , los beneficios relativos de la base de datos columnares comparados con los de las bases de datos orientadas a filas pueden llegar a ser menos importantes.","title":"Basado en columnas"},{"location":"sa/01nosql.html#grafos","text":"Las bases de datos de grafos almacenan entidades y las relaciones entre estas entidades. Las entidades se conocen como nodos, los cuales tienen propiedades. Cada nodo es similar a una instancia de un objeto. Las relaciones, tambi\u00e9n conocidas como v\u00e9rtices, a su vez tienen propiedades, y su sentido es importante. Representaci\u00f3n de un grafo Los nodos se organizan mediante relaciones que facilitan encontrar patrones de informaci\u00f3n existente entre los nodos. Este tipo de organizaci\u00f3n permite almacenar los datos una vez e interpretar los datos de diferentes maneras dependiendo de sus relaciones. Los nodos son entidades que tienen propiedades, tales como el nombre. Por ejemplo, en el gr\u00e1fico cada nodo tiene una propiedad name . Tambi\u00e9n podemos ver que las relaciones tienen tipos, como label , since , etc\u2026\u200b Estas propiedades permiten organizar los nodos. Las relaciones pueden tener m\u00faltiples propiedades, y adem\u00e1s tienen direcci\u00f3n, con lo cual si queremos incluir bidireccionalidad tenemos que a\u00f1adir dos relaciones en sentidos opuestos. Tanto los nodos como las relaciones tienen un atributo id que los identifica. Por ejemplo, podemos comenzar a crear el grafo anterior mediante Neo4J de la siguiente manera: Node alice = graphDb . createNode (); alice . setProperty ( \"name\" , \"Alice\" ); Node bob = graphDb . createNode (); bob . setProperty ( \"name\" , \"Bob\" ); alice . createRelationshipTo ( bob , FRIEND ); bob . createRelationshipTo ( alice , FRIEND ); Los nodos permiten tener diferentes tipos de relaciones entre ellos y as\u00ed representar relaciones entre las entidades del dominio, y tener relaciones secundarias para caracter\u00edsticas como categor\u00eda, camino, \u00e1rboles de tiempo, listas enlazas para acceso ordenado, etc\u2026\u200b Al no existir un l\u00edmite en el n\u00famero ni en el tipo de relaciones que puede tener un nodo, todas se pueden representar en la misma base de datos.","title":"Grafos"},{"location":"sa/01nosql.html#consistencia","text":"En un sistema consistente, las escrituras de una aplicaci\u00f3n son visibles en siguientes consultas. Con una consistencia eventual, las escrituras no son visibles inmediatamente. Por ejemplo, en un sistema de control de stock, si el sistema es consistente, cada consulta obtendr\u00e1 el estado real del inventario, mientras que si tiene consistencia eventual, puede que no sea el estado real en un momento concreto pero terminar\u00e1 si\u00e9ndolo en breve.","title":"Consistencia"},{"location":"sa/01nosql.html#sistemas-consistentes","text":"Cada aplicaci\u00f3n tiene diferentes requisitos para la consistencia de los datos. Para muchas aplicaciones, es imprescindible que los datos sean consistentes en todo momento. Como los equipos de desarrollo han estado trabajo con un modelo de datos relacional durante d\u00e9cadas, este enfoque parece natural. Sin embargo, en otras ocasiones, la consistencia eventual es un traspi\u00e9s aceptable si conlleva una mayor flexibilidad en la disponibilidad del sistema. Las bases de datos documentales y de grafos pueden ser consistentes o eventualmente consistentes. Por ejemplo, MongoDB ofrece un consistencia configurable. De manera predeterminada, los datos son consistentes, de modo que todas las escrituras y lecturas se realizan sobre la copia principal de los datos. Pero como opci\u00f3n, las consultas de lectura, se pueden realizar con las copias secundarias donde los datos tendr\u00e1n consistencia eventual. La elecci\u00f3n de la consistencia se realiza a nivel de consulta.","title":"Sistemas consistentes"},{"location":"sa/01nosql.html#sistemas-de-consistencia-eventual","text":"Con los sistemas eventualmente consistentes, hay un per\u00edodo de tiempo en el que todas las copias de los datos no est\u00e1n sincronizados. Esto puede ser aceptable para aplicaciones de s\u00f3lo-lectura y almacenes de datos que no cambian frecuentemente, como los archivos hist\u00f3ricos. Dentro del mismo saco podemos meter las aplicaciones con alta tasa de escritura donde las lecturas sean poco frecuentes, como un archivo de log. Un claro ejemplo de sistema eventualmente consistente es el servicio DNS, donde tras registrar un dominio, puede tardar varios d\u00edas en propagar los datos a trav\u00e9s de Internet, pero siempre est\u00e1n disponibles aunque contenga una versi\u00f3n antigua de los datos. Respecto a las bases de datos NoSQL, los almacenes de clave-valor y los basados en columnas son sistemas eventualmente consistentes. Estos tienen que soportar conflictos en las actualizaciones de registros individuales. Como las escrituras se pueden aplicar a cualquier copia de los datos, puede ocurrir, y no ser\u00eda muy extra\u00f1o, que hubiese un conflicto de escritura. Algunos sistemas como Riak utilizan vectores de reloj para determinar el orden de los eventos y asegurar que la operaci\u00f3n m\u00e1s reciente gana en caso de un conflicto. Otros sistemas como CouchDB , retienen todos los valores conflictivos y permiten al usuario resolver el conflicto. Otro enfoque seguido por Cassandra sencillamente asume que el valor m\u00e1s grande es el correcto. Por estos motivos, las escrituras tienden a comportarse bien en sistemas eventualmente consistentes, pero las actualizaciones pueden conllevar sacrificios que complican la aplicaci\u00f3n.","title":"Sistemas de consistencia eventual"},{"location":"sa/01nosql.html#teorema-de-cap","text":"Propuesto por Eric Brewer en el a\u00f1o 2000, prueba que podemos crear una base de datos distribuida que elija dos de las siguientes tres caracter\u00edsticas: C onsistencia: las escrituras son at\u00f3micas y todas las peticiones posteriores obtienen el nuevo valor, independientemente del lugar de la petici\u00f3n. Disponibilidad ( A vailable ): la base de datos devolver\u00e1 siempre un valor. En la pr\u00e1ctica significa que no hay downtime . Tolerancia a P articiones: el sistema funcionar\u00e1 incluso si la comunicaci\u00f3n con un servidor se interrumpe de manera temporal (para ello, ha de dividir los datos entre diferentes nodos). Es decir, implica que se pueden recibir lecturas desde unos nodos que no contienen informaci\u00f3n escrita en otros. En otras palabras, podemos crear un sistema de base de datos que sea consistente y tolerante a particiones (CP), un sistema que sea disponible y tolerante a particiones (AP), o un sistema que sea consistente y disponible (CA). Pero no es posible crear una base de datos distribuida que sea consistente, disponible y tolerante a particiones al mismo tiempo. Teorema de CAP El teorema CAP es \u00fatil cuando consideramos el sistema de base de datos que necesitamos, ya que nos permite decidir cual de las tres caracter\u00edsticas vamos a descartar. La elecci\u00f3n realmente se centra entre la disponibilidad y la consistencia, ya que la tolerancia a particiones es una decisi\u00f3n de arquitectura (sea o no distribuida). Aunque el teorema dicte que si en un sistema distribuido elegimos disponibilidad no podemos tener consistencia, todav\u00eda podemos obtener consistencia eventual. Es decir, cada nodo siempre estar\u00e1 disponible para servir peticiones, aunque estos nodos no puedan asegurar que la informaci\u00f3n que contienen sea consistente (pero si bastante precisa), en alg\u00fan momento lo ser\u00e1. Algunas bases de datos tolerantes a particiones se pueden ajustar para ser m\u00e1s o menos consistentes o disponible a nivel de petici\u00f3n. Por ejemplo, Riak trabaja de esta manera, permitiendo a los clientes decidir en tiempo de petici\u00f3n que nivel de consistencia necesitan.","title":"Teorema de CAP"},{"location":"sa/01nosql.html#clasificacion-segun-cap","text":"El siguiente gr\u00e1fico muestra como dependiendo de estos atributos podemos clasificar los sistemas NoSQL: Clasificaci\u00f3n seg\u00fan CAP As\u00ed pues, las bases de datos NoSQL se clasifican en: CP : Consistente y tolerantes a particiones. Tanto MongoDB como HBase son CP, ya que dentro de una partici\u00f3n pueden no estar disponibles para responder una determinada consulta (por ejemplo, evitando lecturas en los nodos secundarios), aunque son tolerantes a fallos porque cualquier nodo secundario se puede convertir en principal y asumir el rol del nodo ca\u00eddo. AP : Disponibles y tolerantes a particiones. DynamoDB permite replicar los datos entre sus nodos aunque no garantiza la consistencia en ninguno de los sus servidores. CA : Consistentes y disponible. Aqu\u00ed es donde situar\u00edamos a los SGDB relacionales. Por ejemplo, PostreSQL es CA (aunque ambas soluciones ofrecen productos complementarios para dar soporte al particionado, como Redis Cluster y PgCluster), ya que no distribuyen los datos y por tanto la partici\u00f3n no es una restricci\u00f3n. Lo bueno es que la gran mayor\u00eda de sistemas permiten configurarse para cambiar su tipo CAP, lo que permite que MongoDB pase de CP a AP, o CouchDB de AP a CP.","title":"Clasificaci\u00f3n seg\u00fan CAP"},{"location":"sa/01nosql.html#base","text":"Partiendo del teorema de CAP, de forma an\u00e1loga al modelo transaccional ACID para las bases de datos relacionales que dan soporte a la transaccionalidad ofreciendo en todo momento un sistema consistente, las bases de datos distribuidas siguen el modelo transaccional BASE, el cual se centra en la alta disponibilidad y significa: B\u00e1sicamente disponible ( B asically A vailable ): la base de datos siempre responde a las solicitudes recibidas, ya sea con una respuesta exitosa o con un error, a\u00fan en el caso de que el sistema soporte la tolerancia a particiones (de manera que caiga alg\u00fan nodo o no est\u00e9 accesible por problemas de la red). Esto puedo implicar lecturas desde nodos que no han recibido la \u00faltima escritura, por lo que el resultado puede no ser consistente. Estado blando ( S oft State ): la base de datos puede encontrarse en un estado inconsistente cuando se produce una lectura, de modo que es posible realizar dos veces la misma lectura y obtener dos resultados distintos a pesar de que no haya habido ninguna escritura entre ambas operaciones, sino que la escritura se hab\u00eda realizado antes en el tiempo y no se hab\u00eda persistido hasta ese momento. Consistencia eventual ( E ventual consistency ): tras cada escritura, la consistencia de la base de datos s\u00f3lo se alcanza una vez el cambio ha sido propagado a todos los nodos. Durante el tiempo que tarda en producirse la consistencia, observamos un estado blando de la base de datos. Una base de datos que sigue el modelo transaccional BASE prefiere la disponibilidad antes que la consistencia (es decir, desde el punto de vista del teorema CAP es AP).","title":"BASE"},{"location":"sa/01nosql.html#referencias","text":"Next Generation Databases : NoSQL, NewSQL, and Big Data NoSQL Distilled : A Brief Guide to the Emerging World of Polyglot Persistence Row vs Column Oriented Databases","title":"Referencias"},{"location":"sa/01nosql.html#actividades","text":"( RA5075.2 / CE5.2a / 2p) Contesta a las siguientes preguntas: \u00bfQu\u00e9 significa el prefijo No del acr\u00f3nimo NoSQL ? \u00bfUn sistema puede soportar al mismo tiempo replicaci\u00f3n y particionado? Para los siguientes supuestos, indica qu\u00e9 modelo de datos emplear\u00edas y justifica tu respuesta: Wiki sobre de personajes de c\u00f3mics. Informaci\u00f3n acad\u00e9mica de un pa\u00eds (centros, alumnos, profesores, asignaturas, calificaciones, \u2026\u200b) Investiga en qu\u00e9 consiste la persistencia pol\u00edglota . Clasifica las siguientes bases de datos seg\u00fan el teorema de CAP en CA, CP o AP: BigTable, Cassandra, CouchDB, DynamoDB, HBase, MongoDB, Redis, Riak, Voldemort . ( RA5075.2 / CE5.2a / 1p) Crea una presentaci\u00f3n de 5-6 diapositivas donde expliques en qu\u00e9 consiste el movimiento NewSQL , su relaci\u00f3n con NoSQL y qu\u00e9 ofrecen bases de datos como CockroachDB y/o VoltDB .","title":"Actividades"},{"location":"sa/02mongo.html","text":"MongoDB ( http://www.mongodb.com ) es una de las bases de datos NoSQL m\u00e1s conocidas. Sigue un modelo de datos documental, donde los documentos se basan en JSON. huMONGOus Como curiosidad, su nombre viene de la palabra inglesa humongous , que significa gigantesco/enorme. MongoDB destaca porque: Soporta esquemas din\u00e1micos: diferentes documentos de una misma colecci\u00f3n pueden tener atributos diferentes. Soporte limitado de joins , ya que no escalan bien. No soporta transacciones. Lo que en un RDMS puede suponer m\u00faltiples operaciones, con MongoDB se puede hacer en una sola operaci\u00f3n al insertar/actualizar todo un documento de una sola vez. Conceptos \u00b6 Hay una serie de conceptos que conviene conocer antes de entrar en detalle: MongoDB tienen el mismo concepto de base de datos que un RDMS . Dentro de una instancia de MongoDB podemos tener 0 o m\u00e1s bases de datos, actuando cada una como un contenedor de alto nivel. Una base de datos tendr\u00e1 0 o m\u00e1s colecciones. Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un RDMS . MongoDB ofrece diferentes tipos de colecciones, desde las normales cuyo tama\u00f1o crece conforme lo hace el n\u00famero de documentos, como las colecciones capped , las cuales tienen un tama\u00f1o predefinido y que pueden contener una cierta cantidad de informaci\u00f3n que se sustituir\u00e1 por nueva cuando se llene. Las colecciones contienen 0 o m\u00e1s documentos , por lo que es similar a una fila o registro de un RDMS . Cada documento contiene 0 o m\u00e1s atributos, compuestos de parejas clave/valor . Cada uno de estos documentos no sigue ning\u00fan esquema, por lo que dos documentos de una misma colecci\u00f3n pueden contener todos los atributos diferentes entre s\u00ed. Elementos de MongoDB As\u00ed pues, tenemos que una base de datos va a contener varias colecciones, donde cada colecci\u00f3n contendr\u00e1 un conjunto de documentos: Modelo de MongoDB Adem\u00e1s, MongoDB soporta \u00edndices , igual que cualquier RDMS , para acelerar la b\u00fasqueda de datos. Al realizar cualquier consulta, se devuelve un cursor , con el cual podemos hacer cosas tales como contar, ordenar, limitar o saltar documentos. BSON \u00b6 Mediante JavaScript podemos crear objetos que se representan con JSON. Internamente, MongoDB almacena los documentos mediante BSON ( Binary JSON ). Podemos consultar la especificaci\u00f3n en http://BSONSpec.org Especificaci\u00f3n BSON BSON representa un superset de JSON ya que: Permite almacenar datos en binario Incluye un conjunto de tipos de datos no incluidos en JSON, como pueden ser ObjectId , Date o BinData . Podemos consultar todos los tipos que soporta un objeto BSON en http://docs.mongodb.org/manual/reference/bson-types/ Un ejemplo de un objeto BSON podr\u00eda ser: var yo = { n ombre : \"Aitor\" , apellidos : \"Medrano\" , fna c : ne w Da te ( \"Oct 3, 1977\" ) , hobbies : [ \"programaci\u00f3n\" , \"videojuegos\" , \"baloncesto\" ], casado : true , hijos : 2 , co nta c t o : { t wi tter : \"@aitormedrano\" , email : \"a.medrano@edu.gva.es\" }, fe chaCreacio n : ne w Times ta mp() } Los documentos BSON tienen las siguientes restricciones: No pueden tener un tama\u00f1o superior a 16 MB. El atributo _id queda reservado para la clave primaria. Desde MongoDB 5.0 los nombres de los campos pueden empezar por $ y/o contener el . , aunque en la medida de lo posible, es recomendable evitar su uso. Adem\u00e1s MongoDB: No asegura que el orden de los campos se respete. Es sensible a los tipos de los datos Es sensible a las may\u00fasculas. Por lo que estos documentos son distintos: { \"edad\" : \"18\" } { \"edad\" : 18 } { \"Edad\" : 18 } Si queremos validar si un documento JSON es v\u00e1lido, podemos usar http://jsonlint.com/ . Hemos de tener en cuenta que s\u00f3lo valida JSON y no BSON, por tanto nos dar\u00e1 errores en los tipos de datos propios de BSON. Puesta en marcha \u00b6 En la actualidad, MongoDB se comercializa mediante tres productos: Mongo Atlas , como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB. MongoDB Community Edition , versi\u00f3n gratuita para trabajar on-premise, con versiones para Windows, MacOS y Linux. MongoDB Enterprise Advanced , versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada. Instalaci\u00f3n \u00b6 Desde https://www.mongodb.com/try/download/community podemos descargar la versi\u00f3n Community acorde a nuestro sistema operativo. Independientemente de nuestro sistema operativo, por defecto, el demonio se lanza sobre el puerto 27017. Una vez instalado, si accedemos a http://localhost:27017 podremos ver que nos indica c\u00f3mo estamos intentando acceder mediante HTTP a MongoDB mediante el puerto reservado al driver nativo. Acceso al puerto 27017 En vez de instalarlo como un servicio en nuestra m\u00e1quina, a d\u00eda de hoy, es mucho m\u00e1s c\u00f3modo hacer uso de contenedores Docker o utilizar una soluci\u00f3n cloud . Docker \u00b6 Para lanzar el contenedor de Docker al que llamaremos iadb-mongo mediante el siguiente comando: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo A continuaci\u00f3n vamos a descargar el conjunto de datos sampledata.archive.gz que ofrece MongoDB a modo de prueba, el cual vamos a emplear a lo largo de las diferentes sesiones. Volvemos al terminal de nuestro sistema y copiamos los datos desde nuestro sistema a la carpeta /tmp del contenedor: docker cp sampledata.archive.gz iabd-mongo:/tmp Posteriormente abrimos un terminal dentro de nuestro contenedor (o mediante Attach Shell en VSCode ): docker exec -it iabd-mongo bash Y finalmente, restauramos los datos mediante mongorestore : mongorestore --gzip --archive = /tmp/sampledata.archive.gz Una vez cargados, nos informar\u00e1 que se han restaurado 433281 documentos. Mongo Atlas \u00b6 Y si preferimos una soluci\u00f3n cloud , disponemos de Mongo Atlas , que nos ofrece de manera gratuita un cluster compartido de servidores con 3 nodos y 512 MB para datos. Si queremos una soluci\u00f3n serverless o un servidor dedicado, ya tendremos que pasar por caja . Registro en Mongo Atlas Para comenzar a utilizar Mongo Atlas el primer paso es registrarnos y completar un cuestionario sobre nuestro uso. Tras ello: Creamos el cluster de despliegue. En nuestro caso, hemos realizado el despliegue en AWS en la regi\u00f3n de Paris ( eu-west-3 ) y dejado el nombre por defecto, Cluster 0 . Elecci\u00f3n del cluster Creamos un usuario/contrase\u00f1a para autenticar nuestra conexi\u00f3n. En nuestro caso, hemos creado el usuario iabd con la contrase\u00f1a iabdiabd (despu\u00e9s la podemos modificar desde el men\u00fa Security -> Database Access ): Configuraci\u00f3n del usuario En la misma pantalla, indicamos que permitimos las conexiones desde todas las direcciones IP (esta decisi\u00f3n s\u00f3lo la tomamos por comodidad, para poder conectarnos desde casa y el centro) mediante la IP 0.0.0.0 (despu\u00e9s podemos modificar la configuraci\u00f3n desde el men\u00fa Security -> Network Access ). Una vez realizados los dos pasos anteriores, comenzar\u00e1 la creaci\u00f3n del cluster, la cual puede tardar de 2 a 3 minutos. Dashboard del cluster A continuaci\u00f3n, cargaremos los datos de ejemplo. Para ello, en el men\u00fa con los tres puntos ( ... ), elegiremos la opci\u00f3n Load Sample Dataset . Una vez haya finalizado, podremos ver los datos cargados pulsando sobre el bot\u00f3n Browse Collections : Colecciones con los datos de prueba Conexi\u00f3n segura Mediante srv se establece una conexi\u00f3n segura Finalmente, para obtener la cadena de conexi\u00f3n, desde el dashboard del cluster con la opci\u00f3n Connect o desde la pesta\u00f1a Cmd Line Tools del propio cluster, podremos obtener la cadena de conexi\u00f3n , que tendr\u00e1 un formato similar a : mongodb+srv://usuario:password@host/basededatos A continuaci\u00f3n, vamos a conocer las diferentes herramientas que nos ofrece MongoDB para posteriormente estudiar todas las operaciones que podemos realizar. mongosh \u00b6 Tras arrancar el demonio mongod (el cual se lanza autom\u00e1ticamente mediante Docker o con el cluster de Mongo Atlas ) llega el momento de acceder mediante el cliente mongosh (en versiones anteriores el comando utilizado era mongo ), el cual funciona igual que un shell, de modo que con la fecha hacia arriba visualizaremos el \u00faltimo comando. El cliente utiliza JavaScript como lenguaje de interacci\u00f3n con la base de datos. Si nos conectamos desde Docker, no necesitamos instalarlo. Primero nos conectamos al contenedor: docker exec -it iabd-mongo bash Al conectar con mongosh si no le indicamos nada se conectar\u00e1 por defecto a la base de datos test de localhost . Si queremos conectarnos a una base de datos concreta, por ejemplo a sample_training , la pasaremos como par\u00e1metro: root @ 3 ad17b675fb1 : /# mongosh sample_training Current Mongosh Log ID : 6316498 f30f8283fedcfabc2 Connecting to : mongodb : //127.0.0.1:27017/sample_training?directConnection=true&serverSelectionTimeoutMS=2000 Using MongoDB : 5.0.4 Using Mongosh : 1.1.2 Si queremos ver las bases de datos que existen ejecutaremos el comando show dbs : sample_training> show dbs ; admin 41 kB config 73 .7 kB local 73 .7 kB sample_airbnb 55 .1 MB sample_analytics 9 .9 MB sample_geospatial 999 kB sample_mflix 48 .5 MB sample_restaurants 6 .2 MB sample_supplies 991 kB sample_training 43 .4 MB sample_weatherdata 2 .49 MB Si nos quisi\u00e9ramos conectar a nuestro cluster de Mongo Atlas utilizaremos la cadena de conexi\u00f3n tras el comando mongosh : root@3ad17b675fb1:/# mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/test Current Mongosh Log ID: 63164ac26030844c1576f8b4 Connecting to: mongodb+srv://<credentials>@cluster0.dfaz5er.mongodb.net/test Using MongoDB: 5 .0.11 Using Mongosh: 1 .1.2 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ Atlas atlas-4wikkb-shard-0 [ primary ] test> Uso externo Si no queremos tener que conectarnos al contenedor o vamos a trabajar con un servidor remoto, podemos instalar \u00fanicamente el shell desde https://www.mongodb.com/try/download/shell MongoDB Database Tools \u00b6 Adem\u00e1s del propio servidor de MongoDB y el cliente para conectarse a \u00e9l, MongoDB ofrece un conjunto de herramientas para interactuar con las bases de datos, permitiendo crear y restaurar copias de seguridad. Si estamos interesados en introducir o exportar una colecci\u00f3n de datos mediante JSON, podemos emplear los comandos mongoimport y mongoexport : mongoimport -d nombreBaseDatos -c coleccion \u2013-file nombreFichero.json mongoexport -d nombreBaseDatos -c coleccion nombreFichero.json Estas herramientas interact\u00faan con datos JSON y no sobre toda la base de datos. Un caso particular y muy com\u00fan es importar datos que se encuentran en formato CSV/TSV. Para ello, emplearemos el par\u00e1metro --type csv : mongoimport --type tsv -d test -c poblacion --headerline --drop poblacionEspanya2013.tsv En vez de realizar un export , es m\u00e1s conveniente realizar un backup en binario mediante mongodump , el cual genera ficheros BSON. Estos archivos posteriormente se restauran mediante mongorestore : mongodump -d nombreBaseDatos nombreFichero.bson mongorestore -d nombreBaseDatos nombreFichero.bson Autoevaluaci\u00f3n Intenta exportar los datos de la base de datos sample_training desde MongoAtlas. Veras que ha creado una carpeta que contiene dos archivos \u00bfCu\u00e1les son? \u00bfQu\u00e9 contiene cada uno de ellos y cual es su formato? Si necesitamos transformar un fichero BSON a JSON (de binario a texto), tenemos el comando bsondump : bsondump file.bson > file.json Info M\u00e1s informaci\u00f3n sobre copias de seguridad en https://www.mongodb.com/docs/manual/core/backups/ . Para poder trabajar con MongoDB desde cualquier aplicaci\u00f3n necesitamos un driver. MongoDB ofrece drivers oficiales para casi todos los lenguajes de programaci\u00f3n actuales. En la sesi\u00f3n 30 de 'MongoDB y Python' trabajaremos con PyMongo . Monitorizaci\u00f3n Tanto mongostat como mongotop permiten visualizar el estado del servidor MongoDB, as\u00ed como algunas estad\u00edsticas sobre su rendimiento. Si trabajamos con MongoAtlas estas herramientas est\u00e1n integradas en las diferentes herramientas de monitorizaci\u00f3n de la plataforma. En versiones anteriores, una herramienta de terceros bastante utilizada era RoboMongo / Robo3T / Studio3T el cual extiende el shell y ofrece un IDE m\u00e1s amigable. A d\u00edas de hoy, MongoDB tiene su propio IDE conocido como Mongo Compass . Mongo Compass \u00b6 En el curso nos vamos a centrar en el uso del shell y la conectividad de MongoDB mediante Python, pero no est\u00e1 de m\u00e1s conocer las herramientas visuales que facilitan el trabajo con MongoDB en el d\u00eda a d\u00eda. Una de ellas es Mongo Compass , que facilita la exploraci\u00f3n y manipulaci\u00f3n de los datos. De una manera flexible e intuitiva, Compass ofrece visualizaciones detalladas de los esquemas, m\u00e9tricas de rendimiento en tiempo real as\u00ed como herramientas para la creaci\u00f3n de consultas. Existen tres versiones de Compass, una completa con todas las caracter\u00edsticas, una de s\u00f3lo lectura sin posibilidad de insertar, modificar o eliminar datos (perfecta para anal\u00edtica de datos) y una \u00faltima versi\u00f3n isolated que solo permite la conexi\u00f3n a una instancia local. Una vez descargada e instalada la versi\u00f3n que nos interesa, tras crear la conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), veremos en el men\u00fa de la izquierda un resumen del cluster, as\u00ed como las consultas que vayamos almacenando y las diferentes bases de datos almacenadas: GUI de Mongo Compass Si seleccionamos una base de datos concreta, y de ella, una colecci\u00f3n en el men\u00fa de la izquierda, en el panel central tendremos una visualizaci\u00f3n de los datos contenidos, as\u00ed como opciones para ver su esquema, realizar consultas agregadas, editar los \u00edndices, etc... Adem\u00e1s, podremos realizar consultas sobre los datos: Opciones desde una colecci\u00f3n mongosh en Compass Si te fijas, en la barra inferior podemos desplegar un panel para interactuar mediante comandos como lo har\u00edamos desde mongosh . MongoDB for VSCode \u00b6 Tambi\u00e9n podemos utilizar la extensi\u00f3n que lleva VSCode para trabajar con MongoDB . Tras su instalaci\u00f3n creamos una conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), y una vez conectados, podremos recorrer las colecciones con los datos as\u00ed como utilizar un playground para interactuar de manera similar al shell: Uso de la extensi\u00f3n de VSCode Hola MongoDB \u00b6 Pues una vez que ya nos hemos conectado a MongoDB mediante mongosh , vamos a empezar a interactuar con los datos. En cualquier momento podemos cambiar la base de datos activa mediante use nombreBaseDatos . Si la base de datos no existiese, MongoDB crear\u00e1 dicha base de datos. Esto es una verdad a medias, ya que la base de datos realmente se crea al insertar datos dentro de alguna colecci\u00f3n. As\u00ed pues, vamos a crear nuestra base de datos iabd : use iabd Una vez creada, podemos crear nuestra primera colecci\u00f3n, que llamaremos people , e insertaremos un persona con nuestros datos personales mediante el m\u00e9todo insertOne , al que le pasamos un objeto JSON: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Tras ejecutar el comando, veremos que nos devuelve un objeto JSON con su ACK y el identificador del documento insertado: { ack n owledged : true , i nserte dId : Objec t Id( \"6316fc938cc2bc168bfed066\" ) } Una vez insertada, s\u00f3lo nos queda realizar una consulta para recuperar los datos y comprobar que todo funciona correctamente mediante el m\u00e9todo findOne : db . people . findOne () Lo que nos dar\u00e1 como resultado un objeto JSON que contiene un atributo _id con el mismo identificador mostrado anteriormente, adem\u00e1s de los que le a\u00f1adimos al insertar la persona: { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' } Como podemos observar, todas las instrucciones van a seguir el patr\u00f3n de db.nombreColeccion.operacion() . Trabajando con el shell \u00b6 Antes de entrar en detalles en las instrucciones necesarias para realizar las operaciones CRUD, veamos algunos comandos que nos ser\u00e1n muy \u00fatiles al interactuar con el shell: Comando Funci\u00f3n show dbs Muestra el nombre de las bases de datos show collections Muestra el nombre de las colecciones db Muestra el nombre de la base de datos que estamos utilizando db.dropDatabase() Elimina la base de datos actual db.help() Muestra los comandos disponibles db.version() Muestra la versi\u00f3n actual del servidor En el resto de la sesi\u00f3n vamos a hacer un uso intenso del shell de MongoDB. Por ejemplo, si nos basamos en el objeto definido en el apartado de BSON, podemos ejecutar las siguientes instrucciones: > db . people . insertOne ( yo ) // (1) < { acknowledged : true , insertedId : ObjectId ( \"631704a042aae0893122f2d6\" ) } > db . people . find () // (2) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) } ] > yo . profesion = \"Profesor\" < Profesor > db . people . insertOne ( yo ) // (3) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) }, { _id : ObjectId ( \"6317056d42aae0893122f2d7\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662453101 , i : 1 }), profesion : 'Profesor' } ] > db . people . countDocuments () // (4) < 3 Si queremos insertar un documento en una colecci\u00f3n, hemos de utilizar el m\u00e9todo insertOne pas\u00e1ndole como par\u00e1metro el documento que queremos insertar, ya sea a partir de una variable o el propio documento en s\u00ed. find recupera todos los documentos de la colecci\u00f3n Modificamos nuestro documento y los volvemos a insertar. Realmente va a crear un nuevo documento, y no se va a quejar de que ya exista, porque nuestro documento no contiene ning\u00fan atributo identificador, por lo que considera que se trata de una nueva persona. Obtenemos la cantidad de documentos de la colecci\u00f3n mediante countDocuments . Con este ejemplo, hemos podido observar como los documentos de una misma colecci\u00f3n no tienen por qu\u00e9 tener el mismo esquema, ni hemos necesitado definirlo expl\u00edcitamente antes de insertar datos. As\u00ed pues, el esquema se ir\u00e1 generando y actualizando conforme se inserten documentos. M\u00e1s adelante veremos que podemos definir un esquema para validar que los datos que insertamos cumplan restricciones de tipos de datos o elementos que obligatoriamente deben estar rellenados. Empleando JavaScript \u00b6 Ya hemos comentado que el shell utiliza JavaScript como lenguaje de interacci\u00f3n, por lo que podemos almacenar los comandos en un script externo y ejecutarlo mediante load() : load ( \"scripts/misDatos.js\" ); load ( \"/data/db/scripts/misDatos.js\" ); Si hacemos una referencia relativa, lo hace respecto a la ruta desde la cual se ejecuta el shell mongosh . Otra manera de lanzar un script es hacerlo desde la l\u00ednea de comandos, pas\u00e1ndole como segundo par\u00e1metro el script a ejecutar: mongosh iabd misDatos.js Si el c\u00f3digo a ejecutar no necesita almacenarse en un script externo, el propio shell permite introducir instrucciones en varias l\u00edneas: > for ( var i = 0 ; i < 10 ; i ++ ) { ... db . espias . insertOne ({ \"nombre\" : \"James Bond \" + i , \"agente\" : \"00\" + i }); ... } < { acknowledged : true , insertedId : ObjectId ( \"63171d2142aae0893122f2e1\" ) } > db . espias . find () < [ { _id : ObjectId ( \"63171d2142aae0893122f2d8\" ), nombre : 'James Bond 0' , agente : '000' }, { _id : ObjectId ( \"63171d2142aae0893122f2d9\" ), nombre : 'James Bond 1' , agente : '001' }, ... { _id : ObjectId ( \"63171d2142aae0893122f2e1\" ), nombre : 'James Bond 9' , agente : '009' } ] ObjectId \u00b6 En MongoDB , el atributo _id es \u00fanico dentro de la colecci\u00f3n, y hace la funci\u00f3n de clave primaria. Se le asocia un ObjectId , el cual es un tipo BSON de 12 bytes que se crea mediante: el timestamp actual (4 bytes) un valor aleatorio y \u00fanico por m\u00e1quina y proceso (5 bytes) un contador inicializado a n\u00famero aleatorio (3 bytes). Este objeto lo crea el driver y no MongoDB , por lo cual no deberemos considerar que siguen un orden concreto, ya que clientes diferentes pueden tener timestamps desincronizados. Lo que s\u00ed que podemos obtener a partir del ObjectId es la fecha de creaci\u00f3n del documento, mediante el m\u00e9todo getTimestamp() del atributo _id . Obteniendo la fecha de creaci\u00f3n de un documento > db.people.findOne () ._id < ObjectId ( \"6316fc938cc2bc168bfed066\" ) > db.people.findOne () ._id.getTimestamp () < ISODate ( \"2022-09-06T07:53:55.000Z\" ) Este identificador es global, \u00fanico e inmutable. Esto es, no habr\u00e1 dos repetidos y una vez un documento tiene un _id , \u00e9ste no se puede modificar. Si en la definici\u00f3n del objeto a insertar no ponemos el atributo identificador, MongoDB crear\u00e1 uno de manera autom\u00e1tica. Si lo ponemos nosotros de manera expl\u00edcita, MongoDB no a\u00f1adir\u00e1 ning\u00fan ObjectId . Eso s\u00ed, debemos asegurarnos que sea \u00fanico (podemos usar n\u00fameros, cadenas, etc\u2026\u200b). Por lo tanto, podemos asignar un identificador al insertar: > db . people . insert ({ _id : 4 , nombre : \"Marina\" , edad : 14 }) < { acknowledged : true , insertedIds : { '0' : 4 } } Tipos de datos Cuidado con los tipos, ya que no es lo mismo insertar un atributo con edad:14 (se considera el campo como entero) que con edad:\"14\" , ya que considera el campo como texto. O tambi\u00e9n, si queremos podemos hacer que el _id de un documento sea un documento en s\u00ed, y no un entero, para ello, al insertarlo, podemos asignarle un objeto JSON al atributo identificador: > db . people . insertOne ({ _id : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' }, ciudad : 'Elx' }) < { acknowledged : true , insertedId : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' } } Recuperando datos \u00b6 Para recuperar los datos de una colecci\u00f3n o un documento en concreto usaremos el m\u00e9todo find() : > db . people . find () < { _id : ObjectId ( \"6316fc1597eb703de2add36e\" ), nombre : 'Aitor' , edad : 45 , profesion : 'Profesor' } { _id : ObjectId ( \"6317048697eb703de2add36f\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : 1977 - 10 - 02 T23 : 00 : 00 .000 Z , hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452870 , i : 3 }) } El m\u00e9todo find() sobre una colecci\u00f3n devuelve un cursor a los datos obtenidos, el cual se queda abierto con el servidor y que se cierra autom\u00e1ticamente a los 30 minutos de inactividad o al finalizar su recorrido. Si hay muchos resultados, la consola nos mostrar\u00e1 un subconjunto de los datos (20). Si queremos seguir obteniendo resultados, solo tenemos que introducir it , para que contin\u00fae iterando el cursor. En cambio, si s\u00f3lo queremos recuperar un documento hemos de utilizar findOne() : > db.people. f i n dO ne () > { _id : Objec t Id( \"6316fc1597eb703de2add36e\" ) , n ombre : 'Ai t or' , edad : 45 , pro fes io n : 'Pro fes or' } Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de 10.000 documentos sobre los viajes realizados por los usuarios de una empresa de alquiler de bicicletas, los cuales han sido extra\u00eddos de https://ride.citibikenyc.com/system-data . Esta colecci\u00f3n ( trips ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de viaje ser\u00eda: > use sample_training < 'switched to db sample_training' > db . trips . findOne () < { _id : ObjectId ( \"572bb8222b288919b68abf5b\" ), tripduration : 889 , 'start station id' : 268 , 'start station name' : 'Howard St & Centre St' , 'end station id' : 3002 , 'end station name' : 'South End Ave & Liberty St' , bikeid : 22794 , usertype : 'Subscriber' , 'birth year' : 1961 , 'start station location' : { type : 'Point' , coordinates : [ - 73.99973337 , 40.71910537 ] }, 'end station location' : { type : 'Point' , coordinates : [ - 74.015756 , 40.711512 ] }, 'start time' : 2016 - 01 - 01 T00 : 01 : 06 .000 Z , 'stop time' : 2016 - 01 - 01 T00 : 15 : 56.000 Z } Criterios en consultas \u00b6 Al hacer una consulta, si queremos obtener datos mediante m\u00e1s de un criterio, en el primer par\u00e1metro del find podemos pasar un objeto JSON con los campos a cumplir (condici\u00f3n Y). Consulta Resultado db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) { _id : Objec t Id( \"572bb8222b288919b68ad197\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 23724 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 45.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 48.000 Z } { _id : Objec t Id( \"572bb8222b288919b68ad191\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 17075 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 38.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 42.000 Z } Consejo de Rendimiento Las consultas disyuntivas, es decir, con varios criterios u operador $and , deben filtrar el conjunto m\u00e1s peque\u00f1o cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar documentos que cumplen los criterios A, B y C. Digamos que el criterio A lo cumplen 40.000 documentos, el B lo hacen 9.000 y el C s\u00f3lo 200. Si filtramos A, luego B, y finalmente C, el conjunto que trabaja cada criterio es muy grande. Restringiendo consultas AND En cambio, si hacemos una consulta que primero empiece por el criterio m\u00e1s restrictivo, el resultado con lo que se intersecciona el siguiente criterio es menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas AND de menor a mayor MongoDB tambi\u00e9n ofrece operadores l\u00f3gicos para los campos num\u00e9ricos: Comparador Operador menor que ( < ) $lt menor o igual que ( \u2264 ) $lte mayor que ( > ) $gt mayor o igual que ( \u2265 ) $gte Estos operadores se pueden utilizar de forma simult\u00e1nea sobre un mismo campo o sobre diferentes campos, sobre campos anidados o que forman parte de un array, y se colocan como un nuevo documento en el valor del campo a filtrar, compuesto del operador y del valor a comparar mediante la siguiente sintaxis: db . < coleccion > . find ({ < campo >: { < operador >: < valor > } }) Por ejemplo, para recuperar los viajes que han durado menos de 5 minutos o comprendidos entre 3 y 5 minutos (el campo almacena el tiempo en segundos), podemos hacer: db . trips . find ({ \"tripduration\" : { $lt : 300 } }) db . trips . find ({ \"tripduration\" : { $gt : 180 , $lte : 300 } }) Para los campos de texto, adem\u00e1s de la comparaci\u00f3n directa, podemos usar el operador $ne para obtener los documentos cuyo campos no tienen un determinado valor ( not equal ). As\u00ed pues, podemos usarlo para averiguar todos los trayectos realizados por usuarios que no son subscriptores ( Subscriber ): db . trips . find ({ \"usertype\" : { $ne : \"Subscriber\" } }) Por supuesto, podemos tener diferentes operadores en campos distintos. Por ejemplo, si queremos ver los viajes de menos de un minuto y medio realizado por usuarios que no son subscriptores har\u00edamos: db . trips . find ({ \"tripduration\" : { $lt : 90 }, \"usertype\" : { $ne : \"Subscriber\" } }) Case sensitive Las comparaciones de cadenas se realizan siguiendo el orden UTF8, similar a ASCII, con lo cual no es lo mismo buscar un rango entre may\u00fasculas que min\u00fasculas. Con cierto parecido a la condici\u00f3n de valor no nulo de las BBDD relacionales y teniendo en cuenta que la libertad de esquema puede provocar que un documento tenga unos campos determinados y otro no lo tenga, podemos utilizar el operador $exists si queremos averiguar si un campo existe (y por tanto tiene alg\u00fan valor). db . people . find ({ \"edad\" : { $exists : true }}) Polimorfismo Mucho cuidado al usar polimorfismo y almacenar en un mismo campo un entero y una cadena, ya que al hacer comparaciones para recuperar datos, no vamos a poder mezclar cadenas con valores num\u00e9ricos. Se considera un antipatr\u00f3n el mezclar tipos de datos en un campo. Pese a que ciertos operadores contengan su correspondiente operador negado, MongoDB ofrece el operador $not . \u00c9ste puede utilizarse conjuntamente con otros operadores para negar el resultado de los documentos obtenidos. Por ejemplo, si queremos obtener todas las personas cuya edad no sea m\u00faltiplo de 5, podr\u00edamos hacerlo as\u00ed: db . people . find ({ edad : { $not : { $mod : [ 5 , 0 ]}}}) Expresiones regulares \u00b6 Finalmente, si queremos realizar consultas sobre partes de un campo de texto, hemos de emplear expresiones regulares. Para ello, tenemos el operador $regexp o, de manera m\u00e1s sencilla, indicando como valor la expresi\u00f3n regular a cumplir: Por ejemplo, para buscar la cantidad de viajes que salen da alguna estaci\u00f3n cuyo nombre contenga Central Park podemos hacer: db . trips . find ({ \"start station name\" : /Central Park/ }). count () db . trips . find ({ \"start station name\" : /central park/i }). count () db . trips . find ({ \"start station name\" : { $regex : /central park/i }}). count () B\u00fasquedas sobre textos Si vamos a realizar b\u00fasquedas intensivas sobre texto, desde MongoDB han creado un producto espec\u00edfico dentro del ecosistema de Mongo Atlas el cual ofrece un mejor rendimiento y mayor funcionalidad que el uso de expresiones regulares, conocido con Mongo Atlas Search . Si usamos una soluci\u00f3n on-premise , mediante \u00edndices de texto y el operator $text podemos realizar b\u00fasquedas. Operador $expr \u00b6 El operador $expr es un operador de consulta expresiva que permite utilizar expresiones de agregaci\u00f3n dentro de las consultas. Permite utilizar variables y sentencias condicionales, asi como comparar campos dentro de un documento. As\u00ed pues, si queremos comparar valores entre dos campos, podemos hacerlo mediante $expr referenciando a los campos anteponiendo un dolar ( $ ) delante del campo, de manera que si queremos obtener los viajes que comienzan y finalizan en la misma estaci\u00f3n podemos hacer: db . trips . find ({ \"$expr\" : { \"$eq\" : [ \"$end station id\" , \"$start station id\" ]}}) Al poner el $ delante de un campo, en vez de referenciar al campo, lo que hace es referenciar a su valor, por lo que $end station id est\u00e1 referenciando al valor del campo end station id . Otros operadores El operador $type permite recuperar documentos que dependan del tipo de campo que contiene. El operador $where permite introducir una expresi\u00f3n JavaScript . Proyecci\u00f3n de campos \u00b6 Las consultas realizadas hasta ahora devuelven los documentos completos. Si queremos que devuelva un campo o varios campos en concreto, hemos de pasar un segundo par\u00e1metro de tipo JSON con aquellos campos que deseamos mostrar con el valor true o 1 . Destacar que si no se indica nada, por defecto siempre mostrar\u00e1 el campo _id > db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }, { tripduration : 1 }) < { _id : ObjectId ( \"572bb8222b288919b68ad191\" ), tripduration : 1143 } Por lo tanto, si queremos que no se muestre el _id , lo podremos a false o 0 : > db. tr ips. f i n d( { 's tart s tat io n id' : 405 , 'e n d s tat io n id' : 146 }, { tr ipdura t io n : 1 , _id : 0 } ) < { tr ipdura t io n : 1143 } No mezcles Al hacer una proyecci\u00f3n, no podemos mezclar campos que se vean ( 1 ) con los que no ( 0 ). Es decir, hemos de hacer algo similar a: db . < coleccion > . find ({ < consulta > }, { < campo1 >: 1 , < campo2 >: 1 }) db . < coleccion > . find ({ < consulta > }, { < campo1 >: 0 , < campo2 >: 0 }) As\u00ed pues, s\u00f3lo se mezclar\u00e1 la visibilidad de los campos cuando queramos ocultar el _id . Condiciones compuestas con Y / O \u00b6 Para usar la conjunci\u00f3n o la disyunci\u00f3n, tenemos los operadores $and y $or . Son operadores prefijo, de modo que se ponen antes de las subconsultas que se van a evaluar. Estos operadores trabajan con arrays, donde cada uno de los elementos es un documento con la condici\u00f3n a evaluar, de modo que se realiza la uni\u00f3n entre estas condiciones, aplicando la l\u00f3gica asociada a AND y a OR. db . trips . find ({ $or : [{ 'start station id' : 405 }, { 'end station id' : 146 }] }) db . trips . find ({ $or : [{ \"tripduration\" : { $lte : 70 }}, { \"tripduration\" : { $gte : 3600 }}] }) Realmente el operador $and no se suele usar porque podemos anidar en la consulta dos criterios, al poner uno dentro del otro. As\u00ed pues, estas dos consultas hacen lo mismo: db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) db . trips . find ({ $and : [ { 'start station id' : 405 }, { 'end station id' : 146 } ] }) Consejo de Rendimiento Las consultas conjuntivas, es decir, con varios criterios excluyentes u operador $or , deben filtrar el conjunto m\u00e1s grande cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar los mismos documentos que cumplen los criterios A (40.000 documentos), B (9.000 documentos) y C (200 documentos). Si filtramos C, luego B, y finalmente A, el conjunto de documentos que tiene que comprobar MongoDB es muy grande. Restringiendo consultas OR de menor a mayor En cambio, si hacemos una consulta que primero empiece por el criterio menos restrictivo, el conjunto de documentos sobre el cual va a tener que comprobar siguientes criterios va a ser menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas OR de mayor a menor Tambi\u00e9n podemos utilizar el operado $nor , que no es m\u00e1s que la negaci\u00f3n de $or y que obtendr\u00e1 aquellos documentos que no cumplan ninguna de las condiciones. Autoevaluaci\u00f3n Que obtendr\u00edamos al ejecutar la siguiente consulta: db . trips . find ({ \"tripduration\" : { $lte : 65 }, $nor : [ { usertype : \"Customer\" }, { \"birth year\" : 1989 } ] }) Finalmente, si queremos indicar mediante un array los diferentes valores que puede cumplir un campo, podemos utilizar el operador $in : db . trips . find ({ \"birth year\" : { $in : [ 1977 , 1980 ]} }) Por supuesto, tambi\u00e9n existe su negaci\u00f3n mediante $nin . Condiciones sobre objetos anidados \u00b6 Si queremos acceder a campos de subdocumentos, siguiendo la sintaxis de JSON, se utiliza la notaci\u00f3n punto. Esta notaci\u00f3n permite acceder al campo de un documento anidado, da igual el nivel en el que est\u00e9 y su orden respecto al resto de campos. Preparando los ejemplos Para los siguientes ejemplos sobre documentos anidados y arrays, vamos a utilizar una colecci\u00f3n de 500 documentos sobre mensajes de un blog. Esta colecci\u00f3n ( posts ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de mensaje ser\u00eda: > use sample_training 'switched to db sample_training' > db . posts . findOne () < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc3f9\" ), body : 'Amendment I\\n<p>Congress shall make ....\"\\n<p>\\n' , permalink : 'aRjNnLZkJkTyspAIoRGe' , author : 'machine' , title : 'Bill of Rights' , tags : 'santa' , 'xylophone' , 'math' , 'dream' , 'action' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'HvizfYVx@pKvLaagH.com' , author : 'Santiago Dollins' }, { body : 'Lorem ipsum dolor sit...' , email : 'WpOUCpdD@hccdxJvT.com' , author : 'Jaclyn Morado' }, { body : 'Lorem ipsum dolor sit amet...' , email : 'OgDzHfFN@cWsDtCtx.com' , author : 'Houston Valenti' }], date : 2012 - 11 - 20 T05 : 05 : 15.231 Z } Para acceder al autor de los comentarios de un mensaje usar\u00edamos la propiedad comments.author . Por ejemplo, para averiguar los mensajes titulados 'Bill of Rights' y que tienen alg\u00fan comentario creado por Santiago Dollins har\u00edamos: db . posts . find ({ title : 'Bill of Rights' , \"comments.author\" : 'Santiago Dollins' }) Consultas sobre arrays \u00b6 Si trabajamos con arrays, vamos a poder consultar el contenido de una posici\u00f3n del mismo tal como si fuera un campo normal, siempre que sea un campo de primer nivel, es decir, no sea un documento embebido dentro de un array. Si queremos filtrar teniendo en cuenta el n\u00famero de ocurrencias del array, podemos utilizar: $all para filtrar ocurrencias que tienen todos los valores del array, es decir, los valores pasados a la consulta ser\u00e1n un subconjunto del resultado. Puede que devuelva los mismos, o un array con m\u00e1s campos (el orden no importa) $in , igual que SQL, para obtener las ocurrencias que cumple con alguno de los valores pasados (similar a usar $or sobre un conjunto de valores de un mismo campo). Si queremos su negaci\u00f3n, usaremos $nin , para obtener los documentos que no cumplen ninguno de los valores. Por ejemplo, si queremos obtener los mensajes que contenga las etiquetas dream y action tendr\u00edamos: db . posts . find ( { tags : { $all : [ \"dream\" , \"action\" ]}} ) En cambio, si queremos los mensajes que contengan alguna de esas etiquetas har\u00edamos: db . posts . find ( { tags : { $in : [ \"dream\" , \"action\" ]}} ) Si el array contiene documentos y queremos filtrar la consulta sobre los campos de los documentos del array, tenemos que utilizar $elemMatch , de manera que obtengamos aquellos que al menos encuentre un elemento que cumpla el criterio. As\u00ed pues, si queremos recuperar los mensajes que tienen un comentario cuyo autor sea Santiago Dollins har\u00edamos: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Criterio con notaci\u00f3n punto En el ejemplo anterior, si s\u00f3lo hubi\u00e9ramos tenido un campo para el filtrado, podr\u00edamos haber utilizado la notaci\u00f3n punto comments.author . Si s\u00f3lo queremos los comentarios escritos por un determinado autor, en vez de en el filtrado, hemos de indicarlo en la proyecci\u00f3n: db . posts . find ( {}, { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Si lo que nos interesa es la cantidad de elementos que contiene un array, emplearemos el operador $size . Por ejemplo, para obtener los mensajes que tienen 10 etiquetas har\u00edamos: db . posts . find ( { tags : { $size : 10 }} ) Finalmente, a la hora de proyectar los datos, si no estamos interesados en todos los valores de un campo que es un array, podemos restringir el resultado mediante el operador $slice . As\u00ed pues, si quisi\u00e9ramos obtener las mensajes titulados US Constitution y que de esos mensajes, mostrar s\u00f3lo tres etiquetas y dos comentarios, har\u00edamos: > db . posts . find ( { title : \"US Constitution\" }, { comments : { $slice : 2 }, tags : { $slice : 3 }} ) < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc416\" ), body : 'We the People ...' , permalink : 'NhWDUNColpvxFjovsgqU' , author : 'machine' , title : 'US Constitution' , tags : [ 'engineer' , 'granddaughter' , 'sundial' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'ftRlVMZN@auLhwhlj.com' , author : 'Leonida Lafond' }, { body : 'Lorem ipsum dolor sit...' , email : 'dsoLAdFS@VGBBuDVs.com' , author : 'Nobuko Linzey' } ], date : 2012 - 11 - 20 T05 : 05 : 15.276 Z } Conjunto de valores \u00b6 Igual que en SQL, a partir de un colecci\u00f3n, si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct > db . trips . distinct ( 'usertype' ) < [ 'Customer' , 'Subscriber' ] Si queremos filtrar los datos sobre los que se obtienen los valores, le pasaremos un segundo par\u00e1metro con el criterio a aplicar: > db . trips . distinct ( 'usertype' , { \"birth year\" : { $gt : 1990 } } ) < [ 'Subscriber' ] Cursores \u00b6 Al hacer una consulta en el shell , se devuelve un cursor. Este cursor lo podemos guardar en un variable, y partir de ah\u00ed trabajar con \u00e9l como har\u00edamos mediante cualquier lenguaje de programaci\u00f3n. Si cur es la variable que referencia al cursor, podremos utilizar los siguientes m\u00e9todos: M\u00e9todo Uso Lugar de ejecuci\u00f3n cur.hasNext() true / false para saber si quedan elementos Cliente cur.next() Pasa al siguiente documento Cliente cur.limit(cantidad) Restringe el n\u00famero de resultados a cantidad Servidor cur.sort({campo:1}) Ordena los datos por campo: 1 ascendente o -1 o descendente Servidor cur.skip(cantidad) Permite saltar cantidad elementos con el cursor Servidor La consulta no se ejecuta hasta que el cursor comprueba o pasa al siguiente documento ( next / hasNext ), por ello que tanto limit como sort (ambos modifican el cursor) s\u00f3lo se pueden realizar antes de recorrer cualquier elemento del cursor. Como tras realizar una consulta con find , realmente se devuelve un cursor, un uso muy habitual es encadenar una operaci\u00f3n de find con sort y/o limit para ordenar el resultado por uno o m\u00e1s campos y posteriormente limitar el n\u00famero de documentos a devolver. As\u00ed pues, si quisi\u00e9ramos obtener los tres viajes que m\u00e1s han durado, podr\u00edamos hacerlo as\u00ed: db . trips . find (). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Tambi\u00e9n podemos filtrar previamente a ordenar y limitar: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Finalmente, podemos paginar utilizando el m\u00e9todo skip , para mostrar viajes de 10 en 10 y a partir de la tercera p\u00e1gina, podr\u00edamos hacer algo as\u00ed: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 10 ). skip ( 20 ) Autoevaluaci\u00f3n A partir de la colecci\u00f3n trips , escribe un consulta que recupere los viajes realizados por subscriptores ordenados descendentemente por su duraci\u00f3n y que obtenga los documentos de 15 al 20. Contando Documentos \u00b6 Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo countDocuments . Por ejemplo: > db . trips . countDocuments ({ \"birth year\" : 1977 }) < 186 > db . trips . countDocuments ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 116 count Desde la versi\u00f3n 4.0, los m\u00e9todos count a nivel de colecci\u00f3n y de cursor est\u00e1n caducados ( deprecated ), y no se recomienda su utilizaci\u00f3n. A\u00fan as\u00ed, es muy com\u00fan utilizarlo como m\u00e9todo de un cursor: db . trips . find ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}). count () Cuando tenemos much\u00edsimos datos, si no necesitamos exactitud pero queremos un valor estimado el cual tarde menos en conseguirse (utiliza los metadatos de las colecciones), podemos usar estimatedDocumentCount > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 }) < 10.000 > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 10.000 Modificando documentos \u00b6 Preparando un persona Para este apartado, vamos a insertar dos veces la misma persona sobre la cual realizaremos las modificaciones: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Para actualizar (y fusionar datos), se utilizan los m\u00e9todos updateOne / updateMany dependiendo de cuantos documentos queremos que modifique. Ambos m\u00e9todos requieren 2 par\u00e1metros: el primero es la consulta para averiguar sobre qu\u00e9 documentos, y en el segundo par\u00e1metro, los campos a modificar utilizando los operadores de actualizaci\u00f3n: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $set : { nombre : \"Marina Medrano\" , salario : 123456 }}) Al realizar la modificaci\u00f3n, el shell nos devolver\u00e1 informaci\u00f3n sobre cuantos documentos ha encontrado, modificado y m\u00e1s informaci\u00f3n: { ack n owledged : true , i nserte dId : null , ma t chedCou nt : 1 , modi f iedCou nt : 1 , upser te dCou nt : 0 } Como hay m\u00e1s de una persona con el mismo nombre, al haber utilizado updateOne s\u00f3lo modificar\u00e1 el primer documento que ha encontrado. \u00a1Cuidado! En versiones antiguas de MongoDB, adem\u00e1s de utilizar los operadores de actualizaci\u00f3n, pod\u00edamos pasarle como par\u00e1metro un documento, de manera que MongoDB realizaba un reemplazo de los campos, es decir, si en el origen hab\u00eda 100 campos y en la operaci\u00f3n de modificaci\u00f3n s\u00f3lo pon\u00edamos 2, el resultado \u00fanicamente contendr\u00eda 2 campos. Es por ello, que ahora es obligatorio utilizar los operadores. Si cuando vamos a actualizar, en el criterio de selecci\u00f3n no encuentra el documento sobre el que hacer los cambios, no se realiza ninguna acci\u00f3n. Si quisi\u00e9ramos que en el caso de no encontrar nada insertase un nuevo documento, acci\u00f3n conocida como upsert ( update + insert ), hay que pasarle un tercer par\u00e1metro al m\u00e9todo con el objeto {upsert:true} . Si encuentra el documento, lo modificar\u00e1, pero si no, crear\u00e1 uno nuevo: db . people . updateOne ({ nombre : \"Andreu Medrano\" }, { name : \"Andreu Medrano\" , twitter : \"@andreumedrano\" }, { upsert : true }) Operadores de actualizaci\u00f3n \u00b6 MongoDB ofrece un conjunto de operadores para simplificar la modificaci\u00f3n de campos. El operador m\u00e1s utiilzar es el operador $set , el cual admite los campos que se van a modificar. Si el campo no existe, lo crear\u00e1. Por ejemplo, para modificar el salario har\u00edamos: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $se t :{ salario : 1000000 }} ) Mediante $inc podemos incrementar el valor de una variable: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $i n c :{ salario : 1000 }} ) Para eliminar un campo de un documento, usaremos el operador $unset . De este modo, para eliminar el campo twitter de una persona har\u00edamos: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $unset : { twitter : '' }}) Otros operadores que podemos utilizar son $mul , $min , $max y $currentDate . Podemos consultar todos los operadores disponibles en https://www.mongodb.com/docs/manual/reference/operator/update/ Autoevaluaci\u00f3n Tras realizar la siguiente operaci\u00f3n sobre una colecci\u00f3n vac\u00eda: db . people . updateOne ({ nombre : 'yo' }, { '$set' : { 'hobbies' : [ 'gaming' , 'sofing' ]}}, { upsert : true } ); \u00bfCu\u00e1l es el estado de la colecci\u00f3n? Finalmente, un caso particular de las actualizaciones es la posibilidad de renombrar un campo mediante el operador $rename : db . people . update ( { _id : 1 }, { $rename : { 'nickname' : 'alias' , 'cell' : 'movil' }}) Podemos consultar todas las opciones de configuraci\u00f3n de una actualizaci\u00f3n en https://www.mongodb.com/docs/manual/reference/method/db.collection.update/ . Control de la concurrencia \u00b6 Cuando se hace una actualizaci\u00f3n m\u00faltiple, MongoDB no realiza la operaci\u00f3n de manera at\u00f3mica (a no ser que utilicemos transacciones desde el driver), lo que provoca que se puedan producir pausas ( pause yielding ). Cada documento en s\u00ed es at\u00f3mico, por lo que ninguno se va a quedar a la mitad. MongoDB ofrece el m\u00e9todo findAndModify para encontrar y modificar un documento de manera at\u00f3mica, y as\u00ed evitar que, entre la b\u00fasqueda y la modificaci\u00f3n, el estado del documento se vea afectado. Adem\u00e1s, devuelve el documento modificado. Un caso de uso muy com\u00fan es para contadores y casos similares. db . people . findAndModify ({ query : { nombre : \"Marina Medrano\" }, update : { $inc : { salario : 100 , edad :- 30 }}, new : true }) Por defecto, el documento devuelto ser\u00e1 el resultado que ha encontrado con la consulta. Si queremos que nos devuelva el documento modificado con los cambios deseados, necesitamos utilizar el par\u00e1metro new a true . Si no lo indicamos o lo ponemos a false , tendremos el comportamiento por defecto. Actualizaciones sobre Arrays \u00b6 Para trabajar con arrays necesitamos nuevos operadores que nos permitan tanto introducir como eliminar elementos de una manera m\u00e1s sencilla que sustituir todos los elementos del array. Los operadores que podemos emplear para trabajar con arrays son: Operador Prop\u00f3sito $push A\u00f1ade uno o varios elementos $addToSet A\u00f1ade un elemento sin duplicados $pull Elimina un elemento $pullAll Elimina varios elementos $pop Elimina el primer o el \u00faltimo Preparando los ejemplos Para trabajar con los arrays, vamos a suponer que tenemos una colecci\u00f3n de enlaces donde vamos a almacenar un documento por cada site, con un atributo tags con etiquetas sobre el enlace en cuesti\u00f3n db . enlaces . insertOne ({ titulo : \"www.google.es\" , tags : [ \"mapas\" , \"videos\" ]}) De modo que tendr\u00edamos el siguiente documento: { _id : Objec t Id( \"633c60e8ac452ac9d7f9fe74\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' ] } A\u00f1adiendo elementos \u00b6 Si queremos a\u00f1adir uno o varios elementos, usaremos el operador $push . Cuando queremos a\u00f1adir varios elementos a la vez, mediante el operador $each le pasamos un array con los datos: db . enlaces . updateOne ({ titulo : \"www.google.es\" }, { $push : { tags : \"blog\" }} ) db . enlaces . updateOne ({ titulo : \"www.google.es\" }, { $push : { tags : { $each : [ \"calendario\" , \"email\" , \"mapas\" ]}}}) Al hacer estar modificaci\u00f3n, el resultado del documento ser\u00eda: { _id : Objec t Id( \"633c61b5ac452ac9d7f9fe75\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' , 'blog' , 'cale n dario' , 'email' , 'mapas' ] } Al utilizar $push no se tiene en cuenta lo que contiene el array, por tanto, si un elemento ya existe, se repetir\u00e1 y tendremos duplicados. Si queremos evitar los duplicados, usaremos $addToSet : db . enlaces . update ({ titulo : \"www.google.es\" }, { $addToSet : { tags : \"buscador\" }}) Si queremos a\u00f1adir m\u00e1s de un campo a la vez sin duplicados, debemos anidar el operador $each igual que hemos hecho antes: db . enlaces . update ({ titulo : \"www.google.es\" }, { $addToSet : { tags : { $each : [ \"drive\" , \"traductor\" ]}}}) En cambio, si queremos eliminar elementos de un array, usaremos el operador $pull : db . enlaces . update ({ titulo : \"www.google.es\" }, { $pull : { tags : \"traductor\" }}) Similar al caso anterior, con $pullAll , eliminaremos varios elementos de una sola vez: db . enlaces . update ({ titulo : \"www.google.es\" }, { $pullAll : { tags : [ \"calendario\" , \"email\" ]}}) Otra manera de eliminar elementos del array es mediante $pop , el cual elimina el primero ( -1 ) o el \u00faltimo ( 1 ) elemento del array: db . enlaces . update ({ titulo : \"www.google.es\" }, { $pop : { tags :- 1 }}) Operador posicional \u00b6 Por \u00faltimo, tenemos el operador posicional , el cual se expresa con el s\u00edmbolo $ y nos permite modificar el elemento que ocupa una determinada posici\u00f3n del array sin saber exactamente cual es esa posici\u00f3n. Supongamos que tenemos las calificaciones de los estudiantes (colecci\u00f3n students ) en un documento con una estructura similar a la siguiente: { \"_id\" : 1 , \"notas\" : [ 80 , 85 , 90 ] } y queremos cambiar la calificaci\u00f3n de 80 por 82. Mediante el operador posicional haremos: db . students . update ( { _id : 1 , notas : 80 }, { $set : { \"notas.$\" : 82 } } ) De manera similar, si queremos modificar parte de un documento el cual forma parte de un array, debemos usar la notaci\u00f3n punto tras el $ . Por ejemplo, supongamos que tenemos estas calificaciones de un determinado alumno, las cuales forman parte de un objeto dentro de un array: { \"_id\" : 4 , \"notas\" : [ { n o ta : 80 , media : 75 }, { n o ta : 85 , media : 90 }, { n o ta : 90 , media : 85 } ] } Podemos observar como tenemos cada calificaci\u00f3n como parte de un objeto dentro de un array. Si queremos cambiar el valor de media a 89 de la calificaci\u00f3n cuya nota es 85 , haremos: db . students . update ( { _id : 4 , \"notas.nota\" : 85 }, { $set : { \"notas.$.media\" : 89 } } ) Es decir, el $ referencia al documento que ha cumplido el filtro de b\u00fasqueda. M\u00e1s operadores posicionales Adem\u00e1s del operador posicional $, tenemos disponible el operador posicional $[] que indica que afecta a todos los elementos del array, y el operador posicional $[identificador] que identifica que elementos del array cumplen una condici\u00f3n para su filtrado. Podemos consultar toda la documentaci\u00f3n disponible sobre estos operadores en http://docs.mongodb.org/manual/reference/operator/update-array/ Borrando documentos \u00b6 Para borrar, usaremos los m\u00e9todo deleteOne o deleteMany , los cuales funcionan de manera similar a findOne y find . Si no pasamos ning\u00fan par\u00e1metro, deleteOne borrar\u00e1 el primer documento, o en el caso de deleteMany toda la colecci\u00f3n documento a documento. Si le pasamos un par\u00e1metro, \u00e9ste ser\u00e1 el criterio de selecci\u00f3n de documentos a eliminar. db . people . deleteOne ({ nombre : \"Marina Medrano\" }) Al eliminar un documento, no podemos olvidar que cualquier referencia al documento que exista en la base de datos seguir\u00e1 existiendo. Por este motivo, manualmente tambi\u00e9n hay que eliminar o modificar esas referencias. Si queremos borrar toda la colecci\u00f3n, es m\u00e1s eficiente usar el m\u00e9todo drop , ya que tambi\u00e9n elimina los \u00edndices. db . people . drop () Eliminar un campo Recordad que eliminar un determinado campo de un documento no se considera un operaci\u00f3n de borrado, sino una actualizaci\u00f3n mediante el operador $unset . Referencias \u00b6 Manual de MongoDB Cheatsheet oficial Comparaci\u00f3n entre SQL y MongoDB Cursos gratuitos de Mongo University Consultas solucionadas sobre la colecci\u00f3n sample_restaurants.restaurants en w3resource Actividades \u00b6 ( RA5075.2 / CE5.2b / 1p) Crea un cluster en MongoAtlas , carga los datos de ejemplo y adjunta capturas de pantalla de: Dashboard del cluster Bases de datos / colecciones creadas A continuaci\u00f3n, con\u00e9ctate mediante MongoDB Compass y adjunta una captura de pantalla tras conectar con el cl\u00faster. ( RA5075.1 / CE5.1d / 2p) Haciendo uso de mongosh , escribe los comandos necesarios para: Obtener las bases de datos creadas. Sobre la base de datos sample_training y la colecci\u00f3n zips averigua: Cuantos documentos hay en la ciudad de SAN DIEGO . Cuantos documentos tienen menos de 100 personas (campo pop ). Obt\u00e9n los estados de la ciudad de SAN DIEGO (Soluci\u00f3n: [ 'CA', 'TX' ] ). Cual es el c\u00f3digo postal de la ciudad de ALLEN que no tiene habitantes (s\u00f3lo recupera el zip , no nos interesa ning\u00fan otro campo, ni el _id ). Listado con los 5 c\u00f3digos postales m\u00e1s poblados (muestra los documentos completos). Cantidad de documentos que no tienen menos de 5.000 habitantes ni m\u00e1s de 1.000.000 (debes utilizar el operador $nor ). Cuantos documentos tienen m\u00e1s habitantes que su propio c\u00f3digo postal (campo zip ). Sobre la colecci\u00f3n posts averigua: Cuantos mensajes tienen las etiquetas restaurant o moon . Los comentarios que ha escrito el usuario Salena Olmos . Recupera los mensajes que en body contengan la palabra earth , y devuelve el t\u00edtulo, 3 comentarios y 5 etiquetas. ( RA5075.1 / CE5.1d / 2p) Escribe los comandos necesarios para realizar las siguientes operaciones sobre la colecci\u00f3n zips : Crea una entrada con los siguientes datos: { ci t y : 'ELX' , zip : ' 03206 ' , loc : { x : 38.265500 , y : -0.698459 }, pop : 230224 , s tate : 'Espa\u00f1a' } Crea una entrada con los datos del c\u00f3digo postal donde vives (si es el mismo c\u00f3digo postal, crea uno diferente). Modifica la poblaci\u00f3n de tu c\u00f3digo postal a 1.000.000 . Incrementa la poblaci\u00f3n de todas los documentos de Espa\u00f1a en 666 personas. A\u00f1ade un campo prov a ambos documentos con valor Alicante . Modifica los documentos de Espa\u00f1a y a\u00f1ade un atributo tags que contenga un array vac\u00edo. Modifica todos los documentos de la provincia de Alicante y a\u00f1ade al atributo tags el valor sun . Modifica el valor de sun de tu c\u00f3digo postal y sustit\u00fayelo por house . Renombra en los documentos de la provincia de Alicante el atributo prov por provincia Elimina las coordenadas del zip 03206 . Elimina tu entrada.","title":"S24.- MongoDB"},{"location":"sa/02mongo.html#conceptos","text":"Hay una serie de conceptos que conviene conocer antes de entrar en detalle: MongoDB tienen el mismo concepto de base de datos que un RDMS . Dentro de una instancia de MongoDB podemos tener 0 o m\u00e1s bases de datos, actuando cada una como un contenedor de alto nivel. Una base de datos tendr\u00e1 0 o m\u00e1s colecciones. Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un RDMS . MongoDB ofrece diferentes tipos de colecciones, desde las normales cuyo tama\u00f1o crece conforme lo hace el n\u00famero de documentos, como las colecciones capped , las cuales tienen un tama\u00f1o predefinido y que pueden contener una cierta cantidad de informaci\u00f3n que se sustituir\u00e1 por nueva cuando se llene. Las colecciones contienen 0 o m\u00e1s documentos , por lo que es similar a una fila o registro de un RDMS . Cada documento contiene 0 o m\u00e1s atributos, compuestos de parejas clave/valor . Cada uno de estos documentos no sigue ning\u00fan esquema, por lo que dos documentos de una misma colecci\u00f3n pueden contener todos los atributos diferentes entre s\u00ed. Elementos de MongoDB As\u00ed pues, tenemos que una base de datos va a contener varias colecciones, donde cada colecci\u00f3n contendr\u00e1 un conjunto de documentos: Modelo de MongoDB Adem\u00e1s, MongoDB soporta \u00edndices , igual que cualquier RDMS , para acelerar la b\u00fasqueda de datos. Al realizar cualquier consulta, se devuelve un cursor , con el cual podemos hacer cosas tales como contar, ordenar, limitar o saltar documentos.","title":"Conceptos"},{"location":"sa/02mongo.html#bson","text":"Mediante JavaScript podemos crear objetos que se representan con JSON. Internamente, MongoDB almacena los documentos mediante BSON ( Binary JSON ). Podemos consultar la especificaci\u00f3n en http://BSONSpec.org Especificaci\u00f3n BSON BSON representa un superset de JSON ya que: Permite almacenar datos en binario Incluye un conjunto de tipos de datos no incluidos en JSON, como pueden ser ObjectId , Date o BinData . Podemos consultar todos los tipos que soporta un objeto BSON en http://docs.mongodb.org/manual/reference/bson-types/ Un ejemplo de un objeto BSON podr\u00eda ser: var yo = { n ombre : \"Aitor\" , apellidos : \"Medrano\" , fna c : ne w Da te ( \"Oct 3, 1977\" ) , hobbies : [ \"programaci\u00f3n\" , \"videojuegos\" , \"baloncesto\" ], casado : true , hijos : 2 , co nta c t o : { t wi tter : \"@aitormedrano\" , email : \"a.medrano@edu.gva.es\" }, fe chaCreacio n : ne w Times ta mp() } Los documentos BSON tienen las siguientes restricciones: No pueden tener un tama\u00f1o superior a 16 MB. El atributo _id queda reservado para la clave primaria. Desde MongoDB 5.0 los nombres de los campos pueden empezar por $ y/o contener el . , aunque en la medida de lo posible, es recomendable evitar su uso. Adem\u00e1s MongoDB: No asegura que el orden de los campos se respete. Es sensible a los tipos de los datos Es sensible a las may\u00fasculas. Por lo que estos documentos son distintos: { \"edad\" : \"18\" } { \"edad\" : 18 } { \"Edad\" : 18 } Si queremos validar si un documento JSON es v\u00e1lido, podemos usar http://jsonlint.com/ . Hemos de tener en cuenta que s\u00f3lo valida JSON y no BSON, por tanto nos dar\u00e1 errores en los tipos de datos propios de BSON.","title":"BSON"},{"location":"sa/02mongo.html#puesta-en-marcha","text":"En la actualidad, MongoDB se comercializa mediante tres productos: Mongo Atlas , como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB. MongoDB Community Edition , versi\u00f3n gratuita para trabajar on-premise, con versiones para Windows, MacOS y Linux. MongoDB Enterprise Advanced , versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada.","title":"Puesta en marcha"},{"location":"sa/02mongo.html#instalacion","text":"Desde https://www.mongodb.com/try/download/community podemos descargar la versi\u00f3n Community acorde a nuestro sistema operativo. Independientemente de nuestro sistema operativo, por defecto, el demonio se lanza sobre el puerto 27017. Una vez instalado, si accedemos a http://localhost:27017 podremos ver que nos indica c\u00f3mo estamos intentando acceder mediante HTTP a MongoDB mediante el puerto reservado al driver nativo. Acceso al puerto 27017 En vez de instalarlo como un servicio en nuestra m\u00e1quina, a d\u00eda de hoy, es mucho m\u00e1s c\u00f3modo hacer uso de contenedores Docker o utilizar una soluci\u00f3n cloud .","title":"Instalaci\u00f3n"},{"location":"sa/02mongo.html#docker","text":"Para lanzar el contenedor de Docker al que llamaremos iadb-mongo mediante el siguiente comando: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo A continuaci\u00f3n vamos a descargar el conjunto de datos sampledata.archive.gz que ofrece MongoDB a modo de prueba, el cual vamos a emplear a lo largo de las diferentes sesiones. Volvemos al terminal de nuestro sistema y copiamos los datos desde nuestro sistema a la carpeta /tmp del contenedor: docker cp sampledata.archive.gz iabd-mongo:/tmp Posteriormente abrimos un terminal dentro de nuestro contenedor (o mediante Attach Shell en VSCode ): docker exec -it iabd-mongo bash Y finalmente, restauramos los datos mediante mongorestore : mongorestore --gzip --archive = /tmp/sampledata.archive.gz Una vez cargados, nos informar\u00e1 que se han restaurado 433281 documentos.","title":"Docker"},{"location":"sa/02mongo.html#mongo-atlas","text":"Y si preferimos una soluci\u00f3n cloud , disponemos de Mongo Atlas , que nos ofrece de manera gratuita un cluster compartido de servidores con 3 nodos y 512 MB para datos. Si queremos una soluci\u00f3n serverless o un servidor dedicado, ya tendremos que pasar por caja . Registro en Mongo Atlas Para comenzar a utilizar Mongo Atlas el primer paso es registrarnos y completar un cuestionario sobre nuestro uso. Tras ello: Creamos el cluster de despliegue. En nuestro caso, hemos realizado el despliegue en AWS en la regi\u00f3n de Paris ( eu-west-3 ) y dejado el nombre por defecto, Cluster 0 . Elecci\u00f3n del cluster Creamos un usuario/contrase\u00f1a para autenticar nuestra conexi\u00f3n. En nuestro caso, hemos creado el usuario iabd con la contrase\u00f1a iabdiabd (despu\u00e9s la podemos modificar desde el men\u00fa Security -> Database Access ): Configuraci\u00f3n del usuario En la misma pantalla, indicamos que permitimos las conexiones desde todas las direcciones IP (esta decisi\u00f3n s\u00f3lo la tomamos por comodidad, para poder conectarnos desde casa y el centro) mediante la IP 0.0.0.0 (despu\u00e9s podemos modificar la configuraci\u00f3n desde el men\u00fa Security -> Network Access ). Una vez realizados los dos pasos anteriores, comenzar\u00e1 la creaci\u00f3n del cluster, la cual puede tardar de 2 a 3 minutos. Dashboard del cluster A continuaci\u00f3n, cargaremos los datos de ejemplo. Para ello, en el men\u00fa con los tres puntos ( ... ), elegiremos la opci\u00f3n Load Sample Dataset . Una vez haya finalizado, podremos ver los datos cargados pulsando sobre el bot\u00f3n Browse Collections : Colecciones con los datos de prueba Conexi\u00f3n segura Mediante srv se establece una conexi\u00f3n segura Finalmente, para obtener la cadena de conexi\u00f3n, desde el dashboard del cluster con la opci\u00f3n Connect o desde la pesta\u00f1a Cmd Line Tools del propio cluster, podremos obtener la cadena de conexi\u00f3n , que tendr\u00e1 un formato similar a : mongodb+srv://usuario:password@host/basededatos A continuaci\u00f3n, vamos a conocer las diferentes herramientas que nos ofrece MongoDB para posteriormente estudiar todas las operaciones que podemos realizar.","title":"Mongo Atlas"},{"location":"sa/02mongo.html#mongosh","text":"Tras arrancar el demonio mongod (el cual se lanza autom\u00e1ticamente mediante Docker o con el cluster de Mongo Atlas ) llega el momento de acceder mediante el cliente mongosh (en versiones anteriores el comando utilizado era mongo ), el cual funciona igual que un shell, de modo que con la fecha hacia arriba visualizaremos el \u00faltimo comando. El cliente utiliza JavaScript como lenguaje de interacci\u00f3n con la base de datos. Si nos conectamos desde Docker, no necesitamos instalarlo. Primero nos conectamos al contenedor: docker exec -it iabd-mongo bash Al conectar con mongosh si no le indicamos nada se conectar\u00e1 por defecto a la base de datos test de localhost . Si queremos conectarnos a una base de datos concreta, por ejemplo a sample_training , la pasaremos como par\u00e1metro: root @ 3 ad17b675fb1 : /# mongosh sample_training Current Mongosh Log ID : 6316498 f30f8283fedcfabc2 Connecting to : mongodb : //127.0.0.1:27017/sample_training?directConnection=true&serverSelectionTimeoutMS=2000 Using MongoDB : 5.0.4 Using Mongosh : 1.1.2 Si queremos ver las bases de datos que existen ejecutaremos el comando show dbs : sample_training> show dbs ; admin 41 kB config 73 .7 kB local 73 .7 kB sample_airbnb 55 .1 MB sample_analytics 9 .9 MB sample_geospatial 999 kB sample_mflix 48 .5 MB sample_restaurants 6 .2 MB sample_supplies 991 kB sample_training 43 .4 MB sample_weatherdata 2 .49 MB Si nos quisi\u00e9ramos conectar a nuestro cluster de Mongo Atlas utilizaremos la cadena de conexi\u00f3n tras el comando mongosh : root@3ad17b675fb1:/# mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/test Current Mongosh Log ID: 63164ac26030844c1576f8b4 Connecting to: mongodb+srv://<credentials>@cluster0.dfaz5er.mongodb.net/test Using MongoDB: 5 .0.11 Using Mongosh: 1 .1.2 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ Atlas atlas-4wikkb-shard-0 [ primary ] test> Uso externo Si no queremos tener que conectarnos al contenedor o vamos a trabajar con un servidor remoto, podemos instalar \u00fanicamente el shell desde https://www.mongodb.com/try/download/shell","title":"mongosh"},{"location":"sa/02mongo.html#mongodb-database-tools","text":"Adem\u00e1s del propio servidor de MongoDB y el cliente para conectarse a \u00e9l, MongoDB ofrece un conjunto de herramientas para interactuar con las bases de datos, permitiendo crear y restaurar copias de seguridad. Si estamos interesados en introducir o exportar una colecci\u00f3n de datos mediante JSON, podemos emplear los comandos mongoimport y mongoexport : mongoimport -d nombreBaseDatos -c coleccion \u2013-file nombreFichero.json mongoexport -d nombreBaseDatos -c coleccion nombreFichero.json Estas herramientas interact\u00faan con datos JSON y no sobre toda la base de datos. Un caso particular y muy com\u00fan es importar datos que se encuentran en formato CSV/TSV. Para ello, emplearemos el par\u00e1metro --type csv : mongoimport --type tsv -d test -c poblacion --headerline --drop poblacionEspanya2013.tsv En vez de realizar un export , es m\u00e1s conveniente realizar un backup en binario mediante mongodump , el cual genera ficheros BSON. Estos archivos posteriormente se restauran mediante mongorestore : mongodump -d nombreBaseDatos nombreFichero.bson mongorestore -d nombreBaseDatos nombreFichero.bson Autoevaluaci\u00f3n Intenta exportar los datos de la base de datos sample_training desde MongoAtlas. Veras que ha creado una carpeta que contiene dos archivos \u00bfCu\u00e1les son? \u00bfQu\u00e9 contiene cada uno de ellos y cual es su formato? Si necesitamos transformar un fichero BSON a JSON (de binario a texto), tenemos el comando bsondump : bsondump file.bson > file.json Info M\u00e1s informaci\u00f3n sobre copias de seguridad en https://www.mongodb.com/docs/manual/core/backups/ . Para poder trabajar con MongoDB desde cualquier aplicaci\u00f3n necesitamos un driver. MongoDB ofrece drivers oficiales para casi todos los lenguajes de programaci\u00f3n actuales. En la sesi\u00f3n 30 de 'MongoDB y Python' trabajaremos con PyMongo . Monitorizaci\u00f3n Tanto mongostat como mongotop permiten visualizar el estado del servidor MongoDB, as\u00ed como algunas estad\u00edsticas sobre su rendimiento. Si trabajamos con MongoAtlas estas herramientas est\u00e1n integradas en las diferentes herramientas de monitorizaci\u00f3n de la plataforma. En versiones anteriores, una herramienta de terceros bastante utilizada era RoboMongo / Robo3T / Studio3T el cual extiende el shell y ofrece un IDE m\u00e1s amigable. A d\u00edas de hoy, MongoDB tiene su propio IDE conocido como Mongo Compass .","title":"MongoDB Database Tools"},{"location":"sa/02mongo.html#mongo-compass","text":"En el curso nos vamos a centrar en el uso del shell y la conectividad de MongoDB mediante Python, pero no est\u00e1 de m\u00e1s conocer las herramientas visuales que facilitan el trabajo con MongoDB en el d\u00eda a d\u00eda. Una de ellas es Mongo Compass , que facilita la exploraci\u00f3n y manipulaci\u00f3n de los datos. De una manera flexible e intuitiva, Compass ofrece visualizaciones detalladas de los esquemas, m\u00e9tricas de rendimiento en tiempo real as\u00ed como herramientas para la creaci\u00f3n de consultas. Existen tres versiones de Compass, una completa con todas las caracter\u00edsticas, una de s\u00f3lo lectura sin posibilidad de insertar, modificar o eliminar datos (perfecta para anal\u00edtica de datos) y una \u00faltima versi\u00f3n isolated que solo permite la conexi\u00f3n a una instancia local. Una vez descargada e instalada la versi\u00f3n que nos interesa, tras crear la conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), veremos en el men\u00fa de la izquierda un resumen del cluster, as\u00ed como las consultas que vayamos almacenando y las diferentes bases de datos almacenadas: GUI de Mongo Compass Si seleccionamos una base de datos concreta, y de ella, una colecci\u00f3n en el men\u00fa de la izquierda, en el panel central tendremos una visualizaci\u00f3n de los datos contenidos, as\u00ed como opciones para ver su esquema, realizar consultas agregadas, editar los \u00edndices, etc... Adem\u00e1s, podremos realizar consultas sobre los datos: Opciones desde una colecci\u00f3n mongosh en Compass Si te fijas, en la barra inferior podemos desplegar un panel para interactuar mediante comandos como lo har\u00edamos desde mongosh .","title":"Mongo Compass"},{"location":"sa/02mongo.html#mongodb-for-vscode","text":"Tambi\u00e9n podemos utilizar la extensi\u00f3n que lleva VSCode para trabajar con MongoDB . Tras su instalaci\u00f3n creamos una conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), y una vez conectados, podremos recorrer las colecciones con los datos as\u00ed como utilizar un playground para interactuar de manera similar al shell: Uso de la extensi\u00f3n de VSCode","title":"MongoDB for VSCode"},{"location":"sa/02mongo.html#hola-mongodb","text":"Pues una vez que ya nos hemos conectado a MongoDB mediante mongosh , vamos a empezar a interactuar con los datos. En cualquier momento podemos cambiar la base de datos activa mediante use nombreBaseDatos . Si la base de datos no existiese, MongoDB crear\u00e1 dicha base de datos. Esto es una verdad a medias, ya que la base de datos realmente se crea al insertar datos dentro de alguna colecci\u00f3n. As\u00ed pues, vamos a crear nuestra base de datos iabd : use iabd Una vez creada, podemos crear nuestra primera colecci\u00f3n, que llamaremos people , e insertaremos un persona con nuestros datos personales mediante el m\u00e9todo insertOne , al que le pasamos un objeto JSON: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Tras ejecutar el comando, veremos que nos devuelve un objeto JSON con su ACK y el identificador del documento insertado: { ack n owledged : true , i nserte dId : Objec t Id( \"6316fc938cc2bc168bfed066\" ) } Una vez insertada, s\u00f3lo nos queda realizar una consulta para recuperar los datos y comprobar que todo funciona correctamente mediante el m\u00e9todo findOne : db . people . findOne () Lo que nos dar\u00e1 como resultado un objeto JSON que contiene un atributo _id con el mismo identificador mostrado anteriormente, adem\u00e1s de los que le a\u00f1adimos al insertar la persona: { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' } Como podemos observar, todas las instrucciones van a seguir el patr\u00f3n de db.nombreColeccion.operacion() .","title":"Hola MongoDB"},{"location":"sa/02mongo.html#trabajando-con-el-shell","text":"Antes de entrar en detalles en las instrucciones necesarias para realizar las operaciones CRUD, veamos algunos comandos que nos ser\u00e1n muy \u00fatiles al interactuar con el shell: Comando Funci\u00f3n show dbs Muestra el nombre de las bases de datos show collections Muestra el nombre de las colecciones db Muestra el nombre de la base de datos que estamos utilizando db.dropDatabase() Elimina la base de datos actual db.help() Muestra los comandos disponibles db.version() Muestra la versi\u00f3n actual del servidor En el resto de la sesi\u00f3n vamos a hacer un uso intenso del shell de MongoDB. Por ejemplo, si nos basamos en el objeto definido en el apartado de BSON, podemos ejecutar las siguientes instrucciones: > db . people . insertOne ( yo ) // (1) < { acknowledged : true , insertedId : ObjectId ( \"631704a042aae0893122f2d6\" ) } > db . people . find () // (2) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) } ] > yo . profesion = \"Profesor\" < Profesor > db . people . insertOne ( yo ) // (3) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) }, { _id : ObjectId ( \"6317056d42aae0893122f2d7\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662453101 , i : 1 }), profesion : 'Profesor' } ] > db . people . countDocuments () // (4) < 3 Si queremos insertar un documento en una colecci\u00f3n, hemos de utilizar el m\u00e9todo insertOne pas\u00e1ndole como par\u00e1metro el documento que queremos insertar, ya sea a partir de una variable o el propio documento en s\u00ed. find recupera todos los documentos de la colecci\u00f3n Modificamos nuestro documento y los volvemos a insertar. Realmente va a crear un nuevo documento, y no se va a quejar de que ya exista, porque nuestro documento no contiene ning\u00fan atributo identificador, por lo que considera que se trata de una nueva persona. Obtenemos la cantidad de documentos de la colecci\u00f3n mediante countDocuments . Con este ejemplo, hemos podido observar como los documentos de una misma colecci\u00f3n no tienen por qu\u00e9 tener el mismo esquema, ni hemos necesitado definirlo expl\u00edcitamente antes de insertar datos. As\u00ed pues, el esquema se ir\u00e1 generando y actualizando conforme se inserten documentos. M\u00e1s adelante veremos que podemos definir un esquema para validar que los datos que insertamos cumplan restricciones de tipos de datos o elementos que obligatoriamente deben estar rellenados.","title":"Trabajando con el shell"},{"location":"sa/02mongo.html#empleando-javascript","text":"Ya hemos comentado que el shell utiliza JavaScript como lenguaje de interacci\u00f3n, por lo que podemos almacenar los comandos en un script externo y ejecutarlo mediante load() : load ( \"scripts/misDatos.js\" ); load ( \"/data/db/scripts/misDatos.js\" ); Si hacemos una referencia relativa, lo hace respecto a la ruta desde la cual se ejecuta el shell mongosh . Otra manera de lanzar un script es hacerlo desde la l\u00ednea de comandos, pas\u00e1ndole como segundo par\u00e1metro el script a ejecutar: mongosh iabd misDatos.js Si el c\u00f3digo a ejecutar no necesita almacenarse en un script externo, el propio shell permite introducir instrucciones en varias l\u00edneas: > for ( var i = 0 ; i < 10 ; i ++ ) { ... db . espias . insertOne ({ \"nombre\" : \"James Bond \" + i , \"agente\" : \"00\" + i }); ... } < { acknowledged : true , insertedId : ObjectId ( \"63171d2142aae0893122f2e1\" ) } > db . espias . find () < [ { _id : ObjectId ( \"63171d2142aae0893122f2d8\" ), nombre : 'James Bond 0' , agente : '000' }, { _id : ObjectId ( \"63171d2142aae0893122f2d9\" ), nombre : 'James Bond 1' , agente : '001' }, ... { _id : ObjectId ( \"63171d2142aae0893122f2e1\" ), nombre : 'James Bond 9' , agente : '009' } ]","title":"Empleando JavaScript"},{"location":"sa/02mongo.html#objectid","text":"En MongoDB , el atributo _id es \u00fanico dentro de la colecci\u00f3n, y hace la funci\u00f3n de clave primaria. Se le asocia un ObjectId , el cual es un tipo BSON de 12 bytes que se crea mediante: el timestamp actual (4 bytes) un valor aleatorio y \u00fanico por m\u00e1quina y proceso (5 bytes) un contador inicializado a n\u00famero aleatorio (3 bytes). Este objeto lo crea el driver y no MongoDB , por lo cual no deberemos considerar que siguen un orden concreto, ya que clientes diferentes pueden tener timestamps desincronizados. Lo que s\u00ed que podemos obtener a partir del ObjectId es la fecha de creaci\u00f3n del documento, mediante el m\u00e9todo getTimestamp() del atributo _id . Obteniendo la fecha de creaci\u00f3n de un documento > db.people.findOne () ._id < ObjectId ( \"6316fc938cc2bc168bfed066\" ) > db.people.findOne () ._id.getTimestamp () < ISODate ( \"2022-09-06T07:53:55.000Z\" ) Este identificador es global, \u00fanico e inmutable. Esto es, no habr\u00e1 dos repetidos y una vez un documento tiene un _id , \u00e9ste no se puede modificar. Si en la definici\u00f3n del objeto a insertar no ponemos el atributo identificador, MongoDB crear\u00e1 uno de manera autom\u00e1tica. Si lo ponemos nosotros de manera expl\u00edcita, MongoDB no a\u00f1adir\u00e1 ning\u00fan ObjectId . Eso s\u00ed, debemos asegurarnos que sea \u00fanico (podemos usar n\u00fameros, cadenas, etc\u2026\u200b). Por lo tanto, podemos asignar un identificador al insertar: > db . people . insert ({ _id : 4 , nombre : \"Marina\" , edad : 14 }) < { acknowledged : true , insertedIds : { '0' : 4 } } Tipos de datos Cuidado con los tipos, ya que no es lo mismo insertar un atributo con edad:14 (se considera el campo como entero) que con edad:\"14\" , ya que considera el campo como texto. O tambi\u00e9n, si queremos podemos hacer que el _id de un documento sea un documento en s\u00ed, y no un entero, para ello, al insertarlo, podemos asignarle un objeto JSON al atributo identificador: > db . people . insertOne ({ _id : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' }, ciudad : 'Elx' }) < { acknowledged : true , insertedId : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' } }","title":"ObjectId"},{"location":"sa/02mongo.html#recuperando-datos","text":"Para recuperar los datos de una colecci\u00f3n o un documento en concreto usaremos el m\u00e9todo find() : > db . people . find () < { _id : ObjectId ( \"6316fc1597eb703de2add36e\" ), nombre : 'Aitor' , edad : 45 , profesion : 'Profesor' } { _id : ObjectId ( \"6317048697eb703de2add36f\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : 1977 - 10 - 02 T23 : 00 : 00 .000 Z , hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452870 , i : 3 }) } El m\u00e9todo find() sobre una colecci\u00f3n devuelve un cursor a los datos obtenidos, el cual se queda abierto con el servidor y que se cierra autom\u00e1ticamente a los 30 minutos de inactividad o al finalizar su recorrido. Si hay muchos resultados, la consola nos mostrar\u00e1 un subconjunto de los datos (20). Si queremos seguir obteniendo resultados, solo tenemos que introducir it , para que contin\u00fae iterando el cursor. En cambio, si s\u00f3lo queremos recuperar un documento hemos de utilizar findOne() : > db.people. f i n dO ne () > { _id : Objec t Id( \"6316fc1597eb703de2add36e\" ) , n ombre : 'Ai t or' , edad : 45 , pro fes io n : 'Pro fes or' } Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de 10.000 documentos sobre los viajes realizados por los usuarios de una empresa de alquiler de bicicletas, los cuales han sido extra\u00eddos de https://ride.citibikenyc.com/system-data . Esta colecci\u00f3n ( trips ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de viaje ser\u00eda: > use sample_training < 'switched to db sample_training' > db . trips . findOne () < { _id : ObjectId ( \"572bb8222b288919b68abf5b\" ), tripduration : 889 , 'start station id' : 268 , 'start station name' : 'Howard St & Centre St' , 'end station id' : 3002 , 'end station name' : 'South End Ave & Liberty St' , bikeid : 22794 , usertype : 'Subscriber' , 'birth year' : 1961 , 'start station location' : { type : 'Point' , coordinates : [ - 73.99973337 , 40.71910537 ] }, 'end station location' : { type : 'Point' , coordinates : [ - 74.015756 , 40.711512 ] }, 'start time' : 2016 - 01 - 01 T00 : 01 : 06 .000 Z , 'stop time' : 2016 - 01 - 01 T00 : 15 : 56.000 Z }","title":"Recuperando datos"},{"location":"sa/02mongo.html#criterios-en-consultas","text":"Al hacer una consulta, si queremos obtener datos mediante m\u00e1s de un criterio, en el primer par\u00e1metro del find podemos pasar un objeto JSON con los campos a cumplir (condici\u00f3n Y). Consulta Resultado db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) { _id : Objec t Id( \"572bb8222b288919b68ad197\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 23724 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 45.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 48.000 Z } { _id : Objec t Id( \"572bb8222b288919b68ad191\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 17075 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 38.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 42.000 Z } Consejo de Rendimiento Las consultas disyuntivas, es decir, con varios criterios u operador $and , deben filtrar el conjunto m\u00e1s peque\u00f1o cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar documentos que cumplen los criterios A, B y C. Digamos que el criterio A lo cumplen 40.000 documentos, el B lo hacen 9.000 y el C s\u00f3lo 200. Si filtramos A, luego B, y finalmente C, el conjunto que trabaja cada criterio es muy grande. Restringiendo consultas AND En cambio, si hacemos una consulta que primero empiece por el criterio m\u00e1s restrictivo, el resultado con lo que se intersecciona el siguiente criterio es menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas AND de menor a mayor MongoDB tambi\u00e9n ofrece operadores l\u00f3gicos para los campos num\u00e9ricos: Comparador Operador menor que ( < ) $lt menor o igual que ( \u2264 ) $lte mayor que ( > ) $gt mayor o igual que ( \u2265 ) $gte Estos operadores se pueden utilizar de forma simult\u00e1nea sobre un mismo campo o sobre diferentes campos, sobre campos anidados o que forman parte de un array, y se colocan como un nuevo documento en el valor del campo a filtrar, compuesto del operador y del valor a comparar mediante la siguiente sintaxis: db . < coleccion > . find ({ < campo >: { < operador >: < valor > } }) Por ejemplo, para recuperar los viajes que han durado menos de 5 minutos o comprendidos entre 3 y 5 minutos (el campo almacena el tiempo en segundos), podemos hacer: db . trips . find ({ \"tripduration\" : { $lt : 300 } }) db . trips . find ({ \"tripduration\" : { $gt : 180 , $lte : 300 } }) Para los campos de texto, adem\u00e1s de la comparaci\u00f3n directa, podemos usar el operador $ne para obtener los documentos cuyo campos no tienen un determinado valor ( not equal ). As\u00ed pues, podemos usarlo para averiguar todos los trayectos realizados por usuarios que no son subscriptores ( Subscriber ): db . trips . find ({ \"usertype\" : { $ne : \"Subscriber\" } }) Por supuesto, podemos tener diferentes operadores en campos distintos. Por ejemplo, si queremos ver los viajes de menos de un minuto y medio realizado por usuarios que no son subscriptores har\u00edamos: db . trips . find ({ \"tripduration\" : { $lt : 90 }, \"usertype\" : { $ne : \"Subscriber\" } }) Case sensitive Las comparaciones de cadenas se realizan siguiendo el orden UTF8, similar a ASCII, con lo cual no es lo mismo buscar un rango entre may\u00fasculas que min\u00fasculas. Con cierto parecido a la condici\u00f3n de valor no nulo de las BBDD relacionales y teniendo en cuenta que la libertad de esquema puede provocar que un documento tenga unos campos determinados y otro no lo tenga, podemos utilizar el operador $exists si queremos averiguar si un campo existe (y por tanto tiene alg\u00fan valor). db . people . find ({ \"edad\" : { $exists : true }}) Polimorfismo Mucho cuidado al usar polimorfismo y almacenar en un mismo campo un entero y una cadena, ya que al hacer comparaciones para recuperar datos, no vamos a poder mezclar cadenas con valores num\u00e9ricos. Se considera un antipatr\u00f3n el mezclar tipos de datos en un campo. Pese a que ciertos operadores contengan su correspondiente operador negado, MongoDB ofrece el operador $not . \u00c9ste puede utilizarse conjuntamente con otros operadores para negar el resultado de los documentos obtenidos. Por ejemplo, si queremos obtener todas las personas cuya edad no sea m\u00faltiplo de 5, podr\u00edamos hacerlo as\u00ed: db . people . find ({ edad : { $not : { $mod : [ 5 , 0 ]}}})","title":"Criterios en consultas"},{"location":"sa/02mongo.html#proyeccion-de-campos","text":"Las consultas realizadas hasta ahora devuelven los documentos completos. Si queremos que devuelva un campo o varios campos en concreto, hemos de pasar un segundo par\u00e1metro de tipo JSON con aquellos campos que deseamos mostrar con el valor true o 1 . Destacar que si no se indica nada, por defecto siempre mostrar\u00e1 el campo _id > db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }, { tripduration : 1 }) < { _id : ObjectId ( \"572bb8222b288919b68ad191\" ), tripduration : 1143 } Por lo tanto, si queremos que no se muestre el _id , lo podremos a false o 0 : > db. tr ips. f i n d( { 's tart s tat io n id' : 405 , 'e n d s tat io n id' : 146 }, { tr ipdura t io n : 1 , _id : 0 } ) < { tr ipdura t io n : 1143 } No mezcles Al hacer una proyecci\u00f3n, no podemos mezclar campos que se vean ( 1 ) con los que no ( 0 ). Es decir, hemos de hacer algo similar a: db . < coleccion > . find ({ < consulta > }, { < campo1 >: 1 , < campo2 >: 1 }) db . < coleccion > . find ({ < consulta > }, { < campo1 >: 0 , < campo2 >: 0 }) As\u00ed pues, s\u00f3lo se mezclar\u00e1 la visibilidad de los campos cuando queramos ocultar el _id .","title":"Proyecci\u00f3n de campos"},{"location":"sa/02mongo.html#condiciones-compuestas-con-y-o","text":"Para usar la conjunci\u00f3n o la disyunci\u00f3n, tenemos los operadores $and y $or . Son operadores prefijo, de modo que se ponen antes de las subconsultas que se van a evaluar. Estos operadores trabajan con arrays, donde cada uno de los elementos es un documento con la condici\u00f3n a evaluar, de modo que se realiza la uni\u00f3n entre estas condiciones, aplicando la l\u00f3gica asociada a AND y a OR. db . trips . find ({ $or : [{ 'start station id' : 405 }, { 'end station id' : 146 }] }) db . trips . find ({ $or : [{ \"tripduration\" : { $lte : 70 }}, { \"tripduration\" : { $gte : 3600 }}] }) Realmente el operador $and no se suele usar porque podemos anidar en la consulta dos criterios, al poner uno dentro del otro. As\u00ed pues, estas dos consultas hacen lo mismo: db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) db . trips . find ({ $and : [ { 'start station id' : 405 }, { 'end station id' : 146 } ] }) Consejo de Rendimiento Las consultas conjuntivas, es decir, con varios criterios excluyentes u operador $or , deben filtrar el conjunto m\u00e1s grande cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar los mismos documentos que cumplen los criterios A (40.000 documentos), B (9.000 documentos) y C (200 documentos). Si filtramos C, luego B, y finalmente A, el conjunto de documentos que tiene que comprobar MongoDB es muy grande. Restringiendo consultas OR de menor a mayor En cambio, si hacemos una consulta que primero empiece por el criterio menos restrictivo, el conjunto de documentos sobre el cual va a tener que comprobar siguientes criterios va a ser menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas OR de mayor a menor Tambi\u00e9n podemos utilizar el operado $nor , que no es m\u00e1s que la negaci\u00f3n de $or y que obtendr\u00e1 aquellos documentos que no cumplan ninguna de las condiciones. Autoevaluaci\u00f3n Que obtendr\u00edamos al ejecutar la siguiente consulta: db . trips . find ({ \"tripduration\" : { $lte : 65 }, $nor : [ { usertype : \"Customer\" }, { \"birth year\" : 1989 } ] }) Finalmente, si queremos indicar mediante un array los diferentes valores que puede cumplir un campo, podemos utilizar el operador $in : db . trips . find ({ \"birth year\" : { $in : [ 1977 , 1980 ]} }) Por supuesto, tambi\u00e9n existe su negaci\u00f3n mediante $nin .","title":"Condiciones compuestas con Y / O"},{"location":"sa/02mongo.html#condiciones-sobre-objetos-anidados","text":"Si queremos acceder a campos de subdocumentos, siguiendo la sintaxis de JSON, se utiliza la notaci\u00f3n punto. Esta notaci\u00f3n permite acceder al campo de un documento anidado, da igual el nivel en el que est\u00e9 y su orden respecto al resto de campos. Preparando los ejemplos Para los siguientes ejemplos sobre documentos anidados y arrays, vamos a utilizar una colecci\u00f3n de 500 documentos sobre mensajes de un blog. Esta colecci\u00f3n ( posts ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de mensaje ser\u00eda: > use sample_training 'switched to db sample_training' > db . posts . findOne () < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc3f9\" ), body : 'Amendment I\\n<p>Congress shall make ....\"\\n<p>\\n' , permalink : 'aRjNnLZkJkTyspAIoRGe' , author : 'machine' , title : 'Bill of Rights' , tags : 'santa' , 'xylophone' , 'math' , 'dream' , 'action' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'HvizfYVx@pKvLaagH.com' , author : 'Santiago Dollins' }, { body : 'Lorem ipsum dolor sit...' , email : 'WpOUCpdD@hccdxJvT.com' , author : 'Jaclyn Morado' }, { body : 'Lorem ipsum dolor sit amet...' , email : 'OgDzHfFN@cWsDtCtx.com' , author : 'Houston Valenti' }], date : 2012 - 11 - 20 T05 : 05 : 15.231 Z } Para acceder al autor de los comentarios de un mensaje usar\u00edamos la propiedad comments.author . Por ejemplo, para averiguar los mensajes titulados 'Bill of Rights' y que tienen alg\u00fan comentario creado por Santiago Dollins har\u00edamos: db . posts . find ({ title : 'Bill of Rights' , \"comments.author\" : 'Santiago Dollins' })","title":"Condiciones sobre objetos anidados"},{"location":"sa/02mongo.html#consultas-sobre-arrays","text":"Si trabajamos con arrays, vamos a poder consultar el contenido de una posici\u00f3n del mismo tal como si fuera un campo normal, siempre que sea un campo de primer nivel, es decir, no sea un documento embebido dentro de un array. Si queremos filtrar teniendo en cuenta el n\u00famero de ocurrencias del array, podemos utilizar: $all para filtrar ocurrencias que tienen todos los valores del array, es decir, los valores pasados a la consulta ser\u00e1n un subconjunto del resultado. Puede que devuelva los mismos, o un array con m\u00e1s campos (el orden no importa) $in , igual que SQL, para obtener las ocurrencias que cumple con alguno de los valores pasados (similar a usar $or sobre un conjunto de valores de un mismo campo). Si queremos su negaci\u00f3n, usaremos $nin , para obtener los documentos que no cumplen ninguno de los valores. Por ejemplo, si queremos obtener los mensajes que contenga las etiquetas dream y action tendr\u00edamos: db . posts . find ( { tags : { $all : [ \"dream\" , \"action\" ]}} ) En cambio, si queremos los mensajes que contengan alguna de esas etiquetas har\u00edamos: db . posts . find ( { tags : { $in : [ \"dream\" , \"action\" ]}} ) Si el array contiene documentos y queremos filtrar la consulta sobre los campos de los documentos del array, tenemos que utilizar $elemMatch , de manera que obtengamos aquellos que al menos encuentre un elemento que cumpla el criterio. As\u00ed pues, si queremos recuperar los mensajes que tienen un comentario cuyo autor sea Santiago Dollins har\u00edamos: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Criterio con notaci\u00f3n punto En el ejemplo anterior, si s\u00f3lo hubi\u00e9ramos tenido un campo para el filtrado, podr\u00edamos haber utilizado la notaci\u00f3n punto comments.author . Si s\u00f3lo queremos los comentarios escritos por un determinado autor, en vez de en el filtrado, hemos de indicarlo en la proyecci\u00f3n: db . posts . find ( {}, { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Si lo que nos interesa es la cantidad de elementos que contiene un array, emplearemos el operador $size . Por ejemplo, para obtener los mensajes que tienen 10 etiquetas har\u00edamos: db . posts . find ( { tags : { $size : 10 }} ) Finalmente, a la hora de proyectar los datos, si no estamos interesados en todos los valores de un campo que es un array, podemos restringir el resultado mediante el operador $slice . As\u00ed pues, si quisi\u00e9ramos obtener las mensajes titulados US Constitution y que de esos mensajes, mostrar s\u00f3lo tres etiquetas y dos comentarios, har\u00edamos: > db . posts . find ( { title : \"US Constitution\" }, { comments : { $slice : 2 }, tags : { $slice : 3 }} ) < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc416\" ), body : 'We the People ...' , permalink : 'NhWDUNColpvxFjovsgqU' , author : 'machine' , title : 'US Constitution' , tags : [ 'engineer' , 'granddaughter' , 'sundial' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'ftRlVMZN@auLhwhlj.com' , author : 'Leonida Lafond' }, { body : 'Lorem ipsum dolor sit...' , email : 'dsoLAdFS@VGBBuDVs.com' , author : 'Nobuko Linzey' } ], date : 2012 - 11 - 20 T05 : 05 : 15.276 Z }","title":"Consultas sobre arrays"},{"location":"sa/02mongo.html#conjunto-de-valores","text":"Igual que en SQL, a partir de un colecci\u00f3n, si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct > db . trips . distinct ( 'usertype' ) < [ 'Customer' , 'Subscriber' ] Si queremos filtrar los datos sobre los que se obtienen los valores, le pasaremos un segundo par\u00e1metro con el criterio a aplicar: > db . trips . distinct ( 'usertype' , { \"birth year\" : { $gt : 1990 } } ) < [ 'Subscriber' ]","title":"Conjunto de valores"},{"location":"sa/02mongo.html#cursores","text":"Al hacer una consulta en el shell , se devuelve un cursor. Este cursor lo podemos guardar en un variable, y partir de ah\u00ed trabajar con \u00e9l como har\u00edamos mediante cualquier lenguaje de programaci\u00f3n. Si cur es la variable que referencia al cursor, podremos utilizar los siguientes m\u00e9todos: M\u00e9todo Uso Lugar de ejecuci\u00f3n cur.hasNext() true / false para saber si quedan elementos Cliente cur.next() Pasa al siguiente documento Cliente cur.limit(cantidad) Restringe el n\u00famero de resultados a cantidad Servidor cur.sort({campo:1}) Ordena los datos por campo: 1 ascendente o -1 o descendente Servidor cur.skip(cantidad) Permite saltar cantidad elementos con el cursor Servidor La consulta no se ejecuta hasta que el cursor comprueba o pasa al siguiente documento ( next / hasNext ), por ello que tanto limit como sort (ambos modifican el cursor) s\u00f3lo se pueden realizar antes de recorrer cualquier elemento del cursor. Como tras realizar una consulta con find , realmente se devuelve un cursor, un uso muy habitual es encadenar una operaci\u00f3n de find con sort y/o limit para ordenar el resultado por uno o m\u00e1s campos y posteriormente limitar el n\u00famero de documentos a devolver. As\u00ed pues, si quisi\u00e9ramos obtener los tres viajes que m\u00e1s han durado, podr\u00edamos hacerlo as\u00ed: db . trips . find (). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Tambi\u00e9n podemos filtrar previamente a ordenar y limitar: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Finalmente, podemos paginar utilizando el m\u00e9todo skip , para mostrar viajes de 10 en 10 y a partir de la tercera p\u00e1gina, podr\u00edamos hacer algo as\u00ed: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 10 ). skip ( 20 ) Autoevaluaci\u00f3n A partir de la colecci\u00f3n trips , escribe un consulta que recupere los viajes realizados por subscriptores ordenados descendentemente por su duraci\u00f3n y que obtenga los documentos de 15 al 20.","title":"Cursores"},{"location":"sa/02mongo.html#contando-documentos","text":"Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo countDocuments . Por ejemplo: > db . trips . countDocuments ({ \"birth year\" : 1977 }) < 186 > db . trips . countDocuments ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 116 count Desde la versi\u00f3n 4.0, los m\u00e9todos count a nivel de colecci\u00f3n y de cursor est\u00e1n caducados ( deprecated ), y no se recomienda su utilizaci\u00f3n. A\u00fan as\u00ed, es muy com\u00fan utilizarlo como m\u00e9todo de un cursor: db . trips . find ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}). count () Cuando tenemos much\u00edsimos datos, si no necesitamos exactitud pero queremos un valor estimado el cual tarde menos en conseguirse (utiliza los metadatos de las colecciones), podemos usar estimatedDocumentCount > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 }) < 10.000 > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 10.000","title":"Contando Documentos"},{"location":"sa/02mongo.html#modificando-documentos","text":"Preparando un persona Para este apartado, vamos a insertar dos veces la misma persona sobre la cual realizaremos las modificaciones: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Para actualizar (y fusionar datos), se utilizan los m\u00e9todos updateOne / updateMany dependiendo de cuantos documentos queremos que modifique. Ambos m\u00e9todos requieren 2 par\u00e1metros: el primero es la consulta para averiguar sobre qu\u00e9 documentos, y en el segundo par\u00e1metro, los campos a modificar utilizando los operadores de actualizaci\u00f3n: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $set : { nombre : \"Marina Medrano\" , salario : 123456 }}) Al realizar la modificaci\u00f3n, el shell nos devolver\u00e1 informaci\u00f3n sobre cuantos documentos ha encontrado, modificado y m\u00e1s informaci\u00f3n: { ack n owledged : true , i nserte dId : null , ma t chedCou nt : 1 , modi f iedCou nt : 1 , upser te dCou nt : 0 } Como hay m\u00e1s de una persona con el mismo nombre, al haber utilizado updateOne s\u00f3lo modificar\u00e1 el primer documento que ha encontrado. \u00a1Cuidado! En versiones antiguas de MongoDB, adem\u00e1s de utilizar los operadores de actualizaci\u00f3n, pod\u00edamos pasarle como par\u00e1metro un documento, de manera que MongoDB realizaba un reemplazo de los campos, es decir, si en el origen hab\u00eda 100 campos y en la operaci\u00f3n de modificaci\u00f3n s\u00f3lo pon\u00edamos 2, el resultado \u00fanicamente contendr\u00eda 2 campos. Es por ello, que ahora es obligatorio utilizar los operadores. Si cuando vamos a actualizar, en el criterio de selecci\u00f3n no encuentra el documento sobre el que hacer los cambios, no se realiza ninguna acci\u00f3n. Si quisi\u00e9ramos que en el caso de no encontrar nada insertase un nuevo documento, acci\u00f3n conocida como upsert ( update + insert ), hay que pasarle un tercer par\u00e1metro al m\u00e9todo con el objeto {upsert:true} . Si encuentra el documento, lo modificar\u00e1, pero si no, crear\u00e1 uno nuevo: db . people . updateOne ({ nombre : \"Andreu Medrano\" }, { name : \"Andreu Medrano\" , twitter : \"@andreumedrano\" }, { upsert : true })","title":"Modificando documentos"},{"location":"sa/02mongo.html#operadores-de-actualizacion","text":"MongoDB ofrece un conjunto de operadores para simplificar la modificaci\u00f3n de campos. El operador m\u00e1s utiilzar es el operador $set , el cual admite los campos que se van a modificar. Si el campo no existe, lo crear\u00e1. Por ejemplo, para modificar el salario har\u00edamos: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $se t :{ salario : 1000000 }} ) Mediante $inc podemos incrementar el valor de una variable: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $i n c :{ salario : 1000 }} ) Para eliminar un campo de un documento, usaremos el operador $unset . De este modo, para eliminar el campo twitter de una persona har\u00edamos: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $unset : { twitter : '' }}) Otros operadores que podemos utilizar son $mul , $min , $max y $currentDate . Podemos consultar todos los operadores disponibles en https://www.mongodb.com/docs/manual/reference/operator/update/ Autoevaluaci\u00f3n Tras realizar la siguiente operaci\u00f3n sobre una colecci\u00f3n vac\u00eda: db . people . updateOne ({ nombre : 'yo' }, { '$set' : { 'hobbies' : [ 'gaming' , 'sofing' ]}}, { upsert : true } ); \u00bfCu\u00e1l es el estado de la colecci\u00f3n? Finalmente, un caso particular de las actualizaciones es la posibilidad de renombrar un campo mediante el operador $rename : db . people . update ( { _id : 1 }, { $rename : { 'nickname' : 'alias' , 'cell' : 'movil' }}) Podemos consultar todas las opciones de configuraci\u00f3n de una actualizaci\u00f3n en https://www.mongodb.com/docs/manual/reference/method/db.collection.update/ .","title":"Operadores de actualizaci\u00f3n"},{"location":"sa/02mongo.html#control-de-la-concurrencia","text":"Cuando se hace una actualizaci\u00f3n m\u00faltiple, MongoDB no realiza la operaci\u00f3n de manera at\u00f3mica (a no ser que utilicemos transacciones desde el driver), lo que provoca que se puedan producir pausas ( pause yielding ). Cada documento en s\u00ed es at\u00f3mico, por lo que ninguno se va a quedar a la mitad. MongoDB ofrece el m\u00e9todo findAndModify para encontrar y modificar un documento de manera at\u00f3mica, y as\u00ed evitar que, entre la b\u00fasqueda y la modificaci\u00f3n, el estado del documento se vea afectado. Adem\u00e1s, devuelve el documento modificado. Un caso de uso muy com\u00fan es para contadores y casos similares. db . people . findAndModify ({ query : { nombre : \"Marina Medrano\" }, update : { $inc : { salario : 100 , edad :- 30 }}, new : true }) Por defecto, el documento devuelto ser\u00e1 el resultado que ha encontrado con la consulta. Si queremos que nos devuelva el documento modificado con los cambios deseados, necesitamos utilizar el par\u00e1metro new a true . Si no lo indicamos o lo ponemos a false , tendremos el comportamiento por defecto.","title":"Control de la concurrencia"},{"location":"sa/02mongo.html#actualizaciones-sobre-arrays","text":"Para trabajar con arrays necesitamos nuevos operadores que nos permitan tanto introducir como eliminar elementos de una manera m\u00e1s sencilla que sustituir todos los elementos del array. Los operadores que podemos emplear para trabajar con arrays son: Operador Prop\u00f3sito $push A\u00f1ade uno o varios elementos $addToSet A\u00f1ade un elemento sin duplicados $pull Elimina un elemento $pullAll Elimina varios elementos $pop Elimina el primer o el \u00faltimo Preparando los ejemplos Para trabajar con los arrays, vamos a suponer que tenemos una colecci\u00f3n de enlaces donde vamos a almacenar un documento por cada site, con un atributo tags con etiquetas sobre el enlace en cuesti\u00f3n db . enlaces . insertOne ({ titulo : \"www.google.es\" , tags : [ \"mapas\" , \"videos\" ]}) De modo que tendr\u00edamos el siguiente documento: { _id : Objec t Id( \"633c60e8ac452ac9d7f9fe74\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' ] }","title":"Actualizaciones sobre Arrays"},{"location":"sa/02mongo.html#borrando-documentos","text":"Para borrar, usaremos los m\u00e9todo deleteOne o deleteMany , los cuales funcionan de manera similar a findOne y find . Si no pasamos ning\u00fan par\u00e1metro, deleteOne borrar\u00e1 el primer documento, o en el caso de deleteMany toda la colecci\u00f3n documento a documento. Si le pasamos un par\u00e1metro, \u00e9ste ser\u00e1 el criterio de selecci\u00f3n de documentos a eliminar. db . people . deleteOne ({ nombre : \"Marina Medrano\" }) Al eliminar un documento, no podemos olvidar que cualquier referencia al documento que exista en la base de datos seguir\u00e1 existiendo. Por este motivo, manualmente tambi\u00e9n hay que eliminar o modificar esas referencias. Si queremos borrar toda la colecci\u00f3n, es m\u00e1s eficiente usar el m\u00e9todo drop , ya que tambi\u00e9n elimina los \u00edndices. db . people . drop () Eliminar un campo Recordad que eliminar un determinado campo de un documento no se considera un operaci\u00f3n de borrado, sino una actualizaci\u00f3n mediante el operador $unset .","title":"Borrando documentos"},{"location":"sa/02mongo.html#referencias","text":"Manual de MongoDB Cheatsheet oficial Comparaci\u00f3n entre SQL y MongoDB Cursos gratuitos de Mongo University Consultas solucionadas sobre la colecci\u00f3n sample_restaurants.restaurants en w3resource","title":"Referencias"},{"location":"sa/02mongo.html#actividades","text":"( RA5075.2 / CE5.2b / 1p) Crea un cluster en MongoAtlas , carga los datos de ejemplo y adjunta capturas de pantalla de: Dashboard del cluster Bases de datos / colecciones creadas A continuaci\u00f3n, con\u00e9ctate mediante MongoDB Compass y adjunta una captura de pantalla tras conectar con el cl\u00faster. ( RA5075.1 / CE5.1d / 2p) Haciendo uso de mongosh , escribe los comandos necesarios para: Obtener las bases de datos creadas. Sobre la base de datos sample_training y la colecci\u00f3n zips averigua: Cuantos documentos hay en la ciudad de SAN DIEGO . Cuantos documentos tienen menos de 100 personas (campo pop ). Obt\u00e9n los estados de la ciudad de SAN DIEGO (Soluci\u00f3n: [ 'CA', 'TX' ] ). Cual es el c\u00f3digo postal de la ciudad de ALLEN que no tiene habitantes (s\u00f3lo recupera el zip , no nos interesa ning\u00fan otro campo, ni el _id ). Listado con los 5 c\u00f3digos postales m\u00e1s poblados (muestra los documentos completos). Cantidad de documentos que no tienen menos de 5.000 habitantes ni m\u00e1s de 1.000.000 (debes utilizar el operador $nor ). Cuantos documentos tienen m\u00e1s habitantes que su propio c\u00f3digo postal (campo zip ). Sobre la colecci\u00f3n posts averigua: Cuantos mensajes tienen las etiquetas restaurant o moon . Los comentarios que ha escrito el usuario Salena Olmos . Recupera los mensajes que en body contengan la palabra earth , y devuelve el t\u00edtulo, 3 comentarios y 5 etiquetas. ( RA5075.1 / CE5.1d / 2p) Escribe los comandos necesarios para realizar las siguientes operaciones sobre la colecci\u00f3n zips : Crea una entrada con los siguientes datos: { ci t y : 'ELX' , zip : ' 03206 ' , loc : { x : 38.265500 , y : -0.698459 }, pop : 230224 , s tate : 'Espa\u00f1a' } Crea una entrada con los datos del c\u00f3digo postal donde vives (si es el mismo c\u00f3digo postal, crea uno diferente). Modifica la poblaci\u00f3n de tu c\u00f3digo postal a 1.000.000 . Incrementa la poblaci\u00f3n de todas los documentos de Espa\u00f1a en 666 personas. A\u00f1ade un campo prov a ambos documentos con valor Alicante . Modifica los documentos de Espa\u00f1a y a\u00f1ade un atributo tags que contenga un array vac\u00edo. Modifica todos los documentos de la provincia de Alicante y a\u00f1ade al atributo tags el valor sun . Modifica el valor de sun de tu c\u00f3digo postal y sustit\u00fayelo por house . Renombra en los documentos de la provincia de Alicante el atributo prov por provincia Elimina las coordenadas del zip 03206 . Elimina tu entrada.","title":"Actividades"},{"location":"sa/03modelado.html","text":"things you need to keep in the back of your mind before you model: The document model lets, and encourages, you to keep information that is used together inside one single document. Atomicity, consistency, isolation, and durability (ACID) can be implemented by leveraging the document model, which is preferred, or by using MongoDB transactions. Making early decisions and compromises about very large datasets will help make the data more manageable. For example, deleting or archiving documents after a given period of time may reduce the resources needed for the project. Using a distributed system, like MongoDB, means that data can be written and read on servers all over the world. Carefully planning the location of the servers will improve performance and resilience of the applications. Definir la carga ( workload ): Comprender para qu\u00e9 operaciones estamos modelando Cuantificar y calificar las operaciones de lectura y escritura Listar las operaciones m\u00e1s importantes Diferentes cargas pueden provocar diferentes soluciones de modelado. Modelar las relaciones de forma similar a los modelos relacionales, las relaciones 1:1, 1:M y N:M. Las relaciones 1:1 normalmente se modelan con un documento embebido las relaciones 1:M y N:M mediante un array de documentos o referencias a documentos de otra colecci\u00f3n. El hecho de decidir si utilizar un documento embebido o crear un enlace entre diferentes documentos es la decisi\u00f3n m\u00e1s cr\u00edtica a la hora de modelar. Reconocer y Aplicar patrones de dise\u00f1o Ofrecen transformaciones sobre el esquema, que se centran en el rendimiento, mantenimiento o simplificaci\u00f3n de los requisitos. MongoDB es una base de datos documental, no relacional, donde el esquema no se debe basar en el uso de claves ajenas/joins, ya que no existen. A la hora de dise\u00f1ar un esquema, si nos encontramos que el esquema esta en 3FN o si cuando hacemos consultas (recordad que no hay joins) estamos teniendo que realizar varias consultas de manera programativa (primero acceder a una tabla, con ese _id ir a otra tabla, etc\u2026\u200b.) es que no estamos siguiendo el enfoque adecuado. MongoDB no soporta transacciones, ya que su enfoque distribuido dificultar\u00eda y penalizar\u00eda el rendimiento. En cambio, s\u00ed que asegura que las operaciones sean at\u00f3micas. Los posibles enfoques para solucionar la falta de transacciones son: Restructurar el c\u00f3digo para que toda la informaci\u00f3n est\u00e9 contenida en un \u00fanico documento. Implementar un sistema de bloqueo por software (sem\u00e1foro, etc\u2026\u200b). Tolerar un grado de inconsistencia en el sistema. Dependiendo del tipo de relaci\u00f3n entre dos documentos, normalizaremos los datos para minimizar la redundancia pero manteniendo en la medida de lo posible que mediante operaciones at\u00f3micas se mantenga la integridad de los datos. Para ello, bien crearemos referencias entre dos documentos o embeberemos un documento dentro de otro. Relacionando documentos \u00b6 Las aplicaciones que emplean MongoDB utilizan dos t\u00e9cnicas para relacionar documentos: Referencias Manuales Uso de DBRef Referencias manuales \u00b6 De manera similar a una base de datos relacional, se almacena el campo _id de un documento en otro documento a modo de clave ajena. De este modo, la aplicaci\u00f3n realiza una segunda consulta para obtener los datos relacionados. Estas referencias son sencillas y suficientes para la mayor\u00eda de casos de uso. Referencias manuales Por ejemplo, si nos basamos en el gr\u00e1fico anterior, podemos conseguir referenciar manualmente estos objetos del siguiente modo: var idUsuario = ObjectId (); db . usuario . insert ({ _id : idUsuario , nombre : \"123xyz\" }); db . contacto . insert ({ usuario_id : idUsuario , telefono : \"123 456 7890\" , email : \"xyz@ejemplo.com\" }); FIXME: Para relacionar los dos documentos, haremos uso de la operaci\u00f3n $lookup para hacer el join , o haremos una segunda consulta para la segunda colecci\u00f3n. FIXME: poner c\u00f3digo DBRef \u00b6 Son referencias de un documento a otro mediante el valor del campo _id , el nombre de la colecci\u00f3n y, opcionalmente, el nombre de la base de datos. Estos objetos siguen una convenci\u00f3n para representar un documento mediante la notaci\u00f3n { \"$ref\" : <nombreColeccion>, \"$id\" : <valorCampo_id>, \"$db\" : <nombreBaseDatos> } . Al incluir estos nombres, las DBRef permite referenciar documentos localizados en diferentes colecciones. As\u00ed pues, si reescribimos el c\u00f3digo anterior mediante DBRef tendr\u00edamos que el contacto queda de la siguiente manera: db . contacto . insert ({ usuario_id : new DBRef ( \"usuario\" , idUsuario ), telefono : \"123-456-7890\" , email : \"xyz@example.com\" }); De manera similar a las referencias manuales, mediante consultas adicionales se obtendr\u00e1n los documentos referenciados. Muchos drivers (incluido el de Python, mediante la clase DBRef ) contienen m\u00e9todos auxiliares que realizan las consultas con referencias DBRef autom\u00e1ticamennte. Desde la propia documentaci\u00f3n de MongoDB recomiendan el uso de referencias manuales y el uso de $lookup , a no ser que dispongamos documentos de una colecci\u00f3n que referencian a documentos que se encuentran en varias colecciones diferentes. Datos embebidos \u00b6 En cambio, si dentro de un documento almacenamos los datos mediante sub-documentos, ya sea dentro de un atributo o un array, podremos obtener todos los datos mediante un \u00fanico acceso, sin necesidad de claves ajenas ni comprobaciones de integridad referencial. Datos embebidos Generalmente, emplearemos datos embebidos cuando tengamos: relaciones \"contiene\" entre entidades, entre relaciones de documentos \"uno a uno\" o \"uno a pocos\". relaciones \"uno a muchos\" entre entidades. En estas relaciones los documentos hijo (o \"muchos\") siempre aparecen dentro del contexto del padre o del documento \"uno\". Los datos embebidos ofrecen mejor rendimiento al permitir obtener los datos mediante una \u00fanica operaci\u00f3n, as\u00ed como modificar datos relacionados en una sola operaci\u00f3n at\u00f3mica de escritura. Un aspecto a tener en cuenta es que un documento BSON puede contener un m\u00e1ximo de 16MB. Si quisi\u00e9ramos que un atributo contenga m\u00e1s informaci\u00f3n, tendr\u00edamos que utilizar el API de GridFS. Relaciones \u00b6 Vamos a estudiar en detalle cada uno de los tipos de relaciones, para intentar clarificar cuando es conveniente utilizar referencias o datos embebidos. 1:1 \u00b6 Cuando existe una relaci\u00f3n 1:1, como pueda ser entre Persona y Curriculum, o Persona y Direccion hay que embeber un documento dentro del otro, como parte de un atributo. Ejemplo relaci\u00f3n 1:1 - Persona/Direcci\u00f3n { nombre : \"Aitor\" , edad : 38 , direccion : { calle : \"Mayor\" , ciudad : \"Elx\" } } La principal ventaja de este planteamiento es que mediante una \u00fanica consulta podemos obtener tanto los detalles del usuario como su direcci\u00f3n. Un par de aspectos que nos pueden llevar a no embeberlos son: la frecuencia de acceso. Si a uno de ellos se accede raramente, puede que convenga tenerlos separados para liberar memoria. el tama\u00f1o de los elementos. Si hay uno que es mucho m\u00e1s grande que el otro, o uno lo modificamos muchas m\u00e1s veces que el otro, para que cada vez que hagamos un cambio en un documento no tengamos que modificar el otro ser\u00e1 mejor separarlos en documentos separados. Pero siempre teniendo en cuenta la atomicidad de los datos, ya que si necesitamos modificar los dos documentos al mismo tiempo, tendremos que embeber uno dentro del otro. 1:N \u00b6 Vamos a distinguir dos tipos: 1 a muchos , como puede ser entre Editorial y Libro. Para este tipo de relaci\u00f3n es mejor usar referencias entre los documentos colocando la referencia en el lado del muchos: Ejemplo relaci\u00f3n 1:N - Editorial { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } Ejemplo relaci\u00f3n 1:N - Libro { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial_id : 1 , } { _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial_id : 1 , } 1 a pocos , como por ejemplo, dentro de un blog, la relaci\u00f3n entre Mensaje y Comentario. En este caso, la mejor soluci\u00f3n es crear un array dentro de la entidad 1 ( en nuestro caso, Mensaje). De este modo, el Mensaje contiene un array de Comentario: Ejemplo relaci\u00f3n 1:N - Mensaje/Comentario { titulo : \"La broma asesina\" , url : \"http://es.wikipedia.org/wiki/Batman:_The_Killing_Joke\" , texto : \"La dualidad de Batman y Joker\" , comentarios : [ { autor : \"Bruce Wayne\" , fecha : ISODate ( \"2015-04-01T09:31:32Z\" ), comentario : \"A mi me encant\u00f3\" }, { autor : \"Bruno D\u00edaz\" , fecha : ISODate ( \"2015-04-03T10:07:28Z\" ), comentario : \"El mejor\" } ] } Hay que tener siempre en mente la restricci\u00f3n de los 16 MB de BSON. Si vamos a embeber muchos documentos y estos son grandes, hay que vigilar no llegar a dicho tama\u00f1o. En ocasiones las relaciones 1 a muchos se traducen en documentos embebidos cuando la informaci\u00f3n que nos interesa es la que contiene en un momento determinado. Por ejemplo, dentro de Pedido, el precio de los productos debe embeberse, ya que si en un futuro se modifica el precio de un producto determinado debido a una oferta, el pedido realizado no debe modificar su precio total. Del mismo modo, al almacenar la direcci\u00f3n de una persona, tambi\u00e9n es conveniente embeberla. No queremos que la direcci\u00f3n de env\u00edo de un pedido se modique si un usuario modifica sus datos personales. N:M \u00b6 M\u00e1s que relaciones muchos a muchos, suelen ser relaciones pocos a pocos, como por ejemplo, Libro y Autor, o Profesor y Estudiante. Supongamos que tenemos libros de la siguiente manera y autores con la siguiente estructura: Ejemplo relaci\u00f3n N:N - Libro { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 } Ejemplo relaci\u00f3n N:M - Autor { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" } Podemos resolver estas relaciones de tres maneras: Siguiendo un enfoque relacional, empleando un documento como la entidad que agrupa con referencias manuales a los dos documentos. Ejemplo relaci\u00f3n N:M - Autor/Libro { autor_id : 1 , libro_id : 1 } Este enfoque se desaconseja porque necesita tres consultas para obtener toda la informaci\u00f3n. Mediante 2 documentos, cada uno con un array que contenga los ids del otro documento (2 Way Embedding). Hay que tener cuidado porque podemos tener problemas de inconsistencia de datos si no actualizamos correctamente. Ejemplo relaci\u00f3n N:N - Libro referencia a Autor { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [ 1 ] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [ 1 ] } Ejemplo relaci\u00f3n N:M - Autor referencia a Libro { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" , libros : [ 1 , 2 ] } Embeber un documento dentro de otro (One Way Embedding). Por ejemplo: Ejemplo relaci\u00f3n N:M - Autor embebido en Libro { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] } En principio este enfoque no se recomienda porque el documento puede crecer mucho y provocar anomal\u00edas de modificaciones donde la informaci\u00f3n no es consistente. Si se opta por esta soluci\u00f3n, hay que tener en cuenta que si un documento depende de otro para su creaci\u00f3n (por ejemplo, si metemos los profesores dentro de los estudiantes, no vamos a poder dar de alta a profesores sin haber dado de alta previamente a un alumno). A modo de resumen, en las relaciones N:M, hay que establecer el tama\u00f1o de N y M. Si N como m\u00e1ximo vale 3 y M 500000, entonces deber\u00edamos seguir un enfoque de embeber la N dentro de la M (One Way Embedding). En cambio, si N vale 3 y M vale 5, entonces podemos hacer que ambos embeban al otro documento (Two Way Embedding). Rendimiento e Integridad A modo de resumen, embeber documentos ofrece un mejor rendimiento que referenciar, ya que con una \u00fanica operaci\u00f3n (ya sea una lectura o una escritura) podemos acceder a varios documentos. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/applications/data-models-relationships/ Jer\u00e1rquicas \u00b6 Si tenemos que modelar alguna entidad que tenga hijos y nos importa las relaciones padre-hijos (categor\u00eda-subcategor\u00eda), podemos tanto embeber un array con los hijos de un documento (children), como embeber un array con los padres de un documento (ancestors) M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/applications/data-models-tree-structures/ Patrones \u00b6 https://www.mongodb.com/blog/post/building-with-patterns-a-summary https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-summary/ Patrones y Casos de Uso - mongodb.com Duplicidad, caducidad e integridad \u00b6 Attribute Pattern \u00b6 Extended Reference Pattern \u00b6 In this case, the Extended Reference Pattern will easily take care of the additional queries our application is making. To implement the pattern we can modify the inventory item documents by adding frequently-accessed order data directly to them. This will result in lowering the number of related queries on the orders collection, since the relevant data about the orders will now be part of the inventory item documents. Subset Pattern \u00b6 Computed Pattern \u00b6 En vez de calcular un dato agregado en cada lectura, en una colecci\u00f3n aparte se guardan los datos calculados, de manera que cuando llega un nuevo registro, se recalcula este valor y se modifica el documento oportuno. Tiene sentido en aplicaciones donde hay muchas m\u00e1s lecturas que escrituras. The Computed Pattern allows your application to calculate values at write time. In this case, the sum of the number of views would be calculated in a rolling fashion by book genre. Bucket Pattern \u00b6 Agrupar los datos por fecha o categor\u00eda para reducir la cantidad de documentos o el tama\u00f1o de los mismos. The Bucket pattern allows us to record data in hour interval documents, which can then be stored in yearly collections. This makes it easy to store, analyze, and purge the data within the given time requirements. Schema Versioning Pattern \u00b6 A\u00f1ade un atributo schema_version para indicar que versi\u00f3n del esquema cumplen los datos. Permite evitar downtime al realizar la actualizaci\u00f3n del esquema This pattern allows for the application to quickly identify which document structure it is dealing with, the old one or the new. This helps to minimize downtime for the application user, while allowing the database to smoothly transition to the new schema. Tree Pattern \u00b6 Representaci\u00f3n de datos jer\u00e1rquicos. Referencias de hijos, padre, array de ancestors , materilized paths . Polymorphic Pattern \u00b6 The problem states that bodegas sell a variety of items from different categories, with different purposes and properties. In this case, the Polymorphic Pattern will be the best candidate to catalog this set of goods. Otros patrones asdf Validaci\u00f3n de esquemas \u00b6 Aunque los esquemas son din\u00e1micos y podemos a\u00f1adir nuevos campos confome evoluciona nuestro modelo, podemos validar los esquemas para: asegurar la existencia de un campo asegurar que un campo est\u00e1 rellenado (no nulo) asegurar el tipo de datos de un campo restringir entre un conjunto de valores Para ello se utiliza el operador $jsonSchema . Por ejemplo: { $jso n Schema : { required : [ \"name\" , \"major\" , \"gpa\" , \"address\" ], proper t ies : { na me : { bso n Type : \"string\" , descrip t io n : \"must be a string and is required\" }, address : { bso n Type : \"object\" , required : [ \"zipcode\" ], proper t ies : { \"street\" : { bso n Type : \"string\" }, \"zipcode\" : { bso n Type : \"string\" } } } } } } De este modo, podemos validar un documento contra uno o m\u00e1s esquemas a la vez, lo que facilita la evoluci\u00f3n de los esquemas a lo lago del tiempo. https://www.mongodb.com/docs/manual/core/schema-validation/specify-json-schema/#std-label-schema-validation-json https://www.mongodb.com/docs/manual/core/schema-validation/use-json-schema-query-conditions/#std-label-use-json-schema-query-conditions FIXME: revisar la validaci\u00f3n de los esquemas https://www.mongodb.com/docs/manual/core/schema-validation/ https://www.digitalocean.com/community/tutorials/how-to-use-schema-validation-in-mongodb Referencias \u00b6 V\u00eddeo A Complete Methodology of Data Modeling for MongoDB https://andreshevia.com/2020/10/18/diseno-de-modelos-de-datos-nosql/ https://www.mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices/ https://medium.com/geekculture/mongodb-data-modeling-matters-first-step-to-optimization-data-modeling-series-1-158be911ecb8 https://www.mongodb.com/developer/products/mongodb/schema/tutorials/ https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-summary/ https://www.mongodb.com/blog/post/performance-best-practices-mongodb-data-modeling-and-memory-sizing Actividades \u00b6 (1p) Estimaci\u00f3n de la carga de trabajo en un proceso de ingesta de datos. (Metodolog\u00eda curso MongoDB University) (2p) Ejercicios de modelado de relaciones 1:1, 1:N, N:M que impliquen documentos embebidos y/o relaciones (1p) Consultas agregadas sobre documentos con o sin referencias a otras colecciones.","title":"Modelado NoSQL"},{"location":"sa/03modelado.html#relacionando-documentos","text":"Las aplicaciones que emplean MongoDB utilizan dos t\u00e9cnicas para relacionar documentos: Referencias Manuales Uso de DBRef","title":"Relacionando documentos"},{"location":"sa/03modelado.html#referencias-manuales","text":"De manera similar a una base de datos relacional, se almacena el campo _id de un documento en otro documento a modo de clave ajena. De este modo, la aplicaci\u00f3n realiza una segunda consulta para obtener los datos relacionados. Estas referencias son sencillas y suficientes para la mayor\u00eda de casos de uso. Referencias manuales Por ejemplo, si nos basamos en el gr\u00e1fico anterior, podemos conseguir referenciar manualmente estos objetos del siguiente modo: var idUsuario = ObjectId (); db . usuario . insert ({ _id : idUsuario , nombre : \"123xyz\" }); db . contacto . insert ({ usuario_id : idUsuario , telefono : \"123 456 7890\" , email : \"xyz@ejemplo.com\" }); FIXME: Para relacionar los dos documentos, haremos uso de la operaci\u00f3n $lookup para hacer el join , o haremos una segunda consulta para la segunda colecci\u00f3n. FIXME: poner c\u00f3digo","title":"Referencias manuales"},{"location":"sa/03modelado.html#dbref","text":"Son referencias de un documento a otro mediante el valor del campo _id , el nombre de la colecci\u00f3n y, opcionalmente, el nombre de la base de datos. Estos objetos siguen una convenci\u00f3n para representar un documento mediante la notaci\u00f3n { \"$ref\" : <nombreColeccion>, \"$id\" : <valorCampo_id>, \"$db\" : <nombreBaseDatos> } . Al incluir estos nombres, las DBRef permite referenciar documentos localizados en diferentes colecciones. As\u00ed pues, si reescribimos el c\u00f3digo anterior mediante DBRef tendr\u00edamos que el contacto queda de la siguiente manera: db . contacto . insert ({ usuario_id : new DBRef ( \"usuario\" , idUsuario ), telefono : \"123-456-7890\" , email : \"xyz@example.com\" }); De manera similar a las referencias manuales, mediante consultas adicionales se obtendr\u00e1n los documentos referenciados. Muchos drivers (incluido el de Python, mediante la clase DBRef ) contienen m\u00e9todos auxiliares que realizan las consultas con referencias DBRef autom\u00e1ticamennte. Desde la propia documentaci\u00f3n de MongoDB recomiendan el uso de referencias manuales y el uso de $lookup , a no ser que dispongamos documentos de una colecci\u00f3n que referencian a documentos que se encuentran en varias colecciones diferentes.","title":"DBRef"},{"location":"sa/03modelado.html#datos-embebidos","text":"En cambio, si dentro de un documento almacenamos los datos mediante sub-documentos, ya sea dentro de un atributo o un array, podremos obtener todos los datos mediante un \u00fanico acceso, sin necesidad de claves ajenas ni comprobaciones de integridad referencial. Datos embebidos Generalmente, emplearemos datos embebidos cuando tengamos: relaciones \"contiene\" entre entidades, entre relaciones de documentos \"uno a uno\" o \"uno a pocos\". relaciones \"uno a muchos\" entre entidades. En estas relaciones los documentos hijo (o \"muchos\") siempre aparecen dentro del contexto del padre o del documento \"uno\". Los datos embebidos ofrecen mejor rendimiento al permitir obtener los datos mediante una \u00fanica operaci\u00f3n, as\u00ed como modificar datos relacionados en una sola operaci\u00f3n at\u00f3mica de escritura. Un aspecto a tener en cuenta es que un documento BSON puede contener un m\u00e1ximo de 16MB. Si quisi\u00e9ramos que un atributo contenga m\u00e1s informaci\u00f3n, tendr\u00edamos que utilizar el API de GridFS.","title":"Datos embebidos"},{"location":"sa/03modelado.html#relaciones","text":"Vamos a estudiar en detalle cada uno de los tipos de relaciones, para intentar clarificar cuando es conveniente utilizar referencias o datos embebidos.","title":"Relaciones"},{"location":"sa/03modelado.html#11","text":"Cuando existe una relaci\u00f3n 1:1, como pueda ser entre Persona y Curriculum, o Persona y Direccion hay que embeber un documento dentro del otro, como parte de un atributo. Ejemplo relaci\u00f3n 1:1 - Persona/Direcci\u00f3n { nombre : \"Aitor\" , edad : 38 , direccion : { calle : \"Mayor\" , ciudad : \"Elx\" } } La principal ventaja de este planteamiento es que mediante una \u00fanica consulta podemos obtener tanto los detalles del usuario como su direcci\u00f3n. Un par de aspectos que nos pueden llevar a no embeberlos son: la frecuencia de acceso. Si a uno de ellos se accede raramente, puede que convenga tenerlos separados para liberar memoria. el tama\u00f1o de los elementos. Si hay uno que es mucho m\u00e1s grande que el otro, o uno lo modificamos muchas m\u00e1s veces que el otro, para que cada vez que hagamos un cambio en un documento no tengamos que modificar el otro ser\u00e1 mejor separarlos en documentos separados. Pero siempre teniendo en cuenta la atomicidad de los datos, ya que si necesitamos modificar los dos documentos al mismo tiempo, tendremos que embeber uno dentro del otro.","title":"1:1"},{"location":"sa/03modelado.html#1n","text":"Vamos a distinguir dos tipos: 1 a muchos , como puede ser entre Editorial y Libro. Para este tipo de relaci\u00f3n es mejor usar referencias entre los documentos colocando la referencia en el lado del muchos: Ejemplo relaci\u00f3n 1:N - Editorial { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } Ejemplo relaci\u00f3n 1:N - Libro { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial_id : 1 , } { _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial_id : 1 , } 1 a pocos , como por ejemplo, dentro de un blog, la relaci\u00f3n entre Mensaje y Comentario. En este caso, la mejor soluci\u00f3n es crear un array dentro de la entidad 1 ( en nuestro caso, Mensaje). De este modo, el Mensaje contiene un array de Comentario: Ejemplo relaci\u00f3n 1:N - Mensaje/Comentario { titulo : \"La broma asesina\" , url : \"http://es.wikipedia.org/wiki/Batman:_The_Killing_Joke\" , texto : \"La dualidad de Batman y Joker\" , comentarios : [ { autor : \"Bruce Wayne\" , fecha : ISODate ( \"2015-04-01T09:31:32Z\" ), comentario : \"A mi me encant\u00f3\" }, { autor : \"Bruno D\u00edaz\" , fecha : ISODate ( \"2015-04-03T10:07:28Z\" ), comentario : \"El mejor\" } ] } Hay que tener siempre en mente la restricci\u00f3n de los 16 MB de BSON. Si vamos a embeber muchos documentos y estos son grandes, hay que vigilar no llegar a dicho tama\u00f1o. En ocasiones las relaciones 1 a muchos se traducen en documentos embebidos cuando la informaci\u00f3n que nos interesa es la que contiene en un momento determinado. Por ejemplo, dentro de Pedido, el precio de los productos debe embeberse, ya que si en un futuro se modifica el precio de un producto determinado debido a una oferta, el pedido realizado no debe modificar su precio total. Del mismo modo, al almacenar la direcci\u00f3n de una persona, tambi\u00e9n es conveniente embeberla. No queremos que la direcci\u00f3n de env\u00edo de un pedido se modique si un usuario modifica sus datos personales.","title":"1:N"},{"location":"sa/03modelado.html#nm","text":"M\u00e1s que relaciones muchos a muchos, suelen ser relaciones pocos a pocos, como por ejemplo, Libro y Autor, o Profesor y Estudiante. Supongamos que tenemos libros de la siguiente manera y autores con la siguiente estructura: Ejemplo relaci\u00f3n N:N - Libro { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 } Ejemplo relaci\u00f3n N:M - Autor { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" } Podemos resolver estas relaciones de tres maneras: Siguiendo un enfoque relacional, empleando un documento como la entidad que agrupa con referencias manuales a los dos documentos. Ejemplo relaci\u00f3n N:M - Autor/Libro { autor_id : 1 , libro_id : 1 } Este enfoque se desaconseja porque necesita tres consultas para obtener toda la informaci\u00f3n. Mediante 2 documentos, cada uno con un array que contenga los ids del otro documento (2 Way Embedding). Hay que tener cuidado porque podemos tener problemas de inconsistencia de datos si no actualizamos correctamente. Ejemplo relaci\u00f3n N:N - Libro referencia a Autor { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [ 1 ] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [ 1 ] } Ejemplo relaci\u00f3n N:M - Autor referencia a Libro { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" , libros : [ 1 , 2 ] } Embeber un documento dentro de otro (One Way Embedding). Por ejemplo: Ejemplo relaci\u00f3n N:M - Autor embebido en Libro { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] } En principio este enfoque no se recomienda porque el documento puede crecer mucho y provocar anomal\u00edas de modificaciones donde la informaci\u00f3n no es consistente. Si se opta por esta soluci\u00f3n, hay que tener en cuenta que si un documento depende de otro para su creaci\u00f3n (por ejemplo, si metemos los profesores dentro de los estudiantes, no vamos a poder dar de alta a profesores sin haber dado de alta previamente a un alumno). A modo de resumen, en las relaciones N:M, hay que establecer el tama\u00f1o de N y M. Si N como m\u00e1ximo vale 3 y M 500000, entonces deber\u00edamos seguir un enfoque de embeber la N dentro de la M (One Way Embedding). En cambio, si N vale 3 y M vale 5, entonces podemos hacer que ambos embeban al otro documento (Two Way Embedding). Rendimiento e Integridad A modo de resumen, embeber documentos ofrece un mejor rendimiento que referenciar, ya que con una \u00fanica operaci\u00f3n (ya sea una lectura o una escritura) podemos acceder a varios documentos. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/applications/data-models-relationships/","title":"N:M"},{"location":"sa/03modelado.html#jerarquicas","text":"Si tenemos que modelar alguna entidad que tenga hijos y nos importa las relaciones padre-hijos (categor\u00eda-subcategor\u00eda), podemos tanto embeber un array con los hijos de un documento (children), como embeber un array con los padres de un documento (ancestors) M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/applications/data-models-tree-structures/","title":"Jer\u00e1rquicas"},{"location":"sa/03modelado.html#patrones","text":"https://www.mongodb.com/blog/post/building-with-patterns-a-summary https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-summary/ Patrones y Casos de Uso - mongodb.com","title":"Patrones"},{"location":"sa/03modelado.html#duplicidad-caducidad-e-integridad","text":"","title":"Duplicidad, caducidad e integridad"},{"location":"sa/03modelado.html#attribute-pattern","text":"","title":"Attribute Pattern"},{"location":"sa/03modelado.html#extended-reference-pattern","text":"In this case, the Extended Reference Pattern will easily take care of the additional queries our application is making. To implement the pattern we can modify the inventory item documents by adding frequently-accessed order data directly to them. This will result in lowering the number of related queries on the orders collection, since the relevant data about the orders will now be part of the inventory item documents.","title":"Extended Reference Pattern"},{"location":"sa/03modelado.html#subset-pattern","text":"","title":"Subset Pattern"},{"location":"sa/03modelado.html#computed-pattern","text":"En vez de calcular un dato agregado en cada lectura, en una colecci\u00f3n aparte se guardan los datos calculados, de manera que cuando llega un nuevo registro, se recalcula este valor y se modifica el documento oportuno. Tiene sentido en aplicaciones donde hay muchas m\u00e1s lecturas que escrituras. The Computed Pattern allows your application to calculate values at write time. In this case, the sum of the number of views would be calculated in a rolling fashion by book genre.","title":"Computed Pattern"},{"location":"sa/03modelado.html#bucket-pattern","text":"Agrupar los datos por fecha o categor\u00eda para reducir la cantidad de documentos o el tama\u00f1o de los mismos. The Bucket pattern allows us to record data in hour interval documents, which can then be stored in yearly collections. This makes it easy to store, analyze, and purge the data within the given time requirements.","title":"Bucket Pattern"},{"location":"sa/03modelado.html#schema-versioning-pattern","text":"A\u00f1ade un atributo schema_version para indicar que versi\u00f3n del esquema cumplen los datos. Permite evitar downtime al realizar la actualizaci\u00f3n del esquema This pattern allows for the application to quickly identify which document structure it is dealing with, the old one or the new. This helps to minimize downtime for the application user, while allowing the database to smoothly transition to the new schema.","title":"Schema Versioning Pattern"},{"location":"sa/03modelado.html#tree-pattern","text":"Representaci\u00f3n de datos jer\u00e1rquicos. Referencias de hijos, padre, array de ancestors , materilized paths .","title":"Tree Pattern"},{"location":"sa/03modelado.html#polymorphic-pattern","text":"The problem states that bodegas sell a variety of items from different categories, with different purposes and properties. In this case, the Polymorphic Pattern will be the best candidate to catalog this set of goods. Otros patrones asdf","title":"Polymorphic Pattern"},{"location":"sa/03modelado.html#validacion-de-esquemas","text":"Aunque los esquemas son din\u00e1micos y podemos a\u00f1adir nuevos campos confome evoluciona nuestro modelo, podemos validar los esquemas para: asegurar la existencia de un campo asegurar que un campo est\u00e1 rellenado (no nulo) asegurar el tipo de datos de un campo restringir entre un conjunto de valores Para ello se utiliza el operador $jsonSchema . Por ejemplo: { $jso n Schema : { required : [ \"name\" , \"major\" , \"gpa\" , \"address\" ], proper t ies : { na me : { bso n Type : \"string\" , descrip t io n : \"must be a string and is required\" }, address : { bso n Type : \"object\" , required : [ \"zipcode\" ], proper t ies : { \"street\" : { bso n Type : \"string\" }, \"zipcode\" : { bso n Type : \"string\" } } } } } } De este modo, podemos validar un documento contra uno o m\u00e1s esquemas a la vez, lo que facilita la evoluci\u00f3n de los esquemas a lo lago del tiempo. https://www.mongodb.com/docs/manual/core/schema-validation/specify-json-schema/#std-label-schema-validation-json https://www.mongodb.com/docs/manual/core/schema-validation/use-json-schema-query-conditions/#std-label-use-json-schema-query-conditions FIXME: revisar la validaci\u00f3n de los esquemas https://www.mongodb.com/docs/manual/core/schema-validation/ https://www.digitalocean.com/community/tutorials/how-to-use-schema-validation-in-mongodb","title":"Validaci\u00f3n de esquemas"},{"location":"sa/03modelado.html#referencias","text":"V\u00eddeo A Complete Methodology of Data Modeling for MongoDB https://andreshevia.com/2020/10/18/diseno-de-modelos-de-datos-nosql/ https://www.mongodb.com/developer/products/mongodb/mongodb-schema-design-best-practices/ https://medium.com/geekculture/mongodb-data-modeling-matters-first-step-to-optimization-data-modeling-series-1-158be911ecb8 https://www.mongodb.com/developer/products/mongodb/schema/tutorials/ https://www.mongodb.com/developer/products/mongodb/schema-design-anti-pattern-summary/ https://www.mongodb.com/blog/post/performance-best-practices-mongodb-data-modeling-and-memory-sizing","title":"Referencias"},{"location":"sa/03modelado.html#actividades","text":"(1p) Estimaci\u00f3n de la carga de trabajo en un proceso de ingesta de datos. (Metodolog\u00eda curso MongoDB University) (2p) Ejercicios de modelado de relaciones 1:1, 1:N, N:M que impliquen documentos embebidos y/o relaciones (1p) Consultas agregadas sobre documentos con o sin referencias a otras colecciones.","title":"Actividades"},{"location":"sa/04formatos.html","text":"En el primer bloque ya vimos una peque\u00f1a introducci\u00f3n a los diferentes formatos de datos . Las propiedades que ha de tener un formato de datos son: independiente del lenguaje expresivo, con soporte para estructuras complejas y anidadas eficiente, r\u00e1pido y reducido din\u00e1mico, de manera que los programas puedan procesar y definir nuevos tipos de datos. formato de fichero standalone y que permita dividirlo y comprimirlo. Para que Hadoop pueda procesar documento, es imprescindible que el formato del fichero permita su divisi\u00f3n en fragmentos ( splittable in chunks ). Si los clasificamos respecto al formato de almacenamiento tenemos: texto (m\u00e1s lentos, ocupan m\u00e1s pero son m\u00e1s expresivos y permiten su interoperabilidad): CSV, XML, JSON, etc... binarios (mejor rendimiento, ocupan menos, menos expresivos): Avro, Parquet, ORC, etc... Si comparamos los formatos m\u00e1s empleados a partir de las propiedades descritas tenemos: Caracter\u00edstica CSV XML / JSON SequenceFile Avro Independencia del lenguaje :thumbsup: :thumbsup: :fontawesome-regular-thumbs-down: :thumbsup: Expresivo :fontawesome-regular-thumbs-down: :thumbsup: :thumbsup: :thumbsup: Eficiente :fontawesome-regular-thumbs-down: :fontawesome-regular-thumbs-down: :thumbsup: :thumbsup: Din\u00e1mico :thumbsup: :thumbsup: :fontawesome-regular-thumbs-down: :thumbsup: Standalone :grey_question: :thumbsup: :fontawesome-regular-thumbs-down: :thumbsup: Dividible :grey_question: :grey_question: :thumbsup: :thumbsup: Las ventajas de elegir el formato correcto son: Mayor rendimiento en la lectura y/o escritura Ficheros trozeables ( splittables ) Soporte para esquemas que evolucionan Soporte para compresi\u00f3n de los datos (por ejemplo, mediante Snappy ). Filas vs Columnas \u00b6 Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV, con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet, lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes. Avro \u00b6 Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Avro se basa en esquemas, los cuales se realizan mediante JSON para definir los tipos de datos y protocolos. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Schema \u00b6 Los esquemas se componen de tipos primitivos ( null , boolean , int , long , float , double , bytes , y string ) y compuestos ( record , enum , array , map , union , y fixed ). Un ejemplo de esquema podr\u00eda ser: empleado.avsc { \"type\" : \"record\" , \"namespace\" : \"SeveroOchoa\" , \"name\" : \"Empleado\" , \"fields\" : [ { \"name\" : \"Nombre\" , \"type\" : \"string\" }, { \"name\" : \"Altura\" , \"type\" : \"float\" } { \"name\" : \"Edad\" , \"type\" : \"int\" } ] } Python \u00b6 Para poder serializar y deserializar documentos Avro mediante Python , previamente debemos instalar la librer\u00eda avro : pip install avro-python3 # o si utilizamos Anaconda conda install -c conda-forge avro-python3 Vamos a realizar un ejemplo donde primero leemos un esquema de un archivo Avro , y con dicho esquema, escribiremos nuevos datos en un fichero. A continuaci\u00f3n, abrimos el fichero escrito y leemos y mostramos los datos: C\u00f3digo Python Resultado import avro import copy import json from avro.datafile import DataFileReader , DataFileWriter from avro.io import DatumReader , DatumWriter # abrimos el fichero en modo binario y leemos el esquema schema = avro . schema . parse ( open ( \"empleado.avsc\" , \"rb\" ) . read ()) # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleados.avro' , 'wb' ) as f : writer = DataFileWriter ( f , DatumWriter (), schema ) writer . append ({ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }) writer . append ({ \"nombre\" : \"Juan\" , \"altura\" : 175 }) writer . close () # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleados.avro\" , \"rb\" ) as f : reader = DataFileReader ( f , DatumReader ()) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . meta ) # obtenemos el schema del fichero le\u00eddo schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] reader . close () print ( f 'Schema de empleado.avsc: \\n { schema } ' ) print ( f 'Schema del fichero empleados.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { \"type\" : \"record\" , \"name\" : \"empleado\" , \"namespace\" : \"SeveroOchoa\" , \"fields\" : [{ \"type\" : \"string\" , \"name\" : \"nombre\" }, { \"type\" : \"int\" , \"name\" : \"altura\" }, { \"type\" : [ \"null\" , \"int\" ], \"name\" : \"edad\" , \"default\" : null }]} Schema del f ichero empleados.avro : { ' t ype' : 'record' , ' na me' : 'empleado' , ' na mespace' : 'SeveroOchoa' , ' f ields' : [{ ' t ype' : 's tr i n g' , ' na me' : ' n ombre' }, { ' t ype' : 'i nt ' , ' na me' : 'al tura ' }, { ' t ype' : [ ' null ' , 'i nt ' ], ' na me' : 'edad' , 'de fault ' : No ne }]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Fastavro \u00b6 Para trabajar con Avro y grandes vol\u00famenes de datos, se utiliza la librer\u00eda Fastavro ( https://github.com/fastavro/fastavro ) la cual ofrece un rendimiento mucho mejor (en vez de estar codificada en Python puro, tiene algunos fragmentos realizados mediante Cython ). Primero, hemos de instalar la librer\u00eda: pip install fastavro # o si utilizamos Anaconda conda install -c conda-forge fastavro Como pod\u00e9is observar a continuaci\u00f3n, hemos repetido el ejemplo y el c\u00f3digo es muy similar: C\u00f3digo Python Resultado import fastavro import copy import json from fastavro import reader # abrimos el fichero en modo binario y leemos el esquema with open ( \"empleado.avsc\" , \"rb\" ) as f : schemaJSON = json . load ( f ) schemaDict = fastavro . parse_schema ( schemaJSON ) empleados = [{ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }, { \"nombre\" : \"Juan\" , \"altura\" : 175 }] # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleadosf.avro' , 'wb' ) as f : fastavro . writer ( f , schemaDict , empleados ) # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleadosf.avro\" , \"rb\" ) as f : reader = fastavro . reader ( f ) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . metadata ) # obtenemos el schema del fichero le\u00eddo schemaReader = copy . deepcopy ( reader . writer_schema ) schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] print ( f 'Schema de empleado.avsc: \\n { schemaDict } ' ) print ( f 'Schema del fichero empleadosf.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}], '__ fasta vro_parsed' : True , '__ na med_schemas' : { 'SeveroOchoa.empleado' : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]}}} Schema del f ichero empleados f .avro : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Fastavro y Pandas \u00b6 Finalmente, vamos a realizar un \u00faltimo ejemplo con las dos librer\u00edas m\u00e1s utilizadas. Vamos a leer un fichero CSV de ventas mediante Pandas, y tras limpiar los datos y quedarnos \u00fanicamente con las ventas de Alemania, almacenaremos el resultado del procesamiento en Avro. Acceso Local Acceso HDFS import pandas as pd from fastavro import writer , parse_schema # Leemos el csv mediante pandas df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 1. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 2. Convertimos el dataframe a una lista de diccionarios records = df . to_dict ( 'records' ) # 3. Persistimos en un fichero avro with open ( 'sales.avro' , 'wb' ) as f : writer ( f , schemaParseado , records ) import pandas as pd from fastavro import parse_schema from hdfs import InsecureClient from hdfs.ext.avro import AvroWriter from hdfs.ext.dataframe import write_dataframe # 1. Nos conectamos a HDFS HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # 2. Leemos el dataframe with hdfs_client . read ( '/user/iabd/pdi_sales.csv' ) as reader : df = pd . read_csv ( reader , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 3. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 4a. Persistimos en un fichero avro dentro de HDFS mediante la extension AvroWriter de hdfs with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado ) as writer : records = df . to_dict ( 'records' ) # diccionario for record in records : writer . write ( record ) # 4b. O directamente persistimos el dataframe mediante la extension write_dataframe de hdfs write_dataframe ( hdfs_client , '/user/iabd/sales2.avro' , df ) # infiere el esquema write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado ) Para el acceso HDFS hemos utilizados las extensiones Fastavro y Pandas de la librer\u00eda HDFS del apartado anterior. Comprimiendo los datos \u00b6 \u00bfY s\u00ed comprimimos los datos para ocupen menos espacio en nuestro cl\u00faster y por tanto, nos cuesten menos dinero? Fastavro soporta dos tipos de compresi\u00f3n: gzip (mediante el algoritmo deflate ) y snappy . Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data, la cual hemos de instalar previamente mediante pip install python-snappy . Para indicar el tipo de compresi\u00f3n, \u00fanicamente hemos de a\u00f1adir un par\u00e1metros extra con el algoritmo de compresi\u00f3n en la funci\u00f3n/constructor de persistencia: Fastavro y gzip AvroWriter y snappy write_dataframe y snappy writer ( f , schemaParseado , records , 'deflate' ) with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado , 'snappy' ) as writer : write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado , codec = 'snappy' ) Comparando algoritmos de compresi\u00f3n Respecto a la compresi\u00f3n, sobre un fichero de 100GB, podemos considerar media si ronda los 50GB y alta si baja a los 40GB. Algoritmo Velocidad Compresi\u00f3n Gzip Media Media Bzip2 Lenta Alta Snappy Alta Media M\u00e1s que un tema de espacio, necesitamos que los procesos sean eficientes y por eso priman los algoritmos que son m\u00e1s r\u00e1pidos. Si te interesa el tema, es muy interesante el art\u00edculo Data Compression in Hadoop . Por ejemplo, si realizamos el ejemplo de Fast Avro y Pandas con acceso local obtenemos los siguientes tama\u00f1os: Sin compresi\u00f3n: 6,9 MiB Gzip: 1,9 MiB Snappy: 2,8 MiB Parquet \u00b6 Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas. Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc... Parquet y Python \u00b6 Para interactuar con el formato Parquet mediante Python, la librer\u00eda m\u00e1s utilizada es la que ofrece Apache Arrow , en concreto la librer\u00eda PyArrow . As\u00ed pues, la instalamos mediante pip: pip install pyarrow Apache Arrow usa un tipo de estructura denominada tabla para almacenar los datos bidimentsional (ser\u00eda muy similar a un dataframe de Pandas ). La documentaci\u00f3n de PyArrow dispone de un libro de recetas con ejemplos con c\u00f3digo para los diferentes casos de uso que se nos puedan plantear. Vamos a simular el mismo ejemplo que hemos realizado previamente mediante Avro , y vamos a crear un fichero en formato JSON con empleados, y tras persistirlo en formato Parquet , lo vamos a recuperar: Empleados en columnas Empleados en Filas dict-parquet.py import pyarrow.parquet as pq import pyarrow as pa # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Almacenamos los empleados por columnas empleados = { \"nombre\" : [ \"Carlos\" , \"Juan\" ], \"altura\" : [ 180 , 44 ], \"edad\" : [ None , 34 ]} # 3.- Creamos una tabla Arrow y la persistimos mediante Parquet tabla = pa . Table . from_pydict ( empleados , schema ) pq . write_table ( tabla , 'empleados.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) Para que pyarrow pueda leer los empleados como documentos JSON, a d\u00eda de hoy s\u00f3lo puede hacerlo leyendo documentos individuales almacenados en fichero: Por lo tanto, creamos el fichero empleados.json con la siguiente informaci\u00f3n: empleados.json { \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 } { \"nombre\" : \"Juan\" , \"altura\" : 175 } De manera que podemos leer los datos JSON y persistirlos en Parquet del siguiente modo: json-parquet.py import pyarrow.parquet as pq import pyarrow as pa from pyarrow import json # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Leemos los empleados tabla = json . read_json ( \"empleados.json\" ) # 3.- Persistimos la tabla en Parquet pq . write_table ( tabla , 'empleados-json.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados-json.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados-json.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) En ambos casos obtendr\u00edamos algo similar a: Schema del fichero empleados.parquet: nombre: string altura: int32 edad: int32 Tabla de Empleados: pyarrow.Table nombre: string altura: int32 edad: int32 ---- nombre: [[\"Carlos\",\"Juan\"]] altura: [[180,44]] edad: [[null,34]] Parquet y Pandas \u00b6 En el caso del uso de Pandas el c\u00f3digo todav\u00eda se simplifica m\u00e1s. Si reproducimos el mismo ejemplo que hemos realizado con Avro tenemos que los Dataframes ofrecen el m\u00e9todo to_parquet para exportar a un fichero Parquet : csv-parquet.py import pandas as pd df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # A partir de un DataFrame, persistimos los datos df . to_parquet ( 'sales.parquet' ) Si quisi\u00e9ramos almacenar el archivo directamente en HDFS, necesitamos indicarle a Pandas la direcci\u00f3n del sistema de archivos que tenemos configurado en core-site.xml : core-site.ml <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> As\u00ed pues, \u00fanicamente necesitamos modificar el nombre del archivo donde serializamos los datos a Parquet : df . to_parquet ( 'hdfs://iabd-virtualbox:9000/sales.parquet' ) Comparando tama\u00f1os \u00b6 Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiBa Referencias \u00b6 Handling Avro files in Python Actividades \u00b6 Mediante Python , carga los datos de los taxis XXX y crea dentro de /user/iabd/datos los siguientes archivos con el formato adecuado: taxis.avro : la fecha ( tpep_pickup_datetime ), el VendorID y el coste de cada viaje ( total_amount ) (opcional) taxis.parquet con los mismos atributos.","title":"Formatos de datos"},{"location":"sa/04formatos.html#filas-vs-columnas","text":"Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV, con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet, lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes.","title":"Filas vs Columnas"},{"location":"sa/04formatos.html#avro","text":"Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Avro se basa en esquemas, los cuales se realizan mediante JSON para definir los tipos de datos y protocolos. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos.","title":"Avro"},{"location":"sa/04formatos.html#schema","text":"Los esquemas se componen de tipos primitivos ( null , boolean , int , long , float , double , bytes , y string ) y compuestos ( record , enum , array , map , union , y fixed ). Un ejemplo de esquema podr\u00eda ser: empleado.avsc { \"type\" : \"record\" , \"namespace\" : \"SeveroOchoa\" , \"name\" : \"Empleado\" , \"fields\" : [ { \"name\" : \"Nombre\" , \"type\" : \"string\" }, { \"name\" : \"Altura\" , \"type\" : \"float\" } { \"name\" : \"Edad\" , \"type\" : \"int\" } ] }","title":"Schema"},{"location":"sa/04formatos.html#python","text":"Para poder serializar y deserializar documentos Avro mediante Python , previamente debemos instalar la librer\u00eda avro : pip install avro-python3 # o si utilizamos Anaconda conda install -c conda-forge avro-python3 Vamos a realizar un ejemplo donde primero leemos un esquema de un archivo Avro , y con dicho esquema, escribiremos nuevos datos en un fichero. A continuaci\u00f3n, abrimos el fichero escrito y leemos y mostramos los datos: C\u00f3digo Python Resultado import avro import copy import json from avro.datafile import DataFileReader , DataFileWriter from avro.io import DatumReader , DatumWriter # abrimos el fichero en modo binario y leemos el esquema schema = avro . schema . parse ( open ( \"empleado.avsc\" , \"rb\" ) . read ()) # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleados.avro' , 'wb' ) as f : writer = DataFileWriter ( f , DatumWriter (), schema ) writer . append ({ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }) writer . append ({ \"nombre\" : \"Juan\" , \"altura\" : 175 }) writer . close () # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleados.avro\" , \"rb\" ) as f : reader = DataFileReader ( f , DatumReader ()) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . meta ) # obtenemos el schema del fichero le\u00eddo schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] reader . close () print ( f 'Schema de empleado.avsc: \\n { schema } ' ) print ( f 'Schema del fichero empleados.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { \"type\" : \"record\" , \"name\" : \"empleado\" , \"namespace\" : \"SeveroOchoa\" , \"fields\" : [{ \"type\" : \"string\" , \"name\" : \"nombre\" }, { \"type\" : \"int\" , \"name\" : \"altura\" }, { \"type\" : [ \"null\" , \"int\" ], \"name\" : \"edad\" , \"default\" : null }]} Schema del f ichero empleados.avro : { ' t ype' : 'record' , ' na me' : 'empleado' , ' na mespace' : 'SeveroOchoa' , ' f ields' : [{ ' t ype' : 's tr i n g' , ' na me' : ' n ombre' }, { ' t ype' : 'i nt ' , ' na me' : 'al tura ' }, { ' t ype' : [ ' null ' , 'i nt ' ], ' na me' : 'edad' , 'de fault ' : No ne }]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }]","title":"Python"},{"location":"sa/04formatos.html#fastavro","text":"Para trabajar con Avro y grandes vol\u00famenes de datos, se utiliza la librer\u00eda Fastavro ( https://github.com/fastavro/fastavro ) la cual ofrece un rendimiento mucho mejor (en vez de estar codificada en Python puro, tiene algunos fragmentos realizados mediante Cython ). Primero, hemos de instalar la librer\u00eda: pip install fastavro # o si utilizamos Anaconda conda install -c conda-forge fastavro Como pod\u00e9is observar a continuaci\u00f3n, hemos repetido el ejemplo y el c\u00f3digo es muy similar: C\u00f3digo Python Resultado import fastavro import copy import json from fastavro import reader # abrimos el fichero en modo binario y leemos el esquema with open ( \"empleado.avsc\" , \"rb\" ) as f : schemaJSON = json . load ( f ) schemaDict = fastavro . parse_schema ( schemaJSON ) empleados = [{ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }, { \"nombre\" : \"Juan\" , \"altura\" : 175 }] # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleadosf.avro' , 'wb' ) as f : fastavro . writer ( f , schemaDict , empleados ) # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleadosf.avro\" , \"rb\" ) as f : reader = fastavro . reader ( f ) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . metadata ) # obtenemos el schema del fichero le\u00eddo schemaReader = copy . deepcopy ( reader . writer_schema ) schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] print ( f 'Schema de empleado.avsc: \\n { schemaDict } ' ) print ( f 'Schema del fichero empleadosf.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}], '__ fasta vro_parsed' : True , '__ na med_schemas' : { 'SeveroOchoa.empleado' : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]}}} Schema del f ichero empleados f .avro : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }]","title":"Fastavro"},{"location":"sa/04formatos.html#fastavro-y-pandas","text":"Finalmente, vamos a realizar un \u00faltimo ejemplo con las dos librer\u00edas m\u00e1s utilizadas. Vamos a leer un fichero CSV de ventas mediante Pandas, y tras limpiar los datos y quedarnos \u00fanicamente con las ventas de Alemania, almacenaremos el resultado del procesamiento en Avro. Acceso Local Acceso HDFS import pandas as pd from fastavro import writer , parse_schema # Leemos el csv mediante pandas df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 1. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 2. Convertimos el dataframe a una lista de diccionarios records = df . to_dict ( 'records' ) # 3. Persistimos en un fichero avro with open ( 'sales.avro' , 'wb' ) as f : writer ( f , schemaParseado , records ) import pandas as pd from fastavro import parse_schema from hdfs import InsecureClient from hdfs.ext.avro import AvroWriter from hdfs.ext.dataframe import write_dataframe # 1. Nos conectamos a HDFS HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # 2. Leemos el dataframe with hdfs_client . read ( '/user/iabd/pdi_sales.csv' ) as reader : df = pd . read_csv ( reader , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 3. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 4a. Persistimos en un fichero avro dentro de HDFS mediante la extension AvroWriter de hdfs with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado ) as writer : records = df . to_dict ( 'records' ) # diccionario for record in records : writer . write ( record ) # 4b. O directamente persistimos el dataframe mediante la extension write_dataframe de hdfs write_dataframe ( hdfs_client , '/user/iabd/sales2.avro' , df ) # infiere el esquema write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado ) Para el acceso HDFS hemos utilizados las extensiones Fastavro y Pandas de la librer\u00eda HDFS del apartado anterior.","title":"Fastavro y Pandas"},{"location":"sa/04formatos.html#comprimiendo-los-datos","text":"\u00bfY s\u00ed comprimimos los datos para ocupen menos espacio en nuestro cl\u00faster y por tanto, nos cuesten menos dinero? Fastavro soporta dos tipos de compresi\u00f3n: gzip (mediante el algoritmo deflate ) y snappy . Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data, la cual hemos de instalar previamente mediante pip install python-snappy . Para indicar el tipo de compresi\u00f3n, \u00fanicamente hemos de a\u00f1adir un par\u00e1metros extra con el algoritmo de compresi\u00f3n en la funci\u00f3n/constructor de persistencia: Fastavro y gzip AvroWriter y snappy write_dataframe y snappy writer ( f , schemaParseado , records , 'deflate' ) with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado , 'snappy' ) as writer : write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado , codec = 'snappy' ) Comparando algoritmos de compresi\u00f3n Respecto a la compresi\u00f3n, sobre un fichero de 100GB, podemos considerar media si ronda los 50GB y alta si baja a los 40GB. Algoritmo Velocidad Compresi\u00f3n Gzip Media Media Bzip2 Lenta Alta Snappy Alta Media M\u00e1s que un tema de espacio, necesitamos que los procesos sean eficientes y por eso priman los algoritmos que son m\u00e1s r\u00e1pidos. Si te interesa el tema, es muy interesante el art\u00edculo Data Compression in Hadoop . Por ejemplo, si realizamos el ejemplo de Fast Avro y Pandas con acceso local obtenemos los siguientes tama\u00f1os: Sin compresi\u00f3n: 6,9 MiB Gzip: 1,9 MiB Snappy: 2,8 MiB","title":"Comprimiendo los datos"},{"location":"sa/04formatos.html#parquet","text":"Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas. Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc...","title":"Parquet"},{"location":"sa/04formatos.html#parquet-y-python","text":"Para interactuar con el formato Parquet mediante Python, la librer\u00eda m\u00e1s utilizada es la que ofrece Apache Arrow , en concreto la librer\u00eda PyArrow . As\u00ed pues, la instalamos mediante pip: pip install pyarrow Apache Arrow usa un tipo de estructura denominada tabla para almacenar los datos bidimentsional (ser\u00eda muy similar a un dataframe de Pandas ). La documentaci\u00f3n de PyArrow dispone de un libro de recetas con ejemplos con c\u00f3digo para los diferentes casos de uso que se nos puedan plantear. Vamos a simular el mismo ejemplo que hemos realizado previamente mediante Avro , y vamos a crear un fichero en formato JSON con empleados, y tras persistirlo en formato Parquet , lo vamos a recuperar: Empleados en columnas Empleados en Filas dict-parquet.py import pyarrow.parquet as pq import pyarrow as pa # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Almacenamos los empleados por columnas empleados = { \"nombre\" : [ \"Carlos\" , \"Juan\" ], \"altura\" : [ 180 , 44 ], \"edad\" : [ None , 34 ]} # 3.- Creamos una tabla Arrow y la persistimos mediante Parquet tabla = pa . Table . from_pydict ( empleados , schema ) pq . write_table ( tabla , 'empleados.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) Para que pyarrow pueda leer los empleados como documentos JSON, a d\u00eda de hoy s\u00f3lo puede hacerlo leyendo documentos individuales almacenados en fichero: Por lo tanto, creamos el fichero empleados.json con la siguiente informaci\u00f3n: empleados.json { \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 } { \"nombre\" : \"Juan\" , \"altura\" : 175 } De manera que podemos leer los datos JSON y persistirlos en Parquet del siguiente modo: json-parquet.py import pyarrow.parquet as pq import pyarrow as pa from pyarrow import json # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Leemos los empleados tabla = json . read_json ( \"empleados.json\" ) # 3.- Persistimos la tabla en Parquet pq . write_table ( tabla , 'empleados-json.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados-json.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados-json.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) En ambos casos obtendr\u00edamos algo similar a: Schema del fichero empleados.parquet: nombre: string altura: int32 edad: int32 Tabla de Empleados: pyarrow.Table nombre: string altura: int32 edad: int32 ---- nombre: [[\"Carlos\",\"Juan\"]] altura: [[180,44]] edad: [[null,34]]","title":"Parquet y Python"},{"location":"sa/04formatos.html#parquet-y-pandas","text":"En el caso del uso de Pandas el c\u00f3digo todav\u00eda se simplifica m\u00e1s. Si reproducimos el mismo ejemplo que hemos realizado con Avro tenemos que los Dataframes ofrecen el m\u00e9todo to_parquet para exportar a un fichero Parquet : csv-parquet.py import pandas as pd df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # A partir de un DataFrame, persistimos los datos df . to_parquet ( 'sales.parquet' ) Si quisi\u00e9ramos almacenar el archivo directamente en HDFS, necesitamos indicarle a Pandas la direcci\u00f3n del sistema de archivos que tenemos configurado en core-site.xml : core-site.ml <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> As\u00ed pues, \u00fanicamente necesitamos modificar el nombre del archivo donde serializamos los datos a Parquet : df . to_parquet ( 'hdfs://iabd-virtualbox:9000/sales.parquet' )","title":"Parquet y Pandas"},{"location":"sa/04formatos.html#comparando-tamanos","text":"Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiBa","title":"Comparando tama\u00f1os"},{"location":"sa/04formatos.html#referencias","text":"Handling Avro files in Python","title":"Referencias"},{"location":"sa/04formatos.html#actividades","text":"Mediante Python , carga los datos de los taxis XXX y crea dentro de /user/iabd/datos los siguientes archivos con el formato adecuado: taxis.avro : la fecha ( tpep_pickup_datetime ), el VendorID y el coste de cada viaje ( total_amount ) (opcional) taxis.parquet con los mismos atributos.","title":"Actividades"},{"location":"sa/05mongo-avanzado.html","text":"MongoDB Avanzado \u00b6 En esta sesi\u00f3n estudiaremos como realizar consultas agregadas utilizando el framework de agregaci\u00f3n, y temas relacionados con la escalibilidad del sistema mediante la replicaci\u00f3n y el particionado de los datos. Agregaciones \u00b6 Para poder agrupar datos y realizar c\u00e1lculos sobre \u00e9stos, MongoDB ofrece diferentes alternativas: Mediante operaciones Map-reduce con la operaci\u00f3n mapreduce() cuyo uso est\u00e1 deprecated desde MongoBD 5.0 . Mediante el uso conjunto de $function y $accumulator que permiten definir expresiones de agregaci\u00f3n mediante JavaScript . Mediante operaciones de agrupaci\u00f3n sencilla, como pueden ser las operaciones count() o distinct() . Mediante el uso del Aggregation Framework , basado en el uso de pipelines , el cual permite realizar diversas operaciones sobre los datos. Este framework es mecanismo m\u00e1s eficiente y usable para la realizaci\u00f3n de agregaciones, y por tanto, en el que nos vamos a centrar en esta sesi\u00f3n. Para ello, a partir de una colecci\u00f3n, mediante el m\u00e9todo aggregate le pasaremos un array con las fases a realizar: db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , numProductos : { $sum : 1 }} }, { $sort : { numProductos :- 1 }} ]) Pipeline de agregaci\u00f3n \u00b6 Las agregaciones usan un pipeline, conocido como Aggregation Pipeline , de ah\u00ed el uso de un array con [ ] donde cada elemento es una fase del pipeline , de modo que la salida de una fase es la entrada de la siguiente: db . coleccion . aggregate ([ op1 , op2 , ... opN ]) El resultado del pipeline es un documento y por lo tanto est\u00e1 sujeto a la restricci\u00f3n de BSON, que limita su tama\u00f1o a 16MB. En la siguiente imagen se resumen los pasos de una agrupaci\u00f3n donde primero se eligen los elementos que vamos a agrupar mediante $match y posteriormente se agrupan con $group para hacer $sum sobre el total: Ejemplo de pipeline con $match y $group Operadores del pipeline \u00b6 Antes de nada destacar que las fases se pueden repetir, por lo que una consulta puede repetir operadores . A continuaci\u00f3n vamos a estudiar todos estos operadores: Operador Descripci\u00f3n Cardinalidad $project Proyecci\u00f3n de campos, es decir, propiedades en las que estamos interesados. Tambi\u00e9n nos permite modificar un documento, o crear un subdocumento (reshape) 1:1 $match Filtrado de campos, similar a where N:1 $group Para agrupar los datos, similar a group by N:1 $sort Ordenar 1:1 $skip Saltar N:1 $limit Limitar los resultados N:1 $unwind Separa los datos que hay dentro de un array 1:N Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de productos (productos.js) de un tienda de electr\u00f3nica con las caracter\u00edsticas y precios de los mismos. Un ejemplo de un producto ser\u00eda: > db . productos . findOne () { \"_id\" : ObjectId ( \"5345afc1176f38ea4eda4787\" ), \"nombre\" : \"iPad 16GB Wifi\" , \"fabricante\" : \"Apple\" , \"categoria\" : \"Tablets\" , \"precio\" : 499 } Para cargar este archivo desde la consola podemos realizar: mongosh < productos.js $group \u00b6 Agrupa los documentos con el prop\u00f3sito de calcular valores agregrados de una colecci\u00f3n de documentos. Por ejemplo, podemos usar $group para calcular la media de p\u00e1ginas visitas de manera diaria. La salida de $group esta desordenada La salida de $group depende de como se definan los grupos. Se empieza especificando un identificador (por ejemplo, un campo _id) para el grupo que creamos con el pipeline. Para este campo_id, podemos especificar varias expresiones, incluyendo un \u00fanico campo proveniente de un documento del pipeline, un valor calculado de una fase anterior, un documento con muchos campos y otras expresiones v\u00e1lidas, tales como constantes o campos de subdocumentos. Tambi\u00e9n podemos usar operadores de $project para el campo_id. Cuando referenciemos al valor de un campo lo haremos poniendo entre comillas un $ delante del nombre del campo. As\u00ed pues, para referenciar al fabricante de un producto lo haremos mediante $fabricante. > db . productos . aggregate ([{ $group : { _id : \"$fabricante\" , total : { $sum : 1 } } }]) { \"_id\" : \"Sony\" , \"total\" : 1 } { \"_id\" : \"Amazon\" , \"total\" : 2 } { \"_id\" : \"Google\" , \"total\" : 1 } { \"_id\" : \"Samsung\" , \"total\" : 2 } { \"_id\" : \"Apple\" , \"total\" : 4 } Si lo que queremos es que el valor del identificador contenga un objeto, lo podemos hacer asociandolo como valor: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, total : { $sum : 1 } } }]) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"total\" : 2 } { \"_id\" : { \"empresa\" : \"Sony\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"total\" : 2 } { \"_id\" : { \"empresa\" : \"Google\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"total\" : 4 } Tambi\u00e9n podemos agrupar m\u00e1s de un atributo, de tal modo que tengamos un _id compuesto. Por ejemplo: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" , \"tipo\" : \"$categoria\" }, total : { $sum : 1 } } }]) { \"_id\" : { \"empresa\" : \"Amazon\" , \"tipo\" : \"Tablets\" }, \"total\" : 2 } { \"_id\" : { \"empresa\" : \"Google\" , \"tipo\" : \"Tablets\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Apple\" , \"tipo\" : \"Port\u00e1tiles\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Sony\" , \"tipo\" : \"Port\u00e1tiles\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Samsung\" , \"tipo\" : \"Tablets\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Samsung\" , \"tipo\" : \"Smartphones\" }, \"total\" : 1 } { \"_id\" : { \"empresa\" : \"Apple\" , \"tipo\" : \"Tablets\" }, \"total\" : 3 } Cada expresi\u00f3n de $group debe especificar un campo _id. Acumuladores \u00b6 Adem\u00e1s del campo _id , la expresi\u00f3n $group puede incluir campos calculados. Estos otros campos deben utilizar uno de los siguientes acumuladores. Nombre Descripci\u00f3n $addToSet Devuelve un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones) $first Devuelve el primer valor del grupo. Se suele usar despu\u00e9s de ordenar. $last Devuelve el \u00faltimo valor del grupo. Se suele usar despu\u00e9s de ordenar. $max Devuelve el mayor valor de un grupo $min Devuelve el menor valor de un grupo. $avg Devuelve el promedio de todos los valores de un grupo $push Devuelve un array con todos los valores del campo seleccionado entre cada documento del grupo (puede haber repeticiones) $sum Devuelve la suma de todos los valores del grupo A continuaci\u00f3n vamos a ver ejemplos de cada uno de estos acumuladores. $sum \u00b6 El operador $sum acumula los valores y devuelve la suma. Por ejemplo, para obtener el montante total de los prodyctos agrupados por fabricante, har\u00edamos: Agrupaci\u00f3n con $sum db.productos.aggregate([{ $group: { _id: { \"empresa\":\"$fabricante\" }, totalPrecio: {$sum:\"$precio\"} } }]) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"totalPrecio\" : 328 } { \"_id\" : { \"empresa\" : \"Sony\" }, \"totalPrecio\" : 499 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"totalPrecio\" : 1014.98 } { \"_id\" : { \"empresa\" : \"Google\" }, \"totalPrecio\" : 199 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"totalPrecio\" : 2296 } $avg \u00b6 Mediante $avg podemos obtener el promedio de los valores de un campo num\u00e9rico. Por ejemplo, para obtener el precio medio de los productos agrupados por categor\u00eda, har\u00edamos: db.productos.aggregate([{ $group: { _id: { \"categoria\":\"$categoria\" }, precioMedio: {$avg:\"$precio\"} } }]) { \"_id\" : { \"categoria\" : \"Port\u00e1tiles\" }, \"precioMedio\" : 499 } { \"_id\" : { \"categoria\" : \"Smartphones\" }, \"precioMedio\" : 563.99 } { \"_id\" : { \"categoria\" : \"Tablets\" }, \"precioMedio\" : 396.4271428571428 } $addToSet Mediante addToSet obtendremos un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones). Por ejemplo, para obtener para cada empresa las categor\u00edas en las que tienen productos, har\u00edamos: db.productos.aggregate([{ $group: { _id: { \"fabricante\":\"$fabricante\" }, categorias: {$addToSet:\"$categoria\"} } }]) { \"_id\" : { \"fabricante\" : \"Amazon\" }, \"categorias\" : [ \"Tablets\" ] } { \"_id\" : { \"fabricante\" : \"Sony\" }, \"categorias\" : [ \"Port\u00e1tiles\" ] } { \"_id\" : { \"fabricante\" : \"Samsung\" }, \"categorias\" : [ \"Tablets\", \"Smartphones\" ] } { \"_id\" : { \"fabricante\" : \"Google\" }, \"categorias\" : [ \"Tablets\" ] } { \"_id\" : { \"fabricante\" : \"Apple\" }, \"categorias\" : [ \"Port\u00e1tiles\", \"Tablets\" ] } $push Mediante $push tambi\u00e9n obtendremos un array con todos los valores para los campos seleccionados entre cada documento del grupo, pero con repeticiones. Es decir, funciona de manera similar a $addToSet pero permitiendo elementos repetidos. Por ello, si reescribimos la consulta anterior pero haciendo uso de $push obtendremos categor\u00edas repetidas: db.productos.aggregate([{ $group: { _id: { \"empresa\":\"$fabricante\" }, categorias: {$push:\"$categoria\"} } }]) { \"_id\" : {\"empresa\" : \"Amazon\"}, \"categorias\" : [\"Tablets\", \"Tablets\"] } { \"_id\" : {\"empresa\" : \"Sony\"}, \"categorias\" : [\"Port\u00e1tiles\"] } { \"_id\" : {\"empresa\" : \"Samsung\"}, \"categorias\" : [\"Smartphones\", \"Tablets\"] } { \"_id\" : {\"empresa\" : \"Google\"}, \"categorias\" : [\"Tablets\"] } { \"_id\" : {\"empresa\" : \"Apple\"}, \"categorias\" : [\"Tablets\", \"Tablets\", \"Tablets\", \"Port\u00e1tiles\"] } $max y $min Los operadores $max y $min permiten obtener el mayor y el menor valor, respectivamente, del campo por el que se agrupan los documentos. Por ejemplo, para obtener el precio del producto m\u00e1s caro que tiene cada empresa har\u00edamos: db.productos.aggregate([{ $group: { _id: { \"empresa\":\"$fabricante\" }, precioMaximo: {$max:\"$precio\"}, precioMinimo: {$min:\"$precio\"}, } }]) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"precioMaximo\" : 199, \"precioMinimo\" : 129 } { \"_id\" : { \"empresa\" : \"Sony\" }, \"precioMaximo\" : 499, \"precioMinimo\" : 499 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"precioMaximo\" : 563.99, \"precioMinimo\" : 450.99 } { \"_id\" : { \"empresa\" : \"Google\" }, \"precioMaximo\" : 199, \"precioMinimo\" : 199 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"precioMaximo\" : 699, \"precioMinimo\" : 499 } Doble $group Si queremos obtener el resultado de una agrupaci\u00f3n podemos aplicar el operador $group sobre otro $group. Por ejemplo, para obtener el precio medio de los precios medios de los tipos de producto por empresa har\u00edamos: db.productos.aggregate([ {$group: { _id: { \"empresa\":\"$fabricante\", \"categoria\":\"$categoria\" }, precioMedio: {$avg:\"$precio\"} } }, {$group: { _id: \"$_id.empresa\", precioMedio: {$avg: \"$precioMedio\"} } } ]) { \"_id\" : \"Samsung\", \"precioMedio\" : 507.49 } { \"_id\" : \"Sony\", \"precioMedio\" : 499 } { \"_id\" : \"Apple\", \"precioMedio\" : 549 } { \"_id\" : \"Google\", \"precioMedio\" : 199 } { \"_id\" : \"Amazon\", \"precioMedio\" : 164 } Precio medio por empresa y categor\u00eda Precio medio por empresa en base al precio medio anterior $first y $last Estos operadores devuelven el valor resultante de aplicar la expresi\u00f3n al primer/\u00faltimo elemento de un grupo de elementos que comparten el mismo grupo por clave. Por ejemplo, para obtener para cada empresa, cual es el tipo de producto que m\u00e1s tiene y la cantidad de dicho tipo har\u00edamos: db.productos.aggregate([ {$group: { _id: { \"empresa\": \"$fabricante\", \"tipo\" : \"$categoria\" }, total: {$sum:1} } }, {$sort: {\"total\":-1}}, {$group: { _id:\"$_id.empresa\", producto: {$first: \"$_id.tipo\" }, cantidad: {$first:\"$total\"} } } ]) { \"_id\" : \"Samsung\", \"producto\" : \"Tablets\", \"cantidad\" : 1 } { \"_id\" : \"Sony\", \"producto\" : \"Port\u00e1tiles\", \"cantidad\" : 1 } { \"_id\" : \"Amazon\", \"producto\" : \"Tablets\", \"cantidad\" : 2 } { \"_id\" : \"Google\", \"producto\" : \"Tablets\", \"cantidad\" : 1 } { \"_id\" : \"Apple\", \"producto\" : \"Tablets\", \"cantidad\" : 3 } Agrupamos por empresa y categor\u00eda de producto Al agrupar por empresa, elegimos la categor\u00eda producto que tiene m\u00e1s unidades M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/operator/aggregation/first/ y http://docs.mongodb.org/manual/reference/operator/aggregation/last/ $project \u00b6 Si queremos realizar una proyecci\u00f3n sobre el conjunto de resultados y quedarnos con un subconjunto de los campos usaremos el operador $project. Como resultado obtendremos el mismo n\u00famero de documentos, y en el orden indicado en la proyecci\u00f3n. La proyecci\u00f3n dentro del framework de agregaci\u00f3n es mucho m\u00e1s potente que dentro de las consultas normales. Se emplea para: renombrar campos. introducir campos calculados en el documento resultante, mediante $add, $substract, $multiply, $divide o $mod transformar campos a may\u00fasculas $toUpper o min\u00fasculas $toLower, concatenar campos mediante $concat u obtener subcadenas con $substr. transformar campos en base a valores obtenidos a partir de una condici\u00f3n mediante expresiones l\u00f3gicas con los operadores de comparaci\u00f3n vistos en las consultas. > db . productos . aggregate ([ { $project : { _id : 0 , 'empresa' : { $toUpper : \"$fabricante\" }, 'detalles' : { 'categoria' : \"$categoria\" , 'precio' : { \"$multiply\" : [ \"$precio\" , 1.1 ]} }, 'elemento' : '$nombre' } } ]) { \"empresa\" : \"APPLE\" , \"detalles\" : { \"categoria\" : \"Tablets\" , \"precio\" : 548.9000000000001 }, \"elemento\" : \"iPad 16GB Wifi\" } { \"empresa\" : \"APPLE\" , \"detalles\" : { \"categoria\" : \"Tablets\" , \"precio\" : 658.9000000000001 }, \"elemento\" : \"iPad 32GB Wifi\" } Transforma un campo y lo pasa a may\u00fasculas Crea un documento anidado Incrementa el precio el 10% Renombra el campo M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/operator/aggregation/project/ $match \u00b6 Se utiliza principalmente para filtrar los documentos que pasar\u00e1n a la siguiente etapa del pipeline o a la salida final. Por ejemplo, para seleccionar s\u00f3lo las tabletas har\u00edamos: > db . productos . aggregate ([{ $match : { categoria : \"Tablets\" }}]) Aparte de igualar un valor a un campo, podemos emplear los operadores usuales de consulta, como $gt, $lt, $in, etc\u2026\u200b Se recomienda poner el operador match al principio del pipeline para limitar los documentos a procesar en siguientess fases. Si usamos este operador como primera fase podremos hacer uso de los indices de la colecci\u00f3n de una manera eficiente. As\u00ed pues, para obtener la cantidad de Tablets de menos de 500 euros har\u00edamos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" , precio : { $lt : 500 }}}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, cantidad : { $sum : 1 }} }] ) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"cantidad\" : 2 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"cantidad\" : 1 } { \"_id\" : { \"empresa\" : \"Google\" }, \"cantidad\" : 1 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"cantidad\" : 1 } M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/operator/aggregation/match/ $sort \u00b6 El operador $sort ordena los documentos recibidos por el campo y el orden indicado por la expresi\u00f3n indicada al pipeline. Por ejemplo, para ordenar los productos por precio descendentemente har\u00edamos: > db . productos . aggregate ({ $sort : { precio :- 1 }}) El operador $sort ordena los datos en memoria, por lo que hay que tener cuidado con el tama\u00f1o de los datos. Por ello, se emplea en las \u00faltimas fases del pipeline, cuando el conjunto de resultados es el menor posible. Si retomamos el ejemplo anterior, y ordenamos los datos por el precio total tenemos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" }}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, totalPrecio : { $sum : \"$precio\" }} }, { $sort : { totalPrecio :- 1 }} ]) { \"_id\" : { \"empresa\" : \"Apple\" }, \"totalPrecio\" : 1797 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"totalPrecio\" : 450.99 } { \"_id\" : { \"empresa\" : \"Amazon\" }, \"totalPrecio\" : 328 } { \"_id\" : { \"empresa\" : \"Google\" }, \"totalPrecio\" : 199 } Al ordenar los datos, referenciamos al campo que hemos creado en la fase de $group M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/operator/aggregation/sort/ $skip y $limit \u00b6 El operador $limit \u00fanicamente limita el n\u00famero de documentos que pasan a trav\u00e9s del pipeline. El operador recibe un n\u00famero como par\u00e1metro: > db . productos . aggregate ([{ $limit : 3 }]) Este operador no modifica los documentos, s\u00f3lo restringe quien pasa a la siguiente fase. De manera similar, con el operador $skip, saltamos un n\u00famero determinado de documentos: > db . productos . aggregate ([{ $skip : 3 }]) El orden en el que empleemos estos operadores importa, y mucho, ya que no es lo mismo saltar y luego limitar, donde la cantidad de elementos la fija $limit: > db . productos . aggregate ([{ $skip : 2 },{ $limit : 4 }]) { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6e7\" ), \"nombre\" : \"iPad 64GB Wifi\" , \"categoria\" : \"Tablets\" , \"fabricante\" : \"Apple\" , \"precio\" : 699 } { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6e8\" ), \"nombre\" : \"Galaxy S3\" , \"categoria\" : \"Smartphones\" , \"fabricante\" : \"Samsung\" , \"precio\" : 563.99 } { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6e9\" ), \"nombre\" : \"Galaxy Tab 10\" , \"categoria\" : \"Tablets\" , \"fabricante\" : \"Samsung\" , \"precio\" : 450.99 } { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6ea\" ), \"nombre\" : \"Vaio\" , \"categoria\" : \"Port\u00e1tiles\" , \"fabricante\" : \"Sony\" , \"precio\" : 499 } En cambio, si primero limitamos y luego saltamos, la cantidad de elementos se obtiene de la diferencia entre el l\u00edmite y el salto: > db . productos . aggregate ([{ $limit : 4 },{ $skip : 2 }]) { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6e7\" ), \"nombre\" : \"iPad 64GB Wifi\" , \"categoria\" : \"Tablets\" , \"fabricante\" : \"Apple\" , \"precio\" : 699 } { \"_id\" : ObjectId ( \"54ffff889836d613eee9a6e8\" ), \"nombre\" : \"Galaxy S3\" , \"categoria\" : \"Smartphones\" , \"fabricante\" : \"Samsung\" , \"precio\" : 563.99 } M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/operator/aggregation/limit/ y http://docs.mongodb.org/manual/reference/operator/aggregation/skip/ $unwind \u00b6 Este operador es muy interesante y se utiliza solo con operadores array. Al usarlo con un campo array de tama\u00f1o N en un documento, lo transforma en N documentos con el campo tomando el valor individual de cada uno de los elementos del array. Si retomamos el ejemplo de la segunda sesi\u00f3n donde actualizabamos una colecci\u00f3n de enlaces, ten\u00edamos un enlace con la siguiente informaci\u00f3n: db.enlaces.findOne() { \"_id\" : ObjectId(\"54f9769212b1897ae84190cf\"), \"titulo\" : \"www.google.es\", \"tags\" : [ \"mapas\", \"videos\", \"blog\", \"calendario\", \"email\", \"mapas\" ] } Podemos observar como el campo tags contiene 6 valores dentro del array (con un valor repetido). A continuaci\u00f3n vamos a desenrollar el array: > db . enlaces . aggregate ( { $match : { titulo : \"www.google.es\" }}, { $unwind : \"$tags\" }) { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"mapas\" } { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"videos\" } { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"blog\" } { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"calendario\" } { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"email\" } { \"_id\" : ObjectId ( \"54f9769212b1897ae84190cf\" ), \"titulo\" : \"www.google.es\" , \"tags\" : \"mapas\" } As\u00ed pues hemos obtenido 6 documentos con el mismo _id y titulo , es decir , un documento por elemento del array . De este modo, podemos realizar consultas que sumen/cuenten los elementos del array. Por ejemplo, si queremos obtener las 3 etiquetas que m\u00e1s aparecen en todos los enlaces har\u00edamos: > db . enlaces . aggregate ([ { \"$unwind\" : \"$tags\" }, { \"$group\" : { \"_id\" : \"$tags\" , \"total\" : { $sum : 1 } } }, { \"$sort\" : { \"total\" :- 1 }}, { \"$limit\" : 3 } ]) { \"_id\" : \"mapas\" , \"total\" : 3 } { \"_id\" : \"email\" , \"total\" : 2 } { \"_id\" : \"calendario\" , \"total\" : 1 } Doble $unwind Si trabajamos con documentos que tienen varios arrays, podemos necesitar desenrollar los dos array. Al hacer un doble unwind se crea un producto cartesiano entre los elementos de los 2 arrays. Supongamos que tenemos los datos del siguiente inventario de ropa: db.inventario.drop(); db.inventario.insert({'nombre':\"Camiseta\", 'tallas':[\"S\", \"M\", \"L\"], 'colores':['azul', 'blanco', 'naranja', 'rojo']}) db.inventario.insert({'nombre':\"Jersey\", 'tallas':[\"S\", \"M\", \"L\", \"XL\"], 'colores':['azul', 'negro', 'naranja', 'rojo']}) db.inventario.insert({'nombre':\"Pantalones\", 'tallas':[\"32x32\", \"32x30\", \"36x32\"], 'colores':['azul', 'blanco', 'naranja', 'negro']}) Para obtener un listado de cantidad de pares talla/color har\u00edamos: > db . inventario . aggregate ([ { $unwind : \"$tallas\" }, { $unwind : \"$colores\" }, { $group : { '_id' : { 'talla' : '$tallas' , 'color' : '$colores' }, 'total' : { '$sum' : 1 } } } ]) { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"rojo\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"L\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"M\" , \"color\" : \"negro\" }, \"total\" : 1 } De SQL al Pipeline de agregaciones \u00b6 Ya hemos visto que el pipeline ofrece operadores para realizar la misma funcionalidad de agrupaci\u00f3n que ofrece SQL. Si relacionamos los comandos SQL con el pipeline de agregaciones tenemos las siguientes equivalencias: Table 3. Equivalencia con SQL SQL Pipeline de Agregaciones WHERE $match GROUP BY $group HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum Podemos encontrar ejemplos de consultas SQL transformadas al pipeline en http://docs.mongodb.org/manual/reference/sql-aggregation-comparison/ Limitaciones \u00b6 Hay que tener en cuenta las siguiente limitaciones: En versiones anteriores a la 2.6, el pipeline devolv\u00eda en cada fase un objeto BSON, y por tanto, el resultado estaba limitado a 16MB Las fases tienen un l\u00edmite de 100MB en memor\u00eda. Si una fase excede dicho l\u00edmite, se producir\u00e1 un error. En este caso, hay que habilitar el uso de disco mediante allowDiskUse en las opciones de la agregaci\u00f3n. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/method/db.collection.aggregate Agregaciones con Compass \u00b6 Using Compass' Aggregation Pipeline Builder feature https://docs.mongodb.com/compass/master/aggregation-pipeline-builder/ , we can easily create, delete and rearrange the stages in a pipeline, and evaluate the output documents in real time. Then we can produce a version of that pipeline in Python, Java, C# or Node.js, using Compass' Export-to-Language feature. https://docs.mongodb.com/compass/master/export-pipeline-to-language Replicaci\u00f3n \u00b6 Un aspecto muy importante de MongoDB es que soporta la replicaci\u00f3n de los datos de forma nativa mediante el uso de conjuntos de r\u00e9plicas. FIXME: pensar como explicarlo via MongoAtlas... nada de instalar. con Docker Compose \u00bfse podr\u00eda hacer f\u00e1cil? Conjunto de r\u00e9plicas \u00b6 En MongoDB se replican los datos mediante un conjunto de r\u00e9plicas ( Replica Set ) el cual es un grupo de servidores (nodos mongod) donde hay uno que ejerce la funci\u00f3n de primario y por tanto recibe las peticiones de los clientes, y el resto de servidores hace de secundarios, manteniendo copias de los datos del primario. rs.status() rs.initiate({\"_id\": \"replicaTest\", members: [ {_id: 0, host: \"127.0.0.1:27017\" }, { _id: 1, host: \"127.0.0.1:27018\" }, {_id: 2, host: \"127.0.0.1:27019\", arbiterOnly:true }] }) rs.add(\"mongodbd1.example.net:27017\") rs.addArb(\"mongodbd2.example.net:27017\") rs.remove(\"mongodbd1.example.net:27017\") rs.conf() rs.isMaster() rs.printReplicationInfo() rs.printSlaveReplicationInfo() rs.reconfig( ) rs.slaveOk() rs.stepDown(20, 5) // (stepDownSecs, secondaryCatchUpPeriodSecs) Conjunto de R\u00e9plicas Figure 2. Conjunto de R\u00e9plicas Si el nodo primario se cae, los secundarios eligen un nuevo primario entre ellos mismos, en un proceso que se conoce como votaci\u00f3n. La aplicaci\u00f3n se conectar\u00e1 al nuevo primario de manera transparente. Cuando el antiguo nodo primario vuelva en s\u00ed, ser\u00e1 un nuevo nodo secundario. Arbitraje de un secundario Figure 3. Arbitraje de un secundario Al usar replicaci\u00f3n, si un servidor se cae, siempre vamos a poder obtener los datos a partir de otros servidores del conjunto. Si los datos de un servidor se da\u00f1an o son inaccesibles, podemos crear una nueva copia desde uno de los miembros del conjunto. Elementos de un conjunto de r\u00e9plicas \u00b6 Los tipos de nodos que podemos encontrar en un conjunto de r\u00e9plica son: Regular: Es el tipo de nodo m\u00e1s com\u00fan. Primario: Acepta todas las operaciones de escritura de los clientes. Cada conjunto de r\u00e9plicas tendr\u00e1 s\u00f3lo un primario, y como s\u00f3lo un miembro acepta operaciones de escritura, ofrece consitencia estricta para todas las lecturas realizadas desde \u00e9l. Secundario: Los secundarios replican el oplog primario y aplican las operaciones a sus conjuntos de datos. De este modo, los nodos secundarios son un espejo del primario. Si el primario deja de estar disponible, el conjunto de r\u00e9plica elegir\u00e1 a un secundario para que sea el nuevo primario, mediante un proceso de votaci\u00f3n. Por defecto, los clientes realizan las lecturas desde el nodo primario. Sin embargo, los clientes pueden indicar que quieren realizar lecturas desde los nodos secundarios. Es posible que al realizar lecturas de un nodo secundario la informaci\u00f3n que se obtenga no refleje el estado del nodo primario. \u00c1rbitro: se emplea s\u00f3lo para votar. No contiene copia de los datos y no se puede convertir en primario. Los conjuntos de r\u00e9plica pueden tener \u00e1rbitros para a\u00f1adir votos en las elecciones de un nuevo primario. Siempre tienen un voto, y permiten que los conjuntos de r\u00e9plica tengan un n\u00famero impar de nodos, sin la necesidad de tener un miembro que replique los datos. Adem\u00e1s, no requieren hardware dedicado. No ejecutar un \u00e1rbitro en sistemas que tambi\u00e9n ejecutan los miembros primarios y secundarios del conjunto de r\u00e9plicas. S\u00f3lo a\u00f1adir un \u00e1rbitro a un conjunto con un n\u00famero par de miembros. Si se a\u00f1ade un \u00e1rbitro a un conjunto con un n\u00famero impar de miembros, el conjunto puede sufrir un empate. Retrasado (delayed): nodo que se emplea para la recuperaci\u00f3n del sistema ante un fallo. Para ello, hay que asignar la propiedad priority:0. Este nodo nunca ser\u00e1 un nodo primario. Oculto: empleado para anal\u00edticas del sistema. oplog Para soportar la replicaci\u00f3n, el nodo primario almacena todos los cambios en su oplog. De manera simplificada, el oplog es un diario de todos los cambios que la instancia principal realiza en las bases de datos con el prop\u00f3sito de replicar dichos cambios en un nodo secundario para asegurar que las dos bases de datos sean id\u00e9nticas. El servidor principal mantiene el oplog, y el secundario consulta al principal por nuevas entradas que aplicar a sus propias copias de las bases de datos replicadas. El oplog crea un timestamp para cada entrada. Esto permite que un secundario controle la cantidad de informaci\u00f3n que se ha modificado desde una lectura anterior, y qu\u00e9 entradas necesita transferir para ponerse al d\u00eda. Si paramos un secundario y lo reiniciamos m\u00e1s adelante, utilizar\u00e1 el oplog para obtener todos los cambios que ha perdido mientras estaba offline. El oplog se almacena en una colecci\u00f3n limitada (capped) y ordenada de un tama\u00f1o determinado. La opci\u00f3n oplogSize define en MB el tama\u00f1o del archivo. Para un sistema de 64 bits con comportamiento de lectura/escritura normales, el oplogSize deber\u00eda ser de al menos un 5% del espacio de disco disponible. Si el sistema tiene m\u00e1s escrituras que lecturas, puede que necesitemos incrementar este tama\u00f1o para asegurar que cualquier nodo secundario pueda estar offline una cantidad de tiempo razonable sin perder informaci\u00f3n. M\u00e1s informaci\u00f3n de oplog en http://docs.mongodb.org/manual/core/replica-set-oplog/ Creando un conjunto de r\u00e9plicas \u00b6 A la hora de lanzar una instancia, podemos indicarle mediante par\u00e1metros opcionales la siguiente informaci\u00f3n: --dbpath: ruta de la base de datos --port: puerto de la base de datos --replSet: nombre del conjunto de r\u00e9plicas -\u2013fork: indica que se tiene que crear en un hilo --logpath: ruta para almacenar los archivos de log. Normalmente, cada instancia mongod se coloca en un servidor f\u00edsico y todos en el puerto est\u00e1ndar. Como ejemplo vamos a crear un conjunto de tres r\u00e9plicas. Para ello, arrancaremos tres instancias distintas pero que comparten el mismo conjunto de r\u00e9plicas. Adem\u00e1s, en vez de hacerlo en tres m\u00e1quinas distinas, lo haremos en tres puertos diferentes: Las carpeta que se crean tienen que tener los mismos permisos que mongod. Si no existiesen, las tenemos que crear previamente. Script de creaci\u00f3n del conjunto de r\u00e9plicas - (creaConjuntoReplicas.sh) # !/bin/bash mkdir -p /data/db/rs1 /data/db/rs2 /data/db/rs3 /data/logs mongod --replSet replicaExperto --logpath /data/logs/rs1.log --dbpath /data/db/rs1 --port 27017 --oplogSize 64 --smallfiles --fork mongod --replSet replicaExperto --logpath /data/logs/rs2.log --dbpath /data/db/rs2 --port 27018 --oplogSize 64 --smallfiles --fork mongod --replSet replicaExperto --logpath /data/logs/rs3.log --dbpath /data/db/rs3 --port 27019 --oplogSize 64 --smallfiles --fork Y lo lanzamos desde el shell mediante: Lanzando la creaci\u00f3n de la r\u00e9plica bash < creaConjuntoReplicas.sh Al lanzar el script, realmente estamos creando las r\u00e9plicas, por lo que obtendremos que ha creado hijos y que esta a la espera de conexiones: Resultado de crear el conjunto de replicas about to fork child process, waiting until server is ready for connections. forked process: 1811 child process started successfully, parent exiting about to fork child process, waiting until server is ready for connections. forked process: 1814 child process started successfully, parent exiting about to fork child process, waiting until server is ready for connections. forked process: 1817 child process started successfully, parent exiting Una vez lanzados las tres r\u00e9plicas, tenemos que enlazarlas. As\u00ed pues, nos conectaremos al shell de mongo. Puede ser que necesitemos indicar que nos conectamos al puerto adecuado: mongosh --port 27017 Para comprobar su estado emplearemos el comando rs.status(): rs.status() { \"startupStatus\" : 3, \"info\" : \"run rs.initiate(...) if not yet done for the set\", \"ok\" : 0, \"errmsg\" : \"can't get local.system.replset config from self or any seed (EMPTYCONFIG)\" } Dentro del shell de mongo, los comandos que trabajan con r\u00e9plicas comienzan por el prefijo rs.. Mediante rs.help() obtendremos la ayuda de los m\u00e9todos disponibles, A continuaci\u00f3n, crearemos un documento con la configuraci\u00f3n donde el _id tiene que ser igual al usado al crear la r\u00e9plica, y el array de members contiene las replicas creadas donde los puertos han de coincidir. Configurando el conjunto de r\u00e9plicas config = { _id: \"replicaExperto\", members:[ {_id : 0, host : \"localhost:27017\"}, { _id : 1, host : \"localhost:27018\"}, {_id : 2, host : \"localhost:27019\"} ]}; Si en los miembros ponemos slaveDelay: numSeg podemos retrasar un nodo respecto al resto (tambi\u00e9n deberemos indicar que priority : 0 para que no sea un nodo principal). M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/core/replica-set-delayed-member/ Tras crear el documento de configuraci\u00f3n, podemos iniciar el conjunto mediante: > rs . initiate ( config ) { \"info\" : \"Config now saved locally. Should come online in about a minute.\" , \"ok\" : 1 } Si ahora volvemos a consultar el estado de la r\u00e9plica tendremos: replicaExperto:PRIMARY> rs.status() { \"set\" : \"replicaExperto\", \"date\" : ISODate(\"2016-02-09T17:57:52.273Z\"), \"myState\" : 1, \"term\" : NumberLong(1), \"heartbeatIntervalMillis\" : NumberLong(2000), \"members\" : [ { \"_id\" : 0, \"name\" : \"localhost:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 89, \"optime\" : { \"ts\" : Timestamp(1455040665, 2), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2016-02-09T17:57:45Z\"), \"infoMessage\" : \"could not find member to sync from\", \"electionTime\" : Timestamp(1455040665, 1), \"electionDate\" : ISODate(\"2016-02-09T17:57:45Z\"), \"configVersion\" : 1, \"self\" : true }, { \"_id\" : 1, \"name\" : \"localhost:27018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 17, \"optime\" : { \"ts\" : Timestamp(1455040665, 2), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2016-02-09T17:57:45Z\"), \"lastHeartbeat\" : ISODate(\"2016-02-09T17:57:51.287Z\"), \"lastHeartbeatRecv\" : ISODate(\"2016-02-09T17:57:47.860Z\"), \"pingMs\" : NumberLong(0), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 }, { \"_id\" : 2, \"name\" : \"localhost:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 17, \"optime\" : { \"ts\" : Timestamp(1455040665, 2), \"t\" : NumberLong(1) }, \"optimeDate\" : ISODate(\"2016-02-09T17:57:45Z\"), \"lastHeartbeat\" : ISODate(\"2016-02-09T17:57:51.287Z\"), \"lastHeartbeatRecv\" : ISODate(\"2016-02-09T17:57:47.869Z\"), \"pingMs\" : NumberLong(1), \"syncingTo\" : \"localhost:27017\", \"configVersion\" : 1 } ], \"ok\" : 1 } La pr\u00f3xima vez que lancemos las r\u00e9plicas ya no deberemos configurarlas. As\u00ed pues, el proceso de enlazar e iniciar las r\u00e9plicas s\u00f3lo se realiza una vez. Trabajando con las r\u00e9plicas \u00b6 Una vez que hemos visto que las tres r\u00e9plicas est\u00e1n funcionando, vamos a comprobar como podemos trabajar con ellas. Para ello, nos conectamos al nodo principal (al ser el puerto predeterminado, podemos omitirlo): $ mongosh --port 27017 Al conectarnos al nodo principal, nos aparece como s\u00edmbolo del shell el nombre del conjunto de la r\u00e9plica seguido de dos puntos y PRIMARY si nos hemos conectado al nodo principal, o SECONDARY en caso contrario. replicaExperto:PRIMARY> Para saber si nos hemos conectado al nodo correcto, mediante rs.isMaster() obtendremos el tipo del nodo (propiedad ismaster) e informaci\u00f3n sobre el resto de nodos: replicaExperto:PRIMARY> rs.isMaster() { \"hosts\" : [ \"localhost:27017\", \"localhost:27018\", \"localhost:27019\" ], \"setName\" : \"replicaExperto\", \"setVersion\" : 1, \"ismaster\" : true, \"secondary\" : false, \"primary\" : \"localhost:27017\", \"me\" : \"localhost:27017\", \"electionId\" : ObjectId(\"56ba28990000000000000001\"), \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000, \"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2016-02-09T18:00:46.397Z\"), \"maxWireVersion\" : 4, \"minWireVersion\" : 0, \"ok\" : 1 } Ahora que sabemos que estamos en el nodo principal, vamos a insertar datos. Para ello, vamos a insertar 100 documentos: Insertamos 100 documentos sobre replicaExperto:PRIMARY for (i=0; i<1000; i++) { db.pruebas.insert({num: i}) } Estos 1000 documentos se han insertado en el nodo principal, y se han replicado a los secundarios. Para comprobar la replicaci\u00f3n, abrimos un nuevo terminal y nos conectamos a un nodo secundario: $ mongosh --port 27018 replicaExperto:SECONDARY> Si desde el nodo secundario intentamos consultar el total de documentos de la colecci\u00f3n obtendremos un error: replicaExperto:SECONDARY> db.pruebas.count() count failed: { \"ok\" : 0, \"errmsg\" : \"not master and slaveOk=false\", \"code\" : 13435 } El error indica que no somos un nodo primario y por lo tanto no podemos leer de \u00e9l. Para permitir lecturas en los nodos secundarios, mediante rs.slaveOk() le decimos a mongosh que sabemos que nos hemos conectado a un secundairo y admitimos la posibilidad de obtener datos obsoletos. replicaExperto:SECONDARY> rs.slaveOk() replicaExperto:SECONDARY> db.pruebas.count() 1000 Pero que podamos leer no significa que podamos escribir. Si intentamos escribir en un nodo secundario obtendremos un error: replicaExperto:SECONDARY> db.pruebas.insert({num : 1001}) WriteResult({ \"writeError\" : { \"code\" : 10107, \"errmsg\" : \"not master\" } } Tolerancia a fallos \u00b6 Cuando un nodo primario no se comunica con otros miembros del conjunto durante m\u00e1s de 10 segundos, el conjunto de r\u00e9plicas intentar\u00e1, de entre los secundarios, que un miembro se convierta en el nuevo primario. Para ello se realiza un proceso de votaci\u00f3n, de modo que el nodo que obtenga el mayor n\u00famero de votos se erigir\u00e1 en primario. Este proceso de votaci\u00f3n se realiza bastante r\u00e1pido (menos de 3 segundos), durante el cual no existe ning\u00fan nodo primario y por tanto la r\u00e9plica no acepta escrituras y todos los miembros se convierten en nodos de s\u00f3lo-lectura. Elecci\u00f3n de un nuevo primario Figure 4. Elecci\u00f3n de un nuevo primario Proceso de votaci\u00f3n Cuando un nodo secundario no puede contactar con su nodo primario, contactar\u00e1 con el resto de miembros y les indicar\u00e1 que quiere ser elegido como primario. Es decir, cada nodo que no encuentre un primario se nominar\u00e1 como posible primario, de modo que un nodo no nomina a otro a ser primario, \u00fanicamente vota sobre una nominaci\u00f3n ya existente. Antes de dar su voto, el resto de nodos comprobar\u00e1n: si ellos tienen conectividad con el primario si el nodo que solicita ser primario tienen una r\u00e9plica actualizada de los datos. Todas las operaciones replicadas est\u00e1n ordenadas por el timestamp ascendentemente, de modo los candidatos deben tener operaciones posteriores o iguales a cualquier miembro con el que tengan conectividad. si existe alg\u00fan nodo con una prioridad mayor que deber\u00eda ser elegido. Si alg\u00fan miembro que quiere ser primario recibe una mayor\u00eda de \"s\u00eds\" se convertir\u00e1 en el nuevo primario, siempre y cuando no haya un servidor que vete la votaci\u00f3n. Si un miembro la veta es porque conoce alguna raz\u00f3n por la que el nodo que quiere ser primario no deber\u00eda serlo, es decir, ha conseguido contactar con el antiguo primario. Una vez un candidato recibe una mayor\u00eda de \"s\u00eds\", su estado pasar\u00e1 a ser primario. Cantidad de elementos En la votaci\u00f3n, se necesita una mayor\u00eda de nodos para elegir un primario, ya que una escritura se considera segura cuando ha alcanzado a la mayor\u00eda de los nodos. Esta mayor\u00eda se define como m\u00e1s de la mitad de todos los nodos del conjunto. Hay que destacar que la mayor\u00eda no se basa en los elementos que queden en pie o est\u00e9n disponibles, sino en el conjunto definido en la configuraci\u00f3n del conjunto. Por lo tanto, es importante configurar el conjunto de una manera que siempre se puede elegir un nodo primario. Por ejemplo, en un conjunto de cinco nodos, si los nodos 1, 2 y 3 est\u00e1n en un centro de datos y los miembros 4 y 5 en otro, deber\u00eda haber casi siempre una mayor\u00eda disponible en el primer centro de datos (es m\u00e1s probable que se pierda la conexi\u00f3n de red entre centros de datos que dentro de ellos). Elecci\u00f3n de un nuevo primario Figure 5. Elecci\u00f3n de un nuevo primario Por lo tanto, una configuraci\u00f3n que hay que evitar es aquella compuesta por dos elementos: uno primario y uno secundario. Si uno de los dos miembros deja de estar disponible, el otro miembro no puede verlo. En esta situaci\u00f3n, ninguna parte de la partici\u00f3n de red tiene una mayor\u00eda, con lo que acabar\u00edamos con dos secundarios. Por ello, el n\u00famero m\u00ednimo de nodos es 3, para que al realizar una nueva elecci\u00f3n se pueda elegir un nuevo nodo. Comprobando la tolerancia Para comprobar esto, desde el nodo primario vamos a detenerlo: replicaExperto:PRIMARY> db.adminCommand({\"shutdown\" : 1}) Otra posibilidad en vez de detenerlo es degradarlo a nodo secundario: replicaExperto:PRIMARY> rs.stepDown() Si pasamos al antiguo nodo secundario, y le preguntamos si es el principal obtendremos: replicaExperto:SECONDARY> rs.isMaster() { \"setName\" : \"replicaExperto\", \"setVersion\" : 1, \"ismaster\" : false, \"secondary\" : true, \"hosts\" : [ \"localhost:27018\", \"localhost:27019\", \"localhost:27017\" ], \"primary\" : \"localhost:27019\", \"me\" : \"localhost:27018\", \"maxBsonObjectSize\" : 16777216, \"maxMessageSizeBytes\" : 48000000, \"maxWriteBatchSize\" : 1000, \"localTime\" : ISODate(\"2015-03-24T21:55:27.382Z\"), \"maxWireVersion\" : 2, \"minWireVersion\" : 0, \"ok\" : 1 } Si nos fijamos en la propiedad primary, veremos que tenemos un nuevo primario. Configuraci\u00f3n recomendada Se recomiendan dos configuraciones: Mediante una mayor\u00eda del conjunto en un centro de datos. Este planteamiento es bueno si tenemos un data center donde queremos que siempre se aloje el nodo primario de la r\u00e9plica. Siempre que el centro de datos funcione normalmente, habr\u00e1 un nodo primario. Sin embargo, si el centro primario pierde la conectividad, el centro de datos secundario no podr\u00e1 elegir un nuevo primario. Mediante el mismo n\u00famero de servidores en cada centro de datos, m\u00e1s un servidor que rompe la igualdad en una tercera localizaci\u00f3n. Este dise\u00f1o es conveniente cuando ambos centros de datos tienen el mismo grado de confiabilidad y robustez. Recuperaci\u00f3n del sistema \u00b6 Si en un conjunto de r\u00e9plicas se cae el primario y hay escrituras que se han pasado al oplog de modo que los otros nodos no las han replicado, cuando el nodo primario vuelva en s\u00ed como secundario y se sincronice con el primario, se dar\u00e1 cuenta que hay operaciones de escritura pendientes y las pasar\u00e1 a rollback, para que si se desean se apliquen manualmente. Para evitar este escenario, se necesita emplear consistencia en la escritura, de manera que hasta que la escritura no se haya replicado en la mayor\u00eda de los nodos no se considere como una escritura exitosa. Consistencia en la escritura \u00b6 Ya hemos visto que tanto las lecturas como las escrituras se realizan de manera predeterminada en el nodo primario. Las aplicaciones pueden decidir que las escrituras vayan al nodo primario pero las lecturas al secundario. Esto puede provocar que haya lecturas caducas, con datos obsoletos, pero como beneficio podemos escalar el sistema. La replicaci\u00f3n es un proceso as\u00edncrono. En el per\u00edodo de tiempo en el que el sistema de votaci\u00f3n sucede, no se completa ninguna escritura. MongoDB garantiza la consistencia en la escritura, haciendo que sea un sistema consistente. Para ello, ofrece un sistema que garantiza que una escritura ha sido exitosa. Dependiendo del nivel de configuraci\u00f3n de la consistencia, las inserciones, modificaciones y borrados pueden tardar m\u00e1s o menos. Si reducimos el nivel de consistencia, el rendimiento ser\u00e1 mejor, a costa de poder obtener datos obsoletos u perder datos que no se han terminado de serializar en disco. Con un nivel de consistencia m\u00e1s alto, los clientes esperan tras enviar una operaci\u00f3n de escritura a que MongoDB les confirme la operaci\u00f3n. Los valores que podemos configurar se realizan mediante las siguientes opciones: w : indica el n\u00famero de servidores que se han de replicar para que la inserci\u00f3n devuelva un ACK. j : indica si las escrituras se tienen que trasladar a un diario de bit\u00e1cora (journal) wtimeout : indica el l\u00edmite de tiempo a esperar como m\u00e1ximo, para prevenir que una escritura se bloquee indefinidamente. Niveles de consistencia \u00b6 Con estas opciones, podemos configurar diferentes niveles de consistencia son: Sin confirmaci\u00f3n: w:0 , tambi\u00e9n conocido como fire-and-forget . Con confirmaci\u00f3n: w:1 , el cual es el modo por defecto. Con diario: w:1 , j:true . Cada inserci\u00f3n primero se escribe en el diario y posteriormente en el directorio de datos. Con confirmaci\u00f3n de la mayor\u00eda: w: \"majority\" , es decir, confirman la mitad + 1 de los nodos de la replica. Estas opciones se indican como par\u00e1metro final en las operaciones de insercion y modificaci\u00f3n de datos. Por ejemplo: db . pruebas . insert ( { num : 1002 }, { writeConcern : { w : \"majority\" , wtimeout : 5000 }} ) En resumen, a mayor cantidad de nodos, mayor es la tolerancia a fallos pero cada operaci\u00f3n necesita m\u00e1s tiempo y recursos para realizar la persistencia de los datos. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/core/write-concern/ y https://www.youtube.com/watch?v=49BPAY1Yb5w FIXME: Revisar notebook curso MongoDB_PYTHON Particionado (Sharding) \u00b6 https://www.digitalocean.com/community/tutorials/understanding-database-sharding Ya vimos en la primera sesi\u00f3n que dentro del entorno de las bases de datos, particionar consiste en dividir los datos entre m\u00faltiples m\u00e1quinas. Al poner un subconjunto de los datos en cada m\u00e1quina, vamos a poder almacenar m\u00e1s informaci\u00f3n y soportar m\u00e1s carga sin necesidad de m\u00e1quinas m\u00e1s potentes, sino una mayor cantidad de m\u00e1quinas m\u00e1s modestas (y mucho m\u00e1s baratas). El Sharding es una t\u00e9cnica que fragmenta los datos de la base de datos horizontalmente agrup\u00e1ndolos de alg\u00fan modo que tenga sentido y que permita un direccionamiento m\u00e1s r\u00e1pido. Sharding Figure 6. Sharding Por lo tanto, estos shards (fragmentos) pueden estar localizados en diferentes bases de datos y localizaciones f\u00edsicas. El Sharding no tiene por qu\u00e9 estar basado \u00fanicamente en una colecci\u00f3n y un campo, puede ser a nivel de todas las colecciones. Por ejemplo podr\u00edamos decir \"todos los datos de usuarios cuyo perfil est\u00e1 en los Estados Unidos los redirigimos a la base de datos del servidor en Estados Unidos, y todos los de Asia van a la base de datos de Asia\". sh.status() sh.addShard(\"rs1/mongodbd1.example.net:27017\") sh.shardCollection(\"mydb.coll\", {zipcode: 1}) sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\") sh.splitAt(\"mydb.coll\", {x: 70}) sh.splitFind(\"mydb.coll\", {x: 70}) sh.disableAutoSplit() sh.enableAutoSplit() sh.startBalancer() sh.stopBalancer() sh.disableBalancing(\"mydb.coll\") sh.enableBalancing(\"mydb.coll\") sh.getBalancerState() sh.setBalancerState(true/false) sh.isBalancerRunning() sh.addTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") sh.removeTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") sh.addShardTag(\"shard0000\", \"NYC\") sh.removeShardTag(\"shard0000\", \"NYC\") sh.addShardToZone(\"shard0000\", \"JFK\") sh.removeShardFromZone(\"shard0000\", \"NYC\") sh.removeRangeFromZone(\"mydb.coll\", {a: 1, b: 1}, {a: 10, b: 10}) Particionando con MongoDB \u00b6 MongoDB implementa el sharding de forma nativa y autom\u00e1tica (de ah\u00ed el t\u00e9rmino de auto-sharding), siguiendo un enfoque basado en rangos. Para ello, divide una colecci\u00f3n entre diferentes servidores, utilizando mongos como router de las peticiones entre los sharded clusters. Esto favorece que el desarrollador ignore que la aplicaci\u00f3n no se comunica con un \u00fanico servidor, balanceando de manera autom\u00e1tica los datos y permitiendo incrementar o reducir la capacidad del sistema a conveniencia. Antes de plantearse hacer auto-sharding sobre nuestros datos, es conveniente dominar c\u00f3mo se trabaja con MongoDB y el uso de conjuntos de r\u00e9plica. Sharded Cluster El particionado de MongoDB permite crear un cluster de muchas m\u00e1quinas, dividiendo a nivel de colecci\u00f3n y poniendo un subconjunto de los datos de la colecci\u00f3n en cada uno de los fragmentos. Los componentes de un sharded clusters son: Shards (Fragmentos) Cada una de las m\u00e1quinas del cluster, que almacena un subconjunto de los datos de la colecci\u00f3n. Cada shard es una instancia de mongod o un conjunto de r\u00e9plicas. En un entorno de producci\u00f3n, todos los shards son conjuntos de r\u00e9plica. Servidores de Configuracion Cada servidor de configuraci\u00f3n es una instancia de mongod que almacena metadatos sobre el cluster. Los metadatos mapean los trozos con los shards, definiendo qu\u00e9 rangos de datos definen un trozo (chunk) de la colecci\u00f3n, y qu\u00e9 trozos se encuentran en un determinado shard. En entornos de producci\u00f3n se aconseja tener 3 servidores de configuraci\u00f3n ya que si s\u00f3lo tuvi\u00e9semos uno, al producirse una ca\u00edda el cluster quedar\u00eda inaccesible. Enrutadores Cada router es una instancia mongos que enruta las lecturas y escrituras de las aplicaciones a los shards. Las aplicaciones no acceden directamente a los shards, sino al router. Estos enrutadores funcionan de manera similar a una tabla de contenidos, que nos indica donde se encuentran los datos. Una vez recopilados los datos de los diferentes shards, se fusionan y se encarga de devolverlos a la aplicaci\u00f3n. En entornos de producci\u00f3n es com\u00fan tener varios routers para balancear la carga de los clientes. Componentes de un sharded cluster Figure 7. Componentes de un sharded cluster Autoevaluaci\u00f3n Supongamos que queremos ejecutar m\u00faltiples routers mongos para soportar la redundancia. \u00bfQu\u00e9 elemento asegurar\u00e1 la tolerancia a fallos y cambiar\u00e1 de un mongos a otro dentro de tu aplicaci\u00f3n? [2] mongod mongos Driver Los servidores de configuraci\u00f3n de sharding Shard key Para que MongoDB sepa c\u00f3mo dividir una colecci\u00f3n en trozos, hay que elegir una shard key, normalmente el identificador del documento, por ejemplo, student_id. Este identificador es la clave del chunk (por lo hace la misma funci\u00f3n que una clave primaria). Para las b\u00fasquedas, borrados y actualizaciones, al emplear la shard key, mongos sabe a que shard enviar la petici\u00f3n. En cambio, si la operaci\u00f3n no la indica, se har\u00e1 un broadcast a todas los shards para averiguar donde se encuentra. Por eso, toda inserci\u00f3n debe incluir la shard key. En el caso de tratarse de una clave compuesta, la inserci\u00f3n debe contener la clave completa. Entre los aspectos a tener en cuenta a la hora de elegir una shard key cabe destacar que debe: Tener una alta cardinalidad, para asegurar que los documentos puedan dividirse en los distintos fragmentos. Por ejemplo, si elegimos un shard key que solo tiene 3 valores posibles y tenemos 5 fragmentos, no podr\u00edamos separar los documentos en los 5 fragmentos al solo tener 3 valores posibles para separar. Cuantos m\u00e1s valores posibles pueda tener la clave de fragmentaci\u00f3n, m\u00e1s eficiente ser\u00e1 la divisi\u00f3n de los trozos entre los fragmentos disponibles. Tener un alto nivel de aleatoriedad. Si utilizamos una clave que siga un patr\u00f3n incremental como una fecha o un ID, conllevar\u00e1 que al insertar documentos, el mismo fragmento estar\u00e1 siendo utilizando constantemente durante el rango de valores definido para \u00e9l. Esto provoca que los datos est\u00e9n separados de una manera \u00f3ptima, pero pondr\u00e1 siempre bajo estr\u00e9s a un fragmento en per\u00edodos de tiempo mientras que los otros posiblemente queden con muy poca actividad (comportamiento conocido como hotspotting). Una soluci\u00f3n a las claves que siguen patrones incrementales es aplicar una funcion hash y crear una clave hasheada que si tiene un alto nivel de aleatoriedad. M\u00e1s consejos sobre como elegir la shard key en http://techinsides.blogspot.com.es/2013/09/keynote-concerns-how-to-choose-mongodb.html Finalmente, destacar que toda shard key debe tener un \u00edndice asociado. Preparando el Sharding con MongoDB \u00b6 Para comenzar, vamos a crear un particionado en dos instancias en las carpetas /data/s1/db y /data/s2/db. Los logs los colocaremos en /data/logs y crearemos un servidor para la configuraci\u00f3n de los metadatos del shard en /data/con1/db: mkdir -p /data/s1/db /data/s2/db /data/logs /data/conf1/db chown id -u /data/s1/db /data/s2/db /data/logs /data/conf1/db A continuaci\u00f3n, arrancaremos un proceso mongod por cada uno de los shards (con la opci\u00f3n --shardsvr) y un tercero para la base de datos de configuraci\u00f3n (con la opci\u00f3n --configsvr). Finalmente, tambi\u00e9n lanzaremos un proceso mongos: Script de creaci\u00f3n del Shard - (creaShard.sh) mongod --shardsvr --dbpath /data/s1/db --port 27000 --logpath /data/logs/sh1.log --smallfiles --oplogSize 128 --fork mongod --shardsvr --dbpath /data/s2/db --port 27001 --logpath /data/logs/sh2.log --smallfiles --oplogSize 128 --fork mongod --configsvr --dbpath /data/conf1/db --port 25000 --logpath /data/logs/config.log --fork mongos --configdb localhost:25000 --logpath /data/logs/mongos.log --fork El cual lanzaremos mediante bash < creaShard.sh Una vez creado, arrancaremos un shell del mongo, y observaremos como se lanza mongos: $ mongo MongoDB shell version: 3.2.1 connecting to: test mongos> Finalmente, configuraremos el shard mediante el m\u00e9todo sh.addShard(URI), obteniendo confirmaci\u00f3n tras cada cada uno: mongos> sh.addShard(\"localhost:27000\") { \"shardAdded\" : \"shard0000\", \"ok\" : 1 } mongos> sh.addShard(\"localhost:27001\") { \"shardAdded\" : \"shard0001\", \"ok\" : 1 } El valor de la propiedad shardAdded nos devuelve el identificado un\u00edvoco de cada shard. De manera similar que con el conjunto de r\u00e9plicas se emplean el prefijo rs, para interactuar con los componentes implicados en el sharding se emplea sh. Por ejemplo, mediante sh.help() obtendremos la ayuda de los m\u00e9todos disponibles. As\u00ed pues, en este momento tenemos montada un shard con: dos instancias de mongod para almacenar datos en los puertos 27000 y 27001 (shards) una instancia monogd en el puerto 25000 (servidor de configuraci\u00f3n) encargada de almacenar los metadatos del shard, a la cual s\u00f3lo se deber\u00edan conectar el proceso mongos o los drivers para obtener informaci\u00f3n sobre el shard y la shard key y un proceso mongos (enrutador), encargado de aceptar las peticiones de los clientes y enrutar las peticiones al shard adecuado. Shard con dos m\u00e1quinas Figure 8. Shard con dos m\u00e1quinas Si comprobamos el estado del shard podremos comprobar como tenemos dos shards, con sus identificadores y URIs: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: En un entorno de producci\u00f3n, en vez de tener dos shards, habr\u00e1 un conjunto de r\u00e9plicas para asegurar la alta disponibilidad. Adem\u00e1s, tendremos tres servidores de configuraci\u00f3n para asegurar la disponibilidad de \u00e9stos. Del mismo modo, habr\u00e1 tantos procesos mongos creados para un shard como conexiones de clientes. Sharding en un entorno de Producci\u00f3n Figure 9. Sharding en un entorno de producci\u00f3n En init_sharded_replica.sh pod\u00e9is comprobar como crear sharding sobre un conjunto de r\u00e9plicas. 4.6.3. Habilitando el Sharding Una vez hemos creado la estructura necesaria para soportar el sharding vamos a insertar un conjunto de datos para posteriormente particionarlos. Para ello, vamos a insertar cien mil usuarios en una colecci\u00f3n: mongos> use expertojava switched to db expertojava mongos> for (var i=0; i<100000; i++) { db.usuarios.insert({\"login\":\"usu\" + i,\"nombre\":\"nom\" + i*2, \"fcreacion\": new Date()}); } mongos> db.usuarios.count() 100000 Como podemos observar, interactuar con mongos es igual a hacerlo con mongo. Ahora mismo no sabemos en qu\u00e9 cual de los dos shards se han almacenado los datos. Adem\u00e1s, estos datos no est\u00e1n particionados, es decir residen en s\u00f3lo uno de los shards. Para habilitar el sharding a nivel de base de datos y que los datos se repartan entre los fragmentos disponibles, ejecutaremos el comando sh.enableSharding(nombreDB) : mongos> sh.enableSharding(\"expertojava\") Si volvemos a comprobar el estado del shard, tenemos que se ha creado la nueva base de datos que contiene la propiedad \"partitioned\" : true, la cual nos informa que esta fragmentada. Antes de habilitar el sharding para una determinada colecci\u00f3n, tenemos que crear un \u00edndice sobre la shard key: mongos> db.usuarios.createIndex({\"login\": 1}) { \"raw\" : { \"localhost:27000\" : { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 1, \"numIndexesAfter\" : 2, \"ok\" : 1 } }, \"ok\" : 1 } Una vez habilitado el shard ya podemos fragmentar la colecci\u00f3n: mongos> sh.shardCollection(\"expertojava.usuarios\", {\"login\": 1}, false) El m\u00e9todo shardCollection particiona una colecci\u00f3n a partir de una shard key. Para ello, recibe tres par\u00e1metros: nombre de la colecci\u00f3n, con nomenclatura de nombreBD.nombreColecci\u00f3n nombre del campo para fragmentar la colecci\u00f3n, es decir, el shard key. Uno de los requisitos es que esta clave tengo una alta cardinalidad. Si tenemos una propiedad con una cardinalidad baja, podemos hacer un hash de la propiedad mediante {\"login\": \"hashed\"}. Como en nuestro caso hemos utilizado un campo con valores \u00fanicos hemos puesto {\"login\": 1}. booleano que indica si el valor utilizado como shard key es \u00fanico. Para ello, el \u00edndice que se crea sobre el campo debe ser del tipo unique. Este comando divide la colecci\u00f3n en chunks, la cual es la unidad que utiliza MongoDB para mover los datos. Una vez que se ha ejecutado, MongoDB comenzar\u00e1 a balancear la colecci\u00f3n entre los shards del cluster. Este proceso no es instant\u00e1neo. Si la colecci\u00f3n contiene un gran conjunto de datos puede llevar horas completar el balanceo. Si ahora volvemos a comprobar el estado del shard obtendremos: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: { \"_id\" : \"expertojava\", \"primary\" : \"shard0000\", \"partitioned\" : true } expertojava.usuarios shard key: { \"login\" : 1 } unique: false balancing: true chunks: shard0000 1 { \"login\" : { \"$minKey\" : 1 } } -->> { \"login\" : { \"$maxKey\" : 1 } } on : shard0000 Timestamp(1, 0) la propiedad chunks muestra la cantidad de trozos que alberga cada partici\u00f3n. As\u00ed, pues en este momento tenemos 1 chunk Para cada uno de los fragmentos se muestra el rango de valores que alberga cada chunk, as\u00ed como en que shard se ubica. Las claves $minKey y $maxKey son similares a menos infinito y m\u00e1s infinito, es decir, no hay ning\u00fan valor por debajo ni por encima de ellos. Es decir, indican los topes de la colecci\u00f3n. 4.6.4. Trabajando con el Sharding En este momento, el shard esta creado pero todos los nodos residen en un \u00fanico fragmento dentro de un partici\u00f3n. Vamos a volver a insertar 100.000 usuarios m\u00e1s a ver que sucede. mongos> for (var i=100000; i<200000; i++) { db.usuarios.insert({\"login\":\"usu\" + i,\"nombre\":\"nom\" + i*2, \"fcreacion\": new Date()}); } mongos> db.usuarios.count() 200000 Si ahora comprobamos el estado del shard, los datos se deber\u00edan haber repartido entre los shards disponibles: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 31 : Success databases: { \"_id\" : \"expertojava\", \"primary\" : \"shard0000\", \"partitioned\" : true } expertojava.usuarios shard key: { \"login\" : 1 } unique: false balancing: true chunks: shard0000 32 shard0001 31 too many chunks to print, use verbose if you want to force print Con estos datos se ha forzado a balancear los mismos entre los dos fragmentos, habiendo en cada uno de ellos 32 y 31 trozos respectivamente Si ahora realizamos una consulta y obtenemos su plan de ejecuci\u00f3n veremos como se trata de una consulta que se ejecuta en paralelo: mongos> db.usuarios.find({\"login\":\"usu12345\"}).explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SINGLE_SHARD\", \"shards\" : [ { \"shardName\" : \"shard0001\", \"connectionString\" : \"localhost:27001\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27001, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"login\" : { \"$eq\" : \"usu12345\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"login\" : 1 }, \"indexName\" : \"login_1\", \"isMultiKey\" : false, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 1, \"direction\" : \"forward\", \"indexBounds\" : { \"login\" : [ \"[\\\"usu12345\\\", \\\"usu12345\\\"]\" ] } } } }, \"rejectedPlans\" : [ ] } ] } }, \"ok\" : 1 } Podemos observar como se ha realizado una fase SINGLE_SHARD de manera que ha accedido \u00fanicamente al shard0001, y posteriormente una fase de SHARDING_FILTER en la cual ha empleado un \u00edndice para el escaneo (IXSCAN). Si en vez de obtener un documento concreto, obtenemos el plan de ejecuci\u00f3n de obtener todos los documentos tendremos: mongos> db.usuarios.find().explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SHARD_MERGE\", \"shards\" : [ { \"shardName\" : \"shard0000\", \"connectionString\" : \"localhost:27000\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27000, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ ] }, \"winningPlan\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ ] }, \"direction\" : \"forward\" } }, \"rejectedPlans\" : [ ] }, { \"shardName\" : \"shard0001\", \"connectionString\" : \"localhost:27001\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27001, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ ] }, \"winningPlan\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ ] }, \"direction\" : \"forward\" } }, \"rejectedPlans\" : [ ] } ] } }, \"ok\" : 1 } As\u00ed pues, si en una consulta no le enviamos la shard key como criterio, mongos enviar\u00e1 la consulta a cada shard y realizar\u00e1 un SHARD_MERGE con la informaci\u00f3n devuelta de cada shard. Si la consulta contiene la shard key, la consulta se enruta directamente al shard apropiado. Referencias \u00b6 Curso M121: The MongoDB Aggregation Framework de la Mongo University. Actividades \u00b6 https://docs.mongodb.com/manual/reference/write-concern/ https://docs.mongodb.com/manual/reference/read-concern/ https://docs.mongodb.com/manual/core/read-preference/","title":"MongoDB Avanzado"},{"location":"sa/05mongo-avanzado.html#mongodb-avanzado","text":"En esta sesi\u00f3n estudiaremos como realizar consultas agregadas utilizando el framework de agregaci\u00f3n, y temas relacionados con la escalibilidad del sistema mediante la replicaci\u00f3n y el particionado de los datos.","title":"MongoDB Avanzado"},{"location":"sa/05mongo-avanzado.html#agregaciones","text":"Para poder agrupar datos y realizar c\u00e1lculos sobre \u00e9stos, MongoDB ofrece diferentes alternativas: Mediante operaciones Map-reduce con la operaci\u00f3n mapreduce() cuyo uso est\u00e1 deprecated desde MongoBD 5.0 . Mediante el uso conjunto de $function y $accumulator que permiten definir expresiones de agregaci\u00f3n mediante JavaScript . Mediante operaciones de agrupaci\u00f3n sencilla, como pueden ser las operaciones count() o distinct() . Mediante el uso del Aggregation Framework , basado en el uso de pipelines , el cual permite realizar diversas operaciones sobre los datos. Este framework es mecanismo m\u00e1s eficiente y usable para la realizaci\u00f3n de agregaciones, y por tanto, en el que nos vamos a centrar en esta sesi\u00f3n. Para ello, a partir de una colecci\u00f3n, mediante el m\u00e9todo aggregate le pasaremos un array con las fases a realizar: db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , numProductos : { $sum : 1 }} }, { $sort : { numProductos :- 1 }} ])","title":"Agregaciones"},{"location":"sa/05mongo-avanzado.html#pipeline-de-agregacion","text":"Las agregaciones usan un pipeline, conocido como Aggregation Pipeline , de ah\u00ed el uso de un array con [ ] donde cada elemento es una fase del pipeline , de modo que la salida de una fase es la entrada de la siguiente: db . coleccion . aggregate ([ op1 , op2 , ... opN ]) El resultado del pipeline es un documento y por lo tanto est\u00e1 sujeto a la restricci\u00f3n de BSON, que limita su tama\u00f1o a 16MB. En la siguiente imagen se resumen los pasos de una agrupaci\u00f3n donde primero se eligen los elementos que vamos a agrupar mediante $match y posteriormente se agrupan con $group para hacer $sum sobre el total: Ejemplo de pipeline con $match y $group","title":"Pipeline de agregaci\u00f3n"},{"location":"sa/05mongo-avanzado.html#operadores-del-pipeline","text":"Antes de nada destacar que las fases se pueden repetir, por lo que una consulta puede repetir operadores . A continuaci\u00f3n vamos a estudiar todos estos operadores: Operador Descripci\u00f3n Cardinalidad $project Proyecci\u00f3n de campos, es decir, propiedades en las que estamos interesados. Tambi\u00e9n nos permite modificar un documento, o crear un subdocumento (reshape) 1:1 $match Filtrado de campos, similar a where N:1 $group Para agrupar los datos, similar a group by N:1 $sort Ordenar 1:1 $skip Saltar N:1 $limit Limitar los resultados N:1 $unwind Separa los datos que hay dentro de un array 1:N Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de productos (productos.js) de un tienda de electr\u00f3nica con las caracter\u00edsticas y precios de los mismos. Un ejemplo de un producto ser\u00eda: > db . productos . findOne () { \"_id\" : ObjectId ( \"5345afc1176f38ea4eda4787\" ), \"nombre\" : \"iPad 16GB Wifi\" , \"fabricante\" : \"Apple\" , \"categoria\" : \"Tablets\" , \"precio\" : 499 } Para cargar este archivo desde la consola podemos realizar: mongosh < productos.js","title":"Operadores del pipeline"},{"location":"sa/05mongo-avanzado.html#de-sql-al-pipeline-de-agregaciones","text":"Ya hemos visto que el pipeline ofrece operadores para realizar la misma funcionalidad de agrupaci\u00f3n que ofrece SQL. Si relacionamos los comandos SQL con el pipeline de agregaciones tenemos las siguientes equivalencias: Table 3. Equivalencia con SQL SQL Pipeline de Agregaciones WHERE $match GROUP BY $group HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum Podemos encontrar ejemplos de consultas SQL transformadas al pipeline en http://docs.mongodb.org/manual/reference/sql-aggregation-comparison/","title":"De SQL al Pipeline de agregaciones"},{"location":"sa/05mongo-avanzado.html#limitaciones","text":"Hay que tener en cuenta las siguiente limitaciones: En versiones anteriores a la 2.6, el pipeline devolv\u00eda en cada fase un objeto BSON, y por tanto, el resultado estaba limitado a 16MB Las fases tienen un l\u00edmite de 100MB en memor\u00eda. Si una fase excede dicho l\u00edmite, se producir\u00e1 un error. En este caso, hay que habilitar el uso de disco mediante allowDiskUse en las opciones de la agregaci\u00f3n. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/reference/method/db.collection.aggregate","title":"Limitaciones"},{"location":"sa/05mongo-avanzado.html#agregaciones-con-compass","text":"Using Compass' Aggregation Pipeline Builder feature https://docs.mongodb.com/compass/master/aggregation-pipeline-builder/ , we can easily create, delete and rearrange the stages in a pipeline, and evaluate the output documents in real time. Then we can produce a version of that pipeline in Python, Java, C# or Node.js, using Compass' Export-to-Language feature. https://docs.mongodb.com/compass/master/export-pipeline-to-language","title":"Agregaciones con Compass"},{"location":"sa/05mongo-avanzado.html#replicacion","text":"Un aspecto muy importante de MongoDB es que soporta la replicaci\u00f3n de los datos de forma nativa mediante el uso de conjuntos de r\u00e9plicas. FIXME: pensar como explicarlo via MongoAtlas... nada de instalar. con Docker Compose \u00bfse podr\u00eda hacer f\u00e1cil?","title":"Replicaci\u00f3n"},{"location":"sa/05mongo-avanzado.html#conjunto-de-replicas","text":"En MongoDB se replican los datos mediante un conjunto de r\u00e9plicas ( Replica Set ) el cual es un grupo de servidores (nodos mongod) donde hay uno que ejerce la funci\u00f3n de primario y por tanto recibe las peticiones de los clientes, y el resto de servidores hace de secundarios, manteniendo copias de los datos del primario. rs.status() rs.initiate({\"_id\": \"replicaTest\", members: [ {_id: 0, host: \"127.0.0.1:27017\" }, { _id: 1, host: \"127.0.0.1:27018\" }, {_id: 2, host: \"127.0.0.1:27019\", arbiterOnly:true }] }) rs.add(\"mongodbd1.example.net:27017\") rs.addArb(\"mongodbd2.example.net:27017\") rs.remove(\"mongodbd1.example.net:27017\") rs.conf() rs.isMaster() rs.printReplicationInfo() rs.printSlaveReplicationInfo() rs.reconfig( ) rs.slaveOk() rs.stepDown(20, 5) // (stepDownSecs, secondaryCatchUpPeriodSecs) Conjunto de R\u00e9plicas Figure 2. Conjunto de R\u00e9plicas Si el nodo primario se cae, los secundarios eligen un nuevo primario entre ellos mismos, en un proceso que se conoce como votaci\u00f3n. La aplicaci\u00f3n se conectar\u00e1 al nuevo primario de manera transparente. Cuando el antiguo nodo primario vuelva en s\u00ed, ser\u00e1 un nuevo nodo secundario. Arbitraje de un secundario Figure 3. Arbitraje de un secundario Al usar replicaci\u00f3n, si un servidor se cae, siempre vamos a poder obtener los datos a partir de otros servidores del conjunto. Si los datos de un servidor se da\u00f1an o son inaccesibles, podemos crear una nueva copia desde uno de los miembros del conjunto.","title":"Conjunto de r\u00e9plicas"},{"location":"sa/05mongo-avanzado.html#consistencia-en-la-escritura","text":"Ya hemos visto que tanto las lecturas como las escrituras se realizan de manera predeterminada en el nodo primario. Las aplicaciones pueden decidir que las escrituras vayan al nodo primario pero las lecturas al secundario. Esto puede provocar que haya lecturas caducas, con datos obsoletos, pero como beneficio podemos escalar el sistema. La replicaci\u00f3n es un proceso as\u00edncrono. En el per\u00edodo de tiempo en el que el sistema de votaci\u00f3n sucede, no se completa ninguna escritura. MongoDB garantiza la consistencia en la escritura, haciendo que sea un sistema consistente. Para ello, ofrece un sistema que garantiza que una escritura ha sido exitosa. Dependiendo del nivel de configuraci\u00f3n de la consistencia, las inserciones, modificaciones y borrados pueden tardar m\u00e1s o menos. Si reducimos el nivel de consistencia, el rendimiento ser\u00e1 mejor, a costa de poder obtener datos obsoletos u perder datos que no se han terminado de serializar en disco. Con un nivel de consistencia m\u00e1s alto, los clientes esperan tras enviar una operaci\u00f3n de escritura a que MongoDB les confirme la operaci\u00f3n. Los valores que podemos configurar se realizan mediante las siguientes opciones: w : indica el n\u00famero de servidores que se han de replicar para que la inserci\u00f3n devuelva un ACK. j : indica si las escrituras se tienen que trasladar a un diario de bit\u00e1cora (journal) wtimeout : indica el l\u00edmite de tiempo a esperar como m\u00e1ximo, para prevenir que una escritura se bloquee indefinidamente.","title":"Consistencia en la escritura"},{"location":"sa/05mongo-avanzado.html#niveles-de-consistencia","text":"Con estas opciones, podemos configurar diferentes niveles de consistencia son: Sin confirmaci\u00f3n: w:0 , tambi\u00e9n conocido como fire-and-forget . Con confirmaci\u00f3n: w:1 , el cual es el modo por defecto. Con diario: w:1 , j:true . Cada inserci\u00f3n primero se escribe en el diario y posteriormente en el directorio de datos. Con confirmaci\u00f3n de la mayor\u00eda: w: \"majority\" , es decir, confirman la mitad + 1 de los nodos de la replica. Estas opciones se indican como par\u00e1metro final en las operaciones de insercion y modificaci\u00f3n de datos. Por ejemplo: db . pruebas . insert ( { num : 1002 }, { writeConcern : { w : \"majority\" , wtimeout : 5000 }} ) En resumen, a mayor cantidad de nodos, mayor es la tolerancia a fallos pero cada operaci\u00f3n necesita m\u00e1s tiempo y recursos para realizar la persistencia de los datos. M\u00e1s informaci\u00f3n en http://docs.mongodb.org/manual/core/write-concern/ y https://www.youtube.com/watch?v=49BPAY1Yb5w FIXME: Revisar notebook curso MongoDB_PYTHON","title":"Niveles de consistencia"},{"location":"sa/05mongo-avanzado.html#particionado-sharding","text":"https://www.digitalocean.com/community/tutorials/understanding-database-sharding Ya vimos en la primera sesi\u00f3n que dentro del entorno de las bases de datos, particionar consiste en dividir los datos entre m\u00faltiples m\u00e1quinas. Al poner un subconjunto de los datos en cada m\u00e1quina, vamos a poder almacenar m\u00e1s informaci\u00f3n y soportar m\u00e1s carga sin necesidad de m\u00e1quinas m\u00e1s potentes, sino una mayor cantidad de m\u00e1quinas m\u00e1s modestas (y mucho m\u00e1s baratas). El Sharding es una t\u00e9cnica que fragmenta los datos de la base de datos horizontalmente agrup\u00e1ndolos de alg\u00fan modo que tenga sentido y que permita un direccionamiento m\u00e1s r\u00e1pido. Sharding Figure 6. Sharding Por lo tanto, estos shards (fragmentos) pueden estar localizados en diferentes bases de datos y localizaciones f\u00edsicas. El Sharding no tiene por qu\u00e9 estar basado \u00fanicamente en una colecci\u00f3n y un campo, puede ser a nivel de todas las colecciones. Por ejemplo podr\u00edamos decir \"todos los datos de usuarios cuyo perfil est\u00e1 en los Estados Unidos los redirigimos a la base de datos del servidor en Estados Unidos, y todos los de Asia van a la base de datos de Asia\". sh.status() sh.addShard(\"rs1/mongodbd1.example.net:27017\") sh.shardCollection(\"mydb.coll\", {zipcode: 1}) sh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\") sh.splitAt(\"mydb.coll\", {x: 70}) sh.splitFind(\"mydb.coll\", {x: 70}) sh.disableAutoSplit() sh.enableAutoSplit() sh.startBalancer() sh.stopBalancer() sh.disableBalancing(\"mydb.coll\") sh.enableBalancing(\"mydb.coll\") sh.getBalancerState() sh.setBalancerState(true/false) sh.isBalancerRunning() sh.addTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") sh.removeTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\") sh.addShardTag(\"shard0000\", \"NYC\") sh.removeShardTag(\"shard0000\", \"NYC\") sh.addShardToZone(\"shard0000\", \"JFK\") sh.removeShardFromZone(\"shard0000\", \"NYC\") sh.removeRangeFromZone(\"mydb.coll\", {a: 1, b: 1}, {a: 10, b: 10})","title":"Particionado (Sharding)"},{"location":"sa/05mongo-avanzado.html#particionando-con-mongodb","text":"MongoDB implementa el sharding de forma nativa y autom\u00e1tica (de ah\u00ed el t\u00e9rmino de auto-sharding), siguiendo un enfoque basado en rangos. Para ello, divide una colecci\u00f3n entre diferentes servidores, utilizando mongos como router de las peticiones entre los sharded clusters. Esto favorece que el desarrollador ignore que la aplicaci\u00f3n no se comunica con un \u00fanico servidor, balanceando de manera autom\u00e1tica los datos y permitiendo incrementar o reducir la capacidad del sistema a conveniencia. Antes de plantearse hacer auto-sharding sobre nuestros datos, es conveniente dominar c\u00f3mo se trabaja con MongoDB y el uso de conjuntos de r\u00e9plica. Sharded Cluster El particionado de MongoDB permite crear un cluster de muchas m\u00e1quinas, dividiendo a nivel de colecci\u00f3n y poniendo un subconjunto de los datos de la colecci\u00f3n en cada uno de los fragmentos. Los componentes de un sharded clusters son: Shards (Fragmentos) Cada una de las m\u00e1quinas del cluster, que almacena un subconjunto de los datos de la colecci\u00f3n. Cada shard es una instancia de mongod o un conjunto de r\u00e9plicas. En un entorno de producci\u00f3n, todos los shards son conjuntos de r\u00e9plica. Servidores de Configuracion Cada servidor de configuraci\u00f3n es una instancia de mongod que almacena metadatos sobre el cluster. Los metadatos mapean los trozos con los shards, definiendo qu\u00e9 rangos de datos definen un trozo (chunk) de la colecci\u00f3n, y qu\u00e9 trozos se encuentran en un determinado shard. En entornos de producci\u00f3n se aconseja tener 3 servidores de configuraci\u00f3n ya que si s\u00f3lo tuvi\u00e9semos uno, al producirse una ca\u00edda el cluster quedar\u00eda inaccesible. Enrutadores Cada router es una instancia mongos que enruta las lecturas y escrituras de las aplicaciones a los shards. Las aplicaciones no acceden directamente a los shards, sino al router. Estos enrutadores funcionan de manera similar a una tabla de contenidos, que nos indica donde se encuentran los datos. Una vez recopilados los datos de los diferentes shards, se fusionan y se encarga de devolverlos a la aplicaci\u00f3n. En entornos de producci\u00f3n es com\u00fan tener varios routers para balancear la carga de los clientes. Componentes de un sharded cluster Figure 7. Componentes de un sharded cluster Autoevaluaci\u00f3n Supongamos que queremos ejecutar m\u00faltiples routers mongos para soportar la redundancia. \u00bfQu\u00e9 elemento asegurar\u00e1 la tolerancia a fallos y cambiar\u00e1 de un mongos a otro dentro de tu aplicaci\u00f3n? [2] mongod mongos Driver Los servidores de configuraci\u00f3n de sharding Shard key Para que MongoDB sepa c\u00f3mo dividir una colecci\u00f3n en trozos, hay que elegir una shard key, normalmente el identificador del documento, por ejemplo, student_id. Este identificador es la clave del chunk (por lo hace la misma funci\u00f3n que una clave primaria). Para las b\u00fasquedas, borrados y actualizaciones, al emplear la shard key, mongos sabe a que shard enviar la petici\u00f3n. En cambio, si la operaci\u00f3n no la indica, se har\u00e1 un broadcast a todas los shards para averiguar donde se encuentra. Por eso, toda inserci\u00f3n debe incluir la shard key. En el caso de tratarse de una clave compuesta, la inserci\u00f3n debe contener la clave completa. Entre los aspectos a tener en cuenta a la hora de elegir una shard key cabe destacar que debe: Tener una alta cardinalidad, para asegurar que los documentos puedan dividirse en los distintos fragmentos. Por ejemplo, si elegimos un shard key que solo tiene 3 valores posibles y tenemos 5 fragmentos, no podr\u00edamos separar los documentos en los 5 fragmentos al solo tener 3 valores posibles para separar. Cuantos m\u00e1s valores posibles pueda tener la clave de fragmentaci\u00f3n, m\u00e1s eficiente ser\u00e1 la divisi\u00f3n de los trozos entre los fragmentos disponibles. Tener un alto nivel de aleatoriedad. Si utilizamos una clave que siga un patr\u00f3n incremental como una fecha o un ID, conllevar\u00e1 que al insertar documentos, el mismo fragmento estar\u00e1 siendo utilizando constantemente durante el rango de valores definido para \u00e9l. Esto provoca que los datos est\u00e9n separados de una manera \u00f3ptima, pero pondr\u00e1 siempre bajo estr\u00e9s a un fragmento en per\u00edodos de tiempo mientras que los otros posiblemente queden con muy poca actividad (comportamiento conocido como hotspotting). Una soluci\u00f3n a las claves que siguen patrones incrementales es aplicar una funcion hash y crear una clave hasheada que si tiene un alto nivel de aleatoriedad. M\u00e1s consejos sobre como elegir la shard key en http://techinsides.blogspot.com.es/2013/09/keynote-concerns-how-to-choose-mongodb.html Finalmente, destacar que toda shard key debe tener un \u00edndice asociado.","title":"Particionando con MongoDB"},{"location":"sa/05mongo-avanzado.html#preparando-el-sharding-con-mongodb","text":"Para comenzar, vamos a crear un particionado en dos instancias en las carpetas /data/s1/db y /data/s2/db. Los logs los colocaremos en /data/logs y crearemos un servidor para la configuraci\u00f3n de los metadatos del shard en /data/con1/db: mkdir -p /data/s1/db /data/s2/db /data/logs /data/conf1/db chown id -u /data/s1/db /data/s2/db /data/logs /data/conf1/db A continuaci\u00f3n, arrancaremos un proceso mongod por cada uno de los shards (con la opci\u00f3n --shardsvr) y un tercero para la base de datos de configuraci\u00f3n (con la opci\u00f3n --configsvr). Finalmente, tambi\u00e9n lanzaremos un proceso mongos: Script de creaci\u00f3n del Shard - (creaShard.sh) mongod --shardsvr --dbpath /data/s1/db --port 27000 --logpath /data/logs/sh1.log --smallfiles --oplogSize 128 --fork mongod --shardsvr --dbpath /data/s2/db --port 27001 --logpath /data/logs/sh2.log --smallfiles --oplogSize 128 --fork mongod --configsvr --dbpath /data/conf1/db --port 25000 --logpath /data/logs/config.log --fork mongos --configdb localhost:25000 --logpath /data/logs/mongos.log --fork El cual lanzaremos mediante bash < creaShard.sh Una vez creado, arrancaremos un shell del mongo, y observaremos como se lanza mongos: $ mongo MongoDB shell version: 3.2.1 connecting to: test mongos> Finalmente, configuraremos el shard mediante el m\u00e9todo sh.addShard(URI), obteniendo confirmaci\u00f3n tras cada cada uno: mongos> sh.addShard(\"localhost:27000\") { \"shardAdded\" : \"shard0000\", \"ok\" : 1 } mongos> sh.addShard(\"localhost:27001\") { \"shardAdded\" : \"shard0001\", \"ok\" : 1 } El valor de la propiedad shardAdded nos devuelve el identificado un\u00edvoco de cada shard. De manera similar que con el conjunto de r\u00e9plicas se emplean el prefijo rs, para interactuar con los componentes implicados en el sharding se emplea sh. Por ejemplo, mediante sh.help() obtendremos la ayuda de los m\u00e9todos disponibles. As\u00ed pues, en este momento tenemos montada un shard con: dos instancias de mongod para almacenar datos en los puertos 27000 y 27001 (shards) una instancia monogd en el puerto 25000 (servidor de configuraci\u00f3n) encargada de almacenar los metadatos del shard, a la cual s\u00f3lo se deber\u00edan conectar el proceso mongos o los drivers para obtener informaci\u00f3n sobre el shard y la shard key y un proceso mongos (enrutador), encargado de aceptar las peticiones de los clientes y enrutar las peticiones al shard adecuado. Shard con dos m\u00e1quinas Figure 8. Shard con dos m\u00e1quinas Si comprobamos el estado del shard podremos comprobar como tenemos dos shards, con sus identificadores y URIs: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: En un entorno de producci\u00f3n, en vez de tener dos shards, habr\u00e1 un conjunto de r\u00e9plicas para asegurar la alta disponibilidad. Adem\u00e1s, tendremos tres servidores de configuraci\u00f3n para asegurar la disponibilidad de \u00e9stos. Del mismo modo, habr\u00e1 tantos procesos mongos creados para un shard como conexiones de clientes. Sharding en un entorno de Producci\u00f3n Figure 9. Sharding en un entorno de producci\u00f3n En init_sharded_replica.sh pod\u00e9is comprobar como crear sharding sobre un conjunto de r\u00e9plicas. 4.6.3. Habilitando el Sharding Una vez hemos creado la estructura necesaria para soportar el sharding vamos a insertar un conjunto de datos para posteriormente particionarlos. Para ello, vamos a insertar cien mil usuarios en una colecci\u00f3n: mongos> use expertojava switched to db expertojava mongos> for (var i=0; i<100000; i++) { db.usuarios.insert({\"login\":\"usu\" + i,\"nombre\":\"nom\" + i*2, \"fcreacion\": new Date()}); } mongos> db.usuarios.count() 100000 Como podemos observar, interactuar con mongos es igual a hacerlo con mongo. Ahora mismo no sabemos en qu\u00e9 cual de los dos shards se han almacenado los datos. Adem\u00e1s, estos datos no est\u00e1n particionados, es decir residen en s\u00f3lo uno de los shards. Para habilitar el sharding a nivel de base de datos y que los datos se repartan entre los fragmentos disponibles, ejecutaremos el comando sh.enableSharding(nombreDB) : mongos> sh.enableSharding(\"expertojava\") Si volvemos a comprobar el estado del shard, tenemos que se ha creado la nueva base de datos que contiene la propiedad \"partitioned\" : true, la cual nos informa que esta fragmentada. Antes de habilitar el sharding para una determinada colecci\u00f3n, tenemos que crear un \u00edndice sobre la shard key: mongos> db.usuarios.createIndex({\"login\": 1}) { \"raw\" : { \"localhost:27000\" : { \"createdCollectionAutomatically\" : false, \"numIndexesBefore\" : 1, \"numIndexesAfter\" : 2, \"ok\" : 1 } }, \"ok\" : 1 } Una vez habilitado el shard ya podemos fragmentar la colecci\u00f3n: mongos> sh.shardCollection(\"expertojava.usuarios\", {\"login\": 1}, false) El m\u00e9todo shardCollection particiona una colecci\u00f3n a partir de una shard key. Para ello, recibe tres par\u00e1metros: nombre de la colecci\u00f3n, con nomenclatura de nombreBD.nombreColecci\u00f3n nombre del campo para fragmentar la colecci\u00f3n, es decir, el shard key. Uno de los requisitos es que esta clave tengo una alta cardinalidad. Si tenemos una propiedad con una cardinalidad baja, podemos hacer un hash de la propiedad mediante {\"login\": \"hashed\"}. Como en nuestro caso hemos utilizado un campo con valores \u00fanicos hemos puesto {\"login\": 1}. booleano que indica si el valor utilizado como shard key es \u00fanico. Para ello, el \u00edndice que se crea sobre el campo debe ser del tipo unique. Este comando divide la colecci\u00f3n en chunks, la cual es la unidad que utiliza MongoDB para mover los datos. Una vez que se ha ejecutado, MongoDB comenzar\u00e1 a balancear la colecci\u00f3n entre los shards del cluster. Este proceso no es instant\u00e1neo. Si la colecci\u00f3n contiene un gran conjunto de datos puede llevar horas completar el balanceo. Si ahora volvemos a comprobar el estado del shard obtendremos: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: { \"_id\" : \"expertojava\", \"primary\" : \"shard0000\", \"partitioned\" : true } expertojava.usuarios shard key: { \"login\" : 1 } unique: false balancing: true chunks: shard0000 1 { \"login\" : { \"$minKey\" : 1 } } -->> { \"login\" : { \"$maxKey\" : 1 } } on : shard0000 Timestamp(1, 0) la propiedad chunks muestra la cantidad de trozos que alberga cada partici\u00f3n. As\u00ed, pues en este momento tenemos 1 chunk Para cada uno de los fragmentos se muestra el rango de valores que alberga cada chunk, as\u00ed como en que shard se ubica. Las claves $minKey y $maxKey son similares a menos infinito y m\u00e1s infinito, es decir, no hay ning\u00fan valor por debajo ni por encima de ellos. Es decir, indican los topes de la colecci\u00f3n. 4.6.4. Trabajando con el Sharding En este momento, el shard esta creado pero todos los nodos residen en un \u00fanico fragmento dentro de un partici\u00f3n. Vamos a volver a insertar 100.000 usuarios m\u00e1s a ver que sucede. mongos> for (var i=100000; i<200000; i++) { db.usuarios.insert({\"login\":\"usu\" + i,\"nombre\":\"nom\" + i*2, \"fcreacion\": new Date()}); } mongos> db.usuarios.count() 200000 Si ahora comprobamos el estado del shard, los datos se deber\u00edan haber repartido entre los shards disponibles: mongos> sh.status() --- Sharding Status --- sharding version: { \"_id\" : 1, \"minCompatibleVersion\" : 5, \"currentVersion\" : 6, \"clusterId\" : ObjectId(\"56bc7054ba6728d2673a1755\") } shards: { \"_id\" : \"shard0000\", \"host\" : \"localhost:27000\" } { \"_id\" : \"shard0001\", \"host\" : \"localhost:27001\" } active mongoses: \"3.2.1\" : 1 balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: 31 : Success databases: { \"_id\" : \"expertojava\", \"primary\" : \"shard0000\", \"partitioned\" : true } expertojava.usuarios shard key: { \"login\" : 1 } unique: false balancing: true chunks: shard0000 32 shard0001 31 too many chunks to print, use verbose if you want to force print Con estos datos se ha forzado a balancear los mismos entre los dos fragmentos, habiendo en cada uno de ellos 32 y 31 trozos respectivamente Si ahora realizamos una consulta y obtenemos su plan de ejecuci\u00f3n veremos como se trata de una consulta que se ejecuta en paralelo: mongos> db.usuarios.find({\"login\":\"usu12345\"}).explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SINGLE_SHARD\", \"shards\" : [ { \"shardName\" : \"shard0001\", \"connectionString\" : \"localhost:27001\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27001, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"login\" : { \"$eq\" : \"usu12345\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"login\" : 1 }, \"indexName\" : \"login_1\", \"isMultiKey\" : false, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 1, \"direction\" : \"forward\", \"indexBounds\" : { \"login\" : [ \"[\\\"usu12345\\\", \\\"usu12345\\\"]\" ] } } } }, \"rejectedPlans\" : [ ] } ] } }, \"ok\" : 1 } Podemos observar como se ha realizado una fase SINGLE_SHARD de manera que ha accedido \u00fanicamente al shard0001, y posteriormente una fase de SHARDING_FILTER en la cual ha empleado un \u00edndice para el escaneo (IXSCAN). Si en vez de obtener un documento concreto, obtenemos el plan de ejecuci\u00f3n de obtener todos los documentos tendremos: mongos> db.usuarios.find().explain() { \"queryPlanner\" : { \"mongosPlannerVersion\" : 1, \"winningPlan\" : { \"stage\" : \"SHARD_MERGE\", \"shards\" : [ { \"shardName\" : \"shard0000\", \"connectionString\" : \"localhost:27000\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27000, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ ] }, \"winningPlan\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ ] }, \"direction\" : \"forward\" } }, \"rejectedPlans\" : [ ] }, { \"shardName\" : \"shard0001\", \"connectionString\" : \"localhost:27001\", \"serverInfo\" : { \"host\" : \"MacBook-Air-de-Aitor.local\", \"port\" : 27001, \"version\" : \"3.2.1\", \"gitVersion\" : \"a14d55980c2cdc565d4704a7e3ad37e4e535c1b2\" }, \"plannerVersion\" : 1, \"namespace\" : \"expertojava.usuarios\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ ] }, \"winningPlan\" : { \"stage\" : \"SHARDING_FILTER\", \"inputStage\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ ] }, \"direction\" : \"forward\" } }, \"rejectedPlans\" : [ ] } ] } }, \"ok\" : 1 } As\u00ed pues, si en una consulta no le enviamos la shard key como criterio, mongos enviar\u00e1 la consulta a cada shard y realizar\u00e1 un SHARD_MERGE con la informaci\u00f3n devuelta de cada shard. Si la consulta contiene la shard key, la consulta se enruta directamente al shard apropiado.","title":"Preparando el Sharding con MongoDB"},{"location":"sa/05mongo-avanzado.html#referencias","text":"Curso M121: The MongoDB Aggregation Framework de la Mongo University.","title":"Referencias"},{"location":"sa/05mongo-avanzado.html#actividades","text":"https://docs.mongodb.com/manual/reference/write-concern/ https://docs.mongodb.com/manual/reference/read-concern/ https://docs.mongodb.com/manual/core/read-preference/","title":"Actividades"},{"location":"sa/06pymongo.html","text":"PyMongo \u00b6 Para acceder a MongoDB desde Python nos vamos a centrar en la librer\u00eda PyMongo . Para instalar la librer\u00eda mediante pip usaremos el comando (recuerda hacerlo dentro de un entorno virtual): pip install pymongo Se recomienda consultar la documentaci\u00f3n o el API para cualquier duda o aclaraci\u00f3n. Versi\u00f3n En el momento de escribir los apuntes, estamos utilizando la versi\u00f3n 4.2.0 de PyMongo. MFlix \u00b6 https://s3.amazonaws.com/edu-downloads.10gen.com/M220P/2022/July/static/handouts/m220/mflix-python.zip Estructura del proyecto \u00b6 verything you will implement is located in the mflix/db.py file, which contains all database interfacing methods. The API will make calls to db.py to interact with MongoDB. The unit tests in tests will test these database access methods directly, without going through the API. The UI will run these methods in integration tests, and therefore requires the full application to be running. The API layer is fully implemented, as is the UI. If you need to run on a port other than 5000, you can edit the index.html file in the build directory to modify the value of window.host. Please do not modify the API layer in any way, movies.py and user.py under the mflix/api directory. Doing so will most likely result in the frontend application failing to validate some of the labs. Preparando el entorno \u00b6 Descargamos y descomprimimos el archivo Dentro de la carpeta, vamos a crear un entorno virtual con venv: virtualenv mflix_venv A continuaci\u00f3n, lo activamos: source mflix_venv/bin/activate E instalamos los requisitos: pip install -r requirements.txt Running the Application In the mflix-python directory you can find a file called dotini. Open this file and enter your Atlas SRV connection string as directed in the comment. This is the information the driver will use to connect. Make sure not to wrap your Atlas SRV connection between quotes: COPY MFLIX_DB_URI = mongodb+srv://... Rename this file to .ini with the following command: COPY mv dotini_unix .ini # on Unix ren dotini_win .ini # on Windows Note: Once you rename this file to .ini, it will no longer be visible in Finder or File Explorer. However, it will be visible from Command Prompt or Terminal, so if you need to edit it again, you can open it from there: COPY vi .ini # on Unix notepad .ini # on Windows Arrancando y Probando \u00b6 Para arrancar la aplicaci\u00f3n ejecutaremos el script run.py : python run.py Al ejecutar el script, arrancar\u00e1 la aplicaci\u00f3n y podremos acceder a ella a trav\u00e9s de http://127.0.0.1:5000/ . PANTALLAZO Si queremos ejecutar los test: Running the Unit Tests To run the unit tests for this course, you will use pytest and needs to be run from mflix-python directory. Each course lab contains a module of unit tests that you can call individually with a command like the following: COPY pytest -m LAB_UNIT_TEST_NAME Each ticket will contain the command to run that ticket's specific unit tests. For example to run the Connection Ticket test your shell command will be: COPY pytest -m connection Un ejemplo b\u00e1sico podr\u00eda ser similar a: client = MongoClient ( 'mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false' ) filter = { 'age' : { '$lt' : 30 } } test_db = client . test people_coll = test_db . people result = people_coll . find ( filter = filter ) ```` ## MongoClient A partir de la URI de conexi\u00f3n a MongoDB , hemos de instanciar la clase [ ` MongoClient ` ]( https : // pymongo . readthedocs . io / en / stable / api / pymongo / mongo_client . html #pymongo.mongo_client.MongoClient): ``` python uri = \"mongodb+srv://usuario:contrasenya@host\" cliente = MongoCliente ( uri ) Podemos obtener informaci\u00f3n de la conexi\u00f3n mediante la propiedad state : print ( cliente . state ) Por ejemplo, en nuestro caso, nos hemos conectado a MongoAtlas y de la salida del estado podemos ver los diferentes hosts que forman parte del conjunto de r\u00e9plicas: Database ( MongoClient ( host = [ 'ac-hrdpnx0-shard-00-02.4hm7u8y.mongodb.net:27017' , 'ac-hrdpnx0-shard-00-01.4hm7u8y.mongodb.net:27017' , 'ac-hrdpnx0-shard-00-00.4hm7u8y.mongodb.net:27017' ], document_class = dict , tz_aware = False , connect = True , authsource = 'admin' , replicaset = 'atlas-pxc2m9-shard-0' , ssl = True , connecttimeoutms = 200 , retrywrites = True ), 'stats' ) Par\u00e1metros adicionales A la hora de crear el cliente, tambi\u00e9n podemos indicarle opciones de configuraci\u00f3n: cliente200Retry = MongoClient(uri, connectTimeoutMS=200, retryWrites=True) Tambi\u00e9n podemos obtener un listado de las bases de datos mediante list_database_names() : print ( cliente . list_database_names ()) # ['sample_airbnb', 'sample_analytics', 'sample_geospatial', 'sample_guides', 'sample_mflix', 'sample_restaurants', 'sample_supplies', 'sample_training', 'sample_weatherdata', 'admin', 'local'] Para conectarnos a una base de datos en concreto, \u00fanicamente accederemos a ella como una propiedad del cliente: bd = cliente . sample_mflix bd = cliente [ \"sample_mflix'] # tb podemos acceder como si fuera un diccionario Bases de datos y colecciones Lazy Es conveniente tener en cuenta que tantos las colecciones como las bases de datos se crean y carga de forma perezosa, esto es, hasta que no realizamos una operaci\u00f3n sobre ellas, no se accede realmente a ellas. As\u00ed pues, para crear realmente una colecci\u00f3n, hemos de insertar un documento en ella. Una vez tenemos la base de datos, el siguiente paso es obtener una colecci\u00f3n: coleccion = bd . movies coleccion = bd [ \"movies'] Si queremos obtener el nombre de todas las colecciones usaremos el m\u00e9todo list_collection_names() : print ( bd . list_collection_names ()) # ['sessions', 'theaters', 'movies', 'comments', 'users'] Primeras consultas \u00b6 Finalmente, sobre una colecci\u00f3n ya podemos realizar consultas y otras operaciones: movies = bd . movies movies . count_documents ({}) # 23530 movies . find_one () Por ejemplo, podemos filtrar las pel\u00edculas de Salma Hayek: movies . find ( { \"cast\" : \"Salma Hayek\" } ) # return the count of movies with \"Salma Hayek\" in the \"cast\" movies . find ( { \"cast\" : \"Salma Hayek\" } ) . count () Para mostrar los documentos BSON, vamos a utilizar la funci\u00f3n dumps que transforma el documento a JSON: # find all movies with Salma Hayek # then pretty print cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) from bson.json_util import dumps print ( dumps ( cursor , indent = 2 )) # find all movies with Salma Hayek, but only project the \"_id\" and \"title\" fields cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 } ) print ( dumps ( cursor , indent = 2 )) # find all movies with Salma Hayek, but only project the \"title\" field cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"_id\" : 0 } ) print ( dumps ( cursor , indent = 2 )) Trabajando con cursores \u00b6 A continuaci\u00f3n vamos a realizar algunas operaciones sobre los cursores con PyMongo y a comparar a c\u00f3mo podemos realizar la misma operaci\u00f3n mediante el motor de agregaciones. Limitando \u00b6 Sobre el cursor podemos restringir la cantidad de resultados devueltos mediante el m\u00e9todo .limit() equivalente a la agregaci\u00f3n $limit : PyMongo Agregaci\u00f3n Salida limited_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } ) . limit ( 2 ) print ( dumps ( limited_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } }, { \"$limit\" : 2 } ] limited_aggregation = movies . aggregate ( pipeline ) print ( dumps ( limited_aggregation , indent = 2 )) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" }, { \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] } ] Ordenando \u00b6 Para ordenar usaremos el m\u00e9todo .sort() que adem\u00e1s de los campos de ordenaci\u00f3n, indicaremos si el criterio ser\u00e1 ascendente o descendente, de forma similar a como lo hacemos con la operaci\u00f3n de agregaci\u00f3n $sort : PyMongo Agregaci\u00f3n Salida from pymongo import DESCENDING , ASCENDING sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) print ( dumps ( sorted_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_aggregation , indent = 2 )) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" , \"year\" : 1981 }, { \"year\" : 1987 , \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] }, { \"year\" : 1990 , ... } ] En el caso de tener una clave compuesta de ordenaci\u00f3n, le pasaremos como par\u00e1metro una lista de tuplas clave/criterio: PyMongo Agregaci\u00f3n Salida sorted_cursor = movies . find ( { \"cast\" : \"Tom Hanks\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ([( \"year\" , ASCENDING ), ( \"title\" , ASCENDING )]) print ( dumps ( sorted_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"cast\" : \"Tom Hanks\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING , \"title\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_aggregation , indent = 2 )) [ { \"cast\" : [ \"Tom Hanks\" , \"Daryl Hannah\" , \"Eugene Levy\" , \"John Candy\" ], \"title\" : \"Splash\" , \"year\" : 1984 }, { \"cast\" : [ \"Tom Hanks\" , \"Jackie Gleason\" , \"Eva Marie Saint\" , \"Hector Elizondo\" ], \"title\" : \"Nothing in Common\" , \"year\" : 1986 }, { \"cast\" : [ \"Tom Hanks\" , \"Elizabeth Perkins\" , \"Robert Loggia\" , \"John Heard\" ], \"title\" : \"Big\" , \"year\" : 1988 }, { \"cast\" : [ \"Sally Field\" , \"Tom Hanks\" , \"John Goodman\" , \"Mark Rydell\" ], \"title\" : \"Punchline\" , \"year\" : 1988 }, ... ] Saltando \u00b6 Cuando paginamos los resultados, para saltar los documentos, haremos uso del m\u00e9todo .skip() , el cual es similar a la operaci\u00f3n $skip . Por ejemplo, la siguiente consulta devuelve 13 documentos, de manera que al saltarnos 12, s\u00f3lo nos devolver\u00e1 uno: PyMongo Agregaci\u00f3n Salida skipped_sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"year\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) . skip ( 12 ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } }, { \"$skip\" : 12 } ] sorted_skipped_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_skipped_aggregation , indent = 2 )) [ { \"cast\" : [ \"James Franco\" , \"Mila Kunis\" , \"Rachel Weisz\" , \"Michelle Williams\" ], \"title\" : \"Oz the Great and Powerful\" , \"year\" : 2013 } ] Agregaciones b\u00e1sicas \u00b6 match_stage = { \"$match\" : { \"directors\" : \"Sam Raimi\" } } project_stage = { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } } pipeline = [ match_stage , project_stage ] sam_raimi_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sam_raimi_aggregation , indent = 2 )) unwind_stage = { \"$unwind\" : \"$directors\" } group_stage = { \"$group\" : { \"_id\" : { \"director\" : \"$directors\" }, \"average_rating\" : { \"$avg\" : \"$imdb.rating\" } } } sort_stage = { \"$sort\" : { \"average_rating\" : - 1 } } # create pipeline from four different stages pipeline = [ unwind_stage , group_stage , sort_stage ] # aggregate using pipeline director_ratings = movies . aggregate ( pipeline ) # iterate through the resulting cursor list ( director_ratings ) First Write insert_one update_one -> upsert Si ya exist\u00eda , al obtener raw_result, la propiedad nModified = 0 y updatedExisting ser\u00eda True Write Concern db . users . with_options ( write_concern = WriteConcern ( w = \"majority\" )) . insert_one ({ \"name\" : name , \"email\" : email , \"password\" : hashedpw }) Update operations update_one update_many Las operaciones de modificaci\u00f3n devuelven un UpdateResult que contiene las propiedades aknowledge , matched_count , modified_count y upserted_id . modified_count y matched_count ser\u00e1n 0 en caso de un upsert. Join entre movies y comments $lookup -> from, let, pipeline, as // from, localField, foreignField, as pipeline = [ { \"$match\" : { \"_id\" : ObjectId ( id ) } }, { \"$lookup\" : { \"from\" : 'comments' , \"let\" : { 'id' : '$_id' }, \"pipeline\" : [ { '$match' : { '$expr' : { '$eq' : [ '$movie_id' , '$$id' ]}} }, { \"$sort\" : { \"date\" : - 1 }} ], \"as\" : 'comments' } } ] movie = db . movies . aggregate ( pipeline ) . next () Al hacer join, los comentarios tienen que unirse a las pel\u00edculas con campos de tipo ObjectId: comment_doc = { \"name\" : user . name , \"email\" : user . email , \"movie_id\" : ObjectId ( movie_id ), \"text\" : comment , \"date\" : date } db . comments . insert_one ( comment_doc ) db . comments . update_one ( { \"_id\" : ObjectId ( comment_id ), \"email\" : user_email }, { \"$set\" : { \"text\" : text , \"date\" : date }} ) Al crear un pool de conexiones, especificar tama\u00f1o del pool y timeout: db = getattr ( g , \"_database\" , None ) DB_URI = current_app . config [ \"DB_URI\" ] DB_NAME = current_app . config [ \"DB_NAME\" ] if db is None : db = g . _database = MongoClient ( DB_URI , maxPoolSize = 50 , wtimeout = 2500 )[ DB_NAME ] return db Siempre especificar un wtimemout con se realiza una escritura con un una mayor\u00eda de escrituras, por si fallase alg\u00fan nodo, no se quedase colgado esperando. db.users.with_options(write_concern=WriteConcern(w=\"majority\"), wtimeout=5000).insert_one({ \"name\": name, \"email\": email, \"password\": hashedpw }) Siempre configurar y capturar los errores de tipo serverSelectionTimeout . DuplicateKeyError puede saltar en _id y en otros campos si creamos un \u00edndice unique . Principio de \"menos privilegios\". Referencias \u00b6 https://www.askpython.com/python-modules/python-mongodb Tutorial oficial de PyMongo Actividades \u00b6","title":"PyMongo"},{"location":"sa/06pymongo.html#pymongo","text":"Para acceder a MongoDB desde Python nos vamos a centrar en la librer\u00eda PyMongo . Para instalar la librer\u00eda mediante pip usaremos el comando (recuerda hacerlo dentro de un entorno virtual): pip install pymongo Se recomienda consultar la documentaci\u00f3n o el API para cualquier duda o aclaraci\u00f3n. Versi\u00f3n En el momento de escribir los apuntes, estamos utilizando la versi\u00f3n 4.2.0 de PyMongo.","title":"PyMongo"},{"location":"sa/06pymongo.html#mflix","text":"https://s3.amazonaws.com/edu-downloads.10gen.com/M220P/2022/July/static/handouts/m220/mflix-python.zip","title":"MFlix"},{"location":"sa/06pymongo.html#estructura-del-proyecto","text":"verything you will implement is located in the mflix/db.py file, which contains all database interfacing methods. The API will make calls to db.py to interact with MongoDB. The unit tests in tests will test these database access methods directly, without going through the API. The UI will run these methods in integration tests, and therefore requires the full application to be running. The API layer is fully implemented, as is the UI. If you need to run on a port other than 5000, you can edit the index.html file in the build directory to modify the value of window.host. Please do not modify the API layer in any way, movies.py and user.py under the mflix/api directory. Doing so will most likely result in the frontend application failing to validate some of the labs.","title":"Estructura del proyecto"},{"location":"sa/06pymongo.html#preparando-el-entorno","text":"Descargamos y descomprimimos el archivo Dentro de la carpeta, vamos a crear un entorno virtual con venv: virtualenv mflix_venv A continuaci\u00f3n, lo activamos: source mflix_venv/bin/activate E instalamos los requisitos: pip install -r requirements.txt Running the Application In the mflix-python directory you can find a file called dotini. Open this file and enter your Atlas SRV connection string as directed in the comment. This is the information the driver will use to connect. Make sure not to wrap your Atlas SRV connection between quotes: COPY MFLIX_DB_URI = mongodb+srv://... Rename this file to .ini with the following command: COPY mv dotini_unix .ini # on Unix ren dotini_win .ini # on Windows Note: Once you rename this file to .ini, it will no longer be visible in Finder or File Explorer. However, it will be visible from Command Prompt or Terminal, so if you need to edit it again, you can open it from there: COPY vi .ini # on Unix notepad .ini # on Windows","title":"Preparando el entorno"},{"location":"sa/06pymongo.html#arrancando-y-probando","text":"Para arrancar la aplicaci\u00f3n ejecutaremos el script run.py : python run.py Al ejecutar el script, arrancar\u00e1 la aplicaci\u00f3n y podremos acceder a ella a trav\u00e9s de http://127.0.0.1:5000/ . PANTALLAZO Si queremos ejecutar los test: Running the Unit Tests To run the unit tests for this course, you will use pytest and needs to be run from mflix-python directory. Each course lab contains a module of unit tests that you can call individually with a command like the following: COPY pytest -m LAB_UNIT_TEST_NAME Each ticket will contain the command to run that ticket's specific unit tests. For example to run the Connection Ticket test your shell command will be: COPY pytest -m connection Un ejemplo b\u00e1sico podr\u00eda ser similar a: client = MongoClient ( 'mongodb://localhost:27017/?readPreference=primary&appname=MongoDB%20Compass&ssl=false' ) filter = { 'age' : { '$lt' : 30 } } test_db = client . test people_coll = test_db . people result = people_coll . find ( filter = filter ) ```` ## MongoClient A partir de la URI de conexi\u00f3n a MongoDB , hemos de instanciar la clase [ ` MongoClient ` ]( https : // pymongo . readthedocs . io / en / stable / api / pymongo / mongo_client . html #pymongo.mongo_client.MongoClient): ``` python uri = \"mongodb+srv://usuario:contrasenya@host\" cliente = MongoCliente ( uri ) Podemos obtener informaci\u00f3n de la conexi\u00f3n mediante la propiedad state : print ( cliente . state ) Por ejemplo, en nuestro caso, nos hemos conectado a MongoAtlas y de la salida del estado podemos ver los diferentes hosts que forman parte del conjunto de r\u00e9plicas: Database ( MongoClient ( host = [ 'ac-hrdpnx0-shard-00-02.4hm7u8y.mongodb.net:27017' , 'ac-hrdpnx0-shard-00-01.4hm7u8y.mongodb.net:27017' , 'ac-hrdpnx0-shard-00-00.4hm7u8y.mongodb.net:27017' ], document_class = dict , tz_aware = False , connect = True , authsource = 'admin' , replicaset = 'atlas-pxc2m9-shard-0' , ssl = True , connecttimeoutms = 200 , retrywrites = True ), 'stats' ) Par\u00e1metros adicionales A la hora de crear el cliente, tambi\u00e9n podemos indicarle opciones de configuraci\u00f3n: cliente200Retry = MongoClient(uri, connectTimeoutMS=200, retryWrites=True) Tambi\u00e9n podemos obtener un listado de las bases de datos mediante list_database_names() : print ( cliente . list_database_names ()) # ['sample_airbnb', 'sample_analytics', 'sample_geospatial', 'sample_guides', 'sample_mflix', 'sample_restaurants', 'sample_supplies', 'sample_training', 'sample_weatherdata', 'admin', 'local'] Para conectarnos a una base de datos en concreto, \u00fanicamente accederemos a ella como una propiedad del cliente: bd = cliente . sample_mflix bd = cliente [ \"sample_mflix'] # tb podemos acceder como si fuera un diccionario Bases de datos y colecciones Lazy Es conveniente tener en cuenta que tantos las colecciones como las bases de datos se crean y carga de forma perezosa, esto es, hasta que no realizamos una operaci\u00f3n sobre ellas, no se accede realmente a ellas. As\u00ed pues, para crear realmente una colecci\u00f3n, hemos de insertar un documento en ella. Una vez tenemos la base de datos, el siguiente paso es obtener una colecci\u00f3n: coleccion = bd . movies coleccion = bd [ \"movies'] Si queremos obtener el nombre de todas las colecciones usaremos el m\u00e9todo list_collection_names() : print ( bd . list_collection_names ()) # ['sessions', 'theaters', 'movies', 'comments', 'users']","title":"Arrancando y Probando"},{"location":"sa/06pymongo.html#primeras-consultas","text":"Finalmente, sobre una colecci\u00f3n ya podemos realizar consultas y otras operaciones: movies = bd . movies movies . count_documents ({}) # 23530 movies . find_one () Por ejemplo, podemos filtrar las pel\u00edculas de Salma Hayek: movies . find ( { \"cast\" : \"Salma Hayek\" } ) # return the count of movies with \"Salma Hayek\" in the \"cast\" movies . find ( { \"cast\" : \"Salma Hayek\" } ) . count () Para mostrar los documentos BSON, vamos a utilizar la funci\u00f3n dumps que transforma el documento a JSON: # find all movies with Salma Hayek # then pretty print cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) from bson.json_util import dumps print ( dumps ( cursor , indent = 2 )) # find all movies with Salma Hayek, but only project the \"_id\" and \"title\" fields cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 } ) print ( dumps ( cursor , indent = 2 )) # find all movies with Salma Hayek, but only project the \"title\" field cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"_id\" : 0 } ) print ( dumps ( cursor , indent = 2 ))","title":"Primeras consultas"},{"location":"sa/06pymongo.html#trabajando-con-cursores","text":"A continuaci\u00f3n vamos a realizar algunas operaciones sobre los cursores con PyMongo y a comparar a c\u00f3mo podemos realizar la misma operaci\u00f3n mediante el motor de agregaciones.","title":"Trabajando con cursores"},{"location":"sa/06pymongo.html#limitando","text":"Sobre el cursor podemos restringir la cantidad de resultados devueltos mediante el m\u00e9todo .limit() equivalente a la agregaci\u00f3n $limit : PyMongo Agregaci\u00f3n Salida limited_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } ) . limit ( 2 ) print ( dumps ( limited_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } }, { \"$limit\" : 2 } ] limited_aggregation = movies . aggregate ( pipeline ) print ( dumps ( limited_aggregation , indent = 2 )) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" }, { \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] } ]","title":"Limitando"},{"location":"sa/06pymongo.html#ordenando","text":"Para ordenar usaremos el m\u00e9todo .sort() que adem\u00e1s de los campos de ordenaci\u00f3n, indicaremos si el criterio ser\u00e1 ascendente o descendente, de forma similar a como lo hacemos con la operaci\u00f3n de agregaci\u00f3n $sort : PyMongo Agregaci\u00f3n Salida from pymongo import DESCENDING , ASCENDING sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) print ( dumps ( sorted_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_aggregation , indent = 2 )) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" , \"year\" : 1981 }, { \"year\" : 1987 , \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] }, { \"year\" : 1990 , ... } ] En el caso de tener una clave compuesta de ordenaci\u00f3n, le pasaremos como par\u00e1metro una lista de tuplas clave/criterio: PyMongo Agregaci\u00f3n Salida sorted_cursor = movies . find ( { \"cast\" : \"Tom Hanks\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ([( \"year\" , ASCENDING ), ( \"title\" , ASCENDING )]) print ( dumps ( sorted_cursor , indent = 2 )) pipeline = [ { \"$match\" : { \"cast\" : \"Tom Hanks\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING , \"title\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_aggregation , indent = 2 )) [ { \"cast\" : [ \"Tom Hanks\" , \"Daryl Hannah\" , \"Eugene Levy\" , \"John Candy\" ], \"title\" : \"Splash\" , \"year\" : 1984 }, { \"cast\" : [ \"Tom Hanks\" , \"Jackie Gleason\" , \"Eva Marie Saint\" , \"Hector Elizondo\" ], \"title\" : \"Nothing in Common\" , \"year\" : 1986 }, { \"cast\" : [ \"Tom Hanks\" , \"Elizabeth Perkins\" , \"Robert Loggia\" , \"John Heard\" ], \"title\" : \"Big\" , \"year\" : 1988 }, { \"cast\" : [ \"Sally Field\" , \"Tom Hanks\" , \"John Goodman\" , \"Mark Rydell\" ], \"title\" : \"Punchline\" , \"year\" : 1988 }, ... ]","title":"Ordenando"},{"location":"sa/06pymongo.html#saltando","text":"Cuando paginamos los resultados, para saltar los documentos, haremos uso del m\u00e9todo .skip() , el cual es similar a la operaci\u00f3n $skip . Por ejemplo, la siguiente consulta devuelve 13 documentos, de manera que al saltarnos 12, s\u00f3lo nos devolver\u00e1 uno: PyMongo Agregaci\u00f3n Salida skipped_sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"year\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) . skip ( 12 ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } }, { \"$skip\" : 12 } ] sorted_skipped_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sorted_skipped_aggregation , indent = 2 )) [ { \"cast\" : [ \"James Franco\" , \"Mila Kunis\" , \"Rachel Weisz\" , \"Michelle Williams\" ], \"title\" : \"Oz the Great and Powerful\" , \"year\" : 2013 } ]","title":"Saltando"},{"location":"sa/06pymongo.html#agregaciones-basicas","text":"match_stage = { \"$match\" : { \"directors\" : \"Sam Raimi\" } } project_stage = { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } } pipeline = [ match_stage , project_stage ] sam_raimi_aggregation = movies . aggregate ( pipeline ) print ( dumps ( sam_raimi_aggregation , indent = 2 )) unwind_stage = { \"$unwind\" : \"$directors\" } group_stage = { \"$group\" : { \"_id\" : { \"director\" : \"$directors\" }, \"average_rating\" : { \"$avg\" : \"$imdb.rating\" } } } sort_stage = { \"$sort\" : { \"average_rating\" : - 1 } } # create pipeline from four different stages pipeline = [ unwind_stage , group_stage , sort_stage ] # aggregate using pipeline director_ratings = movies . aggregate ( pipeline ) # iterate through the resulting cursor list ( director_ratings ) First Write insert_one update_one -> upsert Si ya exist\u00eda , al obtener raw_result, la propiedad nModified = 0 y updatedExisting ser\u00eda True Write Concern db . users . with_options ( write_concern = WriteConcern ( w = \"majority\" )) . insert_one ({ \"name\" : name , \"email\" : email , \"password\" : hashedpw }) Update operations update_one update_many Las operaciones de modificaci\u00f3n devuelven un UpdateResult que contiene las propiedades aknowledge , matched_count , modified_count y upserted_id . modified_count y matched_count ser\u00e1n 0 en caso de un upsert. Join entre movies y comments $lookup -> from, let, pipeline, as // from, localField, foreignField, as pipeline = [ { \"$match\" : { \"_id\" : ObjectId ( id ) } }, { \"$lookup\" : { \"from\" : 'comments' , \"let\" : { 'id' : '$_id' }, \"pipeline\" : [ { '$match' : { '$expr' : { '$eq' : [ '$movie_id' , '$$id' ]}} }, { \"$sort\" : { \"date\" : - 1 }} ], \"as\" : 'comments' } } ] movie = db . movies . aggregate ( pipeline ) . next () Al hacer join, los comentarios tienen que unirse a las pel\u00edculas con campos de tipo ObjectId: comment_doc = { \"name\" : user . name , \"email\" : user . email , \"movie_id\" : ObjectId ( movie_id ), \"text\" : comment , \"date\" : date } db . comments . insert_one ( comment_doc ) db . comments . update_one ( { \"_id\" : ObjectId ( comment_id ), \"email\" : user_email }, { \"$set\" : { \"text\" : text , \"date\" : date }} ) Al crear un pool de conexiones, especificar tama\u00f1o del pool y timeout: db = getattr ( g , \"_database\" , None ) DB_URI = current_app . config [ \"DB_URI\" ] DB_NAME = current_app . config [ \"DB_NAME\" ] if db is None : db = g . _database = MongoClient ( DB_URI , maxPoolSize = 50 , wtimeout = 2500 )[ DB_NAME ] return db Siempre especificar un wtimemout con se realiza una escritura con un una mayor\u00eda de escrituras, por si fallase alg\u00fan nodo, no se quedase colgado esperando. db.users.with_options(write_concern=WriteConcern(w=\"majority\"), wtimeout=5000).insert_one({ \"name\": name, \"email\": email, \"password\": hashedpw }) Siempre configurar y capturar los errores de tipo serverSelectionTimeout . DuplicateKeyError puede saltar en _id y en otros campos si creamos un \u00edndice unique . Principio de \"menos privilegios\".","title":"Agregaciones b\u00e1sicas"},{"location":"sa/06pymongo.html#referencias","text":"https://www.askpython.com/python-modules/python-mongodb Tutorial oficial de PyMongo","title":"Referencias"},{"location":"sa/06pymongo.html#actividades","text":"","title":"Actividades"},{"location":"sa/planning.html","text":"Planning (10h) \u00b6 01 NoSQL (1h) \u00b6 Revisar: https://aws.amazon.com/es/nosql/ Revisar: https://www.mongodb.com/es/nosql-explained 02 MongoDB (1h + 1h) \u00b6 Conceptos Uso mediante Docker ... poner enlaces para instalar Mongo desde comandos Restore/dump Consultas sencillas (1h) CRUD 03 Modelado (1h) \u00b6 Modelado Relaciones Patrones Framework de agregaci\u00f3n (1h) \u00b6 04 Formatos (1h) \u00b6 csv json columnar avro parquet orc 05 Escalabilidad y Rendimiento (2h) \u00b6 MongoAtlas / Compass Cluster ReplicaSet Replicaci\u00f3n Sharding Rendimiento \u00cdndices 06 PyMongo (2h) \u00b6 Sesiones \u00b6 NoSQL + MongoDB I MongoDB II. Framework de agregaci\u00f3n Formatos de datos. Modelado NoSQL Escalabilidad y Rendimiento. Mongo y Python Pendiente \u00b6 Capped collections: colecciones limitadas","title":"Planning (10h)"},{"location":"sa/planning.html#planning-10h","text":"","title":"Planning (10h)"},{"location":"sa/planning.html#01-nosql-1h","text":"Revisar: https://aws.amazon.com/es/nosql/ Revisar: https://www.mongodb.com/es/nosql-explained","title":"01 NoSQL (1h)"},{"location":"sa/planning.html#02-mongodb-1h-1h","text":"Conceptos Uso mediante Docker ... poner enlaces para instalar Mongo desde comandos Restore/dump Consultas sencillas (1h) CRUD","title":"02 MongoDB (1h + 1h)"},{"location":"sa/planning.html#03-modelado-1h","text":"Modelado Relaciones Patrones","title":"03 Modelado (1h)"},{"location":"sa/planning.html#framework-de-agregacion-1h","text":"","title":"Framework de agregaci\u00f3n (1h)"},{"location":"sa/planning.html#04-formatos-1h","text":"csv json columnar avro parquet orc","title":"04 Formatos (1h)"},{"location":"sa/planning.html#05-escalabilidad-y-rendimiento-2h","text":"MongoAtlas / Compass Cluster ReplicaSet Replicaci\u00f3n Sharding Rendimiento \u00cdndices","title":"05 Escalabilidad y Rendimiento (2h)"},{"location":"sa/planning.html#06-pymongo-2h","text":"","title":"06 PyMongo (2h)"},{"location":"sa/planning.html#sesiones","text":"NoSQL + MongoDB I MongoDB II. Framework de agregaci\u00f3n Formatos de datos. Modelado NoSQL Escalabilidad y Rendimiento. Mongo y Python","title":"Sesiones"},{"location":"sa/planning.html#pendiente","text":"Capped collections: colecciones limitadas","title":"Pendiente"}]}