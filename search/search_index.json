{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . Este curso se ha dise\u00f1ado mediante un proceso de flexibilizaci\u00f3n del curriculum, mediante el cual hemos repartido los diferentes resultados de aprendizaje a lo largo de tres unidades formativas. Cada una de las unidades formativas contiene varias unidades de trabajo centradas en un \u00e1rea de trabajo concreta. En cada unidad formativa vamos a realizar como hilo conductor un Proyecto de Innovaci\u00f3n Aplicada (PIAFP) que dirija el aprendizaje del alumnado. En este sitio web podr\u00e1s consultar los apuntes y ejercicios que he trabajado directamente con el alumnado durante el curso 22/23: Unidad Formativa I - Toma de decisiones UT2 - Sistemas de almacenamiento UT5 - Ecosistema Hadoop UT6 - Datos en el cloud UT7 - PIA FP Lara Unidad Formativa II - Industria IoT Unidad Formativa III - Sistemas Expertos Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . Este curso se ha dise\u00f1ado mediante un proceso de flexibilizaci\u00f3n del curriculum, mediante el cual hemos repartido los diferentes resultados de aprendizaje a lo largo de tres unidades formativas. Cada una de las unidades formativas contiene varias unidades de trabajo centradas en un \u00e1rea de trabajo concreta. En cada unidad formativa vamos a realizar como hilo conductor un Proyecto de Innovaci\u00f3n Aplicada (PIAFP) que dirija el aprendizaje del alumnado. En este sitio web podr\u00e1s consultar los apuntes y ejercicios que he trabajado directamente con el alumnado durante el curso 22/23: Unidad Formativa I - Toma de decisiones UT2 - Sistemas de almacenamiento UT5 - Ecosistema Hadoop UT6 - Datos en el cloud UT7 - PIA FP Lara Unidad Formativa II - Industria IoT Unidad Formativa III - Sistemas Expertos Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"cloud/index.html","text":"Unidad de Trabajo 6.- Datos en el cloud \u00b6 Resultados de aprendizaje \u00b6 RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 33.- Cloud Lunes 21 Nov 2p + 2o 33.- AWS Lunes 21 Nov 1p + 1o 40.- Almacenamiento en AWS Mi\u00e9rcoles 30 Nov 2p + 2o 44.- Computaci\u00f3n en AWS Lunes 5 Dic 1p + 1o 44.- Datos en AWS Lunes 5 Dic 1p + 1o 46.- Servicios de datos en AWS (Glue y Athena) Mi\u00e9rcoles 7 Dic 2p + 2o","title":"Unidad de Trabajo 6.- Datos en el cloud"},{"location":"cloud/index.html#unidad-de-trabajo-6-datos-en-el-cloud","text":"","title":"Unidad de Trabajo 6.- Datos en el cloud"},{"location":"cloud/index.html#resultados-de-aprendizaje","text":"RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.","title":"Resultados de aprendizaje"},{"location":"cloud/index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 33.- Cloud Lunes 21 Nov 2p + 2o 33.- AWS Lunes 21 Nov 1p + 1o 40.- Almacenamiento en AWS Mi\u00e9rcoles 30 Nov 2p + 2o 44.- Computaci\u00f3n en AWS Lunes 5 Dic 1p + 1o 44.- Datos en AWS Lunes 5 Dic 1p + 1o 46.- Servicios de datos en AWS (Glue y Athena) Mi\u00e9rcoles 7 Dic 2p + 2o","title":"Planificaci\u00f3n"},{"location":"cloud/01cloud.html","text":"Cloud Computing \u00b6 La Nube \u00b6 Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Si buscamos una definici\u00f3n formal, seg\u00fan el NIST (National Institute for Standards and Technology) la computaci\u00f3n en la nube es \"un modelo que permite un acceso ubicuo, conveniente y bajo demanda a una red de recursos de computaci\u00f3n configurables (por ejemplo, redes, servidores, almacenamiento, aplicaciones y servicios) que se pueden aprovisionar r\u00e1pidamente y entregar con una interacci\u00f3n m\u00ednima con el proveedor del servicio\". Ventajas \u00b6 As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar hacia arriba, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo ( pay-as-you go pricing ). S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx. CapEx vs OpEx \u00b6 Hay dos tipos diferentes de gastos que se deben tener en cuenta: CapEx vs OpEx La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Info Una soluci\u00f3n on-premise implica que somos propietarios de los servidores, que hemos de contratar a un equipo de administradores de sistemas para su mantenimiento, que necesitamos unas instalaciones adecuadas y que hemos de protegerlas contra todo tipo de fallos. Esta elasticidad facilita que la capacidad de c\u00f3mputo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes. Consumo respecto a Capacidad / Tiempo Coste total de propiedad \u00b6 El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micas. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala , el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo. Inconvenientes \u00b6 Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...). Servicios en la nube \u00b6 Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. IaaS \u00b6 La infraestructura como servicio ( Infrastructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centros de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la infraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas. PaaS \u00b6 La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo (lenguajes de programaci\u00f3n, librer\u00edas, servicios, ...), administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El proveedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos. SaaS \u00b6 Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. Este concepto se conoce como principio de responsabilidad compartida y lo estudiaremos en detalle en la pr\u00f3xima sesi\u00f3n . \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa. Tipos de arquitectura seg\u00fan la infraestructura \u00b6 Arquitecturas on premise \u00b6 Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto al mantenimiento y actualizaci\u00f3n del hardware. Arquitecturas cloud \u00b6 Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel ( serverless , bases de datos administradas...). Arquitecturas h\u00edbridas \u00b6 Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados. Plataformas Cloud \u00b6 En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://azure.microsoft.com/es-es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesador, memoria y almacenamiento a medida Azure VM Igual pero en Azure Google Compute Engine Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente... Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesarios. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: Posici\u00f3n competitiva plataformas cloud - Synergy Infraestructura cloud \u00b6 Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas. Regiones y Zonas de disponibilidad \u00b6 A lo largo de todo el globo terr\u00e1queo se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante redes troncales privadas que proporciona un menor coste y una latencia de red entre regiones mejor que las conexiones p\u00fablicas de Internet. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Conviene destacar que no todos los servicios de AWS est\u00e1n disponibles en todas las regiones. Ejemplo de infraestructura AWS Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 100.000 servidores f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestro centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Ejemplo de cluster con diferentes AZ Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cl\u00faster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centros de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que si cae un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n. Cada AZ est\u00e1 dise\u00f1ada como una zona de error independiente: Est\u00e1n f\u00edsicamente separadas dentro de una regi\u00f3n metropolitana t\u00edpica. Se ubican en terrenos de menor riesgo de inundaci\u00f3n. Utilizan sistemas de alimentaci\u00f3n ininterrumpida discretos y generaci\u00f3n de copia de seguridad en las instalaciones La alimentaci\u00f3n se suministra desde distintas redes de empresas de servicios p\u00fablicos independientes. Est\u00e1n conectadas de forma redundante a varios ISP. Despliegue \u00b6 Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 28 regiones que tiene AWS que incluyen 90 zonas de disponibilidad (en Noviembre de 2022 han a\u00f1adido a Espa\u00f1a como una nueva regi\u00f3n - eu-south-2 ): Regiones 2021 Regiones 2022 Regiones AWS Regiones AWS Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos. Ubicaciones de borde \u00b6 Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generan vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Por ejemplo, Amazon Cloudfront despliega m\u00e1s de 410 puntos de presencia (m\u00e1s de 400 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional) a trav\u00e9s de 90 ciudades en 47 pa\u00edses. Mapa de ubicaciones de borde de AWS El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , de manera que resuelve los dominios m\u00e1s r\u00e1pido y redirige el tr\u00e1fico a los nodos Cloudfront . Referencias \u00b6 Curso Academy Cloud Foundation de Amazon Web Services . Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2022 Conceptos fundamentales de Azure AWS Certified Cloud Practitioner Training 2020 - Full Course Actividades \u00b6 A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar la cuenta educativa de AWS que habr\u00e1s recibido en tu email. ( RA5075.2 / CE5.2a / 1p) Contesta a las siguientes preguntas justificando tus repuestas: A nivel financiero, \u00bfqu\u00e9 ventajas supone apostar por un modelo cloud respecto a una soluci\u00f3n on-premise ? De los modelos As a Service , \u00bfcu\u00e1l permite crear aplicaciones que est\u00e9n disponibles para cualquier persona en cualquier lugar?. ( RA5075.2 / CE5.2a / 1p) Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS .","title":"S33.- Cloud"},{"location":"cloud/01cloud.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"cloud/01cloud.html#la-nube","text":"Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Si buscamos una definici\u00f3n formal, seg\u00fan el NIST (National Institute for Standards and Technology) la computaci\u00f3n en la nube es \"un modelo que permite un acceso ubicuo, conveniente y bajo demanda a una red de recursos de computaci\u00f3n configurables (por ejemplo, redes, servidores, almacenamiento, aplicaciones y servicios) que se pueden aprovisionar r\u00e1pidamente y entregar con una interacci\u00f3n m\u00ednima con el proveedor del servicio\".","title":"La Nube"},{"location":"cloud/01cloud.html#ventajas","text":"As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar hacia arriba, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo ( pay-as-you go pricing ). S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx.","title":"Ventajas"},{"location":"cloud/01cloud.html#capex-vs-opex","text":"Hay dos tipos diferentes de gastos que se deben tener en cuenta: CapEx vs OpEx La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Info Una soluci\u00f3n on-premise implica que somos propietarios de los servidores, que hemos de contratar a un equipo de administradores de sistemas para su mantenimiento, que necesitamos unas instalaciones adecuadas y que hemos de protegerlas contra todo tipo de fallos. Esta elasticidad facilita que la capacidad de c\u00f3mputo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes. Consumo respecto a Capacidad / Tiempo","title":"CapEx vs OpEx"},{"location":"cloud/01cloud.html#coste-total-de-propiedad","text":"El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micas. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala , el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo.","title":"Coste total de propiedad"},{"location":"cloud/01cloud.html#inconvenientes","text":"Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...).","title":"Inconvenientes"},{"location":"cloud/01cloud.html#servicios-en-la-nube","text":"Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo.","title":"Servicios en la nube"},{"location":"cloud/01cloud.html#iaas","text":"La infraestructura como servicio ( Infrastructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centros de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la infraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas.","title":"IaaS"},{"location":"cloud/01cloud.html#paas","text":"La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo (lenguajes de programaci\u00f3n, librer\u00edas, servicios, ...), administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El proveedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos.","title":"PaaS"},{"location":"cloud/01cloud.html#saas","text":"Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. Este concepto se conoce como principio de responsabilidad compartida y lo estudiaremos en detalle en la pr\u00f3xima sesi\u00f3n . \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa.","title":"SaaS"},{"location":"cloud/01cloud.html#tipos-de-arquitectura-segun-la-infraestructura","text":"","title":"Tipos de arquitectura seg\u00fan la infraestructura"},{"location":"cloud/01cloud.html#arquitecturas-on-premise","text":"Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto al mantenimiento y actualizaci\u00f3n del hardware.","title":"Arquitecturas on premise"},{"location":"cloud/01cloud.html#arquitecturas-cloud","text":"Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel ( serverless , bases de datos administradas...).","title":"Arquitecturas cloud"},{"location":"cloud/01cloud.html#arquitecturas-hibridas","text":"Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados.","title":"Arquitecturas h\u00edbridas"},{"location":"cloud/01cloud.html#plataformas-cloud","text":"En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://azure.microsoft.com/es-es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesador, memoria y almacenamiento a medida Azure VM Igual pero en Azure Google Compute Engine Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente... Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesarios. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: Posici\u00f3n competitiva plataformas cloud - Synergy","title":"Plataformas Cloud"},{"location":"cloud/01cloud.html#infraestructura-cloud","text":"Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas.","title":"Infraestructura cloud"},{"location":"cloud/01cloud.html#regiones-y-zonas-de-disponibilidad","text":"A lo largo de todo el globo terr\u00e1queo se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante redes troncales privadas que proporciona un menor coste y una latencia de red entre regiones mejor que las conexiones p\u00fablicas de Internet. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Conviene destacar que no todos los servicios de AWS est\u00e1n disponibles en todas las regiones. Ejemplo de infraestructura AWS Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 100.000 servidores f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestro centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Ejemplo de cluster con diferentes AZ Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cl\u00faster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centros de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que si cae un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n. Cada AZ est\u00e1 dise\u00f1ada como una zona de error independiente: Est\u00e1n f\u00edsicamente separadas dentro de una regi\u00f3n metropolitana t\u00edpica. Se ubican en terrenos de menor riesgo de inundaci\u00f3n. Utilizan sistemas de alimentaci\u00f3n ininterrumpida discretos y generaci\u00f3n de copia de seguridad en las instalaciones La alimentaci\u00f3n se suministra desde distintas redes de empresas de servicios p\u00fablicos independientes. Est\u00e1n conectadas de forma redundante a varios ISP.","title":"Regiones y Zonas de disponibilidad"},{"location":"cloud/01cloud.html#despliegue","text":"Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 28 regiones que tiene AWS que incluyen 90 zonas de disponibilidad (en Noviembre de 2022 han a\u00f1adido a Espa\u00f1a como una nueva regi\u00f3n - eu-south-2 ): Regiones 2021 Regiones 2022 Regiones AWS Regiones AWS Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos.","title":"Despliegue"},{"location":"cloud/01cloud.html#ubicaciones-de-borde","text":"Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generan vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Por ejemplo, Amazon Cloudfront despliega m\u00e1s de 410 puntos de presencia (m\u00e1s de 400 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional) a trav\u00e9s de 90 ciudades en 47 pa\u00edses. Mapa de ubicaciones de borde de AWS El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , de manera que resuelve los dominios m\u00e1s r\u00e1pido y redirige el tr\u00e1fico a los nodos Cloudfront .","title":"Ubicaciones de borde"},{"location":"cloud/01cloud.html#referencias","text":"Curso Academy Cloud Foundation de Amazon Web Services . Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2022 Conceptos fundamentales de Azure AWS Certified Cloud Practitioner Training 2020 - Full Course","title":"Referencias"},{"location":"cloud/01cloud.html#actividades","text":"A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar la cuenta educativa de AWS que habr\u00e1s recibido en tu email. ( RA5075.2 / CE5.2a / 1p) Contesta a las siguientes preguntas justificando tus repuestas: A nivel financiero, \u00bfqu\u00e9 ventajas supone apostar por un modelo cloud respecto a una soluci\u00f3n on-premise ? De los modelos As a Service , \u00bfcu\u00e1l permite crear aplicaciones que est\u00e9n disponibles para cualquier persona en cualquier lugar?. ( RA5075.2 / CE5.2a / 1p) Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS .","title":"Actividades"},{"location":"cloud/02aws.html","text":"Amazon Web Services \u00b6 Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones. Servicios \u00b6 Los servicios de AWS se clasifican en categor\u00edas: Servicios y regiones Los servicios y los productos de AWS se encuentran disponibles por regi\u00f3n, por lo que es posible que dependiendo de la regi\u00f3n a la que accedamos no veamos todos los servicios. Para obtener una lista de los servicios de AWS ofrecidos por regi\u00f3n, podemos consultar la Lista de servicios regionales . A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados: Almacenamiento \u00b6 Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos (ofrece una durabilidad del 99.999999999%), seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y en nuestro caso, como Data Lake para anal\u00edtica de datos. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento S3 seguro, duradero y de muy bajo coste para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloques de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Cada volumen EBS se replica autom\u00e1ticamente dentro de su AZ para prevenir fallos de hardware y ofrece alta disponibilidad y durabilidad. Se utiliza para una amplia gama de cargas de trabajo que requieren un rendimiento con baja latencia, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia. Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para utilizarlo tanto con los servicios cloud de AWS como con los recursos on-premise . Dise\u00f1ado para escalar a petabytes bajo demanda, aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Almacenamiento en AWS . Inform\u00e1tica / Computaci\u00f3n \u00b6 Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable (de forma el\u00e1stica, tanto hacia arriba como hacia abajo) en forma de m\u00e1quinas virtuales en la nube en cuesti\u00f3n de minutos. Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para desplegar y escalar aplicaciones y/o servicios web desarrolladas en Java / PHP / Python en servidores web conocidos, como Apache , Nginx , Passenger o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). Se sube el c\u00f3digo fuente a ejecutar, y AWS se encarga de ejecutarlo y escalarlo conforme necesite. S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes dentro de AWS. Bases de Datos \u00b6 Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Datos en AWS . Redes \u00b6 Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Global Accelerator : utiliza las ubicaciones de borde para encontrar la ruta \u00f3ptima a la regi\u00f3n donde reside nuestra aplicaci\u00f3n (haciendo uso tanto de protocolos HTTP como TCP/UDP). AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS. Seguridad en AWS \u00b6 Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS. Servicios de administraci\u00f3n de costes \u00b6 Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo. Administraci\u00f3n y gobernanza de datos \u00b6 La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorizar recursos y aplicaciones. Si CloudTrail monitoriza personas, CloudWatch monitoriza servicios. CloudWatch es perfecto para asegurar de que los servicios de la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se pueden pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla: Redes en AWS \u00b6 Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son v\u00e1lidos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad. Amazon VPC \u00b6 AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00edamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalle, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b VPC Wizard \u00b6 Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , el cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad. IP El\u00e1stica \u00b6 Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html Seguridad en la Nube \u00b6 La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero. Modelo de responsabilidad compartida de AWS \u00b6 La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general. Responsabilidad de AWS \u00b6 AWS es responsable de proteger la infraestructura en la que se ejecutan todos los servicios ofrecidos por la nube de AWS (en algunas preguntas de la certificaci\u00f3n se refieren a ellos por servicios de la nube): Seguridad f\u00edsica de los centros de datos con acceso controlado basado en las necesidades en instalaciones sin identificaci\u00f3n, con guardias de seguridad permanentes, autenticaci\u00f3n de dos factores, revisi\u00f3n y registro de accesos, videovigilancia, y destrucci\u00f3n y desmagnetizaci\u00f3n de discos. Infraestructura de hardware , como servidores, dispositivos de almacenamiento y otros dispositivos de los que dependen los servicios de AWS. Infraestructura de software , que aloja sistemas operativos, aplicaciones de servicios y software de virtualizaci\u00f3n. Infraestructura de red , como routers, conmutadores, balanceadores de carga, firewalls y cableado. AWS tambi\u00e9n monitoriza la red en l\u00edmites externos, protege los puntos de acceso y proporciona infraestructura redundante con detecci\u00f3n de intrusiones de forma constante Responsabilidad del cliente \u00b6 El cliente es responsable del cifrado de los datos en reposo y los datos en tr\u00e1nsito, de todo lo que se pone en la nube. Los pasos de seguridad que debe tomar depender\u00e1n de los servicios que utilice y de la complejidad del sistema. Si entramos en m\u00e1s detalle, es responsable de: El sistema operativo de la instancia de Amazon EC2 : incluidas las actualizaciones, los parches de seguridad y su mantenimiento. La protecci\u00f3n de las aplicaciones que se lanzan en los recursos AWS: contrase\u00f1as, acceso basado en roles, etc. Configuraci\u00f3n del grupo de seguridad . SO o firewalls basados en host: incluidos los sistemas de detecci\u00f3n o prevenci\u00f3n de intrusiones. Configuraciones de red . Administraci\u00f3n de cuentas : Configuraci\u00f3n de inicio de sesi\u00f3n y credenciales para cada usuario. Respecto al contenido cr\u00edtico, el cliente es responsable de administrar: El contenido que elige almacenar en AWS. Los servicios de AWS que se utilizan con el contenido. En qu\u00e9 pa\u00eds se almacena ese contenido. El formato y la estructura de ese contenido y si est\u00e1 enmascarado, cifrado o es an\u00f3nimo. Qui\u00e9n tiene acceso a ese contenido y c\u00f3mo se conceden, administran y revocan esos derechos de acceso. AWS IAM \u00b6 AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM. IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz. Autenticaci\u00f3n \u00b6 Cuando se define un usuario de IAM se indica qu\u00e9 tipo de acceso puede utilizar el usuario para obtener acceso a los recursos de AWS: acceso mediante programaci\u00f3n: mediante email y clave de acceso secreta cuando realice una llamada a la API de AWS mediante la CLI de AWS, el SDK de AWS o cualquier otra herramienta de desarrollo. acceso a la consola de administraci\u00f3n de AWS: mediante usuario / contrase\u00f1a m\u00e1s el ID/alias de cuenta. Es recomendable activar MFA ( Multi-Factor Authentication ) para a\u00f1adir una capa m\u00e1s de seguridad. acceso mediante ambos tipos Autorizaci\u00f3n \u00b6 Una vez que el usuario se ha autenticado, se ha de determinar qu\u00e9 permisos debe concederse a un usuario, servicio o aplicaci\u00f3n. De forma predeterminada, los usuarios de IAM no tienen permiso para obtener acceso a los recursos o los datos en una cuenta de AWS. En su lugar, debe conceder permisos de forma expl\u00edcita a un usuario, grupo o rol mediante la creaci\u00f3n de una pol\u00edtica de IAM, ya que por defecto, se denegar\u00e1n todas las acciones que no se hayan permitido expl\u00edcitamente. Consejo Seguir el principio de m\u00ednimo privilegio: conceder \u00fanicamente los privilegios de usuario m\u00ednimos que necesita el usuario. El alcance de las configuraciones del servicio de IAM es global, se aplican en todas las regiones de AWS. Pol\u00edticas IAM \u00b6 Una pol\u00edtica de IAM es una instrucci\u00f3n formal mediante un documento JSON con los permisos que se conceder\u00e1 a una entidad. Las entidad es incluyen usuarios, grupos, roles o recursos. Las pol\u00edticas especifican cu\u00e1les son las acciones permitidas, cu\u00e1les son los recursos a los que estas tienen permiso y cu\u00e1l ser\u00e1 el efecto cuando el usuario solicite acceso a los recursos. Info Una sola pol\u00edtica se puede asociar a varias entidades. Una sola entidad puede tener varias pol\u00edticas asociadas a ella. Hay dos tipos de pol\u00edticas de IAM: pol\u00edticas basadas en identidad : controlan qu\u00e9 acciones puede realizar dicha identidad, en qu\u00e9 recursos y en qu\u00e9 condiciones. A su vez se dividen en administradas (asociada a varios usuarios/grupos/roles) o insertadas (un \u00fanico usuario/grupo/rol). pol\u00edticas basadas en recursos : son documentos de pol\u00edtica JSON que se asocian a un recurso (por ejemplo, un bucket de S3). Estas pol\u00edticas controlan qu\u00e9 acciones puede realizar una entidad principal especificada en dicho recurso y en qu\u00e9 condiciones. Destacar que no todos los servicios de AWS soportan este tipo de pol\u00edticas. Pol\u00edticas y permisos \u00b6 El usuario solo podr\u00e1 realizar la acci\u00f3n si la acci\u00f3n solicitada no est\u00e1 denegada de forma expl\u00edcita y adem\u00e1s est\u00e1 permitida de forma expl\u00edcita. Cuando IAM determina si se concede un permiso, primero comprueba la existencia de cualquier pol\u00edtica de denegaci\u00f3n expl\u00edcita aplicable. Si no existe ninguna denegaci\u00f3n expl\u00edcita, comprueba si existe alguna pol\u00edtica de permisos expl\u00edcitos aplicable. Si no existe una pol\u00edtica de denegaci\u00f3n expl\u00edcita ni de permiso expl\u00edcito, IAM vuelve a la forma predeterminada, que consiste en denegar el acceso. Este proceso se denomina denegaci\u00f3n impl\u00edcita . Permisos IAM Otros servicios relacionados con la seguridad AWS Organizations : Permite configurar los permisos de una organizaci\u00f3n que contiene varias cuentas de usuario en unidades organizativas (UO), y unificar tanto la seguridad como la facturaci\u00f3n AWS Key Management Service (AWS KMS): servicio que permite crear y administrar claves de cifrado Amazon Cognito : permite controlar el acceso a recursos de AWS desde aplicaciones con una credencial \u00fanica mediante SAML. AWS Shield : servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones ejecutadas en AWS. Pr\u00e1cticas recomendadas \u00b6 Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS. AWS CLI \u00b6 AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema operativo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/getting-started-install.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola del curso Leaner Labs , tras arrancar el laboratorio, pulsaremos (1) en la opci\u00f3n AWS Details , y posteriormente veremos los datos de acceso temporales al pulsar (2) en Show de la opci\u00f3n AWS CLI : Credenciales de AWS Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> aws configure Otra forma de configurar estos valores es mediante el comando aws configure , el cual nos preguntar\u00e1 los siguientes datos: AWS Access Key ID [****************6YUJ]: AWS Secret Access Key [****************4TEz]: Default region name [us-east-1]: Default output format [None]: El problema es que no nos solicita el token de sesi\u00f3n, por lo cual s\u00f3lo lo podemos utilizar si tenemos una cuenta real de AWS. Orden Al ejecutar comandos AWS CLI, AWS CLI buscar\u00e1 las credenciales primero en las variables de entorno y, a continuaci\u00f3n, en el archivo de configuraci\u00f3n. Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias. AWS Cloudshell \u00b6 Es un shell integrado en la consola web que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operacionales ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Interfaz de Cloudshell Referencias \u00b6 Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS Actividades \u00b6 ( RA5075.2 / CE5.2a / 1p) Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . ( RA5075.2 / CE5.2a / 1p) Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity .","title":"S33.- AWS"},{"location":"cloud/02aws.html#amazon-web-services","text":"Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones.","title":"Amazon Web Services"},{"location":"cloud/02aws.html#servicios","text":"Los servicios de AWS se clasifican en categor\u00edas: Servicios y regiones Los servicios y los productos de AWS se encuentran disponibles por regi\u00f3n, por lo que es posible que dependiendo de la regi\u00f3n a la que accedamos no veamos todos los servicios. Para obtener una lista de los servicios de AWS ofrecidos por regi\u00f3n, podemos consultar la Lista de servicios regionales . A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados:","title":"Servicios"},{"location":"cloud/02aws.html#almacenamiento","text":"Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos (ofrece una durabilidad del 99.999999999%), seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y en nuestro caso, como Data Lake para anal\u00edtica de datos. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento S3 seguro, duradero y de muy bajo coste para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloques de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Cada volumen EBS se replica autom\u00e1ticamente dentro de su AZ para prevenir fallos de hardware y ofrece alta disponibilidad y durabilidad. Se utiliza para una amplia gama de cargas de trabajo que requieren un rendimiento con baja latencia, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia. Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para utilizarlo tanto con los servicios cloud de AWS como con los recursos on-premise . Dise\u00f1ado para escalar a petabytes bajo demanda, aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Almacenamiento en AWS .","title":"Almacenamiento"},{"location":"cloud/02aws.html#informatica-computacion","text":"Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable (de forma el\u00e1stica, tanto hacia arriba como hacia abajo) en forma de m\u00e1quinas virtuales en la nube en cuesti\u00f3n de minutos. Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para desplegar y escalar aplicaciones y/o servicios web desarrolladas en Java / PHP / Python en servidores web conocidos, como Apache , Nginx , Passenger o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). Se sube el c\u00f3digo fuente a ejecutar, y AWS se encarga de ejecutarlo y escalarlo conforme necesite. S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes dentro de AWS.","title":"Inform\u00e1tica / Computaci\u00f3n"},{"location":"cloud/02aws.html#bases-de-datos","text":"Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n Datos en AWS .","title":"Bases de Datos"},{"location":"cloud/02aws.html#redes","text":"Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Global Accelerator : utiliza las ubicaciones de borde para encontrar la ruta \u00f3ptima a la regi\u00f3n donde reside nuestra aplicaci\u00f3n (haciendo uso tanto de protocolos HTTP como TCP/UDP). AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS.","title":"Redes"},{"location":"cloud/02aws.html#seguridad-en-aws","text":"Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS.","title":"Seguridad en AWS"},{"location":"cloud/02aws.html#servicios-de-administracion-de-costes","text":"Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo.","title":"Servicios de administraci\u00f3n de costes"},{"location":"cloud/02aws.html#administracion-y-gobernanza-de-datos","text":"La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorizar recursos y aplicaciones. Si CloudTrail monitoriza personas, CloudWatch monitoriza servicios. CloudWatch es perfecto para asegurar de que los servicios de la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se pueden pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla:","title":"Administraci\u00f3n y gobernanza de datos"},{"location":"cloud/02aws.html#redes-en-aws","text":"Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son v\u00e1lidos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad.","title":"Redes en AWS"},{"location":"cloud/02aws.html#amazon-vpc","text":"AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00edamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalle, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b","title":"Amazon VPC"},{"location":"cloud/02aws.html#vpc-wizard","text":"Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , el cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad.","title":"VPC Wizard"},{"location":"cloud/02aws.html#ip-elastica","text":"Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"IP El\u00e1stica"},{"location":"cloud/02aws.html#seguridad-en-la-nube","text":"La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero.","title":"Seguridad en la Nube"},{"location":"cloud/02aws.html#modelo-de-responsabilidad-compartida-de-aws","text":"La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general.","title":"Modelo de responsabilidad compartida de AWS"},{"location":"cloud/02aws.html#aws-iam","text":"AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM. IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz.","title":"AWS IAM"},{"location":"cloud/02aws.html#practicas-recomendadas","text":"Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS.","title":"Pr\u00e1cticas recomendadas"},{"location":"cloud/02aws.html#aws-cli","text":"AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema operativo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/getting-started-install.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola del curso Leaner Labs , tras arrancar el laboratorio, pulsaremos (1) en la opci\u00f3n AWS Details , y posteriormente veremos los datos de acceso temporales al pulsar (2) en Show de la opci\u00f3n AWS CLI : Credenciales de AWS Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> aws configure Otra forma de configurar estos valores es mediante el comando aws configure , el cual nos preguntar\u00e1 los siguientes datos: AWS Access Key ID [****************6YUJ]: AWS Secret Access Key [****************4TEz]: Default region name [us-east-1]: Default output format [None]: El problema es que no nos solicita el token de sesi\u00f3n, por lo cual s\u00f3lo lo podemos utilizar si tenemos una cuenta real de AWS. Orden Al ejecutar comandos AWS CLI, AWS CLI buscar\u00e1 las credenciales primero en las variables de entorno y, a continuaci\u00f3n, en el archivo de configuraci\u00f3n. Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias.","title":"AWS CLI"},{"location":"cloud/02aws.html#aws-cloudshell","text":"Es un shell integrado en la consola web que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operacionales ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Interfaz de Cloudshell","title":"AWS Cloudshell"},{"location":"cloud/02aws.html#referencias","text":"Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS","title":"Referencias"},{"location":"cloud/02aws.html#actividades","text":"( RA5075.2 / CE5.2a / 1p) Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . ( RA5075.2 / CE5.2a / 1p) Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity .","title":"Actividades"},{"location":"cloud/03s3.html","text":"Almacenamiento en la nube \u00b6 El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de EC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers , memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar: Servicios de almacenamiento de AWS Amazon EBS \u00b6 Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad. Vol\u00famenes \u00b6 IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS. Instant\u00e1neas \u00b6 Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS. Amazon EFS \u00b6 Amazon Elastic File System ( https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS. Amazon S3 \u00b6 S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas, ofreciendo 99,99% de disponibilidad (4 nueves) y 99,999999999% (11 nueves) de durabilidad . Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que deseemos dentro de un bucket (los objetos pueden ser de hasta 5TB), S3 permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivos se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo, por ende, es un servicio de alto rendimiento, con una latencia de primer byte que se mide en milisegundos para la mayor\u00eda de las clases de almacenamiento. Elementos de un objeto \u00b6 Cada objeto que almacenamos en S3 tiene: una clave: nombre que se le asigna al objeto y que se utiliza para recuperarlo. un id de versi\u00f3n: podemos mantener un hist\u00f3rico de cambios mediante el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html un valor: contenido real que se almacena. metadatos: pares clave-valor, que podemos definir nosotros como usuarios. subrecursos: que utiliza AWS para almacenar informaci\u00f3n adicional. Clases de almacenamiento \u00b6 S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/S3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas. Buckets \u00b6 Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Podemos controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n podemos obtener registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, v\u00eddeos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar los datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre s3severo2223 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato). Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://s3severo2223.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos. Casos de uso \u00b6 Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada. Costes \u00b6 Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ . Sitio web est\u00e1tico \u00b6 Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo8a-web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo8a-web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablico M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html S3 Select \u00b6 Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejecutar consulta SQL . Si nos fijamos en la imagen, se crea una tabla ficticia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html . Acceso \u00b6 Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS) o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo8a-cli --region us-east-1 { \"Location\" : \"/severo8a-cli\" } Otra forma es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Seguridad \u00b6 Por defecto, todos los buckets de S3 son privados y solo pueden acceder los usuarios a los que se les concede acceso expl\u00edcitamente. Normalmente utilizaremos S3 para almacenar datos que utiliza una aplicaci\u00f3n que se ejecuta fuera de S3 o para hacer copias de seguridad de informaci\u00f3n confidencial. Para estos casos de uso comunes, el acceso p\u00fablico a los buckets que contienen datos nunca debe ser otorgado. Disponemos de varias herramientas y posibilidades para controlar el acceso a los buckets u objetos de S3: Uso del bloqueo de acceso p\u00fablico, la cual anula de manera sencilla cualquier otra pol\u00edtica o permisos de objetos. La escritura de pol\u00edticas IAM que indiquen los usuarios o roles que pueden acceder a buckets y objetos espec\u00edficos. La escritura de pol\u00edticas de bucket que definan el acceso a buckets u objetos espec\u00edficos, ya sea para conceder acceso entre cuentas de AWS o para conceder acceso p\u00fablico o an\u00f3nimo a los datos de Amazon S3. Se utiliza cuando el usuario o el sistema no pueden autenticarse mediante IAM. Creaci\u00f3n de puntos de acceso de S3, los cuales son nombres de alojamiento \u00fanicos que aplican permisos y controles de red definidos para las solicitudes que se realizan a trav\u00e9s de \u00e9l. Podemos crear puntos de acceso individualizados con nombres y permisos personalizados para cada aplicaci\u00f3n que accede a los datos. Configuraci\u00f3n de listas de control de acceso (ACL). Se utilizan con menos frecuencia (las ACL preceden a la IAM). Si utilizamos ACL, se recomienda no establecer un acceso muy permisivo. La seguridad es cr\u00edtica Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos. S3 y Python \u00b6 AWS ofrece diferentes interfaces para interactuar con S3, siendo Python una de las formas. A continuaci\u00f3n, vamos a crear una serie de scripts en nuestra m\u00e1quina local que accede, a\u00f1ade o recupere objeto<s de S3. Para autenticarnos en AWS desde nuestro sistema local, recuerda que necesitas copiar las credenciales de acceso en ~/.aws/credentials o mediante las variables de entorno . Para acceder a AWS desde Python , Amazon ofrece el SDK Boto3 . Para poder utilizarlo, la instalaremos mediante pip install boto3 Pod\u00e9is consultar toda la informaci\u00f3n relativa a Boto3 en su documentaci\u00f3n oficial en https://boto3.amazonaws.com/v1/documentation/api/latest/index.html Existen dos posibilidades para acceder a AWS mediante Boto3 : Recursos : representan un interfaz orientado a objetos de AWS, de manera que cada recurso contendr\u00e1 un identificador, unos atributos y un conjunto de operaciones. Un ejemplo de recurso es el S3 . M\u00e1s informaci\u00f3n sobre recursos en la documentaci\u00f3n oficial . Clientes : ofrecen un interfaz de bajo nivel que se mapea 1:1 con el API de cada servicio. Los clientes se generan a partir de la definici\u00f3n JSON del servicio. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial . En resumen, los recursos son una abstracci\u00f3n a m\u00e1s alto nivel de los servicios AWS que los clientes. Se recomienda el uso de los recursos al no tener que preocuparse de c\u00f3mo se realiza por debajo la comunicaci\u00f3n e interacci\u00f3n con los servicios. Sin embargo, a d\u00eda de hoy no hay recursos para todos los servicios AWS, y por ello, en ocasiones no queda otra opci\u00f3n que utilizar los clientes. Para demostrar las diferencias, vamos a ver c\u00f3mo podemos realizar algunas operaciones haciendo uso del cliente o del recurso (en estos ejemplos nos vamos a centrar en el servicio S3 - https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html .): Buckets de un usuario Elementos de un bucket Creaci\u00f3n de un bucket Eliminaci\u00f3n de un recurso/bucket s3-buckets.py import boto3 # Opci\u00f3n 1 print ( 'Buckets mediante resource:' ) s3resource = boto3 . resource ( 's3' , region_name = 'us-east-1' ) buckets = s3resource . buckets . all () for bucket in buckets : print ( f ' \\t { bucket . name } ' ) # Opci\u00f3n 2 print ( 'Buckets mediante el cliente:' ) s3client = boto3 . client ( 's3' ) response = s3client . list_buckets () for bucket in response [ 'Buckets' ]: print ( f ' \\t { bucket [ \"Name\" ] } ' ) s3-bucket-objects.py import boto3 s3 = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3 . Bucket ( 's3severo2223python' ) for obj in bucket . objects . all (): print ( obj . key ) s3-create-bucket.py import boto3 # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2223python-r' ) bucket . create () # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) response = s3c . create_bucket ( Bucket = 's3severo2223python' ) s3-delete.py import boto3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2223python' ) # Elimina todos los objetos del bucket bucket . objects . delete () # Elimina el bucket bucket . delete () Caso de uso - Comunicaci\u00f3n con S3 \u00b6 Vamos a trabajar con el archivo datosPeliculas.json el cual contiene un listado de pel\u00edculas con las que trabajaremos en los siguientes casos de uso. Primero vamos a ver c\u00f3mo podemos subir el archivo a S3 mediante Python: upload-movies-s3.py import boto3 ficheroUpload = \"datosPeliculas.json\" nombreBucket = \"s3severo2223python\" # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) # 1.1 mediante upload_file bucket = s3r . Object ( nombreBucket , 'datosSubidosR1.txt' ) bucket . upload_file ( ficheroUpload ) # 1.2 mediante put object = s3r . Object ( nombreBucket , 'datosSubidosR2.txt' ) object . put ( Body = b 'Ejemplo de datos binarios' ) # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) # 2.1 mediante upload_file response = s3c . upload_file ( ficheroUpload , nombreBucket , \"datosSubidosC1.json\" ) # 2.2 mediante upload_fileobj with open ( ficheroUpload , \"rb\" ) as f : s3c . upload_fileobj ( f , nombreBucket , \"datosSubidosC2.json\" ) # Cliente: Ejemplo de como crear un objeto y a\u00f1adirle contenido desde Python s3c . put_object ( Body = b 'Ejemplo de datos binarios' , Bucket = nombreBucket , Key = \"datosSubidosC3\" ) Si lo que queremos es descargar un recurso de S3 para tenerlo en nuestro sistema local haremos: import boto3 # Opci\u00f3n 1 - descarga s3c = boto3 . client ( 's3' ) s3c . download_file ( 's3severo2223python' , 'datosPeliculas.json' , 'datosDescargados.json' ) # Opci\u00f3n 2 - Abrimos el fichero y metemos el contenido with open ( 'fichero.json' , 'wb' ) as f : s3c . download_fileobj ( 's3severo2223python' , 'datosPeliculas.json' , f ) Actividades \u00b6 Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web. (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado. Referencias \u00b6 Amazon EBS Amazon S3 Amazon EFS","title":"S40.- S3"},{"location":"cloud/03s3.html#almacenamiento-en-la-nube","text":"El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de EC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers , memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar: Servicios de almacenamiento de AWS","title":"Almacenamiento en la nube"},{"location":"cloud/03s3.html#amazon-ebs","text":"Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad.","title":"Amazon EBS"},{"location":"cloud/03s3.html#volumenes","text":"IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS.","title":"Vol\u00famenes"},{"location":"cloud/03s3.html#instantaneas","text":"Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS.","title":"Instant\u00e1neas"},{"location":"cloud/03s3.html#amazon-efs","text":"Amazon Elastic File System ( https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS.","title":"Amazon EFS"},{"location":"cloud/03s3.html#amazon-s3","text":"S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas, ofreciendo 99,99% de disponibilidad (4 nueves) y 99,999999999% (11 nueves) de durabilidad . Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que deseemos dentro de un bucket (los objetos pueden ser de hasta 5TB), S3 permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivos se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo, por ende, es un servicio de alto rendimiento, con una latencia de primer byte que se mide en milisegundos para la mayor\u00eda de las clases de almacenamiento.","title":"Amazon S3"},{"location":"cloud/03s3.html#elementos-de-un-objeto","text":"Cada objeto que almacenamos en S3 tiene: una clave: nombre que se le asigna al objeto y que se utiliza para recuperarlo. un id de versi\u00f3n: podemos mantener un hist\u00f3rico de cambios mediante el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html un valor: contenido real que se almacena. metadatos: pares clave-valor, que podemos definir nosotros como usuarios. subrecursos: que utiliza AWS para almacenar informaci\u00f3n adicional.","title":"Elementos de un objeto"},{"location":"cloud/03s3.html#clases-de-almacenamiento","text":"S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/S3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas.","title":"Clases de almacenamiento"},{"location":"cloud/03s3.html#buckets","text":"Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Podemos controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n podemos obtener registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, v\u00eddeos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar los datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre s3severo2223 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato). Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://s3severo2223.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos.","title":"Buckets"},{"location":"cloud/03s3.html#casos-de-uso","text":"Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada.","title":"Casos de uso"},{"location":"cloud/03s3.html#costes","text":"Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ .","title":"Costes"},{"location":"cloud/03s3.html#sitio-web-estatico","text":"Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo8a-web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo8a-web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablico M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html","title":"Sitio web est\u00e1tico"},{"location":"cloud/03s3.html#s3-select","text":"Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejecutar consulta SQL . Si nos fijamos en la imagen, se crea una tabla ficticia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html .","title":"S3 Select"},{"location":"cloud/03s3.html#acceso","text":"Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS) o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo8a-cli --region us-east-1 { \"Location\" : \"/severo8a-cli\" } Otra forma es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda.","title":"Acceso"},{"location":"cloud/03s3.html#seguridad","text":"Por defecto, todos los buckets de S3 son privados y solo pueden acceder los usuarios a los que se les concede acceso expl\u00edcitamente. Normalmente utilizaremos S3 para almacenar datos que utiliza una aplicaci\u00f3n que se ejecuta fuera de S3 o para hacer copias de seguridad de informaci\u00f3n confidencial. Para estos casos de uso comunes, el acceso p\u00fablico a los buckets que contienen datos nunca debe ser otorgado. Disponemos de varias herramientas y posibilidades para controlar el acceso a los buckets u objetos de S3: Uso del bloqueo de acceso p\u00fablico, la cual anula de manera sencilla cualquier otra pol\u00edtica o permisos de objetos. La escritura de pol\u00edticas IAM que indiquen los usuarios o roles que pueden acceder a buckets y objetos espec\u00edficos. La escritura de pol\u00edticas de bucket que definan el acceso a buckets u objetos espec\u00edficos, ya sea para conceder acceso entre cuentas de AWS o para conceder acceso p\u00fablico o an\u00f3nimo a los datos de Amazon S3. Se utiliza cuando el usuario o el sistema no pueden autenticarse mediante IAM. Creaci\u00f3n de puntos de acceso de S3, los cuales son nombres de alojamiento \u00fanicos que aplican permisos y controles de red definidos para las solicitudes que se realizan a trav\u00e9s de \u00e9l. Podemos crear puntos de acceso individualizados con nombres y permisos personalizados para cada aplicaci\u00f3n que accede a los datos. Configuraci\u00f3n de listas de control de acceso (ACL). Se utilizan con menos frecuencia (las ACL preceden a la IAM). Si utilizamos ACL, se recomienda no establecer un acceso muy permisivo. La seguridad es cr\u00edtica Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos.","title":"Seguridad"},{"location":"cloud/03s3.html#s3-y-python","text":"AWS ofrece diferentes interfaces para interactuar con S3, siendo Python una de las formas. A continuaci\u00f3n, vamos a crear una serie de scripts en nuestra m\u00e1quina local que accede, a\u00f1ade o recupere objeto<s de S3. Para autenticarnos en AWS desde nuestro sistema local, recuerda que necesitas copiar las credenciales de acceso en ~/.aws/credentials o mediante las variables de entorno . Para acceder a AWS desde Python , Amazon ofrece el SDK Boto3 . Para poder utilizarlo, la instalaremos mediante pip install boto3 Pod\u00e9is consultar toda la informaci\u00f3n relativa a Boto3 en su documentaci\u00f3n oficial en https://boto3.amazonaws.com/v1/documentation/api/latest/index.html Existen dos posibilidades para acceder a AWS mediante Boto3 : Recursos : representan un interfaz orientado a objetos de AWS, de manera que cada recurso contendr\u00e1 un identificador, unos atributos y un conjunto de operaciones. Un ejemplo de recurso es el S3 . M\u00e1s informaci\u00f3n sobre recursos en la documentaci\u00f3n oficial . Clientes : ofrecen un interfaz de bajo nivel que se mapea 1:1 con el API de cada servicio. Los clientes se generan a partir de la definici\u00f3n JSON del servicio. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial . En resumen, los recursos son una abstracci\u00f3n a m\u00e1s alto nivel de los servicios AWS que los clientes. Se recomienda el uso de los recursos al no tener que preocuparse de c\u00f3mo se realiza por debajo la comunicaci\u00f3n e interacci\u00f3n con los servicios. Sin embargo, a d\u00eda de hoy no hay recursos para todos los servicios AWS, y por ello, en ocasiones no queda otra opci\u00f3n que utilizar los clientes. Para demostrar las diferencias, vamos a ver c\u00f3mo podemos realizar algunas operaciones haciendo uso del cliente o del recurso (en estos ejemplos nos vamos a centrar en el servicio S3 - https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html .): Buckets de un usuario Elementos de un bucket Creaci\u00f3n de un bucket Eliminaci\u00f3n de un recurso/bucket s3-buckets.py import boto3 # Opci\u00f3n 1 print ( 'Buckets mediante resource:' ) s3resource = boto3 . resource ( 's3' , region_name = 'us-east-1' ) buckets = s3resource . buckets . all () for bucket in buckets : print ( f ' \\t { bucket . name } ' ) # Opci\u00f3n 2 print ( 'Buckets mediante el cliente:' ) s3client = boto3 . client ( 's3' ) response = s3client . list_buckets () for bucket in response [ 'Buckets' ]: print ( f ' \\t { bucket [ \"Name\" ] } ' ) s3-bucket-objects.py import boto3 s3 = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3 . Bucket ( 's3severo2223python' ) for obj in bucket . objects . all (): print ( obj . key ) s3-create-bucket.py import boto3 # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2223python-r' ) bucket . create () # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) response = s3c . create_bucket ( Bucket = 's3severo2223python' ) s3-delete.py import boto3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2223python' ) # Elimina todos los objetos del bucket bucket . objects . delete () # Elimina el bucket bucket . delete ()","title":"S3 y Python"},{"location":"cloud/03s3.html#caso-de-uso-comunicacion-con-s3","text":"Vamos a trabajar con el archivo datosPeliculas.json el cual contiene un listado de pel\u00edculas con las que trabajaremos en los siguientes casos de uso. Primero vamos a ver c\u00f3mo podemos subir el archivo a S3 mediante Python: upload-movies-s3.py import boto3 ficheroUpload = \"datosPeliculas.json\" nombreBucket = \"s3severo2223python\" # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) # 1.1 mediante upload_file bucket = s3r . Object ( nombreBucket , 'datosSubidosR1.txt' ) bucket . upload_file ( ficheroUpload ) # 1.2 mediante put object = s3r . Object ( nombreBucket , 'datosSubidosR2.txt' ) object . put ( Body = b 'Ejemplo de datos binarios' ) # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) # 2.1 mediante upload_file response = s3c . upload_file ( ficheroUpload , nombreBucket , \"datosSubidosC1.json\" ) # 2.2 mediante upload_fileobj with open ( ficheroUpload , \"rb\" ) as f : s3c . upload_fileobj ( f , nombreBucket , \"datosSubidosC2.json\" ) # Cliente: Ejemplo de como crear un objeto y a\u00f1adirle contenido desde Python s3c . put_object ( Body = b 'Ejemplo de datos binarios' , Bucket = nombreBucket , Key = \"datosSubidosC3\" ) Si lo que queremos es descargar un recurso de S3 para tenerlo en nuestro sistema local haremos: import boto3 # Opci\u00f3n 1 - descarga s3c = boto3 . client ( 's3' ) s3c . download_file ( 's3severo2223python' , 'datosPeliculas.json' , 'datosDescargados.json' ) # Opci\u00f3n 2 - Abrimos el fichero y metemos el contenido with open ( 'fichero.json' , 'wb' ) as f : s3c . download_fileobj ( 's3severo2223python' , 'datosPeliculas.json' , f )","title":"Caso de uso - Comunicaci\u00f3n con S3"},{"location":"cloud/03s3.html#actividades","text":"Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web. (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado.","title":"Actividades"},{"location":"cloud/03s3.html#referencias","text":"Amazon EBS Amazon S3 Amazon EFS","title":"Referencias"},{"location":"cloud/04emr.html","text":"AWS Elastic Map Reduce \u00b6 Es un servicio de Amazon Web Services que permite crear clusters Hadoop a demanda. \u2022 Utiliza una distribuci\u00f3n propia de Amazon que permite seleccionar los componentes que van a lanzarse en el cluster (Hive, Spark, etc.). \u2022 Ofrece elasticidad: modificar din\u00e1micamente el dimensionamiento del cluster seg\u00fan necesidades. \u2022 Se ejecuta sobre m\u00e1quinas EC2 (IaaS). \u2022 Pago por uso: el coste asociado es el alquiler de las m\u00e1quinas por horas m\u00e1s un sobrecoste de aproximadamente el 25%. \u2022 Ejemplo de coste aproximado: 20 nodos con 122 Gb RAM, 16 vCPU: 32 \u20ac/h. Para ejecutar una tarea de 10h: 320 \u20ac. Con 200 nodos: duraci\u00f3n = 1 hora, coste = 320 \u20ac. Arrancar EMR con instancias m4.large Seguridad: vockey Descargar claves: Conectar mediante SSH ssh -i labsuser.pem hadoop@ec2-75-101-186-154.compute-1.amazonaws.com The authenticity of host 'ec2-75-101-186-154.compute-1.amazonaws.com (75.101.186.154)' can 't be established. ECDSA key fingerprint is SHA256:ZDeS9KrmmJP1vCdPDgRXUMZUpMuuOiLSGvX8qERbFaI. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' ec2-75-101-186-154.compute-1.amazonaws.com,75.101.186.154 ' ( ECDSA ) to the list of known hosts. Last login: Thu Nov 24 12 :19:03 2022 __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/ 22 package ( s ) needed for security, out of 32 available Run \"sudo yum update\" to apply all updates. EEEEEEEEEEEEEEEEEEEE MMMMMMMM MMMMMMMM RRRRRRRRRRRRRRR E::::::::::::::::::E M:::::::M M:::::::M R::::::::::::::R EE:::::EEEEEEEEE:::E M::::::::M M::::::::M R:::::RRRRRR:::::R E::::E EEEEE M:::::::::M M:::::::::M RR::::R R::::R E::::E M::::::M:::M M:::M::::::M R:::R R::::R E:::::EEEEEEEEEE M:::::M M:::M M:::M M:::::M R:::RRRRRR:::::R E::::::::::::::E M:::::M M:::M:::M M:::::M R:::::::::::RR E:::::EEEEEEEEEE M:::::M M:::::M M:::::M R:::RRRRRR::::R E::::E M:::::M M:::M M:::::M R:::R R::::R E::::E EEEEE M:::::M MMM M:::::M R:::R R::::R EE:::::EEEEEEEE::::E M:::::M M:::::M R:::R R::::R E::::::::::::::::::E M:::::M M:::::M RR::::R R::::R EEEEEEEEEEEEEEEEEEEE MMMMMMM MMMMMMM RRRRRRR RRRRRR sudo chmod 777 /etc/hadoop/conf/hdfs-site.xml nano /etc/hadoop/conf/hdfs-site.xml Editamos la propiedad dfs.webhdfs.enabled y la ponemos a true:\u00e7 <property> <name> dfs.webhdfs.enabled </name> <value> false </value> </property> Reiniciamos el servicios HDFS: sudo systemctl restart hadoop-hdfs-namenode Ahora desde el interfaz de HDFS ya podemos navegar por las carpetas y ver el contenido: ` Ahora vamos a cambiar la configuraci\u00f3n de Hue para indicarle la ruta de HDFS y poder navegar: sudo chmod 777 /etc/hue/conf/hue.ini nano /etc/hue/conf/hue.ini `` Y ponemos bien el puerto al 50070 webhdfs_url = http://ip-172-31-60-228.ec2.internal:50070/webhdfs/v1 ``` bash sudo systemctl restart hue CREATE TABLE impressions2 ( requestBeginTime string, adId string, impressionId string, referrer string, userAgent string, userCookie string, ip string) PARTITIONED BY (dt string) ROW FORMAT serde 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths'='requestBeginTime, adId, impressionId, referrer, userAgent, userCookie, ip') LOCATION '${SAMPLE}/tables/impressions';","title":"AWS Elastic Map Reduce"},{"location":"cloud/04emr.html#aws-elastic-map-reduce","text":"Es un servicio de Amazon Web Services que permite crear clusters Hadoop a demanda. \u2022 Utiliza una distribuci\u00f3n propia de Amazon que permite seleccionar los componentes que van a lanzarse en el cluster (Hive, Spark, etc.). \u2022 Ofrece elasticidad: modificar din\u00e1micamente el dimensionamiento del cluster seg\u00fan necesidades. \u2022 Se ejecuta sobre m\u00e1quinas EC2 (IaaS). \u2022 Pago por uso: el coste asociado es el alquiler de las m\u00e1quinas por horas m\u00e1s un sobrecoste de aproximadamente el 25%. \u2022 Ejemplo de coste aproximado: 20 nodos con 122 Gb RAM, 16 vCPU: 32 \u20ac/h. Para ejecutar una tarea de 10h: 320 \u20ac. Con 200 nodos: duraci\u00f3n = 1 hora, coste = 320 \u20ac. Arrancar EMR con instancias m4.large Seguridad: vockey Descargar claves: Conectar mediante SSH ssh -i labsuser.pem hadoop@ec2-75-101-186-154.compute-1.amazonaws.com The authenticity of host 'ec2-75-101-186-154.compute-1.amazonaws.com (75.101.186.154)' can 't be established. ECDSA key fingerprint is SHA256:ZDeS9KrmmJP1vCdPDgRXUMZUpMuuOiLSGvX8qERbFaI. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added ' ec2-75-101-186-154.compute-1.amazonaws.com,75.101.186.154 ' ( ECDSA ) to the list of known hosts. Last login: Thu Nov 24 12 :19:03 2022 __ | __ | _ ) _ | ( / Amazon Linux 2 AMI ___ | \\_ __ | ___ | https://aws.amazon.com/amazon-linux-2/ 22 package ( s ) needed for security, out of 32 available Run \"sudo yum update\" to apply all updates. EEEEEEEEEEEEEEEEEEEE MMMMMMMM MMMMMMMM RRRRRRRRRRRRRRR E::::::::::::::::::E M:::::::M M:::::::M R::::::::::::::R EE:::::EEEEEEEEE:::E M::::::::M M::::::::M R:::::RRRRRR:::::R E::::E EEEEE M:::::::::M M:::::::::M RR::::R R::::R E::::E M::::::M:::M M:::M::::::M R:::R R::::R E:::::EEEEEEEEEE M:::::M M:::M M:::M M:::::M R:::RRRRRR:::::R E::::::::::::::E M:::::M M:::M:::M M:::::M R:::::::::::RR E:::::EEEEEEEEEE M:::::M M:::::M M:::::M R:::RRRRRR::::R E::::E M:::::M M:::M M:::::M R:::R R::::R E::::E EEEEE M:::::M MMM M:::::M R:::R R::::R EE:::::EEEEEEEE::::E M:::::M M:::::M R:::R R::::R E::::::::::::::::::E M:::::M M:::::M RR::::R R::::R EEEEEEEEEEEEEEEEEEEE MMMMMMM MMMMMMM RRRRRRR RRRRRR sudo chmod 777 /etc/hadoop/conf/hdfs-site.xml nano /etc/hadoop/conf/hdfs-site.xml Editamos la propiedad dfs.webhdfs.enabled y la ponemos a true:\u00e7 <property> <name> dfs.webhdfs.enabled </name> <value> false </value> </property> Reiniciamos el servicios HDFS: sudo systemctl restart hadoop-hdfs-namenode Ahora desde el interfaz de HDFS ya podemos navegar por las carpetas y ver el contenido: ` Ahora vamos a cambiar la configuraci\u00f3n de Hue para indicarle la ruta de HDFS y poder navegar: sudo chmod 777 /etc/hue/conf/hue.ini nano /etc/hue/conf/hue.ini `` Y ponemos bien el puerto al 50070 webhdfs_url = http://ip-172-31-60-228.ec2.internal:50070/webhdfs/v1 ``` bash sudo systemctl restart hue CREATE TABLE impressions2 ( requestBeginTime string, adId string, impressionId string, referrer string, userAgent string, userCookie string, ip string) PARTITIONED BY (dt string) ROW FORMAT serde 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths'='requestBeginTime, adId, impressionId, referrer, userAgent, userCookie, ip') LOCATION '${SAMPLE}/tables/impressions';","title":"AWS Elastic Map Reduce"},{"location":"cloud/05computacion.html","text":"Computaci\u00f3n en la nube \u00b6 Introducci\u00f3n \u00b6 Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio. Amazon EC2 \u00b6 Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual. Paso 1: AMI \u00b6 Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde. Tipos de AMI \u00b6 Puede elegir entre los siguientes tipos de AMI: Quick Start : AWS ofrece una serie de AMI predise\u00f1adas, tanto Linux como Windows, para lanzar las instancias. Mis AMI : estas son las AMI que hemos creado nosotros, ya sea a partir de m\u00e1quinas locales que hayamos creado en VmWare, VirtualBox, o una previa que hemos creado en una instancia EC2, configurado y luego exportado. AWS Marketplace : cat\u00e1logo que incluye miles de soluciones de software creadas por empresas terceras (las cuales pueden cobrar por su uso). Estas AMI pueden ofrecer casos de uso espec\u00edficos para que pueda ponerse en marcha r\u00e1pidamente. AMI de la comunidad : estas son AMI creadas por personas de todo el mundo.AWS no controla estas AMI, as\u00ed que deben utilizarse bajo la propia responsabilidad, evitando su uso en entornos corporativos o de producci\u00f3n. Las AMI dependen de la regi\u00f3n Las AMI que creamos se hacen en la regi\u00f3n en la que estamos conectados. Si la necesitamos en otra regi\u00f3n, debemos realizar un proceso de copia. Paso 2: Tipo de instancias \u00b6 El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino. Categor\u00edas \u00b6 Las categor\u00edas de tipos de instancia incluyen instancias de uso general, optimizadas para inform\u00e1tica, optimizadas para memoria, optimizadas para almacenamiento y de inform\u00e1tica acelerada. Categor\u00eda Tipo de instancia Caso de uso Uso general a1, m4, m5, t2, t3 Amplio Computaci\u00f3n c4, c5 Alto rendimiento Memoria r4, r5 , x1, z1 Big Data Inform\u00e1tica acelerada f1, g3, g4, p2, p3 Machine Learning Almacenamiento d2, h1, i3 Sistemas de archivos distribuidos Tipos de instancias \u00b6 Los tipos de instancias ( https://aws.amazon.com/es/ec2/instance-types/ ) ofrecen familias, generaciones y tama\u00f1os . As\u00ed pues, el tipo de instancia t3.large referencia a la familia T , de la tercera generaci\u00f3n y con un tama\u00f1o large . En general, los tipos de instancia que son de una generaci\u00f3n superior son m\u00e1s potentes y ofrecen una mejor relaci\u00f3n calidad/precio. Comparando tipos de instancias Cuando se comparan los tama\u00f1os hay que examinar la parte del coeficiente en la categor\u00eda de tama\u00f1o. Por ejemplo, una instancia t3.2xlarge tiene el doble de CPU virtual y memoria que una t3.xlarge . A su vez, la instancia t3.xlarge tiene el doble de CPU virtual y memoria que una t3.large . Tambi\u00e9n se debe tener en cuenta que el ancho de banda de red tambi\u00e9n est\u00e1 vinculado al tama\u00f1o de la instancia de Amazon EC2. Si ejecutar\u00e1 trabajos que requieren un uso muy intensivo de la red, es posible que deba aumentar las especificaciones de la instancia para que satisfaga sus necesidades. A la hora de elegir un tipo de instancia, nos centraremos en la cantidad de n\u00facleos, el tama\u00f1o de la memoria, el rendimiento de la red y las tecnolog\u00edas de la propia CPU (si tiene habilitada GPU y FPGA) Paso 3: Configuraci\u00f3n de la instancia / red \u00b6 El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita. Asociar un rol de IAM \u00b6 Si necesitamos que nuestras instancias EC2 ejecuten una aplicaci\u00f3n que debe realizar llamadas seguras de la API a otros servicios de AWS, en vez de dejar anotadas las credenciales en el c\u00f3digo de la aplicaci\u00f3n (esto es una muy mala pr\u00e1ctica que puede acarrear problemas de seguridad), debemos asociar un rol de IAM a una instancia EC2. El rol de IAM asociado a una instancia EC2 se almacena en un perfil de instancia . Si creamos el rol desde esta misma pantalla, AWS crear\u00e1 un perfil de instancia autom\u00e1ticamente y le otorgar\u00e1 el mismo nombre que al rol. En el desplegable la lista que se muestra es, en realidad, una lista de nombres de perfiles de instancia. Cuando definimos un rol que una instancia EC2 puede utilizar, estamos configurando qu\u00e9 cuentas o servicios de AWS pueden asumir dicho rol, as\u00ed como qu\u00e9 acciones y recursos de la API puede utilizar la aplicaci\u00f3n despu\u00e9s de asumir el rol. Si cambia un rol, el cambio se extiende a todas las instancias que tengan el rol asociado. La asociaci\u00f3n del rol no est\u00e1 limitada al momento del lanzamiento de la instancia, tambi\u00e9n se puede asociar un rol a una instancia que ya exista. Script de datos de usuario \u00b6 Al momento de crear las instancias EC2, de forma opcional, podemos especificar un script de datos de usuario durante el lanzamiento de la instancia. Los datos de usuario pueden automatizar la finalizaci\u00f3n de las instalaciones y las configuraciones durante el lanzamiento de la instancia. Por ejemplo, un script de datos de usuario podr\u00eda colocar parches en el sistema operativo de la instancia y actualizarlo, recuperar e instalar claves de licencia de software, o instalar sistemas de software adicionales. Por ejemplo, si queremos instalar un servidor de Apache, de manera que arranque autom\u00e1ticamente y que muestre un Hola Mundo podr\u00edamos poner #!/bin/bash yum update -y yum -y install httpd systemctl enable httpd systemctl start httpd echo '<html><h1>Hola Mundo desde el Severo!</h1></html>' > /var/www/html/index.html Script en Windows Si nuestra instancia es de Windows, el script de datos de usuario debe escribirse en un formato que sea compatible con una ventana del s\u00edmbolo del sistema (comandos por lotes) o con Windows PowerShell. De forma predeterminada, los datos de usuario s\u00f3lo se ejecutan la primera vez que se inicia la instancia. Paso 4: Almacenamiento \u00b6 Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias). Paso 5: Etiquetas \u00b6 Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso. Paso 6: Grupo de seguridad \u00b6 Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. Servidor Web Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad. Paso 7: An\u00e1lisis e identificaci\u00f3n \u00b6 El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia Por ejemplo, si utilizamos la Amazon Linux AMI y descargamos las claves de AWS Academy (suponiendo que la ip p\u00fablica de la m\u00e1quina que hemos creado es 3.83.80.52 ) nos conectar\u00edamos mediante: ssh -i labsuser.pem ec2-user@3.83.80.52 M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Claves en AWS Academy Nuestro usuario tiene creado por defecto un par de claves que se conocen como vockey . Esta claves se pueden descargar desde la opci\u00f3n AWS Details del laboratorio de Learner Lab . M\u00e1s adelante, en esta misma sesi\u00f3n, veremos c\u00f3mo utilizarlas. Uso de la consola \u00b6 En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a comentar un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-04ad2567c9e3d7893 --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia t2.micro key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Comandos AWS CLI Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s \u00fatiles a la hora de trabajar con AWS CLI. Caso de uso mediante AWS CLI \u00b6 A continuaci\u00f3n vamos a crear un grupo de seguridad que permita el acceso via HTTP al puerto 80 y HTTPS al puerto 443 y conexi\u00f3n mediante SSH al puerto 22. Para ello, primero creamos el grupo de seguridad utilizando el comando create-security-group : aws ec2 create-security-group --group-name iabd-front \\ --description \"Grupo de seguridad para frontend\" A continuaci\u00f3n a\u00f1adimos el acceso a ssh utilizando el comando authorize-security-group-ingress : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 22 --cidr 0 .0.0.0/0 A continuaci\u00f3n habilitamos el acceso http : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 80 --cidr 0 .0.0.0/0 A continuaci\u00f3n habilitamos el acceso https : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 443 --cidr 0 .0.0.0/0 Si queremos consultar el grupo de seguridad: aws ec2 describe-security-groups --group-name iabd-front Una vez creado y configurado el grupo de seguridad, vamos a crear una instancia utilizando el comando run-instances a partir de la AMI ami-03ae0589c3c7b8599 (es una imagen Ubuntu 20.04), y crearemos un instancia de tipo t3.large (su coste aproximado es de menos de 10 c\u00e9ntimos por hora) con el grupo de seguridad que acabamos de crear y 30GB de almacenamiento EBS: aws ec2 run-instances --image-id ami-03ae0589c3c7b8599 \\ --count 1 --instance-type t3.large \\ --key-name vockey --security-groups iabd-front \\ --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=iabd}]\" \\ --ebs-optimized \\ --block-device-mapping \"[ { \\\"DeviceName\\\": \\\"/dev/sda1\\\", \\\"Ebs\\\": { \\\"VolumeSize\\\": 30 } } ]\" El comando describe-instances tambi\u00e9n nos permite obtener informaci\u00f3n de nuestras instancias utilizando filtros. Asi pues, por ejemplo, si queremos obtener la ip p\u00fablica de la instancia que acabamos de crear podemos hacer: aws ec2 describe-instances \\ --filters \"Name=tag:Name,Values=iabd\" \\ --query \"Reservations[*].Instances[*].PublicIpAddress\" \\ --output text El siguiente paso es conectarnos a nuestra instancia. Como hemos creado la instancia utilizando las credenciales vockey , vamos a descargar las claves desde la consola de Learner Labs donde antes hab\u00edamos consultado nuestras credenciales. Claves SSH en AWS Academy Una vez descargada la clave, ya sea mediante Download PEM o Download PPK , nos conectaremos utilizando el usuario ubuntu mediante: SSH mediante Linux / Mac Putty mediante Windows Si nos situamos sobre la carpeta que contiene en el archivo descargado chmod 400 labsuser.pem Una vez que ya tenemos permisos de lectura sobre el archivo, nos conectamos mediante el comando ssh : ssh -i labsuser.pem ubuntu@<ip-publica> Descargamos el archivo labuser.ppk , y una vez configurada la ip p\u00fablica, en Connection -> SSH -> Auth , en la parte inferior donde podemos cargarlo mediante el bot\u00f3n Browse : Configuraci\u00f3n de Putty Ciclo de vida de las instancias \u00b6 Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de terminaci\u00f3n / apag\u00e1ndose) Terminated (terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping (deteni\u00e9ndose) : las instancias que cuentan con vol\u00famenes EBS se pueden detener. Stopped (detenida) : no generar\u00e1 los mismos costes que una instancia en el estado running . S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que utilizan como almacenamiento EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asoci\u00e1ndola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se pagan por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n. Monitorizaci\u00f3n \u00b6 Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias. Costes de las instancias \u00b6 Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS Academy es en esa modalidad), pero conviene conocer el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y es apto para la capa gratuita de AWS. bajo coste y flexibilidad. Cargas de trabajo de corto plazo, con picos o impredecibles. Tambi\u00e9n para desarrollo o prueba de aplicaciones. spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot. AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos. Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda. Carga de trabajo din\u00e1mica y a gran escala. Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional. instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve. Descuento en el cargo por hora por el uso de la instancia (hasta 72%). Plazo de 1 o 3 a\u00f1os. Asegura capacidad de c\u00f3mputo disponible cuando se la necesita. Cargas de trabajo de uso predecible o estado estable. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n. host reservado / dedicado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso. Ahorro de dinero en costes de licencia. Asistencia para cumplir los requisitos normativos y de conformidad. Licencia Bring your own (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo. PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cuotas mensuales. NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual. Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/ Optimizaci\u00f3n de costes \u00b6 Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las instancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las instancias EBS que ya no se necesitan o las copias de seguridad ya pasadas. AWS Lambda \u00b6 La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions . Creando una funci\u00f3n \u00b6 Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n (en el caso de AWS Academy, elegimos el rol LabRole ), mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda Cargando c\u00f3digo Adem\u00e1s de poder utilizar el IDE que ofrece AWS, podemos subir nuestras propias funciones en formato zip o desde S3. El fichero que contiene las funciones por defecto se llamar\u00e1 lambda_function y el manejador def_handler . Si queremos cambiar alguno de esos nombres, hay que editar el controlador en la configuraci\u00f3n en tiempo de ejecuci\u00f3n de la funci\u00f3n. Restricciones \u00b6 Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos. AWS Elastic Beanstalk \u00b6 AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Beanstalk autom\u00e1ticamente administra la implementaci\u00f3n, desde el aprovisionamiento de capacidad, el balanceo de carga y el escalado autom\u00e1tico hasta la monitorizaci\u00f3n del estado de las aplicaciones. Al mismo tiempo, si queremos, podemos mantener el control total de los recursos de AWS que alimentan la aplicaci\u00f3n y acceder a los recursos subyacentes en cualquier momento. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker, y se desplegan en servidores como Apache, Nginx o IIS. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga por los recursos que AWS utilice (instancia, base de datos, almacenamiento S3, etc...) Actividades \u00b6 Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Crea una instancia ec2 mediante AWS CLI , siguiendo todos los pasos del apartado Uso de la consola . Adjunta una captura con todos los comandos empleados y el resultado que aparece en la consola. Adem\u00e1s, con\u00e9ctate mediante SSH a la m\u00e1quina creada, y realiza una nueva captura. (opcional) Mediante AWS Lambda, realiza una funci\u00f3n que reciba del evento dos n\u00fameros (por ejemplo, a y b ) y devuelva un objeto JSON con el total de la suma. Adjunta captura del c\u00f3digo fuente, del evento de prueba y de las m\u00e9tricas capturadas tras probar la funci\u00f3n 10 veces. Referencias \u00b6 Amazon EC2 Funciones Lambda en AWS","title":"Servicios de computaci\u00f3n en la nube, EC2 y AWS Lambda."},{"location":"cloud/05computacion.html#computacion-en-la-nube","text":"","title":"Computaci\u00f3n en la nube"},{"location":"cloud/05computacion.html#introduccion","text":"Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio.","title":"Introducci\u00f3n"},{"location":"cloud/05computacion.html#amazon-ec2","text":"Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual.","title":"Amazon EC2"},{"location":"cloud/05computacion.html#paso-1-ami","text":"Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde.","title":"Paso 1: AMI"},{"location":"cloud/05computacion.html#paso-2-tipo-de-instancias","text":"El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino.","title":"Paso 2: Tipo de instancias"},{"location":"cloud/05computacion.html#paso-3-configuracion-de-la-instancia-red","text":"El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita.","title":"Paso 3: Configuraci\u00f3n de la instancia / red"},{"location":"cloud/05computacion.html#paso-4-almacenamiento","text":"Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias).","title":"Paso 4: Almacenamiento"},{"location":"cloud/05computacion.html#paso-5-etiquetas","text":"Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso.","title":"Paso 5: Etiquetas"},{"location":"cloud/05computacion.html#paso-6-grupo-de-seguridad","text":"Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. Servidor Web Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad.","title":"Paso 6: Grupo de seguridad"},{"location":"cloud/05computacion.html#paso-7-analisis-e-identificacion","text":"El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia Por ejemplo, si utilizamos la Amazon Linux AMI y descargamos las claves de AWS Academy (suponiendo que la ip p\u00fablica de la m\u00e1quina que hemos creado es 3.83.80.52 ) nos conectar\u00edamos mediante: ssh -i labsuser.pem ec2-user@3.83.80.52 M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Claves en AWS Academy Nuestro usuario tiene creado por defecto un par de claves que se conocen como vockey . Esta claves se pueden descargar desde la opci\u00f3n AWS Details del laboratorio de Learner Lab . M\u00e1s adelante, en esta misma sesi\u00f3n, veremos c\u00f3mo utilizarlas.","title":"Paso 7: An\u00e1lisis e identificaci\u00f3n"},{"location":"cloud/05computacion.html#uso-de-la-consola","text":"En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a comentar un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-04ad2567c9e3d7893 --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia t2.micro key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Comandos AWS CLI Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s \u00fatiles a la hora de trabajar con AWS CLI.","title":"Uso de la consola"},{"location":"cloud/05computacion.html#ciclo-de-vida-de-las-instancias","text":"Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de terminaci\u00f3n / apag\u00e1ndose) Terminated (terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping (deteni\u00e9ndose) : las instancias que cuentan con vol\u00famenes EBS se pueden detener. Stopped (detenida) : no generar\u00e1 los mismos costes que una instancia en el estado running . S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que utilizan como almacenamiento EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asoci\u00e1ndola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se pagan por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n.","title":"Ciclo de vida de las instancias"},{"location":"cloud/05computacion.html#monitorizacion","text":"Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias.","title":"Monitorizaci\u00f3n"},{"location":"cloud/05computacion.html#costes-de-las-instancias","text":"Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS Academy es en esa modalidad), pero conviene conocer el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y es apto para la capa gratuita de AWS. bajo coste y flexibilidad. Cargas de trabajo de corto plazo, con picos o impredecibles. Tambi\u00e9n para desarrollo o prueba de aplicaciones. spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot. AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos. Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda. Carga de trabajo din\u00e1mica y a gran escala. Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional. instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve. Descuento en el cargo por hora por el uso de la instancia (hasta 72%). Plazo de 1 o 3 a\u00f1os. Asegura capacidad de c\u00f3mputo disponible cuando se la necesita. Cargas de trabajo de uso predecible o estado estable. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n. host reservado / dedicado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso. Ahorro de dinero en costes de licencia. Asistencia para cumplir los requisitos normativos y de conformidad. Licencia Bring your own (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo. PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cuotas mensuales. NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual. Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/","title":"Costes de las instancias"},{"location":"cloud/05computacion.html#optimizacion-de-costes","text":"Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las instancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las instancias EBS que ya no se necesitan o las copias de seguridad ya pasadas.","title":"Optimizaci\u00f3n de costes"},{"location":"cloud/05computacion.html#aws-lambda","text":"La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions .","title":"AWS Lambda"},{"location":"cloud/05computacion.html#creando-una-funcion","text":"Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n (en el caso de AWS Academy, elegimos el rol LabRole ), mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda Cargando c\u00f3digo Adem\u00e1s de poder utilizar el IDE que ofrece AWS, podemos subir nuestras propias funciones en formato zip o desde S3. El fichero que contiene las funciones por defecto se llamar\u00e1 lambda_function y el manejador def_handler . Si queremos cambiar alguno de esos nombres, hay que editar el controlador en la configuraci\u00f3n en tiempo de ejecuci\u00f3n de la funci\u00f3n.","title":"Creando una funci\u00f3n"},{"location":"cloud/05computacion.html#restricciones","text":"Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos.","title":"Restricciones"},{"location":"cloud/05computacion.html#aws-elastic-beanstalk","text":"AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Beanstalk autom\u00e1ticamente administra la implementaci\u00f3n, desde el aprovisionamiento de capacidad, el balanceo de carga y el escalado autom\u00e1tico hasta la monitorizaci\u00f3n del estado de las aplicaciones. Al mismo tiempo, si queremos, podemos mantener el control total de los recursos de AWS que alimentan la aplicaci\u00f3n y acceder a los recursos subyacentes en cualquier momento. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker, y se desplegan en servidores como Apache, Nginx o IIS. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga por los recursos que AWS utilice (instancia, base de datos, almacenamiento S3, etc...)","title":"AWS Elastic Beanstalk"},{"location":"cloud/05computacion.html#actividades","text":"Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Crea una instancia ec2 mediante AWS CLI , siguiendo todos los pasos del apartado Uso de la consola . Adjunta una captura con todos los comandos empleados y el resultado que aparece en la consola. Adem\u00e1s, con\u00e9ctate mediante SSH a la m\u00e1quina creada, y realiza una nueva captura. (opcional) Mediante AWS Lambda, realiza una funci\u00f3n que reciba del evento dos n\u00fameros (por ejemplo, a y b ) y devuelva un objeto JSON con el total de la suma. Adjunta captura del c\u00f3digo fuente, del evento de prueba y de las m\u00e9tricas capturadas tras probar la funci\u00f3n 10 veces.","title":"Actividades"},{"location":"cloud/05computacion.html#referencias","text":"Amazon EC2 Funciones Lambda en AWS","title":"Referencias"},{"location":"cloud/06datos.html","text":"Datos en la nube \u00b6 Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches. Datos relacionales - Amazon RDS \u00b6 AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones. Instancias de bases de datos \u00b6 Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB Alta disponibilidad \u00b6 Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal. R\u00e9plica de lectura \u00b6 RDS tambi\u00e9n admite la creaci\u00f3n de r\u00e9plicas de lectura para MySQL, MariaDB, PostgreSQLy Amazon Aurora. R\u00e9plica de lectura Las actualizaciones que se realizan en la instancia principal se copian de manera as\u00edncrona en la instancia de r\u00e9plica de lectura, de manera que direccionando las consultas a esta nueva r\u00e9plica reduciremos la carga de la instancia principal. Las r\u00e9plicas de lectura tambi\u00e9n pueden convertirse en la instancia de base de datos principal, pero, debido a la replicaci\u00f3n as\u00edncrona, este proceso debe hacerse de forma manual. Las r\u00e9plicas de lectura pueden crearse en una regi\u00f3n diferente a la utilizada por la base de datos principal, lo que puede mejorar la recuperaci\u00f3n de desastres y/o disminuir la latencia al dirigir las lecturas a una r\u00e9plica de lectura lo m\u00e1s cercana al usuario. Casos de uso \u00b6 AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2). Costes \u00b6 El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducir\u00edan en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os). Ejemplo RDS \u00b6 A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndar con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Instancias permitidas en AWS Academy Si quer\u00e9is crear bases de datos con m\u00e1quinas m\u00e1s potentes, pod\u00e9is utilizar instancias hasta nivel medium , y a ser posible a r\u00e1fagas (instancias t ). Dentro de la Configuraci\u00f3n adicional , es importante deshabilitar la monitorizaci\u00f3n mejorada (no tenemos permiso para su uso en AWS Academy ). Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL. Amazon Aurora \u00b6 Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Cabe destacar que si creamos una base de datos serverless, Amazon no permite hacerla p\u00fablica, de manera que \u00fanicamente se puede acceder desde otro servicio de AWS. Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales. Datos NoSQL - DynamoDB \u00b6 DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una. Componentes y particiones \u00b6 Los componentes principales son: las tablas : son conjuntos de datos, formada por los elementos. los elementos : grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos : elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, ya que est\u00e1 formada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. Adem\u00e1s, DynamoDB permite crear \u00edndices para optimizar las consultas que realicemos sobre atributos que no forman parte de la clave de partici\u00f3n u ordenamiento. Infraestructura \u00b6 Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente, distribuyendo los datos entre diferentes dispositivos de almacenamiento. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Otras caracter\u00edsticas clave son las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elijamos, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Costes \u00b6 Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes. Ejemplo DynamoDB \u00b6 A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Tambi\u00e9n pod\u00edamos haber creado la tabla mediante el comando create-table de AWS CLI: aws dynamodb create-table \\ --table-name ProductCatalog \\ --attribute-definitions AttributeName = Id,AttributeType = N \\ --key-schema AttributeName = Id,KeyType = HASH \\ --billing-mode PAY_PER_REQUEST Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos consultar informaci\u00f3n de la tabla mediante el comando describe-table de AWS CLi, ejecutaremos: aws dynamodb describe-table --table-name ProductCatalog Si queremos hacer la consulta de la tabla para ver los datos que contiene desde el comando scan de AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL . As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL . Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Consultas PartiQL mediante Python M\u00e1s adelante mediante Python , accederemos a DynamoDB y realizaremos consultas con PartiQL , adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos. Actividades \u00b6 Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instant\u00e1nea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.t4g.medium , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV. Referencias \u00b6 Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"Gesti\u00f3n de datos en la nube con AWS. RDS y DynamoDB."},{"location":"cloud/06datos.html#datos-en-la-nube","text":"Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches.","title":"Datos en la nube"},{"location":"cloud/06datos.html#datos-relacionales-amazon-rds","text":"AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones.","title":"Datos relacionales - Amazon RDS"},{"location":"cloud/06datos.html#instancias-de-bases-de-datos","text":"Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB","title":"Instancias de bases de datos"},{"location":"cloud/06datos.html#alta-disponibilidad","text":"Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal.","title":"Alta disponibilidad"},{"location":"cloud/06datos.html#casos-de-uso","text":"AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2).","title":"Casos de uso"},{"location":"cloud/06datos.html#costes","text":"El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducir\u00edan en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os).","title":"Costes"},{"location":"cloud/06datos.html#ejemplo-rds","text":"A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndar con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Instancias permitidas en AWS Academy Si quer\u00e9is crear bases de datos con m\u00e1quinas m\u00e1s potentes, pod\u00e9is utilizar instancias hasta nivel medium , y a ser posible a r\u00e1fagas (instancias t ). Dentro de la Configuraci\u00f3n adicional , es importante deshabilitar la monitorizaci\u00f3n mejorada (no tenemos permiso para su uso en AWS Academy ). Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL.","title":"Ejemplo RDS"},{"location":"cloud/06datos.html#amazon-aurora","text":"Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Cabe destacar que si creamos una base de datos serverless, Amazon no permite hacerla p\u00fablica, de manera que \u00fanicamente se puede acceder desde otro servicio de AWS. Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales.","title":"Amazon Aurora"},{"location":"cloud/06datos.html#datos-nosql-dynamodb","text":"DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una.","title":"Datos NoSQL - DynamoDB"},{"location":"cloud/06datos.html#componentes-y-particiones","text":"Los componentes principales son: las tablas : son conjuntos de datos, formada por los elementos. los elementos : grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos : elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, ya que est\u00e1 formada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. Adem\u00e1s, DynamoDB permite crear \u00edndices para optimizar las consultas que realicemos sobre atributos que no forman parte de la clave de partici\u00f3n u ordenamiento.","title":"Componentes y particiones"},{"location":"cloud/06datos.html#infraestructura","text":"Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente, distribuyendo los datos entre diferentes dispositivos de almacenamiento. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Otras caracter\u00edsticas clave son las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elijamos, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos.","title":"Infraestructura"},{"location":"cloud/06datos.html#costes_1","text":"Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes.","title":"Costes"},{"location":"cloud/06datos.html#ejemplo-dynamodb","text":"A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Tambi\u00e9n pod\u00edamos haber creado la tabla mediante el comando create-table de AWS CLI: aws dynamodb create-table \\ --table-name ProductCatalog \\ --attribute-definitions AttributeName = Id,AttributeType = N \\ --key-schema AttributeName = Id,KeyType = HASH \\ --billing-mode PAY_PER_REQUEST Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos consultar informaci\u00f3n de la tabla mediante el comando describe-table de AWS CLi, ejecutaremos: aws dynamodb describe-table --table-name ProductCatalog Si queremos hacer la consulta de la tabla para ver los datos que contiene desde el comando scan de AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL . As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL . Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Consultas PartiQL mediante Python M\u00e1s adelante mediante Python , accederemos a DynamoDB y realizaremos consultas con PartiQL , adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos.","title":"Ejemplo DynamoDB"},{"location":"cloud/06datos.html#actividades","text":"Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instant\u00e1nea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.t4g.medium , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV.","title":"Actividades"},{"location":"cloud/06datos.html#referencias","text":"Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"Referencias"},{"location":"hadoop/index.html","text":"Unidad de Trabajo 5.- Ecosistema Hadoop \u00b6 Resultados de aprendizaje \u00b6 RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. RA5075.4 Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 36.- Arquitecturas Big Data Mi\u00e9rcoles 23 Nov 1p + 1o 36.- ELT / ETL Mi\u00e9rcoles 23 Nov 1p + 1o 38.- Hadoop Lunes 28 Nov 2p + 2o 39.- HDFS Mi\u00e9rcoles 30 Nov 1p + 1o 39.- Formatos de datos Mi\u00e9rcoles 30 Nov 1p + 1o 43.- Sqoop / Flume Lunes 5 Dic 2p + 2o 45.- Hive Mi\u00e9rcoles 7 Dic 2p + 2o","title":"Unidad de Trabajo 5.- Ecosistema Hadoop"},{"location":"hadoop/index.html#unidad-de-trabajo-5-ecosistema-hadoop","text":"","title":"Unidad de Trabajo 5.- Ecosistema Hadoop"},{"location":"hadoop/index.html#resultados-de-aprendizaje","text":"RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. RA5075.4 Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen.","title":"Resultados de aprendizaje"},{"location":"hadoop/index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 36.- Arquitecturas Big Data Mi\u00e9rcoles 23 Nov 1p + 1o 36.- ELT / ETL Mi\u00e9rcoles 23 Nov 1p + 1o 38.- Hadoop Lunes 28 Nov 2p + 2o 39.- HDFS Mi\u00e9rcoles 30 Nov 1p + 1o 39.- Formatos de datos Mi\u00e9rcoles 30 Nov 1p + 1o 43.- Sqoop / Flume Lunes 5 Dic 2p + 2o 45.- Hive Mi\u00e9rcoles 7 Dic 2p + 2o","title":"Planificaci\u00f3n"},{"location":"hadoop/01arq.html","text":"Arquitecturas Big Data \u00b6 Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manejar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda concreta, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas. Caracter\u00edsticas \u00b6 Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central ( SPOF ). Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleadas, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener. Tipos de arquitecturas \u00b6 Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas son el procesamiento batch y el procesamiento en streaming . Procesamiento Batch \u00b6 Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande con m\u00faltiples relaciones, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origen para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante. Procesamiento en Streaming \u00b6 Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. No todo es tiempo real No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instant\u00e1neo. Arquitectura Lambda \u00b6 Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era montar un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streaming ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos . Linaje de los datos Indica las transformaciones que ha sufrido un dato, desde el origen hasta el estado actual, incluyendo las combinaciones con otros datos o el cambio del dato en s\u00ed a lo largo de su ciclo de vida. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realiza modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la propia capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para procesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos, el lento y el r\u00e1pido, convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actualizar con datos m\u00e1s precisos provenientes de la capa batch . Paso a paso \u00b6 Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. Arquitectura Kappa \u00b6 El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos caminos diferenciados, lo que conlleva duplicar la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps en su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming . Esta capa, a diferencia de la de tipo batch , no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Para conseguir un \u00fanico camino, se sustituyen las fuentes de datos por colas de mensajes (por ejemplo, mediante una herramienta como Apache Kafka), obteniendo flujos de datos que facilitan el acceso en tiempo real, leyendo y transformando los datos de los mensajes en un formato que sea f\u00e1cilmente accesible a los usuarios finales. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos. Arquitectura por capas \u00b6 Adem\u00e1s de las dos soluciones que acabamos de conocer, otra forma de dise\u00f1ar las capas de una arquitectura Big Data consiste en separar las diferentes fases del dato en capa diferenciadas. La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.com) Capa de ingesti\u00f3n: es la primera capa que recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almac\u00e9n de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interact\u00faan los usuarios. Tecnolog\u00edas \u00b6 Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka y/o un servicio de flujo de datos como Apache Nifi . El almacenamiento de los datos y modelos lo podemos realizar mediante HDFS o S3 . Dentro de una arquitectura Lambda , en el sistema batch, mediante algoritmos MapReduce de Hadoop o consultas Hive podemos generar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB , Redis o AWS Dynamo DB . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill o Presto . Principio SCV \u00b6 Mientras que el teorema CAP tiene que ver con almacenamiento de datos distribuidos, el principio SCV est\u00e1 relacionado con el procesamiento distribuido de los datos . Es decir, no tiene que ver con la escritura y lectura (consistente o no) de los datos en entornos distribuidos sino con el procesamiento que se realiza sobre ellos dentro de los nodos de un sistema de procesamiento distribuido. De modo similar a lo que ocurr\u00eda con el teorema CAP, el principio SCV establece que un sistema de procesamiento distribuido s\u00f3lo puede soportar como m\u00e1ximo 2 de las siguientes 3 caracter\u00edsticas: Velocidad ( Speed ): Se refiere a cu\u00e1nto tardan en procesarse los datos desde el momento en el que son recibidos en el sistema anal\u00edtico. Por lo general se excluye el tiempo que se tarda en capturar los datos, considerando s\u00f3lo lo que se tarda en generar la estad\u00edstica o ejecutar el algoritmo en cuesti\u00f3n. Esta velocidad es m\u00e1s alta si estamos ante un sistema de anal\u00edtica en tiempo real que si se trata de un sistema de anal\u00edtica por lotes (del ingl\u00e9s batch). Consistencia ( Consistency ): Se refiere en este caso a la precisi\u00f3n de los resultados de la anal\u00edtica (no confundir, por lo tanto, con el significado de la C del teorema CAP). Tal precisi\u00f3n depende de si para la anal\u00edtica se utilizan todos los datos disponibles (precisi\u00f3n alta) o de si por el contrario se emplean t\u00e9cnicas de muestreo para seleccionar s\u00f3lo un subconjunto de los mismos con la intenci\u00f3n de producir resultados (de menor precisi\u00f3n) en un menor tiempo. Volumen ( Volume ). Se refiere a la cantidad de datos que pueden ser procesados. Hay que tener en cuenta que en entornos de Big Data, el alto volumen de datos es una caracter\u00edsticas siempre presente (una de las 5 Vs). De igual modo que hicimos al estudiar el teorema CAP , nos fijaremos en una serie de escenarios para mostrar que no podemos conseguir un sistema que cumpla a la vez las 3 caracter\u00edsticas del principio SCV. Principio SCV Si se requiere velocidad (S) y consistencia (C), no podemos procesar un alto volumen (V) de datos ya que eso aumenta el tiempo de respuesta. Si se requiere consistencia (C) y poder procesar grandes vol\u00famenes de datos (V), no es posible realizar tal procesado a una alta velocidad (S). Si necesitamos procesar un alto volumen de datos (V) a una alta velocidad (S), entonces necesitaremos emplear t\u00e9cnicas de muestreo para seleccionar s\u00f3lo un subconjunto de esos datos, lo cual producir\u00e1 un resultado no consistente (C). Casos de uso \u00b6 \u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfCu\u00e1l encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejecuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para ambas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomendable tenemos un par de casos desarrollados por Ericsson que pod\u00e9is leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha. Buenas pr\u00e1cticas \u00b6 En la ingesta de datos: evaluar los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente de datos, y en alg\u00fan caso lo mejor es combinar varias herramientas para cubrir todo el abanico. En el procesamiento: analizar si el sistema debe ser streaming o batch . Algunos sistemas que no se definen como puramente streaming , es decir, utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming . En la monitorizaci\u00f3n: al trabajar con multitud de herramientas es importante utilizar herramienta para controlar, monitorizar y gestionar la arquitectura. Algunas decisiones que debemos tomar a la hora de elegir la arquitectura son: Enfocar los casos de uso. Cuando tengamos los objetivos claros sabremos qu\u00e9 parte debemos fortalecer en la arquitectura. \u00bfVolumen, variedad, velocidad? Definir la arquitectura: \u00bfbatch o streaming ? \u00bfRealmente es necesario que nuestra arquitectura soporte streaming ? Evaluar las fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que se utilizan? Referencias \u00b6 Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Data processing architectures \u2013 Lambda and Kappa Big Data Architecture \u2013 Detailed Explanation Actividades \u00b6 ( RA5075.1 / CE5.1a / 2p) Contesta a las siguientes preguntas justificando tus repuestas: En una arquitectura Lambda , \u00bfc\u00f3mo consigue la capa de streaming mostrar la m\u00e1s informaci\u00f3n m\u00e1s r\u00e1pida que la de batch? \u00bfA coste de que se consigue la velocidad? Indica las diferencias en cantidad de datos y tiempo entre el procesamiento batch y en streaming . Respecto al procesamiento batch / streaming \u00bfPor qu\u00e9 Spark es una de las herramientas m\u00e1s utilizadas a d\u00eda de hoy?","title":"S36.- Arquitecturas Big Data"},{"location":"hadoop/01arq.html#arquitecturas-big-data","text":"Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manejar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda concreta, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas.","title":"Arquitecturas Big Data"},{"location":"hadoop/01arq.html#caracteristicas","text":"Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central ( SPOF ). Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleadas, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener.","title":"Caracter\u00edsticas"},{"location":"hadoop/01arq.html#tipos-de-arquitecturas","text":"Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas son el procesamiento batch y el procesamiento en streaming .","title":"Tipos de arquitecturas"},{"location":"hadoop/01arq.html#procesamiento-batch","text":"Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande con m\u00faltiples relaciones, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origen para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante.","title":"Procesamiento Batch"},{"location":"hadoop/01arq.html#procesamiento-en-streaming","text":"Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. No todo es tiempo real No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instant\u00e1neo.","title":"Procesamiento en Streaming"},{"location":"hadoop/01arq.html#arquitectura-lambda","text":"Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era montar un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streaming ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos . Linaje de los datos Indica las transformaciones que ha sufrido un dato, desde el origen hasta el estado actual, incluyendo las combinaciones con otros datos o el cambio del dato en s\u00ed a lo largo de su ciclo de vida. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realiza modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la propia capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para procesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos, el lento y el r\u00e1pido, convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actualizar con datos m\u00e1s precisos provenientes de la capa batch .","title":"Arquitectura Lambda"},{"location":"hadoop/01arq.html#paso-a-paso","text":"Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas.","title":"Paso a paso"},{"location":"hadoop/01arq.html#arquitectura-kappa","text":"El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos caminos diferenciados, lo que conlleva duplicar la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps en su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming . Esta capa, a diferencia de la de tipo batch , no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Para conseguir un \u00fanico camino, se sustituyen las fuentes de datos por colas de mensajes (por ejemplo, mediante una herramienta como Apache Kafka), obteniendo flujos de datos que facilitan el acceso en tiempo real, leyendo y transformando los datos de los mensajes en un formato que sea f\u00e1cilmente accesible a los usuarios finales. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos.","title":"Arquitectura Kappa"},{"location":"hadoop/01arq.html#arquitectura-por-capas","text":"Adem\u00e1s de las dos soluciones que acabamos de conocer, otra forma de dise\u00f1ar las capas de una arquitectura Big Data consiste en separar las diferentes fases del dato en capa diferenciadas. La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.com) Capa de ingesti\u00f3n: es la primera capa que recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almac\u00e9n de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interact\u00faan los usuarios.","title":"Arquitectura por capas"},{"location":"hadoop/01arq.html#tecnologias","text":"Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka y/o un servicio de flujo de datos como Apache Nifi . El almacenamiento de los datos y modelos lo podemos realizar mediante HDFS o S3 . Dentro de una arquitectura Lambda , en el sistema batch, mediante algoritmos MapReduce de Hadoop o consultas Hive podemos generar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB , Redis o AWS Dynamo DB . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill o Presto .","title":"Tecnolog\u00edas"},{"location":"hadoop/01arq.html#principio-scv","text":"Mientras que el teorema CAP tiene que ver con almacenamiento de datos distribuidos, el principio SCV est\u00e1 relacionado con el procesamiento distribuido de los datos . Es decir, no tiene que ver con la escritura y lectura (consistente o no) de los datos en entornos distribuidos sino con el procesamiento que se realiza sobre ellos dentro de los nodos de un sistema de procesamiento distribuido. De modo similar a lo que ocurr\u00eda con el teorema CAP, el principio SCV establece que un sistema de procesamiento distribuido s\u00f3lo puede soportar como m\u00e1ximo 2 de las siguientes 3 caracter\u00edsticas: Velocidad ( Speed ): Se refiere a cu\u00e1nto tardan en procesarse los datos desde el momento en el que son recibidos en el sistema anal\u00edtico. Por lo general se excluye el tiempo que se tarda en capturar los datos, considerando s\u00f3lo lo que se tarda en generar la estad\u00edstica o ejecutar el algoritmo en cuesti\u00f3n. Esta velocidad es m\u00e1s alta si estamos ante un sistema de anal\u00edtica en tiempo real que si se trata de un sistema de anal\u00edtica por lotes (del ingl\u00e9s batch). Consistencia ( Consistency ): Se refiere en este caso a la precisi\u00f3n de los resultados de la anal\u00edtica (no confundir, por lo tanto, con el significado de la C del teorema CAP). Tal precisi\u00f3n depende de si para la anal\u00edtica se utilizan todos los datos disponibles (precisi\u00f3n alta) o de si por el contrario se emplean t\u00e9cnicas de muestreo para seleccionar s\u00f3lo un subconjunto de los mismos con la intenci\u00f3n de producir resultados (de menor precisi\u00f3n) en un menor tiempo. Volumen ( Volume ). Se refiere a la cantidad de datos que pueden ser procesados. Hay que tener en cuenta que en entornos de Big Data, el alto volumen de datos es una caracter\u00edsticas siempre presente (una de las 5 Vs). De igual modo que hicimos al estudiar el teorema CAP , nos fijaremos en una serie de escenarios para mostrar que no podemos conseguir un sistema que cumpla a la vez las 3 caracter\u00edsticas del principio SCV. Principio SCV Si se requiere velocidad (S) y consistencia (C), no podemos procesar un alto volumen (V) de datos ya que eso aumenta el tiempo de respuesta. Si se requiere consistencia (C) y poder procesar grandes vol\u00famenes de datos (V), no es posible realizar tal procesado a una alta velocidad (S). Si necesitamos procesar un alto volumen de datos (V) a una alta velocidad (S), entonces necesitaremos emplear t\u00e9cnicas de muestreo para seleccionar s\u00f3lo un subconjunto de esos datos, lo cual producir\u00e1 un resultado no consistente (C).","title":"Principio SCV"},{"location":"hadoop/01arq.html#casos-de-uso","text":"\u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfCu\u00e1l encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejecuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para ambas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomendable tenemos un par de casos desarrollados por Ericsson que pod\u00e9is leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha.","title":"Casos de uso"},{"location":"hadoop/01arq.html#buenas-practicas","text":"En la ingesta de datos: evaluar los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente de datos, y en alg\u00fan caso lo mejor es combinar varias herramientas para cubrir todo el abanico. En el procesamiento: analizar si el sistema debe ser streaming o batch . Algunos sistemas que no se definen como puramente streaming , es decir, utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming . En la monitorizaci\u00f3n: al trabajar con multitud de herramientas es importante utilizar herramienta para controlar, monitorizar y gestionar la arquitectura. Algunas decisiones que debemos tomar a la hora de elegir la arquitectura son: Enfocar los casos de uso. Cuando tengamos los objetivos claros sabremos qu\u00e9 parte debemos fortalecer en la arquitectura. \u00bfVolumen, variedad, velocidad? Definir la arquitectura: \u00bfbatch o streaming ? \u00bfRealmente es necesario que nuestra arquitectura soporte streaming ? Evaluar las fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que se utilizan?","title":"Buenas pr\u00e1cticas"},{"location":"hadoop/01arq.html#referencias","text":"Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Data processing architectures \u2013 Lambda and Kappa Big Data Architecture \u2013 Detailed Explanation","title":"Referencias"},{"location":"hadoop/01arq.html#actividades","text":"( RA5075.1 / CE5.1a / 2p) Contesta a las siguientes preguntas justificando tus repuestas: En una arquitectura Lambda , \u00bfc\u00f3mo consigue la capa de streaming mostrar la m\u00e1s informaci\u00f3n m\u00e1s r\u00e1pida que la de batch? \u00bfA coste de que se consigue la velocidad? Indica las diferencias en cantidad de datos y tiempo entre el procesamiento batch y en streaming . Respecto al procesamiento batch / streaming \u00bfPor qu\u00e9 Spark es una de las herramientas m\u00e1s utilizadas a d\u00eda de hoy?","title":"Actividades"},{"location":"hadoop/02etl.html","text":"Ingesta de Datos \u00b6 Introducci\u00f3n \u00b6 Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, desde diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Ingesta de datos La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de datos puedan construir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Entendemos como pipeline de datos un proceso que consume datos desde un punto de origen, los limpia y los escribe en un nuevo destino. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data , para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y qu\u00e9 se pretende conseguir con ellos. A la hora de dise\u00f1ar un pipeline , se debe empezar desde el problema que negocio quiere solucionar, y retroceder con los datos hasta el origen de los mismos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake , asegur\u00e1ndose que los datos sean compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n. Pipeline de datos \u00b6 Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. En su forma m\u00e1s simple, consisten en recoger los datos, almacenarlos y procesarlos, y construir algo \u00fatil con los datos. Pipeline de datos - AWS Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean (principalmente para evitar problemas de rendimiento). El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (sistemas de procesamiento transaccional online, como un CRM), encargados de capturar y crear datos, y de forma separada, sistemas OLAP (sistemas de procesamiento anal\u00edtico, como un Data Warehouse ), encargados de analizar los datos. Fases del pipeline \u00b6 Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Ingesta. Recogemos los datos y los enviamos a un topic de Apache Kafka . Almacenamiento. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Fases en un pipeline de datos - AWS Procesamiento. Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch , leemos los datos del buffer. An\u00e1lisis y Visualizaci\u00f3n. Por ejemplo, mediante Spark realizamos la anal\u00edtica sobre estos datos (haciendo c\u00e1lculos, filtrados, agrupaciones de datos, etc...). Finalmente, podemos visualizar los resultado obtenidos o almacenarlos en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuido como Amazon S3 . Antes de realizar el an\u00e1lisis de los datos, va a ser muy normal tener que limpiar o normalizar los datos, ya sea porque las fuentes de datos, al ser distintas, utilicen diferentes codificaciones o nomenclaturas, o bien que haya datos sin rellenar o incorrectos. Conforme los datos avanzan a trav\u00e9s del pipeline , los datos se van a transformar (por ejemplo, poniendo ceros en los campos vac\u00edos, o rellenando los huecos con valores que aporten valor). Estas transformaciones se conocen como Data Wrangling (manipulaci\u00f3n o disputa de datos), termino que engloba las acciones realizadas desde los datos en crudo hasta el estado final en el cual el dato cobra valor y sentido para los usuarios. Pipeline iterativo \u00b6 Este proceso de ingesta, almacenamiento, procesamiento y an\u00e1lisis es iterativo. Sobre una hip\u00f3tesis que se nos plantee en negocio, comprobaremos los datos almacenados, y si no disponemos de la informaci\u00f3n necesaria, recogeremos nuevos datos. EStos nuevos datos pasaran por todo el pipeline, integr\u00e1ndose con los datos ya existentes. En la fase de anal\u00edtica, si no obtenemos el resultado esperado, nos tocar\u00e1 volver a la fase de ingesta para obtener o modificar los datos recogidos, y as\u00ed, de forma iterativa, hasta producir el resultado esperado. Desarrollo iterativo de un pipeline de datos - AWS Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL, no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas. ETL \u00b6 Una ETL, entendida como un proceso que lleva la informaci\u00f3n de un punto A a un punto B, puede realizarse mediante diversas herramientas, scripts, Python, etc... Pero cuando nos metemos con Big Data no servir\u00e1 cualquier tipo de herramienta, ya que necesitamos que sean: Flexibles y soporten formatos variados (JSON, CSV, etc...) Escalables y tolerante a fallos. Dispongan de conectores a m\u00faltiples fuentes y destinos de datos. Los procesos ETL, siglas de e xtracci\u00f3n, t ransformaci\u00f3n y carga ( l oad ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Extracci\u00f3n, Transformaci\u00f3n y Carga (load) Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto. Extracci\u00f3n \u00b6 Encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales, pasando por mensajes de redes sociales como Twitter o Reddit . Un paso que forma parte de la extracci\u00f3n es la de analizar que los datos sean veraces, que contiene la informaci\u00f3n que se espera, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser transparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se genera en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n. Transformaci\u00f3n \u00b6 En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n. Eliminar datos duplicados. Cruzar diferentes fuentes de datos para obtener una fuente diferente. Agregar informaci\u00f3n en funci\u00f3n de alguna variable. Tomar parte de los datos para cargarlos. Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n. Estructurar mejor la informaci\u00f3n. Generar indicadores que faciliten el procesamiento y entendimiento. Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados, los datos ya estar\u00e1n listos para su carga. Carga \u00b6 Fase encargada de almacenar los datos en el destino, un data warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga debe realizarse buscando minimizar el tiempo de la transacci\u00f3n. Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift, ...) Cargadores Propios (HDFS, S3, ...) Para mejorar la carga debemos tener en cuenta la: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s ELT \u00b6 ELT cambia el orden de las siglas y se basa en extraer, cargar y transformar. Es un t\u00e9cnica de ingesti\u00f3n de datos donde los datos que se obtienen desde m\u00faltiples fuentes se colocan sin transformar directamente en un data lake o almacenamiento de objetos en la nube. Desde ah\u00ed, los datos se pueden transformar dependiendo de los diferentes objetivos de negocio. En principio un proceso ELT necesita menos ingenieros de datos necesarios. Con la separaci\u00f3n de la extracci\u00f3n y la transformaci\u00f3n, ELT permite que los analistas y cient\u00edficos de datos realicen las transformaciones, ya sea con SQL o mediante Python. De esta manera, m\u00e1s departamentos se involucran en obtener y mejorar los datos. Una de las principales razones de que ELT cueste menos de implementar es que permite una mayor generalizaci\u00f3n de la informaci\u00f3n que se almacena. Los ingenieros de datos generan un data lake con los datos obtenidos de las fuentes de datos m\u00e1s populares, dejando que la transformaci\u00f3n la realicen los expertos en el negocio. Esto tambi\u00e9n implica que los datos est\u00e9n disponibles antes, ya que mediante un proceso ETL los datos no est\u00e1n disponibles para los usuarios hasta que se han transformado, lo que suele implicar un largo proceso de trabajo. En resumen, el mercado se est\u00e1 moviendo desde un desarrollo centralizado mediante ETL a uno m\u00e1s orientado a servicios como ELT, que permite automatizar la carga del data lake y la posterior codificaci\u00f3n de los flujos de datos. Herramientas ETL \u00b6 Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD transaccionales, XML, ficheros CSV / JSON, Teradata, HDFS, Hive, S3, ... Peticiones HTTP, servicios REST... APIs de aplicaciones de terceros, logs\u2026 Permiten la planificaci\u00f3n mediante batch , eventos o en streaming . Capacidad para transformar los datos: Transformaciones simples: tipos de datos, cadenas, codificaciones, c\u00e1lculos simples. Transformaciones intermedias: agregaciones, lookups. Transformaciones complejas: algoritmos de IA, segmentaci\u00f3n, integraci\u00f3n de c\u00f3digo de terceros, integraci\u00f3n con otros lenguajes. Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL La ingesta por dentro \u00b6 La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que tenga que aplicar uno o m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almac\u00e9n de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregaciones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. La ingesta de datos - StreamSets Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: servicios de mensajer\u00eda como Apache Kafka, los cuales han obtenido datos desde fuentes externas, como pueden ser dispositivos IOT o contenido obtenido directamente de las redes sociales. bases de datos relacionales, las cuales se acceden, por ejemplo, mediante JDBC. servicios REST que devuelven los datos en formato JSON. servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: servicios de mensajer\u00eda como Apache Kafka . bases de datos relacionales. bases de datos NoSQL. servicios de almacenamiento distribuido como HDFS o S3. plataformas de datos como Snowflake o Databricks . Batch vs Streaming \u00b6 El movimiento de datos entre los or\u00edgenes y los destinos se puede hacer, tal como vimos en la sesi\u00f3n de Arquitecturas de Big Data , mediante un proceso: Batch : el proceso se ejecuta de forma peri\u00f3dica (normalmente en intervalos fijos) a partir de unos datos est\u00e1ticos . Muy eficiente para grandes vol\u00famenes de datos, y donde la latencia (del orden de minutos) no es el factor m\u00e1s importante. Algunas de las herramientas utilizadas son Apache Sqoop , trabajos en MapReduce o de Spark jobs , etc... Streaming : tambi\u00e9n conocido como en tiempo real, donde los datos se leen, modifican y cargan tan pronto como llegan a la capa de ingesta (la latencia es cr\u00edtica). Algunas de las herramientas utilizadas son Apache Storm , Spark Streaming , Apache Nifi , Apache Kafka , etc... Arquitectura \u00b6 Si nos basamos en la arquitectura por capas , podemos ver como la capa de ingesta es la primera de las capas, la cual recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas: Arquitectura por capas (xenonstack.com) En el primer paso de la ingesta es el paso m\u00e1s pesado, por tiempo y cantidad de recursos necesarios. Es normal realizar la ingesta de flujos de datos desde cientos a miles de fuentes de datos, los cuales se obtiene a velocidades variables y en diferentes formatos. Para ello, es necesario: Priorizar las fuentes de datos Validar de forma individual cada fichero Enrutar cada elemento a su destino correcto. Resumiendo, los cuatro par\u00e1metros en los que debemos centrar nuestros esfuerzos son: Velocidad de los datos: c\u00f3mo fluyen los datos entre m\u00e1quinas, interacci\u00f3n con usuario y redes sociales, si el flujo es continuo o masivo. Tama\u00f1o de los datos: la ingesta de m\u00faltiples fuentes puede incrementarse con el tiempo. Frecuencia de los datos: \u00bfbatch o en streaming ? Formato de los datos: estructurado (tablas), desestructurado (im\u00e1genes, audios, v\u00eddeos, ...) o semi-estructurado (JSON). Herramientas de Ingesta de datos \u00b6 Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Herramientas de ingesta de datos Apache Sqoop : permite la transferencia bidireccional de datos entre Hadoop/Hive/HBase y una bases de datos SQL (datos estructurados). Aunque principalmente se interact\u00faa mediante comandos, proporciona una API Java. Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados sobre HDFS o HBase mediante una arquitectura basada en flujos de datos en streaming. Apache Nifi : herramienta que facilita un interfaz gr\u00e1fico que permite cargar datos de diferentes fuentes (tanto batch como streaming ), los pasa por un flujo de procesos (mediante grafos dirigidos) para su tratamiento y transformaci\u00f3n, y los vuelca en otra fuente. Elastic Logstash : Pensada inicialmente para la ingesta de logs en Elasticsearch , admite entradas y salidas de diferentes tipos (incluso AWS). AWS Glue : servicios gestionado para realizar tareas ETL desde la consola de AWS. Facilita el descubrimiento de datos y esquemas. Tambi\u00e9n se utiliza como almacenamiento de servicios como Amazon Athena o AWS Data Pipeline . Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema de colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services . Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure . Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud . Consideraciones \u00b6 A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data , hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCu\u00e1l va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos a nuestra empresa? \u00bfSer\u00e1n datos estructurados o datos sin estructurar? \u00bfCu\u00e1l es el volumen de los datos? Analizar el volumen diario y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad \u00bfC\u00f3mo de importante es la velocidad con la que se deben obtener los datos? Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/d\u00edas (mediante procesos batch ) o ser real-time (mediante streaming ) Actualizaciones \u00bfLas fuentes de origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes + insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y MongoDB ? \u00bfAlmacenaremos los datos con el mismo formato que los ingestamos? \u00bfLos almacenamos en formatos basados en filas ( Avro ) o columnares ( Parquet )? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / Spark ? \u00bfQu\u00e9 procesos de transformaci\u00f3n se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Respecto a la seguridad de los datos, si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta? \u00bfConocemos los requisitos de seguridad de nuestros datos? \u00bfQui\u00e9n tiene que acceder y en qu\u00e9 estado? Referencias \u00b6 Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility Data Ingestion, Processing and Big Data Architecture Layers Actividades \u00b6 ( RA5075.1 / CE5.1b / 2p) Contesta a las siguientes preguntas justificando tus repuestas: \u00bfQu\u00e9 relaci\u00f3n existe entre un pipeline de datos y una ETL? \u00bfEs lo mismo un proceso ETL que ELT? \u00bfCu\u00e1ndo se realiza un ETL y cuando un ELT? Dentro de un contexto Big data, \u00bfCu\u00e1l crees que se utiliza m\u00e1s? \u00bfPor qu\u00e9? Supongamos que somos una empresa que va a sacar un producto al mercado, y queremos medir las reacciones de las comunidades en las redes sociales. Del apartado Consideraciones contesta todas las preguntas planteadas a partir del supuesto planteado.","title":"S36.- Ingesta de datos"},{"location":"hadoop/02etl.html#ingesta-de-datos","text":"","title":"Ingesta de Datos"},{"location":"hadoop/02etl.html#introduccion","text":"Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, desde diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Ingesta de datos La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de datos puedan construir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Entendemos como pipeline de datos un proceso que consume datos desde un punto de origen, los limpia y los escribe en un nuevo destino. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data , para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y qu\u00e9 se pretende conseguir con ellos. A la hora de dise\u00f1ar un pipeline , se debe empezar desde el problema que negocio quiere solucionar, y retroceder con los datos hasta el origen de los mismos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake , asegur\u00e1ndose que los datos sean compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n.","title":"Introducci\u00f3n"},{"location":"hadoop/02etl.html#pipeline-de-datos","text":"Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. En su forma m\u00e1s simple, consisten en recoger los datos, almacenarlos y procesarlos, y construir algo \u00fatil con los datos. Pipeline de datos - AWS Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean (principalmente para evitar problemas de rendimiento). El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (sistemas de procesamiento transaccional online, como un CRM), encargados de capturar y crear datos, y de forma separada, sistemas OLAP (sistemas de procesamiento anal\u00edtico, como un Data Warehouse ), encargados de analizar los datos.","title":"Pipeline de datos"},{"location":"hadoop/02etl.html#fases-del-pipeline","text":"Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Ingesta. Recogemos los datos y los enviamos a un topic de Apache Kafka . Almacenamiento. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Fases en un pipeline de datos - AWS Procesamiento. Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch , leemos los datos del buffer. An\u00e1lisis y Visualizaci\u00f3n. Por ejemplo, mediante Spark realizamos la anal\u00edtica sobre estos datos (haciendo c\u00e1lculos, filtrados, agrupaciones de datos, etc...). Finalmente, podemos visualizar los resultado obtenidos o almacenarlos en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuido como Amazon S3 . Antes de realizar el an\u00e1lisis de los datos, va a ser muy normal tener que limpiar o normalizar los datos, ya sea porque las fuentes de datos, al ser distintas, utilicen diferentes codificaciones o nomenclaturas, o bien que haya datos sin rellenar o incorrectos. Conforme los datos avanzan a trav\u00e9s del pipeline , los datos se van a transformar (por ejemplo, poniendo ceros en los campos vac\u00edos, o rellenando los huecos con valores que aporten valor). Estas transformaciones se conocen como Data Wrangling (manipulaci\u00f3n o disputa de datos), termino que engloba las acciones realizadas desde los datos en crudo hasta el estado final en el cual el dato cobra valor y sentido para los usuarios.","title":"Fases del pipeline"},{"location":"hadoop/02etl.html#pipeline-iterativo","text":"Este proceso de ingesta, almacenamiento, procesamiento y an\u00e1lisis es iterativo. Sobre una hip\u00f3tesis que se nos plantee en negocio, comprobaremos los datos almacenados, y si no disponemos de la informaci\u00f3n necesaria, recogeremos nuevos datos. EStos nuevos datos pasaran por todo el pipeline, integr\u00e1ndose con los datos ya existentes. En la fase de anal\u00edtica, si no obtenemos el resultado esperado, nos tocar\u00e1 volver a la fase de ingesta para obtener o modificar los datos recogidos, y as\u00ed, de forma iterativa, hasta producir el resultado esperado. Desarrollo iterativo de un pipeline de datos - AWS Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL, no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas.","title":"Pipeline iterativo"},{"location":"hadoop/02etl.html#etl","text":"Una ETL, entendida como un proceso que lleva la informaci\u00f3n de un punto A a un punto B, puede realizarse mediante diversas herramientas, scripts, Python, etc... Pero cuando nos metemos con Big Data no servir\u00e1 cualquier tipo de herramienta, ya que necesitamos que sean: Flexibles y soporten formatos variados (JSON, CSV, etc...) Escalables y tolerante a fallos. Dispongan de conectores a m\u00faltiples fuentes y destinos de datos. Los procesos ETL, siglas de e xtracci\u00f3n, t ransformaci\u00f3n y carga ( l oad ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Extracci\u00f3n, Transformaci\u00f3n y Carga (load) Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto.","title":"ETL"},{"location":"hadoop/02etl.html#extraccion","text":"Encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales, pasando por mensajes de redes sociales como Twitter o Reddit . Un paso que forma parte de la extracci\u00f3n es la de analizar que los datos sean veraces, que contiene la informaci\u00f3n que se espera, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser transparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se genera en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n.","title":"Extracci\u00f3n"},{"location":"hadoop/02etl.html#transformacion","text":"En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n. Eliminar datos duplicados. Cruzar diferentes fuentes de datos para obtener una fuente diferente. Agregar informaci\u00f3n en funci\u00f3n de alguna variable. Tomar parte de los datos para cargarlos. Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n. Estructurar mejor la informaci\u00f3n. Generar indicadores que faciliten el procesamiento y entendimiento. Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados, los datos ya estar\u00e1n listos para su carga.","title":"Transformaci\u00f3n"},{"location":"hadoop/02etl.html#carga","text":"Fase encargada de almacenar los datos en el destino, un data warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga debe realizarse buscando minimizar el tiempo de la transacci\u00f3n. Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift, ...) Cargadores Propios (HDFS, S3, ...) Para mejorar la carga debemos tener en cuenta la: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s","title":"Carga"},{"location":"hadoop/02etl.html#elt","text":"ELT cambia el orden de las siglas y se basa en extraer, cargar y transformar. Es un t\u00e9cnica de ingesti\u00f3n de datos donde los datos que se obtienen desde m\u00faltiples fuentes se colocan sin transformar directamente en un data lake o almacenamiento de objetos en la nube. Desde ah\u00ed, los datos se pueden transformar dependiendo de los diferentes objetivos de negocio. En principio un proceso ELT necesita menos ingenieros de datos necesarios. Con la separaci\u00f3n de la extracci\u00f3n y la transformaci\u00f3n, ELT permite que los analistas y cient\u00edficos de datos realicen las transformaciones, ya sea con SQL o mediante Python. De esta manera, m\u00e1s departamentos se involucran en obtener y mejorar los datos. Una de las principales razones de que ELT cueste menos de implementar es que permite una mayor generalizaci\u00f3n de la informaci\u00f3n que se almacena. Los ingenieros de datos generan un data lake con los datos obtenidos de las fuentes de datos m\u00e1s populares, dejando que la transformaci\u00f3n la realicen los expertos en el negocio. Esto tambi\u00e9n implica que los datos est\u00e9n disponibles antes, ya que mediante un proceso ETL los datos no est\u00e1n disponibles para los usuarios hasta que se han transformado, lo que suele implicar un largo proceso de trabajo. En resumen, el mercado se est\u00e1 moviendo desde un desarrollo centralizado mediante ETL a uno m\u00e1s orientado a servicios como ELT, que permite automatizar la carga del data lake y la posterior codificaci\u00f3n de los flujos de datos.","title":"ELT"},{"location":"hadoop/02etl.html#herramientas-etl","text":"Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD transaccionales, XML, ficheros CSV / JSON, Teradata, HDFS, Hive, S3, ... Peticiones HTTP, servicios REST... APIs de aplicaciones de terceros, logs\u2026 Permiten la planificaci\u00f3n mediante batch , eventos o en streaming . Capacidad para transformar los datos: Transformaciones simples: tipos de datos, cadenas, codificaciones, c\u00e1lculos simples. Transformaciones intermedias: agregaciones, lookups. Transformaciones complejas: algoritmos de IA, segmentaci\u00f3n, integraci\u00f3n de c\u00f3digo de terceros, integraci\u00f3n con otros lenguajes. Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL","title":"Herramientas ETL"},{"location":"hadoop/02etl.html#la-ingesta-por-dentro","text":"La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que tenga que aplicar uno o m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almac\u00e9n de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregaciones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. La ingesta de datos - StreamSets Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: servicios de mensajer\u00eda como Apache Kafka, los cuales han obtenido datos desde fuentes externas, como pueden ser dispositivos IOT o contenido obtenido directamente de las redes sociales. bases de datos relacionales, las cuales se acceden, por ejemplo, mediante JDBC. servicios REST que devuelven los datos en formato JSON. servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: servicios de mensajer\u00eda como Apache Kafka . bases de datos relacionales. bases de datos NoSQL. servicios de almacenamiento distribuido como HDFS o S3. plataformas de datos como Snowflake o Databricks .","title":"La ingesta por dentro"},{"location":"hadoop/02etl.html#batch-vs-streaming","text":"El movimiento de datos entre los or\u00edgenes y los destinos se puede hacer, tal como vimos en la sesi\u00f3n de Arquitecturas de Big Data , mediante un proceso: Batch : el proceso se ejecuta de forma peri\u00f3dica (normalmente en intervalos fijos) a partir de unos datos est\u00e1ticos . Muy eficiente para grandes vol\u00famenes de datos, y donde la latencia (del orden de minutos) no es el factor m\u00e1s importante. Algunas de las herramientas utilizadas son Apache Sqoop , trabajos en MapReduce o de Spark jobs , etc... Streaming : tambi\u00e9n conocido como en tiempo real, donde los datos se leen, modifican y cargan tan pronto como llegan a la capa de ingesta (la latencia es cr\u00edtica). Algunas de las herramientas utilizadas son Apache Storm , Spark Streaming , Apache Nifi , Apache Kafka , etc...","title":"Batch vs Streaming"},{"location":"hadoop/02etl.html#arquitectura","text":"Si nos basamos en la arquitectura por capas , podemos ver como la capa de ingesta es la primera de las capas, la cual recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas: Arquitectura por capas (xenonstack.com) En el primer paso de la ingesta es el paso m\u00e1s pesado, por tiempo y cantidad de recursos necesarios. Es normal realizar la ingesta de flujos de datos desde cientos a miles de fuentes de datos, los cuales se obtiene a velocidades variables y en diferentes formatos. Para ello, es necesario: Priorizar las fuentes de datos Validar de forma individual cada fichero Enrutar cada elemento a su destino correcto. Resumiendo, los cuatro par\u00e1metros en los que debemos centrar nuestros esfuerzos son: Velocidad de los datos: c\u00f3mo fluyen los datos entre m\u00e1quinas, interacci\u00f3n con usuario y redes sociales, si el flujo es continuo o masivo. Tama\u00f1o de los datos: la ingesta de m\u00faltiples fuentes puede incrementarse con el tiempo. Frecuencia de los datos: \u00bfbatch o en streaming ? Formato de los datos: estructurado (tablas), desestructurado (im\u00e1genes, audios, v\u00eddeos, ...) o semi-estructurado (JSON).","title":"Arquitectura"},{"location":"hadoop/02etl.html#herramientas-de-ingesta-de-datos","text":"Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Herramientas de ingesta de datos Apache Sqoop : permite la transferencia bidireccional de datos entre Hadoop/Hive/HBase y una bases de datos SQL (datos estructurados). Aunque principalmente se interact\u00faa mediante comandos, proporciona una API Java. Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados sobre HDFS o HBase mediante una arquitectura basada en flujos de datos en streaming. Apache Nifi : herramienta que facilita un interfaz gr\u00e1fico que permite cargar datos de diferentes fuentes (tanto batch como streaming ), los pasa por un flujo de procesos (mediante grafos dirigidos) para su tratamiento y transformaci\u00f3n, y los vuelca en otra fuente. Elastic Logstash : Pensada inicialmente para la ingesta de logs en Elasticsearch , admite entradas y salidas de diferentes tipos (incluso AWS). AWS Glue : servicios gestionado para realizar tareas ETL desde la consola de AWS. Facilita el descubrimiento de datos y esquemas. Tambi\u00e9n se utiliza como almacenamiento de servicios como Amazon Athena o AWS Data Pipeline . Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema de colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services . Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure . Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud .","title":"Herramientas de Ingesta de datos"},{"location":"hadoop/02etl.html#consideraciones","text":"A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data , hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCu\u00e1l va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos a nuestra empresa? \u00bfSer\u00e1n datos estructurados o datos sin estructurar? \u00bfCu\u00e1l es el volumen de los datos? Analizar el volumen diario y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad \u00bfC\u00f3mo de importante es la velocidad con la que se deben obtener los datos? Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/d\u00edas (mediante procesos batch ) o ser real-time (mediante streaming ) Actualizaciones \u00bfLas fuentes de origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes + insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y MongoDB ? \u00bfAlmacenaremos los datos con el mismo formato que los ingestamos? \u00bfLos almacenamos en formatos basados en filas ( Avro ) o columnares ( Parquet )? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / Spark ? \u00bfQu\u00e9 procesos de transformaci\u00f3n se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Respecto a la seguridad de los datos, si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta? \u00bfConocemos los requisitos de seguridad de nuestros datos? \u00bfQui\u00e9n tiene que acceder y en qu\u00e9 estado?","title":"Consideraciones"},{"location":"hadoop/02etl.html#referencias","text":"Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility Data Ingestion, Processing and Big Data Architecture Layers","title":"Referencias"},{"location":"hadoop/02etl.html#actividades","text":"( RA5075.1 / CE5.1b / 2p) Contesta a las siguientes preguntas justificando tus repuestas: \u00bfQu\u00e9 relaci\u00f3n existe entre un pipeline de datos y una ETL? \u00bfEs lo mismo un proceso ETL que ELT? \u00bfCu\u00e1ndo se realiza un ETL y cuando un ELT? Dentro de un contexto Big data, \u00bfCu\u00e1l crees que se utiliza m\u00e1s? \u00bfPor qu\u00e9? Supongamos que somos una empresa que va a sacar un producto al mercado, y queremos medir las reacciones de las comunidades en las redes sociales. Del apartado Consideraciones contesta todas las preguntas planteadas a partir del supuesto planteado.","title":"Actividades"},{"location":"hadoop/03hadoop.html","text":"Hadoop \u00b6 Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.4), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2. Procesamiento distribuido \u00b6 Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). Su filosof\u00eda es almacenar todos los datos en un lugar y procesarlos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global, es decir, controlar la ejecuci\u00f3n o el almacenamiento de los trabajos y/o datos. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Nodos edge : hacen de puente entre el cl\u00faster y la red exterior, y proporcionan interfaces. Arquitectura hardware de Hadoop Commodity Hardware A veces el concepto hardware commodity suele confundirse con hardware dom\u00e9stico, cuando lo que hace referencia es a hardware no espec\u00edfico, que no tiene unos requerimientos en cuanto a disponibilidad o resiliencia exigentes. El hardware t\u00edpico donde se ejecuta un cluster Hadoop es el siguiente: Nodos worker : 256 Gb RAM \u2013 12 discos duros de 2-4 TB JBOD ( just a bunch of drives ) \u2013 2 CPU x 6-8 cores. Nodos master : 256 Gb RAM \u2013 2 discos duros de 2-3 TB en RAID \u2013 2 CPU x 8 cores. En estos nodos es m\u00e1s importante la capacidad de la CPU que la de almacenamiento. Nodos edge : 256 Gb RAM \u2013 2 discos duros de 2-3 TB en RAID \u2013 2 CPU x 8 cores. Cada vez que a\u00f1adimos un nuevo nodo worker , aumentamos tanto la capacidad como el rendimiento de nuestro sistema. Componentes y Ecosistema \u00b6 El n\u00facleo de Hadoop se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Ecosistema Hadoop Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y sistemas gestores de base de datos relacionales. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop , como ficheros de logs, datos de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop , pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce , lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop . Ambari : Herramienta utilizada para instalar, configurar, mantener y monitorizar Hadoop . Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS . CDH de Cloudera Azure HDInsight de Microsoft . DataProc de Google . HDFS \u00b6 Es la capa de almacenamiento de Hadoop , y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google detallando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop , dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos. Bloques \u00b6 Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop . A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS En la siguiente sesi\u00f3n profundizaremos en la arquitectura de HDFS y c\u00f3mo funcionan y gestionan los datos tanto el namenode como los datanodes . MapReduce \u00b6 Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos pr\u00f3ximamente), que mejora el rendimiento por una orden de magnitud. YARN \u00b6 Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Por un lado, un gestor de los procesos que se ejecutan en el cl\u00faster, que permite coordinar diferentes aplicaciones, asignar recursos y prioridades, permitir su convivencia, etc. Y por otro lado, las aplicaciones, que pueden desarrollarse utilizando un marco de ejecuci\u00f3n m\u00e1s ligero, no atado a un modelo estricto sobre c\u00f3mo ejecutarse, lo que da m\u00e1s libertad para poder desarrollar las aplicaciones. Componentes \u00b6 Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. Componentes en YARN El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados. Resource Manager \u00b6 El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Dicho de otro modo, es el equivalente al Namenode de HDFS. Cuando un cliente quiere ejecutar una aplicaci\u00f3n en YARN, se comunica con el ResourceManager , que ser\u00e1 el encargado de asignarle los recursos en base a las pol\u00edticas de prioridad asignadas y los recursos disponibles, distribuir la aplicaci\u00f3n (el ejecutable) por los diferentes nodos worker que realizar\u00e1n la ejecuci\u00f3n, controlar la ejecuci\u00f3n para detectar si ha habido una ca\u00edda de una de las tareas, para relanzarla en otro nodo, y liberar los recursos una vez la ejecuci\u00f3n haya finalizado. El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. Node Manager \u00b6 El servicio NodeManager se ejecuta en cada nodo worker y realiza las siguientes funciones: Monitoriza y proporciona informaci\u00f3n sobre el consumo de recursos (CPU/memoria) por parte de los contenedores al ResourceManager . Env\u00eda mensajes para notificar al ResourceManager su actividad (no est\u00e1 ca\u00eddo) as\u00ed como la informaci\u00f3n sobre su estado a nivel de recursos. Supervisa el ciclo de vida de los contenedores de aplicaciones. Supervisa la ejecuci\u00f3n de las distintas tareas en contenedores y termina aquellas tareas que se han quedado bloqueadas. Almacena un log (fichero en HDFS) con todas las operaciones que se realizan en el nodo. Lanza procesos ApplicationMaster , que coordinan los trabajos para cada aplicaci\u00f3n. Contenedores Es la unidad m\u00ednima de recursos de ejecuci\u00f3n para las aplicaciones, y que representa una cantidad espec\u00edfica de memoria, n\u00facleos de procesamiento (cores) y otros recursos (disco, red), para procesar las aplicaciones. Por ejemplo, un contenedor puede representar 4 gigabytes de memoria y 1 n\u00facleo de procesamiento. Todas las tareas de las aplicaciones YARN se ejecutan en contenedores. Cada trabajo puede contener m\u00faltiples tareas y cada una de las tareas se ejecuta en su propio contenedor. Cuando una tarea va a arrancar, YARN le asigna un contenedor, y cuando la tarea termina, el contenedor se elimina y sus recursos se asignan a otras tareas. Contenedores en NodeManager La cantidad de tareas y, por lo tanto, la cantidad de aplicaciones de YARN que puede ejecutar en cualquier momento, est\u00e1 limitada por la cantidad de contenedores que tiene un cl\u00faster. Por ejemplo, en un cl\u00faster de 20 nodos, con 256 GB de RAM y 12 cores por nodo, si se le ha asignado a YARN toda la capacidad existente, habr\u00e1 un total de 5 TB de RAM y 240 cores disponibles. Si se ha definido un tama\u00f1o de contenedor de 32 gigabytes, habr\u00e1 un m\u00e1ximo de 160 contenedores disponibles, es decir, se podr\u00e1n ejecutar como m\u00e1ximo 160 tareas de forma concurrente. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Los NodeManager , al igual que los Datanodes en HDFS, son tolerantes a fallos, por lo que en caso de ca\u00edda de alguno de ellos, el ResourceManager detectar\u00e1 que no funciona y redirigir\u00e1 la ejecuci\u00f3n de las aplicaciones al resto de nodos activos. Application Master \u00b6 El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Existe un proceso ApplicationMaster por aplicaci\u00f3n, y se ejecuta en uno de los nodos worker, para garantizar la escalabilidad de YARN, ya que si se ejecutaran todos los ApplicationMaster en el nodo maestro, junto con el ResourceManager , \u00e9ste ser\u00eda un cuello de botella para poder escalar o poder lanzar un gran n\u00famero de aplicaciones sobre el cl\u00faster. Asimismo, a diferencia del ResourceManager y los NodeManager , el ApplicationMaster es espec\u00edfico para una aplicaci\u00f3n por lo que, cuando la aplicaci\u00f3n finaliza, el proceso ApplicationMaster termina. En el caso de los servicios ResourceManager y NodeManager , siempre se est\u00e1n ejecutando aunque no haya aplicaciones activas en el cl\u00faster. Cada vez que se inicia una nueva aplicaci\u00f3n, ResourceManager asigna un contenedor que ejecuta ApplicationMaster en uno de los nodos del cl\u00faster. Funcionamiento \u00b6 Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ). Instalaci\u00f3n \u00b6 Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos. Configuraci\u00f3n \u00b6 Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> Hadoop en Docker Si no quieres (o puedes) ejecutar la m\u00e1quina virtual, bien puedes utilizar un servicio en la nube como AWS EMR (que veremos en la pr\u00f3xima sesi\u00f3n), o lanzar un contenedor Docker . La red contiene m\u00faltiples im\u00e1genes ya creadas, tanto con el core como con los servicios ya configurados. Para esta sesi\u00f3n y la siguiente, con una imagen sencilla como la que podemos crear desde https://www.section.io/engineering-education/set-up-containerize-and-test-a-single-hadoop-cluster-using-docker-and-docker-compose/ es suficiente. Puesta en marcha \u00b6 Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN Hola Mundo \u00b6 El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: Comando Hadoop Comando Yarn hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC yarn jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -11-28 09 :24:02,763 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -11-28 09 :24:03,580 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1669623168732_0001 2022 -11-28 09 :24:04,473 INFO input.FileInputFormat: Total input files to process : 1 2022 -11-28 09 :24:04,623 INFO mapreduce.JobSubmitter: number of splits:1 2022 -11-28 09 :24:05,313 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1669623168732_0001 2022 -11-28 09 :24:05,313 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -11-28 09 :24:05,820 INFO conf.Configuration: resource-types.xml not found 2022 -11-28 09 :24:05,821 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -11-28 09 :24:06,483 INFO impl.YarnClientImpl: Submitted application application_1669623168732_0001 2022 -11-28 09 :24:06,644 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1669623168732_0001/ 2022 -11-28 09 :24:06,645 INFO mapreduce.Job: Running job: job_1669623168732_0001 2022 -11-28 09 :24:20,124 INFO mapreduce.Job: Job job_1669623168732_0001 running in uber mode : false 2022 -11-28 09 :24:20,126 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -11-28 09 :24:28,406 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -11-28 09 :24:35,623 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -11-28 09 :24:36,687 INFO mapreduce.Job: Job job_1669623168732_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -11-28 09 :24:36,824 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241507 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 6848 Total time spent by all reduces in occupied slots ( ms )= 4005 Total time spent by all map tasks ( ms )= 6848 Total time spent by all reduce tasks ( ms )= 4005 Total vcore-milliseconds taken by all map tasks = 6848 Total vcore-milliseconds taken by all reduce tasks = 4005 Total megabyte-milliseconds taken by all map tasks = 7012352 Total megabyte-milliseconds taken by all reduce tasks = 4101120 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 1173 CPU time spent ( ms )= 4890 Physical memory ( bytes ) snapshot = 751063040 Virtual memory ( bytes ) snapshot = 5099941888 Total committed heap usage ( bytes )= 675282944 Peak Map Physical memory ( bytes )= 520818688 Peak Map Virtual memory ( bytes )= 2548346880 Peak Reduce Physical memory ( bytes )= 230244352 Peak Reduce Virtual memory ( bytes )= 2551595008 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el job de MapReduce se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC Autoevaluaci\u00f3n \u00bfQu\u00e9 comando HDFS utilizar\u00edas para obtener el contenido de la carpeta /user/iabd/salidaWC ? 1 \u00bfY para obtener el contenido del archivo generado? 2 MapReduce en Python \u00b6 El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python . Mapper \u00b6 Primero creamos el mapeador , el cual se encarga de parsear l\u00ednea a l\u00ednea el fragmento de documento que reciba, y va a generar una nueva salida con todas las palabras de manera que cada nueva l\u00ednea la compongan una tupla formada por la palabra, un tabulador y el n\u00famero 1 (hay una ocurrencia de dicha palabra) mapper.py #!/usr/bin/python3 import sys for linea in sys . stdin : # eliminamos los espacios de delante y de detr\u00e1s linea = linea . strip () # dividimos la l\u00ednea en palabras palabras = linea . split () # creamos tuplas de (palabra, 1) for palabra in palabras : print ( palabra , \" \\t 1\" ) Si queremos probar el mapper, podr\u00edamos ejecutar el siguiente comando: cat el_quijote.txt | python3 mapper.py Obteniendo un resultado similar a: ... gritos 1 al 1 cielo 1 alli\u0301 1 se 1 renovaron 1 las 1 maldiciones 1 ... Reducer \u00b6 A continuaci\u00f3n, en el reducer vamos a recibir la salida del mapper y parsearemos la cadena para separar la palabra del contador. Para llevar la cuenta de las palabras, vamos a meterlas dentro de un diccionario para incrementar las ocurrencias encontradas. Cuidado con la memoria En un caso real, hemos de evitar almacenar todos los datos que recibimos en memoria, ya que es posible que al trabajar con big data no quepa en la RAM de cada datanode . Para ello, se recomienda el uso de la librer\u00eda itertools , por ejemplo, utilizando la funci\u00f3n groupby() . Finalmente, volvemos a crear tuplas de palabra, tabulador y cantidad de ocurrencias. reducer.py #!/usr/bin/python3 import sys # inicializamos el diccionario dictPalabras = {} for linea in sys . stdin : # quitamos espacios de sobra linea = linea . strip () # parseamos la entrada de mapper.py palabra , cuenta = linea . split ( ' \\t ' , 1 ) # convertimos cuenta de string a int try : cuenta = int ( cuenta ) except ValueError : # cuenta no era un numero, descartamos la linea continue try : dictPalabras [ palabra ] += cuenta except : dictPalabras [ palabra ] = cuenta for palabra in dictPalabras . keys (): print ( palabra , \" \\t \" , dictPalabras [ palabra ]) Para probar el proceso completo, ejecutaremos el siguiente comando: cat el_quijote.txt | python3 mapper.py | python3 reducer.py > salida.tsv Si abrimos el fichero, podemos ver el resultado: salida.tsv don 1072 quijote 812 de 9035 la 5014 mancha 50 miguel 3 cervantes 3 ... Hadoop Streaming \u00b6 Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende , codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming env\u00eda los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <par\u00e1metros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-00000 Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint . Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. ( RA5075.2 / CE5.2b / 1p) Sobre Hadoop, ejecuta el siguiente comando y explica qu\u00e9 sucede: yarn jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 16 32 ( RA5075.2 / CE5.2b / 2p) Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabras de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . ( RA5075.4 / CE5.4a / 1p) Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. hdfs dfs -ls /user/iabd/salidaWC \u21a9 hdfs dfs -cat /user/iabd/salidaWC/part-r-00000 \u21a9","title":"S38.- Hadoop"},{"location":"hadoop/03hadoop.html#hadoop","text":"Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.4), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2.","title":"Hadoop"},{"location":"hadoop/03hadoop.html#procesamiento-distribuido","text":"Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). Su filosof\u00eda es almacenar todos los datos en un lugar y procesarlos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global, es decir, controlar la ejecuci\u00f3n o el almacenamiento de los trabajos y/o datos. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Nodos edge : hacen de puente entre el cl\u00faster y la red exterior, y proporcionan interfaces. Arquitectura hardware de Hadoop Commodity Hardware A veces el concepto hardware commodity suele confundirse con hardware dom\u00e9stico, cuando lo que hace referencia es a hardware no espec\u00edfico, que no tiene unos requerimientos en cuanto a disponibilidad o resiliencia exigentes. El hardware t\u00edpico donde se ejecuta un cluster Hadoop es el siguiente: Nodos worker : 256 Gb RAM \u2013 12 discos duros de 2-4 TB JBOD ( just a bunch of drives ) \u2013 2 CPU x 6-8 cores. Nodos master : 256 Gb RAM \u2013 2 discos duros de 2-3 TB en RAID \u2013 2 CPU x 8 cores. En estos nodos es m\u00e1s importante la capacidad de la CPU que la de almacenamiento. Nodos edge : 256 Gb RAM \u2013 2 discos duros de 2-3 TB en RAID \u2013 2 CPU x 8 cores. Cada vez que a\u00f1adimos un nuevo nodo worker , aumentamos tanto la capacidad como el rendimiento de nuestro sistema.","title":"Procesamiento distribuido"},{"location":"hadoop/03hadoop.html#componentes-y-ecosistema","text":"El n\u00facleo de Hadoop se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Ecosistema Hadoop Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y sistemas gestores de base de datos relacionales. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop , como ficheros de logs, datos de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop , pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce , lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop . Ambari : Herramienta utilizada para instalar, configurar, mantener y monitorizar Hadoop . Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS . CDH de Cloudera Azure HDInsight de Microsoft . DataProc de Google .","title":"Componentes y Ecosistema"},{"location":"hadoop/03hadoop.html#hdfs","text":"Es la capa de almacenamiento de Hadoop , y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google detallando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop , dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos.","title":"HDFS"},{"location":"hadoop/03hadoop.html#bloques","text":"Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop . A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS En la siguiente sesi\u00f3n profundizaremos en la arquitectura de HDFS y c\u00f3mo funcionan y gestionan los datos tanto el namenode como los datanodes .","title":"Bloques"},{"location":"hadoop/03hadoop.html#mapreduce","text":"Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos pr\u00f3ximamente), que mejora el rendimiento por una orden de magnitud.","title":"MapReduce"},{"location":"hadoop/03hadoop.html#yarn","text":"Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Por un lado, un gestor de los procesos que se ejecutan en el cl\u00faster, que permite coordinar diferentes aplicaciones, asignar recursos y prioridades, permitir su convivencia, etc. Y por otro lado, las aplicaciones, que pueden desarrollarse utilizando un marco de ejecuci\u00f3n m\u00e1s ligero, no atado a un modelo estricto sobre c\u00f3mo ejecutarse, lo que da m\u00e1s libertad para poder desarrollar las aplicaciones.","title":"YARN"},{"location":"hadoop/03hadoop.html#componentes","text":"Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. Componentes en YARN El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados.","title":"Componentes"},{"location":"hadoop/03hadoop.html#resource-manager","text":"El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Dicho de otro modo, es el equivalente al Namenode de HDFS. Cuando un cliente quiere ejecutar una aplicaci\u00f3n en YARN, se comunica con el ResourceManager , que ser\u00e1 el encargado de asignarle los recursos en base a las pol\u00edticas de prioridad asignadas y los recursos disponibles, distribuir la aplicaci\u00f3n (el ejecutable) por los diferentes nodos worker que realizar\u00e1n la ejecuci\u00f3n, controlar la ejecuci\u00f3n para detectar si ha habido una ca\u00edda de una de las tareas, para relanzarla en otro nodo, y liberar los recursos una vez la ejecuci\u00f3n haya finalizado. El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores.","title":"Resource Manager"},{"location":"hadoop/03hadoop.html#node-manager","text":"El servicio NodeManager se ejecuta en cada nodo worker y realiza las siguientes funciones: Monitoriza y proporciona informaci\u00f3n sobre el consumo de recursos (CPU/memoria) por parte de los contenedores al ResourceManager . Env\u00eda mensajes para notificar al ResourceManager su actividad (no est\u00e1 ca\u00eddo) as\u00ed como la informaci\u00f3n sobre su estado a nivel de recursos. Supervisa el ciclo de vida de los contenedores de aplicaciones. Supervisa la ejecuci\u00f3n de las distintas tareas en contenedores y termina aquellas tareas que se han quedado bloqueadas. Almacena un log (fichero en HDFS) con todas las operaciones que se realizan en el nodo. Lanza procesos ApplicationMaster , que coordinan los trabajos para cada aplicaci\u00f3n. Contenedores Es la unidad m\u00ednima de recursos de ejecuci\u00f3n para las aplicaciones, y que representa una cantidad espec\u00edfica de memoria, n\u00facleos de procesamiento (cores) y otros recursos (disco, red), para procesar las aplicaciones. Por ejemplo, un contenedor puede representar 4 gigabytes de memoria y 1 n\u00facleo de procesamiento. Todas las tareas de las aplicaciones YARN se ejecutan en contenedores. Cada trabajo puede contener m\u00faltiples tareas y cada una de las tareas se ejecuta en su propio contenedor. Cuando una tarea va a arrancar, YARN le asigna un contenedor, y cuando la tarea termina, el contenedor se elimina y sus recursos se asignan a otras tareas. Contenedores en NodeManager La cantidad de tareas y, por lo tanto, la cantidad de aplicaciones de YARN que puede ejecutar en cualquier momento, est\u00e1 limitada por la cantidad de contenedores que tiene un cl\u00faster. Por ejemplo, en un cl\u00faster de 20 nodos, con 256 GB de RAM y 12 cores por nodo, si se le ha asignado a YARN toda la capacidad existente, habr\u00e1 un total de 5 TB de RAM y 240 cores disponibles. Si se ha definido un tama\u00f1o de contenedor de 32 gigabytes, habr\u00e1 un m\u00e1ximo de 160 contenedores disponibles, es decir, se podr\u00e1n ejecutar como m\u00e1ximo 160 tareas de forma concurrente. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Los NodeManager , al igual que los Datanodes en HDFS, son tolerantes a fallos, por lo que en caso de ca\u00edda de alguno de ellos, el ResourceManager detectar\u00e1 que no funciona y redirigir\u00e1 la ejecuci\u00f3n de las aplicaciones al resto de nodos activos.","title":"Node Manager"},{"location":"hadoop/03hadoop.html#application-master","text":"El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Existe un proceso ApplicationMaster por aplicaci\u00f3n, y se ejecuta en uno de los nodos worker, para garantizar la escalabilidad de YARN, ya que si se ejecutaran todos los ApplicationMaster en el nodo maestro, junto con el ResourceManager , \u00e9ste ser\u00eda un cuello de botella para poder escalar o poder lanzar un gran n\u00famero de aplicaciones sobre el cl\u00faster. Asimismo, a diferencia del ResourceManager y los NodeManager , el ApplicationMaster es espec\u00edfico para una aplicaci\u00f3n por lo que, cuando la aplicaci\u00f3n finaliza, el proceso ApplicationMaster termina. En el caso de los servicios ResourceManager y NodeManager , siempre se est\u00e1n ejecutando aunque no haya aplicaciones activas en el cl\u00faster. Cada vez que se inicia una nueva aplicaci\u00f3n, ResourceManager asigna un contenedor que ejecuta ApplicationMaster en uno de los nodos del cl\u00faster.","title":"Application Master"},{"location":"hadoop/03hadoop.html#funcionamiento","text":"Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ).","title":"Funcionamiento"},{"location":"hadoop/03hadoop.html#instalacion","text":"Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.","title":"Instalaci\u00f3n"},{"location":"hadoop/03hadoop.html#configuracion","text":"Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> Hadoop en Docker Si no quieres (o puedes) ejecutar la m\u00e1quina virtual, bien puedes utilizar un servicio en la nube como AWS EMR (que veremos en la pr\u00f3xima sesi\u00f3n), o lanzar un contenedor Docker . La red contiene m\u00faltiples im\u00e1genes ya creadas, tanto con el core como con los servicios ya configurados. Para esta sesi\u00f3n y la siguiente, con una imagen sencilla como la que podemos crear desde https://www.section.io/engineering-education/set-up-containerize-and-test-a-single-hadoop-cluster-using-docker-and-docker-compose/ es suficiente.","title":"Configuraci\u00f3n"},{"location":"hadoop/03hadoop.html#puesta-en-marcha","text":"Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN","title":"Puesta en marcha"},{"location":"hadoop/03hadoop.html#hola-mundo","text":"El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: Comando Hadoop Comando Yarn hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC yarn jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -11-28 09 :24:02,763 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -11-28 09 :24:03,580 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1669623168732_0001 2022 -11-28 09 :24:04,473 INFO input.FileInputFormat: Total input files to process : 1 2022 -11-28 09 :24:04,623 INFO mapreduce.JobSubmitter: number of splits:1 2022 -11-28 09 :24:05,313 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1669623168732_0001 2022 -11-28 09 :24:05,313 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -11-28 09 :24:05,820 INFO conf.Configuration: resource-types.xml not found 2022 -11-28 09 :24:05,821 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -11-28 09 :24:06,483 INFO impl.YarnClientImpl: Submitted application application_1669623168732_0001 2022 -11-28 09 :24:06,644 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1669623168732_0001/ 2022 -11-28 09 :24:06,645 INFO mapreduce.Job: Running job: job_1669623168732_0001 2022 -11-28 09 :24:20,124 INFO mapreduce.Job: Job job_1669623168732_0001 running in uber mode : false 2022 -11-28 09 :24:20,126 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -11-28 09 :24:28,406 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -11-28 09 :24:35,623 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -11-28 09 :24:36,687 INFO mapreduce.Job: Job job_1669623168732_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -11-28 09 :24:36,824 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241507 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 6848 Total time spent by all reduces in occupied slots ( ms )= 4005 Total time spent by all map tasks ( ms )= 6848 Total time spent by all reduce tasks ( ms )= 4005 Total vcore-milliseconds taken by all map tasks = 6848 Total vcore-milliseconds taken by all reduce tasks = 4005 Total megabyte-milliseconds taken by all map tasks = 7012352 Total megabyte-milliseconds taken by all reduce tasks = 4101120 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 1173 CPU time spent ( ms )= 4890 Physical memory ( bytes ) snapshot = 751063040 Virtual memory ( bytes ) snapshot = 5099941888 Total committed heap usage ( bytes )= 675282944 Peak Map Physical memory ( bytes )= 520818688 Peak Map Virtual memory ( bytes )= 2548346880 Peak Reduce Physical memory ( bytes )= 230244352 Peak Reduce Virtual memory ( bytes )= 2551595008 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el job de MapReduce se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC Autoevaluaci\u00f3n \u00bfQu\u00e9 comando HDFS utilizar\u00edas para obtener el contenido de la carpeta /user/iabd/salidaWC ? 1 \u00bfY para obtener el contenido del archivo generado? 2","title":"Hola Mundo"},{"location":"hadoop/03hadoop.html#mapreduce-en-python","text":"El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python .","title":"MapReduce en Python"},{"location":"hadoop/03hadoop.html#hadoop-streaming","text":"Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende , codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming env\u00eda los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <par\u00e1metros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-00000","title":"Hadoop Streaming"},{"location":"hadoop/03hadoop.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint .","title":"Referencias"},{"location":"hadoop/03hadoop.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. ( RA5075.2 / CE5.2b / 1p) Sobre Hadoop, ejecuta el siguiente comando y explica qu\u00e9 sucede: yarn jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 16 32 ( RA5075.2 / CE5.2b / 2p) Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabras de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . ( RA5075.4 / CE5.4a / 1p) Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. hdfs dfs -ls /user/iabd/salidaWC \u21a9 hdfs dfs -cat /user/iabd/salidaWC/part-r-00000 \u21a9","title":"Actividades"},{"location":"hadoop/04formatos.html","text":"En la primera unidad de trabajo ya vimos una peque\u00f1a introducci\u00f3n a los diferentes formatos de datos . Las propiedades que ha de tener un formato de datos son: independiente del lenguaje expresivo, con soporte para estructuras complejas y anidadas eficiente, r\u00e1pido y reducido din\u00e1mico, de manera que los programas puedan procesar y definir nuevos tipos de datos. formato de fichero standalone y que permita dividirlo y comprimirlo. Para que Hadoop/Spark o cualquier herramienta de anal\u00edtica de datos pueda procesar documentos, es imprescindible que el formato del fichero permita su divisi\u00f3n en fragmentos ( splittable in chunks ). Si los clasificamos respecto al formato de almacenamiento tenemos: texto (m\u00e1s lentos, ocupan m\u00e1s pero son m\u00e1s expresivos y permiten su interoperabilidad): CSV, XML, JSON, etc... binarios (mejor rendimiento, ocupan menos, menos expresivos): Avro, Parquet, ORC, etc... Si comparamos los formatos m\u00e1s empleados a partir de las propiedades descritas tenemos: Caracter\u00edstica CSV XML / JSON SequenceFile Avro Independencia del lenguaje Expresivo Eficiente Din\u00e1mico Standalone Divisible Las ventajas de elegir el formato correcto son: Mayor rendimiento en la lectura y/o escritura Ficheros troceables ( splittables ) Soporte para esquemas que evolucionan Soporte para compresi\u00f3n de los datos (por ejemplo, mediante Snappy ). Filas vs Columnas \u00b6 Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV , con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet , lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes. Avro \u00b6 Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Tiene soporte para la compresi\u00f3n de bloques y es un formato que permite la divisi\u00f3n de los datos ( splittable ). Formato \u00b6 El formato Avro se basa en el uso de esquemas, los cuales definen los tipos de datos y protocolos mediante JSON. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Formato de un archivo Avro Cada fichero Avro almacena el esquema en la cabecera del fichero y luego est\u00e1n los datos en formato binario. Los esquemas se componen de tipos primitivos ( null , boolean , int , long , float , double , bytes , y string ) y compuestos ( record , enum , array , map , union , y fixed ). Un ejemplo de esquema podr\u00eda ser: empleado.avsc { \"type\" : \"record\" , \"namespace\" : \"SeveroOchoa\" , \"name\" : \"Empleado\" , \"fields\" : [ { \"name\" : \"Nombre\" , \"type\" : \"string\" }, { \"name\" : \"Altura\" , \"type\" : \"float\" } { \"name\" : \"Edad\" , \"type\" : \"int\" } ] } Avro y Python \u00b6 Para poder serializar y deserializar documentos Avro mediante Python , previamente debemos instalar la librer\u00eda avro : pip install avro-python3 # o si utilizamos Anaconda conda install -c conda-forge avro-python3 Vamos a realizar un ejemplo donde primero leemos un esquema de un archivo Avro , y con dicho esquema, escribiremos nuevos datos en un fichero. A continuaci\u00f3n, abrimos el fichero escrito y leemos y mostramos los datos: C\u00f3digo Python Resultado Google Colab import avro import copy import json from avro.datafile import DataFileReader , DataFileWriter from avro.io import DatumReader , DatumWriter # abrimos el fichero en modo binario y leemos el esquema schema = avro . schema . parse ( open ( \"empleado.avsc\" , \"rb\" ) . read ()) # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleados.avro' , 'wb' ) as f : writer = DataFileWriter ( f , DatumWriter (), schema ) writer . append ({ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }) writer . append ({ \"nombre\" : \"Juan\" , \"altura\" : 175 }) writer . close () # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleados.avro\" , \"rb\" ) as f : reader = DataFileReader ( f , DatumReader ()) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . meta ) # obtenemos el schema del fichero le\u00eddo schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] reader . close () print ( f 'Schema de empleado.avsc: \\n { schema } ' ) print ( f 'Schema del fichero empleados.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { \"type\" : \"record\" , \"name\" : \"empleado\" , \"namespace\" : \"SeveroOchoa\" , \"fields\" : [{ \"type\" : \"string\" , \"name\" : \"nombre\" }, { \"type\" : \"int\" , \"name\" : \"altura\" }, { \"type\" : [ \"null\" , \"int\" ], \"name\" : \"edad\" , \"default\" : null }]} Schema del f ichero empleados.avro : { ' t ype' : 'record' , ' na me' : 'empleado' , ' na mespace' : 'SeveroOchoa' , ' f ields' : [{ ' t ype' : 's tr i n g' , ' na me' : ' n ombre' }, { ' t ype' : 'i nt ' , ' na me' : 'al tura ' }, { ' t ype' : [ ' null ' , 'i nt ' ], ' na me' : 'edad' , 'de fault ' : No ne }]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Accede al cuaderno en Google Colab y adjunta el archivo del empleado.avsc . Fastavro \u00b6 Para trabajar con Avro y grandes vol\u00famenes de datos, es mejor utilizar la librer\u00eda Fastavro ( https://github.com/fastavro/fastavro ) la cual ofrece un rendimiento mayor (en vez de estar codificada en Python puro, tiene algunos fragmentos realizados mediante Cython ). Primero, hemos de instalar la librer\u00eda: pip install fastavro # o si utilizamos Anaconda conda install -c conda-forge fastavro Como pod\u00e9is observar a continuaci\u00f3n, hemos repetido el ejemplo y el c\u00f3digo es muy similar: C\u00f3digo Python Resultado Google Colab import fastavro import copy import json from fastavro import reader # abrimos el fichero en modo binario y leemos el esquema with open ( \"empleado.avsc\" , \"rb\" ) as f : schemaJSON = json . load ( f ) schemaDict = fastavro . parse_schema ( schemaJSON ) empleados = [{ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }, { \"nombre\" : \"Juan\" , \"altura\" : 175 }] # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleadosf.avro' , 'wb' ) as f : fastavro . writer ( f , schemaDict , empleados ) # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleadosf.avro\" , \"rb\" ) as f : reader = fastavro . reader ( f ) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . metadata ) # obtenemos el schema del fichero le\u00eddo schemaReader = copy . deepcopy ( reader . writer_schema ) schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] print ( f 'Schema de empleado.avsc: \\n { schemaDict } ' ) print ( f 'Schema del fichero empleadosf.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}], '__ fasta vro_parsed' : True , '__ na med_schemas' : { 'SeveroOchoa.empleado' : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]}}} Schema del f ichero empleados f .avro : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Accede al cuaderno en Google Colab y adjunta el archivo del empleado.avsc . Fastavro y Pandas \u00b6 Finalmente, vamos a realizar un \u00faltimo ejemplo con las dos librer\u00edas m\u00e1s utilizadas. Vamos a leer un fichero CSV de ventas mediante Pandas , y tras limpiar los datos y quedarnos \u00fanicamente con las ventas de Alemania, almacenaremos el resultado del procesamiento en Avro . Acceso Local Google Colab import pandas as pd from fastavro import writer , parse_schema # Leemos el csv mediante pandas df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 1. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 2. Convertimos el Dataframe a una lista de diccionarios records = df . to_dict ( 'records' ) # 3. Persistimos en un fichero avro with open ( 'sales.avro' , 'wb' ) as f : writer ( f , schemaParseado , records ) Accede al cuaderno en Google Colab y adjunta el archivo de ventas pdi_sales.csv . Acceso HDFS Aunque todav\u00eda no hayamos estudiado el ecosistema Hadoop, vamos a ver como podemos conectarnos a HDFS para leer y persistir datos en formato Avro : import pandas as pd from fastavro import parse_schema from hdfs import InsecureClient from hdfs.ext.avro import AvroWriter from hdfs.ext.dataframe import write_dataframe # 1. Nos conectamos a HDFS HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # 2. Leemos el Dataframe with hdfs_client . read ( '/user/iabd/pdi_sales.csv' ) as reader : df = pd . read_csv ( reader , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 3. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 4a. Persistimos en un fichero avro dentro de HDFS mediante la extension AvroWriter de hdfs with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado ) as writer : records = df . to_dict ( 'records' ) # diccionario for record in records : writer . write ( record ) # 4b. O directamente persistimos el Dataframe mediante la extension write_dataframe de hdfs write_dataframe ( hdfs_client , '/user/iabd/sales2.avro' , df ) # infiere el esquema write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado ) Para el acceso HDFS hemos utilizados las extensiones Fastavro y Pandas de la librer\u00eda HDFS que veremos en posteriores sesiones. Comprimiendo los datos \u00b6 \u00bfY s\u00ed comprimimos los datos para ocupen menos espacio en nuestro cl\u00faster y por tanto, nos cuesten menos dinero? Fastavro soporta dos tipos de compresi\u00f3n: gzip (mediante el algoritmo deflate ) y snappy . Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data, la cual hemos de instalar previamente mediante pip install python-snappy . Para indicar el tipo de compresi\u00f3n, \u00fanicamente hemos de a\u00f1adir un par\u00e1metros extra con el algoritmo de compresi\u00f3n en la funci\u00f3n/constructor de persistencia: Fastavro y gzip AvroWriter y snappy write_dataframe y snappy writer ( f , schemaParseado , records , 'deflate' ) with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado , 'snappy' ) as writer : write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado , codec = 'snappy' ) Comparando algoritmos de compresi\u00f3n Respecto a la compresi\u00f3n, sobre un fichero de 100GB, podemos considerar media si ronda los 50GB y alta si baja a los 40GB. Algoritmo Velocidad Compresi\u00f3n Gzip Media Media Bzip2 Lenta Alta Snappy Alta Media M\u00e1s que un tema de espacio, necesitamos que los procesos sean eficientes y por eso priman los algoritmos que son m\u00e1s r\u00e1pidos. Si te interesa el tema, es muy interesante el art\u00edculo Data Compression in Hadoop . Por ejemplo, si realizamos el ejemplo de Fast Avro y Pandas con acceso local obtenemos los siguientes tama\u00f1os: Sin compresi\u00f3n: 6,9 MiB Gzip: 1,9 MiB Snappy: 2,8 MiB Parquet \u00b6 Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas, es decir, para lecturas de grandes cargas de trabajo. Tiene un ratio de compresi\u00f3n muy alto (mediante Snappy ronda el 75%), y adem\u00e1s, solo se recorren las columnas necesarias en cada lectura, lo que reduce las operaciones de disco de entrada/salida. Formato \u00b6 Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc... Parquet y Python \u00b6 Para interactuar con el formato Parquet mediante Python, la librer\u00eda m\u00e1s utilizada es la que ofrece Apache Arrow , en concreto la librer\u00eda PyArrow . As\u00ed pues, la instalamos mediante pip: pip install pyarrow Apache Arrow usa un tipo de estructura denominada tabla para almacenar los datos bidimensional (ser\u00eda muy similar a un Dataframe de Pandas ). La documentaci\u00f3n de PyArrow dispone de un libro de recetas con ejemplos con c\u00f3digo para los diferentes casos de uso que se nos puedan plantear. Vamos a simular el mismo ejemplo que hemos realizado previamente mediante Avro , y vamos a crear un fichero en formato JSON con empleados, y tras persistirlo en formato Parquet , lo vamos a recuperar: Empleados en columnas Empleados en Filas dict-parquet.py import pyarrow.parquet as pq import pyarrow as pa # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Almacenamos los empleados por columnas empleados = { \"nombre\" : [ \"Carlos\" , \"Juan\" ], \"altura\" : [ 180 , 44 ], \"edad\" : [ None , 34 ]} # 3.- Creamos una tabla Arrow y la persistimos mediante Parquet tabla = pa . Table . from_pydict ( empleados , schema ) pq . write_table ( tabla , 'empleados.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) Para que pyarrow pueda leer los empleados como documentos JSON, a d\u00eda de hoy s\u00f3lo puede hacerlo leyendo documentos individuales almacenados en fichero: Por lo tanto, creamos el fichero empleados.json con la siguiente informaci\u00f3n: empleados.json { \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 } { \"nombre\" : \"Juan\" , \"altura\" : 175 } De manera que podemos leer los datos JSON y persistirlos en Parquet del siguiente modo: json-parquet.py import pyarrow.parquet as pq import pyarrow as pa from pyarrow import json # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Leemos los empleados tabla = json . read_json ( \"empleados.json\" ) # 3.- Persistimos la tabla en Parquet pq . write_table ( tabla , 'empleados-json.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados-json.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados-json.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) En ambos casos obtendr\u00edamos algo similar a: Schema del fichero empleados.parquet: nombre: string altura: int32 edad: int32 Tabla de Empleados: pyarrow.Table nombre: string altura: int32 edad: int32 ---- nombre: [[\"Carlos\",\"Juan\"]] altura: [[180,44]] edad: [[null,34]] Parquet y Pandas \u00b6 En el caso del uso de Pandas el c\u00f3digo todav\u00eda se simplifica m\u00e1s. Un fragmento de c\u00f3digo muy sencillo y que nos puede ser muy \u00fatil es c\u00f3mo leer un archivo en formato Parquet y pasarlo a Pandas : import pyarrow.parquet as pq trips = pq . read_table ( 'archivo.parquet' ) trips = trips . to_pandas () Si reproducimos el mismo ejemplo que hemos realizado con Avro tenemos que los Dataframes ofrecen el m\u00e9todo to_parquet para exportar a un fichero Parquet : csv-parquet.py import pandas as pd df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # A partir de un DataFrame, persistimos los datos df . to_parquet ( 'sales.parquet' ) Parquet y HDFS Si quisi\u00e9ramos almacenar el archivo directamente en HDFS, necesitamos indicarle a Pandas la direcci\u00f3n del sistema de archivos que tenemos configurado en core-site.xml : core-site.ml <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> As\u00ed pues, \u00fanicamente necesitamos modificar el nombre del archivo donde serializamos los datos a Parquet : df . to_parquet ( 'hdfs://iabd-virtualbox:9000/sales.parquet' ) ORC \u00b6 Formato de un archivo ORC Apache ORC es un formato de datos columnar optimizado para la lectura, escritura y procesamiento de datos en Hive . ORC tiene una tasa de compresi\u00f3n alta (utiliza zlib ), y al basarse en Hive, soporta sus tipos de datos ( datetime , decimal , y tipos complejos como struct , list , map y union ) y es compatible con HiveQL. Los fichero ORC se componen de tiras de datos ( stripes ), donde cada tira contiene un \u00edndice, los datos de la fila y un pie (con estad\u00edsticas como la cantidad, m\u00e1ximos y m\u00ednimos y la suma de cada columna convenientemente cacheadas) ORC y Python \u00b6 Para crear archivos ORC y leerlos, volvemos a necesitar la librer\u00eda PyArrow . As\u00ed pues, para la escritura de datos, por ejemplo, desde un Dataframe, har\u00edamos: import pandas as pd import pyarrow as pa import pyarrow.orc as orc df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] table = pa . Table . from_pandas ( df , preserve_index = False ) orc . write_table ( table , 'pdi_sales.orc' ) Y si queremos leer, la propia librer\u00eda de Pandas tiene su m\u00e9todo read_orc : df_orc = pd . read_orc ( 'pdi_sales.orc' ) to_orc Desde Pandas 1.5, a partir de un DataFrame , podemos persistir los datos mediante el m\u00e9todo to_orc . El problema lo tienen Google Colab o Kaggle , que todav\u00eda no dan soporta a esa versi\u00f3n. Comparando formatos \u00b6 Acabamos de ver que cada uno de los formatos tiene sus puntos fuertes. Los formatos basados en filas ofrecen un rendimiento mayor en las escrituras que en las lecturas, ya que a\u00f1adir nuevos registros en m\u00e1s sencillo. Si s\u00f3lo vamos a hacer consultas sobre un subconjunto de las columnas, entonces un formato columnas se comportar\u00e1 mejor, ya que no necesita recuperar los registros enteros (cosa que s\u00ed hacen los formatos basados en filas). Respecto a la compresi\u00f3n, entendiendo que ofrece una ventaja a la hora de almacenar y transmitir la informaci\u00f3n, es \u00fatil cuando trabajamos con un gran volumen de datos. Los formatos basado en columnas ofrecen un mejor rendimiento ya que todos los datos del mismo tipo se almacenan de forma contigua lo que permite una mayor eficiencia en la compresi\u00f3n (adem\u00e1s, cada tipo de columna tiene su propia codificaci\u00f3n). Respecto a la evoluci\u00f3n del esquema, con operaciones como a\u00f1adir o eliminar columnas o cambiar su nombre, la mejor decisi\u00f3n es decantarse por Avro. Adem\u00e1s, al tener el esquema en JSON facilita su gesti\u00f3n y permite que tengan m\u00e1s de un esquema. Si nuestros documentos tienen una estructura compleja compuesta por columnas anidadas y normalmente realizamos consultas sobre un subconjunto de las subcolumnas, la elecci\u00f3n deber\u00eda ser Parquet, por la estructura que utiliza. Finalmente, recordar que ORC est\u00e1 especialmente enfocado a su uso con Hive, mientras que Spark tiene un amplio soporte para Parquet y que si trabajamos con Kafka, Avro es una buena elecci\u00f3n. Comparativa de formatos Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiBa ger_sales.orc : 6,98 MiB Referencias \u00b6 Introduction to Big Data Formats Handling Avro files in Python Big Data File Formats Demystified Actividades \u00b6 ( RA5075.1 / CE5.1c / 1p) Mediante Python y utilizando Kaggle , crea un notebook a partir de los datos del dataset de retrasos en los vuelos y a partir de uno de los ficheros (el que m\u00e1s te guste) transforma los datos y persiste los siguientes archivos: air<anyo>.parquet : el archivo csv en formato Parquet . air<anyo>.orc : el archivo csv en formato ORC . air<anyo>_small.avro : la fecha ( FL_DATE ), el identificador de la aerol\u00ednea ( OP_CARRIER ) y el retraso de cada vuelo ( DEP_DELAY ) en formato Avro air<anyo>_small.parquet : con los mismos atributos pero en Parquet . Adjunta una captura del cuaderno, anota los tama\u00f1os de los ficheros creados y el tiempo necesario para su creaci\u00f3n, y finalmente, comparte el notebook con el usuario Aitor Medrano de Kaggle .","title":"S39.- Formatos de datos"},{"location":"hadoop/04formatos.html#filas-vs-columnas","text":"Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV , con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet , lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes.","title":"Filas vs Columnas"},{"location":"hadoop/04formatos.html#avro","text":"Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Tiene soporte para la compresi\u00f3n de bloques y es un formato que permite la divisi\u00f3n de los datos ( splittable ).","title":"Avro"},{"location":"hadoop/04formatos.html#formato","text":"El formato Avro se basa en el uso de esquemas, los cuales definen los tipos de datos y protocolos mediante JSON. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Formato de un archivo Avro Cada fichero Avro almacena el esquema en la cabecera del fichero y luego est\u00e1n los datos en formato binario. Los esquemas se componen de tipos primitivos ( null , boolean , int , long , float , double , bytes , y string ) y compuestos ( record , enum , array , map , union , y fixed ). Un ejemplo de esquema podr\u00eda ser: empleado.avsc { \"type\" : \"record\" , \"namespace\" : \"SeveroOchoa\" , \"name\" : \"Empleado\" , \"fields\" : [ { \"name\" : \"Nombre\" , \"type\" : \"string\" }, { \"name\" : \"Altura\" , \"type\" : \"float\" } { \"name\" : \"Edad\" , \"type\" : \"int\" } ] }","title":"Formato"},{"location":"hadoop/04formatos.html#avro-y-python","text":"Para poder serializar y deserializar documentos Avro mediante Python , previamente debemos instalar la librer\u00eda avro : pip install avro-python3 # o si utilizamos Anaconda conda install -c conda-forge avro-python3 Vamos a realizar un ejemplo donde primero leemos un esquema de un archivo Avro , y con dicho esquema, escribiremos nuevos datos en un fichero. A continuaci\u00f3n, abrimos el fichero escrito y leemos y mostramos los datos: C\u00f3digo Python Resultado Google Colab import avro import copy import json from avro.datafile import DataFileReader , DataFileWriter from avro.io import DatumReader , DatumWriter # abrimos el fichero en modo binario y leemos el esquema schema = avro . schema . parse ( open ( \"empleado.avsc\" , \"rb\" ) . read ()) # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleados.avro' , 'wb' ) as f : writer = DataFileWriter ( f , DatumWriter (), schema ) writer . append ({ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }) writer . append ({ \"nombre\" : \"Juan\" , \"altura\" : 175 }) writer . close () # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleados.avro\" , \"rb\" ) as f : reader = DataFileReader ( f , DatumReader ()) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . meta ) # obtenemos el schema del fichero le\u00eddo schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] reader . close () print ( f 'Schema de empleado.avsc: \\n { schema } ' ) print ( f 'Schema del fichero empleados.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { \"type\" : \"record\" , \"name\" : \"empleado\" , \"namespace\" : \"SeveroOchoa\" , \"fields\" : [{ \"type\" : \"string\" , \"name\" : \"nombre\" }, { \"type\" : \"int\" , \"name\" : \"altura\" }, { \"type\" : [ \"null\" , \"int\" ], \"name\" : \"edad\" , \"default\" : null }]} Schema del f ichero empleados.avro : { ' t ype' : 'record' , ' na me' : 'empleado' , ' na mespace' : 'SeveroOchoa' , ' f ields' : [{ ' t ype' : 's tr i n g' , ' na me' : ' n ombre' }, { ' t ype' : 'i nt ' , ' na me' : 'al tura ' }, { ' t ype' : [ ' null ' , 'i nt ' ], ' na me' : 'edad' , 'de fault ' : No ne }]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Accede al cuaderno en Google Colab y adjunta el archivo del empleado.avsc .","title":"Avro y Python"},{"location":"hadoop/04formatos.html#fastavro","text":"Para trabajar con Avro y grandes vol\u00famenes de datos, es mejor utilizar la librer\u00eda Fastavro ( https://github.com/fastavro/fastavro ) la cual ofrece un rendimiento mayor (en vez de estar codificada en Python puro, tiene algunos fragmentos realizados mediante Cython ). Primero, hemos de instalar la librer\u00eda: pip install fastavro # o si utilizamos Anaconda conda install -c conda-forge fastavro Como pod\u00e9is observar a continuaci\u00f3n, hemos repetido el ejemplo y el c\u00f3digo es muy similar: C\u00f3digo Python Resultado Google Colab import fastavro import copy import json from fastavro import reader # abrimos el fichero en modo binario y leemos el esquema with open ( \"empleado.avsc\" , \"rb\" ) as f : schemaJSON = json . load ( f ) schemaDict = fastavro . parse_schema ( schemaJSON ) empleados = [{ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }, { \"nombre\" : \"Juan\" , \"altura\" : 175 }] # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleadosf.avro' , 'wb' ) as f : fastavro . writer ( f , schemaDict , empleados ) # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleadosf.avro\" , \"rb\" ) as f : reader = fastavro . reader ( f ) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . metadata ) # obtenemos el schema del fichero le\u00eddo schemaReader = copy . deepcopy ( reader . writer_schema ) schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] print ( f 'Schema de empleado.avsc: \\n { schemaDict } ' ) print ( f 'Schema del fichero empleadosf.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}], '__ fasta vro_parsed' : True , '__ na med_schemas' : { 'SeveroOchoa.empleado' : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]}}} Schema del f ichero empleados f .avro : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Accede al cuaderno en Google Colab y adjunta el archivo del empleado.avsc .","title":"Fastavro"},{"location":"hadoop/04formatos.html#fastavro-y-pandas","text":"Finalmente, vamos a realizar un \u00faltimo ejemplo con las dos librer\u00edas m\u00e1s utilizadas. Vamos a leer un fichero CSV de ventas mediante Pandas , y tras limpiar los datos y quedarnos \u00fanicamente con las ventas de Alemania, almacenaremos el resultado del procesamiento en Avro . Acceso Local Google Colab import pandas as pd from fastavro import writer , parse_schema # Leemos el csv mediante pandas df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 1. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 2. Convertimos el Dataframe a una lista de diccionarios records = df . to_dict ( 'records' ) # 3. Persistimos en un fichero avro with open ( 'sales.avro' , 'wb' ) as f : writer ( f , schemaParseado , records ) Accede al cuaderno en Google Colab y adjunta el archivo de ventas pdi_sales.csv . Acceso HDFS Aunque todav\u00eda no hayamos estudiado el ecosistema Hadoop, vamos a ver como podemos conectarnos a HDFS para leer y persistir datos en formato Avro : import pandas as pd from fastavro import parse_schema from hdfs import InsecureClient from hdfs.ext.avro import AvroWriter from hdfs.ext.dataframe import write_dataframe # 1. Nos conectamos a HDFS HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # 2. Leemos el Dataframe with hdfs_client . read ( '/user/iabd/pdi_sales.csv' ) as reader : df = pd . read_csv ( reader , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 3. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 4a. Persistimos en un fichero avro dentro de HDFS mediante la extension AvroWriter de hdfs with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado ) as writer : records = df . to_dict ( 'records' ) # diccionario for record in records : writer . write ( record ) # 4b. O directamente persistimos el Dataframe mediante la extension write_dataframe de hdfs write_dataframe ( hdfs_client , '/user/iabd/sales2.avro' , df ) # infiere el esquema write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado ) Para el acceso HDFS hemos utilizados las extensiones Fastavro y Pandas de la librer\u00eda HDFS que veremos en posteriores sesiones.","title":"Fastavro y Pandas"},{"location":"hadoop/04formatos.html#comprimiendo-los-datos","text":"\u00bfY s\u00ed comprimimos los datos para ocupen menos espacio en nuestro cl\u00faster y por tanto, nos cuesten menos dinero? Fastavro soporta dos tipos de compresi\u00f3n: gzip (mediante el algoritmo deflate ) y snappy . Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data, la cual hemos de instalar previamente mediante pip install python-snappy . Para indicar el tipo de compresi\u00f3n, \u00fanicamente hemos de a\u00f1adir un par\u00e1metros extra con el algoritmo de compresi\u00f3n en la funci\u00f3n/constructor de persistencia: Fastavro y gzip AvroWriter y snappy write_dataframe y snappy writer ( f , schemaParseado , records , 'deflate' ) with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado , 'snappy' ) as writer : write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado , codec = 'snappy' ) Comparando algoritmos de compresi\u00f3n Respecto a la compresi\u00f3n, sobre un fichero de 100GB, podemos considerar media si ronda los 50GB y alta si baja a los 40GB. Algoritmo Velocidad Compresi\u00f3n Gzip Media Media Bzip2 Lenta Alta Snappy Alta Media M\u00e1s que un tema de espacio, necesitamos que los procesos sean eficientes y por eso priman los algoritmos que son m\u00e1s r\u00e1pidos. Si te interesa el tema, es muy interesante el art\u00edculo Data Compression in Hadoop . Por ejemplo, si realizamos el ejemplo de Fast Avro y Pandas con acceso local obtenemos los siguientes tama\u00f1os: Sin compresi\u00f3n: 6,9 MiB Gzip: 1,9 MiB Snappy: 2,8 MiB","title":"Comprimiendo los datos"},{"location":"hadoop/04formatos.html#parquet","text":"Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas, es decir, para lecturas de grandes cargas de trabajo. Tiene un ratio de compresi\u00f3n muy alto (mediante Snappy ronda el 75%), y adem\u00e1s, solo se recorren las columnas necesarias en cada lectura, lo que reduce las operaciones de disco de entrada/salida.","title":"Parquet"},{"location":"hadoop/04formatos.html#formato_1","text":"Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc...","title":"Formato"},{"location":"hadoop/04formatos.html#parquet-y-python","text":"Para interactuar con el formato Parquet mediante Python, la librer\u00eda m\u00e1s utilizada es la que ofrece Apache Arrow , en concreto la librer\u00eda PyArrow . As\u00ed pues, la instalamos mediante pip: pip install pyarrow Apache Arrow usa un tipo de estructura denominada tabla para almacenar los datos bidimensional (ser\u00eda muy similar a un Dataframe de Pandas ). La documentaci\u00f3n de PyArrow dispone de un libro de recetas con ejemplos con c\u00f3digo para los diferentes casos de uso que se nos puedan plantear. Vamos a simular el mismo ejemplo que hemos realizado previamente mediante Avro , y vamos a crear un fichero en formato JSON con empleados, y tras persistirlo en formato Parquet , lo vamos a recuperar: Empleados en columnas Empleados en Filas dict-parquet.py import pyarrow.parquet as pq import pyarrow as pa # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Almacenamos los empleados por columnas empleados = { \"nombre\" : [ \"Carlos\" , \"Juan\" ], \"altura\" : [ 180 , 44 ], \"edad\" : [ None , 34 ]} # 3.- Creamos una tabla Arrow y la persistimos mediante Parquet tabla = pa . Table . from_pydict ( empleados , schema ) pq . write_table ( tabla , 'empleados.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) Para que pyarrow pueda leer los empleados como documentos JSON, a d\u00eda de hoy s\u00f3lo puede hacerlo leyendo documentos individuales almacenados en fichero: Por lo tanto, creamos el fichero empleados.json con la siguiente informaci\u00f3n: empleados.json { \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 } { \"nombre\" : \"Juan\" , \"altura\" : 175 } De manera que podemos leer los datos JSON y persistirlos en Parquet del siguiente modo: json-parquet.py import pyarrow.parquet as pq import pyarrow as pa from pyarrow import json # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Leemos los empleados tabla = json . read_json ( \"empleados.json\" ) # 3.- Persistimos la tabla en Parquet pq . write_table ( tabla , 'empleados-json.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados-json.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados-json.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) En ambos casos obtendr\u00edamos algo similar a: Schema del fichero empleados.parquet: nombre: string altura: int32 edad: int32 Tabla de Empleados: pyarrow.Table nombre: string altura: int32 edad: int32 ---- nombre: [[\"Carlos\",\"Juan\"]] altura: [[180,44]] edad: [[null,34]]","title":"Parquet y Python"},{"location":"hadoop/04formatos.html#parquet-y-pandas","text":"En el caso del uso de Pandas el c\u00f3digo todav\u00eda se simplifica m\u00e1s. Un fragmento de c\u00f3digo muy sencillo y que nos puede ser muy \u00fatil es c\u00f3mo leer un archivo en formato Parquet y pasarlo a Pandas : import pyarrow.parquet as pq trips = pq . read_table ( 'archivo.parquet' ) trips = trips . to_pandas () Si reproducimos el mismo ejemplo que hemos realizado con Avro tenemos que los Dataframes ofrecen el m\u00e9todo to_parquet para exportar a un fichero Parquet : csv-parquet.py import pandas as pd df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # A partir de un DataFrame, persistimos los datos df . to_parquet ( 'sales.parquet' ) Parquet y HDFS Si quisi\u00e9ramos almacenar el archivo directamente en HDFS, necesitamos indicarle a Pandas la direcci\u00f3n del sistema de archivos que tenemos configurado en core-site.xml : core-site.ml <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> As\u00ed pues, \u00fanicamente necesitamos modificar el nombre del archivo donde serializamos los datos a Parquet : df . to_parquet ( 'hdfs://iabd-virtualbox:9000/sales.parquet' )","title":"Parquet y Pandas"},{"location":"hadoop/04formatos.html#orc","text":"Formato de un archivo ORC Apache ORC es un formato de datos columnar optimizado para la lectura, escritura y procesamiento de datos en Hive . ORC tiene una tasa de compresi\u00f3n alta (utiliza zlib ), y al basarse en Hive, soporta sus tipos de datos ( datetime , decimal , y tipos complejos como struct , list , map y union ) y es compatible con HiveQL. Los fichero ORC se componen de tiras de datos ( stripes ), donde cada tira contiene un \u00edndice, los datos de la fila y un pie (con estad\u00edsticas como la cantidad, m\u00e1ximos y m\u00ednimos y la suma de cada columna convenientemente cacheadas)","title":"ORC"},{"location":"hadoop/04formatos.html#orc-y-python","text":"Para crear archivos ORC y leerlos, volvemos a necesitar la librer\u00eda PyArrow . As\u00ed pues, para la escritura de datos, por ejemplo, desde un Dataframe, har\u00edamos: import pandas as pd import pyarrow as pa import pyarrow.orc as orc df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] table = pa . Table . from_pandas ( df , preserve_index = False ) orc . write_table ( table , 'pdi_sales.orc' ) Y si queremos leer, la propia librer\u00eda de Pandas tiene su m\u00e9todo read_orc : df_orc = pd . read_orc ( 'pdi_sales.orc' ) to_orc Desde Pandas 1.5, a partir de un DataFrame , podemos persistir los datos mediante el m\u00e9todo to_orc . El problema lo tienen Google Colab o Kaggle , que todav\u00eda no dan soporta a esa versi\u00f3n.","title":"ORC y Python"},{"location":"hadoop/04formatos.html#comparando-formatos","text":"Acabamos de ver que cada uno de los formatos tiene sus puntos fuertes. Los formatos basados en filas ofrecen un rendimiento mayor en las escrituras que en las lecturas, ya que a\u00f1adir nuevos registros en m\u00e1s sencillo. Si s\u00f3lo vamos a hacer consultas sobre un subconjunto de las columnas, entonces un formato columnas se comportar\u00e1 mejor, ya que no necesita recuperar los registros enteros (cosa que s\u00ed hacen los formatos basados en filas). Respecto a la compresi\u00f3n, entendiendo que ofrece una ventaja a la hora de almacenar y transmitir la informaci\u00f3n, es \u00fatil cuando trabajamos con un gran volumen de datos. Los formatos basado en columnas ofrecen un mejor rendimiento ya que todos los datos del mismo tipo se almacenan de forma contigua lo que permite una mayor eficiencia en la compresi\u00f3n (adem\u00e1s, cada tipo de columna tiene su propia codificaci\u00f3n). Respecto a la evoluci\u00f3n del esquema, con operaciones como a\u00f1adir o eliminar columnas o cambiar su nombre, la mejor decisi\u00f3n es decantarse por Avro. Adem\u00e1s, al tener el esquema en JSON facilita su gesti\u00f3n y permite que tengan m\u00e1s de un esquema. Si nuestros documentos tienen una estructura compleja compuesta por columnas anidadas y normalmente realizamos consultas sobre un subconjunto de las subcolumnas, la elecci\u00f3n deber\u00eda ser Parquet, por la estructura que utiliza. Finalmente, recordar que ORC est\u00e1 especialmente enfocado a su uso con Hive, mientras que Spark tiene un amplio soporte para Parquet y que si trabajamos con Kafka, Avro es una buena elecci\u00f3n. Comparativa de formatos Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiBa ger_sales.orc : 6,98 MiB","title":"Comparando formatos"},{"location":"hadoop/04formatos.html#referencias","text":"Introduction to Big Data Formats Handling Avro files in Python Big Data File Formats Demystified","title":"Referencias"},{"location":"hadoop/04formatos.html#actividades","text":"( RA5075.1 / CE5.1c / 1p) Mediante Python y utilizando Kaggle , crea un notebook a partir de los datos del dataset de retrasos en los vuelos y a partir de uno de los ficheros (el que m\u00e1s te guste) transforma los datos y persiste los siguientes archivos: air<anyo>.parquet : el archivo csv en formato Parquet . air<anyo>.orc : el archivo csv en formato ORC . air<anyo>_small.avro : la fecha ( FL_DATE ), el identificador de la aerol\u00ednea ( OP_CARRIER ) y el retraso de cada vuelo ( DEP_DELAY ) en formato Avro air<anyo>_small.parquet : con los mismos atributos pero en Parquet . Adjunta una captura del cuaderno, anota los tama\u00f1os de los ficheros creados y el tiempo necesario para su creaci\u00f3n, y finalmente, comparte el notebook con el usuario Aitor Medrano de Kaggle .","title":"Actividades"},{"location":"hadoop/04hdfs.html","text":"HDFS \u00b6 Namenodes y Datanodes \u00b6 En la sesi\u00f3n anterior ya vimos una peque\u00f1a introducci\u00f3n a la arquitectura de HDFS. Arquitectura HDFS Vamos a profundizar en sus elementos. Namenode \u00b6 Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento. Datanode \u00b6 De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS Funcionamiento de HDFS \u00b6 En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet . Procesos de lectura \u00b6 Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster. Proceso de escritura \u00b6 El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura). HDFS por dentro \u00b6 HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log . Trabajando con HDFS \u00b6 Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp Bloques \u00b6 A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451 Administraci\u00f3n \u00b6 Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos. Snapshots \u00b6 Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos HDFS UI \u00b6 En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd . HDFS y Python \u00b6 Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas . Hue \u00b6 Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. ( RA5075.3 / CE5.3a / 0.5p) Explica paso a paso el proceso de lectura (indicando qu\u00e9 bloques y los datanodes empleados) que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS ( RA5075.3 / CE5.3a / 0.5p) En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. ( RA5075.4 / CE5.4e y CE5.4f / 1p) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/snaps . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/snaps as\u00ed como de sus snapshots . ( RA5075.4 / CE5.4e y CE5.4f / 1p) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales.","title":"S39.- HDFS"},{"location":"hadoop/04hdfs.html#hdfs","text":"","title":"HDFS"},{"location":"hadoop/04hdfs.html#namenodes-y-datanodes","text":"En la sesi\u00f3n anterior ya vimos una peque\u00f1a introducci\u00f3n a la arquitectura de HDFS. Arquitectura HDFS Vamos a profundizar en sus elementos.","title":"Namenodes y Datanodes"},{"location":"hadoop/04hdfs.html#namenode","text":"Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento.","title":"Namenode"},{"location":"hadoop/04hdfs.html#datanode","text":"De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS","title":"Datanode"},{"location":"hadoop/04hdfs.html#funcionamiento-de-hdfs","text":"En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet .","title":"Funcionamiento de HDFS"},{"location":"hadoop/04hdfs.html#procesos-de-lectura","text":"Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster.","title":"Procesos de lectura"},{"location":"hadoop/04hdfs.html#proceso-de-escritura","text":"El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura).","title":"Proceso de escritura"},{"location":"hadoop/04hdfs.html#hdfs-por-dentro","text":"HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log .","title":"HDFS por dentro"},{"location":"hadoop/04hdfs.html#trabajando-con-hdfs","text":"Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp","title":"Trabajando con HDFS"},{"location":"hadoop/04hdfs.html#bloques","text":"A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451","title":"Bloques"},{"location":"hadoop/04hdfs.html#administracion","text":"Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.","title":"Administraci\u00f3n"},{"location":"hadoop/04hdfs.html#snapshots","text":"Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos","title":"Snapshots"},{"location":"hadoop/04hdfs.html#hdfs-ui","text":"En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd .","title":"HDFS UI"},{"location":"hadoop/04hdfs.html#hdfs-y-python","text":"Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas .","title":"HDFS y Python"},{"location":"hadoop/04hdfs.html#hue","text":"Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue","title":"Hue"},{"location":"hadoop/04hdfs.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python","title":"Referencias"},{"location":"hadoop/04hdfs.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. ( RA5075.3 / CE5.3a / 0.5p) Explica paso a paso el proceso de lectura (indicando qu\u00e9 bloques y los datanodes empleados) que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS ( RA5075.3 / CE5.3a / 0.5p) En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. ( RA5075.4 / CE5.4e y CE5.4f / 1p) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/snaps . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/snaps as\u00ed como de sus snapshots . ( RA5075.4 / CE5.4e y CE5.4f / 1p) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales.","title":"Actividades"},{"location":"hadoop/05flume.html","text":"Sqoop / Flume \u00b6 Las dos herramientas principales utilizadas para importar/exportar datos en HDFS son Sqoop y Flume, las cuales vamos a estudiar a continuaci\u00f3n. Sqoop \u00b6 Logo de Apache Sqoop Apache Sqoop ( https://sqoop.apache.org ) es una herramienta dise\u00f1ada para transferir de forma eficiente datos crudos entre un cluster de Hadoop y un almacenamiento estructurado, como una base de datos relacional. Sin continuidad Desde Junio de 2021, el proyecto Sqoop ha dejado de mantenerse como proyecto de Apache y forma parte del \u00e1tico . A\u00fan as\u00ed, creemos conveniente conocer su uso en el estado actual. Gran parte de las funcionalidad que ofrece Sqoop se pueden realizar mediante Nifi o Spark . Un caso t\u00edpico de uso es el de cargar los datos en un data lake (ya sea en HDFS o en S3) con datos que importaremos desde una base de datos, como MariaDB , PostgreSQL o MongoDB . Sqoop utiliza una arquitectura basada en conectores, con soporte para plugins que ofrecen la conectividad a los sistemas externos, como pueden ser Oracle o SqlServer . Internamente, Sqoop utiliza los algoritmos MapReduce para importar y exportar los datos. Por defecto, todos los trabajos Sqoop ejecutan cuatro mapas de trabajo, de manera que los datos se dividen en cuatro nodos de Hadoop. Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos ya tenemos tanto Hadoop como Sqoop instalados, podemos descargar la \u00faltima versi\u00f3n desde http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz . Se recomienda seguir las instrucciones resumidas que tenemos en https://www.tutorialspoint.com/sqoop/sqoop_installation.htm o las de https://riptutorial.com/sqoop . Un par de aspectos que hemos tenido que modificar en nuestra m\u00e1quina virtual son: Copiar el driver de MySQL en $SQOOP_HOME/lib Copiar la librer\u00eda commons-langs-2.6 en $SQOOP_HOME/lib Una vez configurado, podemos comprobar que funciona, por ejemplo, consultando las bases de datos que tenemos en MariaDB (aparecen mensajes de warning por no tener instalados/configurados algunos productos): sqoop list-databases --connect jdbc:mysql://localhost --username=iabd --password=iabd Importando datos \u00b6 La sintaxis b\u00e1sica de Sqoop para importar datos en HDFS es la siguiente: sqoop import -connect jdbc:mysql://host/nombredb -table <nombreTabla> \\ --username <usuarioMariaDB> --password <passwordMariaDB> -m 2 El \u00fanico par\u00e1metro que conviene explicar es -m 2 , el cual est\u00e1 indicando que utilice dos mappers en paralelo para importar los datos. Si no le indicamos este par\u00e1metro, como hemos comentado antes, Sqoop siempre utilizar\u00e1 cuatro mappers . La importaci\u00f3n se realiza en dos pasos: Sqoop escanea la base de datos y colecta los metadatos de la tabla a importar. Env\u00eda un job y transfiere los datos reales utilizando los metadatos necesarios. De forma paralela, cada uno de los mappers se encarga de cargar en HDFS una parte proporcional de los datos. Arquitectura de trabajo de Sqoop Los datos importados se almacenan en carpetas de HDFS, pudiendo especificar otras carpetas, as\u00ed como los caracteres separadores o de terminaci\u00f3n de registro. Adem\u00e1s, podemos utilizar diferentes formatos, como son Avro, ORC, Parquet, ficheros secuenciales o de tipo texto, para almacenar los datos en HDFS. Caso 1 - Importando datos desde MariaDB \u00b6 En el siguiente caso de uso vamos a importar datos que tenemos en una base de datos de MariaDB a HDFS. Sqoop y las zonas horarias Cuando se lanza Sqoop captura los timestamps de nuestra base de datos origen y las convierte a la hora del sistema servidor por lo que tenemos que especificar en nuestra base de datos la zona horaria. Para realizar estos ajustes simplemente editamos el fichero mysqld.cnf que se encuentra en /etc/mysql/my.cnf/ y a\u00f1adimos la siguiente propiedad para asignarle nuestra zona horaria: [mariabd] default_time_zone = 'Europe/Madrid' Primero, vamos a preparar nuestro entorno. Una vez conectados a MariaDB , creamos una base de datos que contenga una tabla con informaci\u00f3n sobre profesores: create database sqoopCaso1 ; use sqoopCaso1 ; CREATE TABLE profesores ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Insertamos datos en la tabla profesores: INSERT INTO profesores ( nombre , edad , materia ) VALUES ( \"Carlos\" , 24 , \"Matem\u00e1ticas\" ), ( \"Pedro\" , 32 , \"Ingl\u00e9s\" ), ( \"Juan\" , 35 , \"Tecnolog\u00eda\" ), ( \"Jose\" , 48 , \"Matem\u00e1ticas\" ), ( \"Paula\" , 24 , \"Inform\u00e1tica\" ), ( \"Susana\" , 32 , \"Inform\u00e1tica\" ), ( \"Lorena\" , 54 , \"Inform\u00e1tica\" ); A continuaci\u00f3n, arrancamos HDFS y YARN : start-dfs.sh start-yarn.sh Con el comando sqoop list-tables listamos todas las tablas de la base de datos sqoopCaso1 : sqoop list-tables --connect jdbc:mysql://localhost/sqoopCaso1 --username = iabd --password = iabd Y finalmente importamos los datos mediante el comando sqoop import : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_hdfs \\ --fields-terminated-by = ',' --lines-terminated-by '\\n' En la primera l\u00ednea, indicamos que vamos a importar datos desde un conexi\u00f3n JDBC, donde se indica el SGBD ( mysql ), el host ( localhost ) y el nombre de la base de datos ( sqoopCaso1 ). En la l\u00ednea dos, se configuran tanto el usuario como la contrase\u00f1a del usuario ( iabd / iabd ) que se conecta a la base de datos. En la tercera l\u00ednea, indicamos la tabla que vamos a leer ( profesores ) y el driver que utilizamos. En la cuarta l\u00ednea configuramos el destino HDFS donde se van a importar los datos. Finalmente, en la \u00faltima l\u00ednea, indicamos el separador de los campos y el car\u00e1cter para separar las l\u00edneas. Si queremos que en el caso de que ya existe la carpeta de destino la borre previamente, a\u00f1adiremos la opci\u00f3n --delete-target-dir . Unhealthy node Nuestra m\u00e1quina virtual tiene el espacio limitado a 30GB, y es probable que en alg\u00fan momento se llene el disco. Adem\u00e1s de eliminar archivos no necesarios, una opci\u00f3n es configurar YARN mediante el archivo yarn-site.xml y configurar las siguientes propiedades para ser m\u00e1s permisivos con la falta de espacio: <property> <name> yarn.nodemanager.disk-health-checker.min-healthy-disks </name> <value> 0.0 </value> </property> <property> <name> yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage </name> <value> 100.0 </value> </property> El resultado que aparece en consola es: 2021-12-14 17:19:04,684 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 2021-12-14 17:19:04,806 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 2021-12-14 17:19:05,057 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time. 2021-12-14 17:19:05,087 INFO manager.SqlManager: Using default fetchSize of 1000 2021-12-14 17:19:05,087 INFO tool.CodeGenTool: Beginning code generation 2021-12-14 17:19:05,793 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,798 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,877 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-3.3.1 Note: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 2021-12-14 17:19:12,153 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.jar 2021-12-14 17:19:12,235 INFO mapreduce.ImportJobBase: Beginning import of profesores 2021-12-14 17:19:12,240 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address 2021-12-14 17:19:12,706 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 2021-12-14 17:19:12,714 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:14,330 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 2021-12-14 17:19:14,608 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2021-12-14 17:19:16,112 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1639498733738_0001 2021-12-14 17:19:22,016 INFO db.DBInputFormat: Using read commited transaction isolation 2021-12-14 17:19:22,018 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM profesores 2021-12-14 17:19:22,022 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 1 to: 7 2021-12-14 17:19:22,214 INFO mapreduce.JobSubmitter: number of splits:4 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639498733738_0001 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2021-12-14 17:19:23,390 INFO conf.Configuration: resource-types.xml not found 2021-12-14 17:19:23,391 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'. 2021-12-14 17:19:24,073 INFO impl.YarnClientImpl: Submitted application application_1639498733738_0001 2021-12-14 17:19:24,300 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1639498733738_0001/ 2021-12-14 17:19:24,303 INFO mapreduce.Job: Running job: job_1639498733738_0001 2021-12-14 17:19:44,015 INFO mapreduce.Job: Job job_1639498733738_0001 running in uber mode : false 2021-12-14 17:19:44,017 INFO mapreduce.Job: map 0% reduce 0% 2021-12-14 17:20:21,680 INFO mapreduce.Job: map 50% reduce 0% 2021-12-14 17:20:23,707 INFO mapreduce.Job: map 100% reduce 0% 2021-12-14 17:20:24,736 INFO mapreduce.Job: Job job_1639498733738_0001 completed successfully 2021-12-14 17:20:24,960 INFO mapreduce.Job: Counters: 34 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=1125124 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=377 HDFS: Number of bytes written=163 HDFS: Number of read operations=24 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 HDFS: Number of bytes read erasure-coded=0 Job Counters Killed map tasks=1 Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=139377 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=139377 Total vcore-milliseconds taken by all map tasks=139377 Total megabyte-milliseconds taken by all map tasks=142722048 Map-Reduce Framework Map input records=7 Map output records=7 Input split bytes=377 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=1218 CPU time spent (ms)=5350 Physical memory (bytes) snapshot=560439296 Virtual memory (bytes) snapshot=10029588480 Total committed heap usage (bytes)=349175808 Peak Map Physical memory (bytes)=142544896 Peak Map Virtual memory (bytes)=2507415552 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=163 2021-12-14 17:20:24,979 INFO mapreduce.ImportJobBase: Transferred 163 bytes in 70,589 seconds (2,3091 bytes/sec) 2021-12-14 17:20:24,986 INFO mapreduce.ImportJobBase: Retrieved 7 records. Vamos a repasar la salida del log para entender el proceso: En la l\u00ednea 5 vemos como se lanza el generador de c\u00f3digo. En las l\u00edneas 6, 7 y 15 vemos como ejecuta la consulta para obtener todos los datos de profesores. En la l\u00ednea 20 obtiene los valores m\u00ednimo y m\u00e1ximo para calcular como dividir los datos. De las l\u00edneas 29 a la 34 se ejecuta el proceso MapReduce . En el resto se puede observar un resumen estad\u00edstico. Si accedemos al interfaz gr\u00e1fico de YARN (en http://iabd-virtualbox:8088/cluster ) podemos ver c\u00f3mo aparece el proceso como realizado: Estado de YARN tras la importaci\u00f3n Si accedemos al interfaz gr\u00e1fico de Hadoop (recuerda que puedes acceder a \u00e9l mediante http://localhost:9870 ) podremos comprobar en el directorio /user/iabd/sqoop que ha creado el directorio que hemos especificado junto con los siguientes archivos: Contenido de /user/iabd/sqoop/profesores_hdfs Si entramos a ver los datos, podemos visualizar el contenido del primer fragmento que contiene los primeros datos de la tabla: Contenido de part-m-0000 Caso 2 - Exportando datos a MariaDB \u00b6 Ahora vamos a hacer el paso contrario, desde HDFS vamos a exportar los ficheros a otra tabla. As\u00ed pues, primero vamos a crear la nueva tabla en una nueva base de datos (aunque pod\u00edamos haber reutilizado la base de datos): create database sqoopCaso2 ; use sqoopCaso2 ; CREATE TABLE profesores2 ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Para exportar los datos de HDFS y cargarlos en esta nueva tabla lanzamos la siguiente orden: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --export-dir = /user/iabd/sqoop/profesores_hdfs Formatos Avro y Parquet \u00b6 Sqoop permite trabajar con diferentes formatos, tanto Avro como Parquet . Avro es un formato de almacenamiento basado en filas para Hadoop que se usa ampliamente como formato de serializaci\u00f3n. Recuerda que Avro almacena la estructura en formato JSON y los datos en binario. Parquet a su vez es un formato de almacenamiento binario basado en columnas que puede almacenar estructuras de datos anidados. Avro y Hadoop Para que funcione la serializaci\u00f3n con Avro hay que copiar el fichero .jar que viene en el directorio de Sqoop para Avro como librer\u00eda de Hadoop , mediante el siguiente comando: cp $SQOOP_HOME /lib/avro-1.8.1.jar $HADOOP_HOME /share/hadoop/common/lib/ rm $HADOOP_HOME /share/hadoop/common/lib/avro-1.7.7.jar En nuestra m\u00e1quina virtual este paso ya est\u00e1 realizado. Para importar los datos en formato Avro , a\u00f1adiremos la opci\u00f3n --as-avrodatafile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_avro --as-avrodatafile Si en vez de Avro , queremos importar los datos en formato Parquet cambiamos el \u00faltimo par\u00e1metro por --as-parquetfile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_parquet --as-parquetfile Si queremos comprobar los archivos, podemos acceder via HDFS y la opci\u00f3n -ls : hdfs dfs -ls /user/iabd/sqoop/profesores_avro Obteniendo: Found 5 items -rw-r--r-- 1 iabd supergroup 0 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/_SUCCESS -rw-r--r-- 1 iabd supergroup 568 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00000.avro -rw-r--r-- 1 iabd supergroup 569 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00001.avro -rw-r--r-- 1 iabd supergroup 547 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00002.avro -rw-r--r-- 1 iabd supergroup 574 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00003.avro Si queremos ver el contenido de una de las partes, utilizamos la opci\u00f3n -text : hdfs dfs -text /user/iabd/sqoop/profesores_avro/part-m-00000.avro Obteniendo el esquema y los datos en formato Avro : {\"id\":{\"int\":1},\"nombre\":{\"string\":\"Carlos\"},\"edad\":{\"int\":24},\"materia\":{\"string\":\"Matem\u00e1ticas\"}} {\"id\":{\"int\":2},\"nombre\":{\"string\":\"Pedro\"},\"edad\":{\"int\":32},\"materia\":{\"string\":\"Ingl\u00e9s\"}} Autoevaluaci\u00f3n \u00bfQu\u00e9 sucede si ejectuamos el comando hdfs dfs -tail /user/iabd/sqoop/profesores_avro/part-m-00000.avro ? \u00bfPor qu\u00e9 aparece contenido en binario? En el caso de ficheros Parquet , primero listamos los archivos generados: hdfs dfs -ls /user/iabd/sqoop/profesores_parquet Obteniendo: Found 6 items drwxr-xr-x - iabd supergroup 0 2021-12-15 16:13 /user/iabd/sqoop/profesores_parquet/.metadata drwxr-xr-x - iabd supergroup 0 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/.signals -rw-r--r-- 1 iabd supergroup 1094 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet -rw-r--r-- 1 iabd supergroup 1114 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/1e12aaad-98c6-4508-9c41-e1599e698385.parquet -rw-r--r-- 1 iabd supergroup 1097 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/6a803503-f3e0-4f2a-8546-a337f7f90e73.parquet -rw-r--r-- 1 iabd supergroup 1073 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/eda459b2-1da4-4790-b649-0f2f8b83ab06.parquet Podemos usar las parquet-tools para ver su contenido. Si la instalamos mediante pip install parquet-tools , podremos acceder a ficheros locales y almacenados en S3. Si queremos acceder de forma remota via HDFS, podemos descargar la versi\u00f3n Java y utilizarla mediante hadoop (aunque da problemas entre las versiones de Sqoop y Parquet): hadoop jar parquet-tools-1.11.2.jar head -n5 hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet Si queremos obtener informaci\u00f3n sobre los documentos, usaremos la opci\u00f3n meta : hadoop jar parquet-tools-1.11.2.jar meta hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet M\u00e1s informaci\u00f3n sobre parquet-tools en https://pypi.org/project/parquet-tools/ . Trabajando con datos comprimidos \u00b6 En un principio, vamos a trabajar siempre con los datos sin comprimir. Cuando tengamos datos que vamos a utilizar durante mucho tiempo (del orden de varios a\u00f1os) es cuando nos plantearemos comprimir los datos. Por defecto, podemos comprimir mediante el formato gzip : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_gzip \\ --compress Si en cambio queremos comprimirlo con formato bzip2 : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_bzip \\ --compress --compression-codec bzip2 Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data. As\u00ed pues, para utilizarlo lo indicaremos mediante el codec snappy : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_snappy \\ --compress --compression-codec snappy Importando con filtros \u00b6 Adem\u00e1s de poder importar todos los datos de una tabla, podemos filtrar los datos. Por ejemplo, podemos indicar mediante la opci\u00f3n --where el filtro a ejecutar en la consulta: sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_materia_info \\ --where \"materia='Inform\u00e1tica'\" Tambi\u00e9n podemos restringir las columnas que queremos recuperar mediante la opci\u00f3n --columns : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_cols \\ --columns \"nombre,materia\" Finalmente, podemos especificar una consulta con clave de particionado (en este caso, ya no indicamos el nombre de la tabla): sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_query \\ --query \"select * from profesores where edad > 40 AND \\$CONDITIONS\" \\ --split-by \"id\" En la consulta, hemos de a\u00f1adir el token \\$CONDITIONS , el cual Hadoop substituir\u00e1 por la columna por la que realiza el particionado. Importaci\u00f3n incremental \u00b6 Si utilizamos procesos batch , es muy com\u00fan realizar importaciones incrementales tras una carga de datos. Para ello, utilizaremos las opciones --incremental append junto con la columna a comprobar mediante --check-column y el \u00faltimo registro cargado mediante --last-value : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_inc \\ --incremental append \\ --check-column id \\ --last-value 4 Despu\u00e9s de ejecutarlo, si vemos la informaci\u00f3n que nos devuelve, en las \u00faltimas l\u00edneas, podemos copiar los par\u00e1metros que tenemos que utilizar para posteriores importaciones. ... 2021-12-15 19:10:59,348 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments: 2021-12-15 19:10:59,348 INFO tool.ImportTool: --incremental append 2021-12-15 19:10:59,348 INFO tool.ImportTool: --check-column id 2021-12-15 19:10:59,348 INFO tool.ImportTool: --last-value 7 2021-12-15 19:10:59,349 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create') Trabajando con Hive \u00b6 Podemos importar los datos en HDFS para que luego puedan ser consultables desde Hive . Para ello hemos de utilizar el par\u00e1metro --hive-import e indicar el nombre de la base de datos mediante --hive-database as\u00ed como la opci\u00f3n de --create-hive-table para que cree la tabla indicada en el par\u00e1metro hive-table . Es importante destacar que ya no ponemos destino con target-dir : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database default \\ --create-hive-table --hive-table profesores_mariadb Para comprobar el resultado, dentro de Hive ejecutaremos el comando: describe formatted profesores_mariadb Para exportar los datos, de forma similar haremos: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --driver = com.mysql.jdbc.Driver \\ --h-catalog-table profesores_mariadb Flume \u00b6 Logo de Apache Flume All\u00e1 por el a\u00f1o 2010 Cloudera present\u00f3 Flume que posteriormente pas\u00f3 a formar parte de Apache ( https://flume.apache.org/ ) como un software para tratamiento e ingesta de datos masivo. Flume permite crear desarrollos complejos que permiten el tratamiento en streaming de datos masivos. Flume funciona como un buffer entre los productores de datos y el destino final. Al utilizar un buffer, evitamos que un productor sature a un consumidor, sin necesidad de preocuparnos de que alg\u00fan destino est\u00e9 inalcanzable o inoperable (por ejemplo, en el caso de que haya ca\u00eddo HDFS), etc... Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos tambi\u00e9n tenemos instalado Flume , podemos descargar la \u00faltima versi\u00f3n desde http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz . A nivel de configuraci\u00f3n s\u00f3lo hemos definido la variable de entorno $FLUME_HOME que apunta a /opt/flume-1.9.0 . Arquitectura \u00b6 Su arquitectura es sencilla, y se basa en el uso de agentes que se dividen en tres componentes los cuales podemos configurar: Source (fuente): Fuente de origen de los datos, ya sea Twitter , Kafka , una petici\u00f3n Http , etc... Las fuentes son un componente activo que recibe datos desde otra aplicaci\u00f3n que produce datos (aunque tambi\u00e9n existen fuentes que pueden producir datos por s\u00ed mismos, cuyo objetivo es poder probar ciertos flujos de datos). Las fuentes puedes escuchar uno o m\u00e1s puertos de red para recibir o leer datos del sistema de arhcivos. Cada fuente debe conectar a al menos un canal. Una fuente puede escribir en varios canales, replicando los eventos a todos o algunos canales en base a alg\u00fan criterio. Channel (canal): la v\u00eda por donde se tratar\u00e1n los datos. Un canal es un componente pasivo que almacena los datos como un buffer. Se comportan como colas, donde las fuentes publican y los sumideros consumen los datos. M\u00faltiples fuentes pueden escribir de forma segura en el mismo canal, y m\u00faltiples sumideros pueden leer desde el mismo canal. Sin embargo, cada sumidero s\u00f3lo puede leer de un \u00fanico canal. Si m\u00faltiples sumideros leen del mismo canal, s\u00f3lo uno de ellos leer\u00e1 el dato. Sink (sumidero): persistencia/movimiento de los datos, a ficheros / base de datos. Toma eventos del canal de manera continua leyendo y eliminando los eventos. A continuaci\u00f3n, los transmite hacia el siguiente componente, ya sea a HDFS, Hive, etc... Una vez los datos han llegado al siguiente destino, el sumidero informa al canal mediante un commit transaccional para que elimine dichos eventos del canal. Arquitectura Flume - imagen extra\u00edda de https://www.diegocalvo.es/flume/ Es muy recomendable acceder a la gu\u00eda de usuario oficial para consultar todas las fuentes de datos, canales y sumideros disponibles en la actualidad. A continuaci\u00f3n se nombran algunos de los m\u00e1s destacados: Sources Channels Sinks Avro Source Memory Channel HDFS Sink Thrift Source JDBC Channel Hive Sink Exec Source Kafka Channel Logger Sink JMS Source File Channel Avro Sink Spooling Directory Source Spillable Memory Channel Thrift Sink Twitter 1% firehose Source Pseudo Transaction Channel Kafka Sink Kafka Source File Roll Sink NetCat Source Null Sink Sequence Generator Source HBaseSink Syslog Sources AsyncHBaseSink HTTP Source MorphlineSolrSink Multiport Syslog TCP Source ElasticSearchSink Syslog UDP Source Kite Dataset Sink Flume se complica cuando queremos utilizarlo para obtener datos de manera paralela (o multiplexada) y/o necesitamos crear nuestros propios sumideros o interceptores. Pero por lo general, su uso es sencillo y se trata de una herramienta muy recomendada como ayuda/alternativa a herramientas como Pentaho . Algunas de sus caracter\u00edsticas son: Dise\u00f1o flexible basado en flujos de datos de transmisi\u00f3n. Resistente a fallos y robusto con m\u00faltiples conmutaciones por error y mecanismos de recuperaci\u00f3n. Lleva datos desde origen a destino: incluidos HDFS y HBase . Agentes \u00b6 Un agente es la unidad m\u00e1s sencilla con la que trabaja Flume , permitiendo conectar un agente Flume a uno o m\u00e1s agentes, encanden\u00e1ndolos. Adem\u00e1s, un agente puede recibir datos de uno o m\u00e1s agentes. Conectando m\u00faltiples agentes entre s\u00ed podemos crear un flujo de datos para mover datos de un lugar a otro, de aplicaciones que producen datos a HDFS, HBase o donde necesitemos. Para lanzar un tarea en Flume , debemos definir un agente, el cual funciona como un contenedor para alojar subcomponentes que permiten mover los datos. Estos agentes tienen cuatro partes bien diferenciadas asociadas a la arquitectura de Flume. En la primera parte, definiremos los componente del agente ( sources , channels y sinks ), y luego, para cada uno de ellos, configuraremos sus propiedades: sources : responsables de colocar los eventos/datos en el agente channels : buffer que almacena los eventos/datos recibidos por los sources hasta que un sink lo saca para enviarlo al siguiente destino. sinks : responsable de sacar los eventos/datos del agente y reenviarlo al siguiente agente (HDFS, HBase, etc...) Evento \u00b6 El evento es la unidad m\u00e1s peque\u00f1a del procesamiento de eventos de Flume. Cuando Flume lee una fuente de datos, envuelve una fila de datos (es decir, encuentra los saltos de l\u00ednea) en un evento. Un evento es una estructura de datos que se compone de dos partes: Encabezado, se utiliza principalmente para registrar informaci\u00f3n mediante un mapa en forma de clave y valor. No transfieren datos, pero contienen informaci\u00f3n util para el enrutado y gesti\u00f3n de la prioridad o importancia de los mensajes. Cuerpo: array de bytes que almacena los datos reales. Probando Flume \u00b6 Por ejemplo, vamos a crear un agente el cual llamaremos ExecLoggerAgent el cual va a ejecutar un comando y mostrar\u00e1 el resultado por el log de Flume . Para ello, creamos la configuraci\u00f3n del agente en el fichero agente.conf (todas las propiedades comenzar\u00e1n con el nombre del agente): agente.conf # Nombramos los componentes del agente ExecLoggerAgent.sources = Exec ExecLoggerAgent.channels = MemChannel ExecLoggerAgent.sinks = LoggerSink # Describimos el tipo de origen ExecLoggerAgent.sources.Exec.type = exec ExecLoggerAgent.sources.Exec.command = ls /home/iabd/ # Describimos el destino ExecLoggerAgent.sinks.LoggerSink.type = logger # Describimos la configuraci\u00f3n del canal ExecLoggerAgent.channels.MemChannel.type = memory ExecLoggerAgent.channels.MemChannel.capacity = 1000 ExecLoggerAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal ExecLoggerAgent.sources.Exec.channels = MemChannel ExecLoggerAgent.sinks.LoggerSink.channel = MemChannel Antes de lanzar el agente Flume, recuerda que debes arrancar tanto Hadoop como YARN , por ejemplo, mediante el comando start-all.sh . A continuaci\u00f3n ya podemos lanzar Flume con el agente mediante el comando (la opci\u00f3n -n sirve para indicar el nombre del agente, y con -f indicamos el nombre del archivo de configuraci\u00f3n): flume-ng agent -n ExecLoggerAgent -f agente.conf Configurando un agente \u00b6 Si te has fijado en el ejemplo anterior, los ficheros de configuraci\u00f3n de los agentes siguen el mismo formato. Para definir un flujo dentro de un agente, necesitamos enlazar las fuentes y los sumideros con un canal. Para ello, listaremos las fuentes, sumideros y canales del agente, y entonces apuntaremos la fuente y el sumidero a un canal. 1 - N - 1 Una fuente puede indicar m\u00faltiples canales, pero un sumidero s\u00f3lo puede indicar un \u00fanico canal. As\u00ed pues, el formato ser\u00e1 similar al siguiente archivo: # Listamos las fuentes, sumideros y canales <Agent>.sources = <Source> <Agent>.sinks = <Sink> <Agent>.channels = <Channel1> <Channel2> # Configuramos los canales de la fuente <Agent>.sources.<Source>.channels = <Channel1> <Channel2> ... # Configuramos el canal para el sumidero <Agent>.sinks.<Sink>.channel = <Channel1> Adem\u00e1s de definir el flujo, es necesario configurar las propiedades de cada fuente, sumidero y canal. Para elo se sigue la misma nomenclatura donde fijamos el tipo de componente (mediante la propiedad type ) y el resto de propiedades espec\u00edficas de cada componente: # Propiedades de las fuentes <Agent>.sources.<Source>.<someProperty> = <someValue> # Propiedades de los canales <Agent>.channel.<Channel>.<someProperty> = <someValue> # Propiedades de los sumideros <Agent>.sources.<Sink>.<someProperty> = <someValue> Para cada tipo de fuente , canal y sumidero es recomendable revisar la documentaci\u00f3n para validar todas las propiedades disponibles. Caso 3a - Almacenando en HDFS \u00b6 En este caso de uso vamos generar datos de forma secuencial y los vamos a ingestar en HDFS . Una buena pr\u00e1ctica es colocar los archivos de configuraci\u00f3n dentro de $FLUME_HOME/conf . As\u00ed pues, vamos a crear el agente SeqGenAgent y almacenar la configuraci\u00f3n en el fichero seqgen.conf : seqgen.conf # Nombramos a los componentes del agente SeqGenAgent.sources = SeqSource SeqGenAgent.channels = MemChannel SeqGenAgent.sinks = HDFS # Describimos el tipo de origen SeqGenAgent.sources.SeqSource.type = seq # Describimos el destino SeqGenAgent.sinks.HDFS.type = hdfs SeqGenAgent.sinks.HDFS.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/seqgen_data/ SeqGenAgent.sinks.HDFS.hdfs.filePrefix = flume-caso3-seqgen SeqGenAgent.sinks.HDFS.hdfs.rollInterval = 0 SeqGenAgent.sinks.HDFS.hdfs.rollCount = 1000 SeqGenAgent.sinks.HDFS.hdfs.fileType = DataStream # Describimos la configuraci\u00f3n del canal SeqGenAgent.channels.MemChannel.type = memory SeqGenAgent.channels.MemChannel.capacity = 1000 SeqGenAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal SeqGenAgent.sources.SeqSource.channels = MemChannel SeqGenAgent.sinks.HDFS.channel = MemChannel Ejecutamos el siguiente comando desde $FLUME_HOME y a los pocos segundo lo paramos mediante CTRL + C para que detenga la generaci\u00f3n de n\u00fameros, ya que si no seguir\u00e1 generando archivos en HDFS: ./bin/flume-ng agent --conf ./conf/ --conf-file conf/seqgen.conf \\ --name SeqGenAgent \\ -Dflume.root.logger = INFO,console Vaciando HDFS Si queremos eliminar los ficheros generados en HDFS, recuerda que puedes realizar un borrado recursivo mediante el comando: hdfs dfs -rm -r /user/iabd/flume Si comprobamos por ejemplo el contenido de la carpeta ( hdfs dfs -ls /user/iabd/flume/seqgen_data ) veremos que se han generado m\u00faltiples archivos: Found 10 items -rw-r--r-- 1 iabd supergroup 1402 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 -rw-r--r-- 1 iabd supergroup 1368 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740934 -rw-r--r-- 1 iabd supergroup 1350 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740935 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740936 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740937 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740938 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740939 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740940 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740941 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740942 Y si comprobamos el contenido del primero ( hdfs dfs -cat /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 ) veremos como contiene la secuencia generada: 0 1 2 3 ... Caso 3b - De Netcat a HDFS \u00b6 Ahora vamos a crear otro ejemplo de generaci\u00f3n de informaci\u00f3n, pero esta vez, en vez que utilizar la memoria del servidor como canal, vamos a utilizar el sistema de archivos. Adem\u00e1s, para generar la informaci\u00f3n nos basamos en una fuente Netcat , en la cual debemos especificar un puerto de escucha. Mediante esta fuente, Flume quedar\u00e1 a la escucha en dicho puerto y recibir\u00e1 cada l\u00ednea introducida como un evento individual que transferir\u00e1 al canal especificado. En el mismo directorio $FLUME_HOME\\conf , creamos un nuevo fichero con el nombre netcat.conf y creamos otro agente que se va a encargar de generar informaci\u00f3n: netcat.conf # Nombramos a los componentes del agente NetcatAgent.sources = Netcat NetcatAgent.channels = FileChannel NetcatAgent.sinks = HdfsSink # Describimos el origen netcat en localhost:44444 NetcatAgent.sources.Netcat.type = netcat NetcatAgent.sources.Netcat.bind = localhost NetcatAgent.sources.Netcat.port = 44444 NetcatAgent.sources.Netcat.channels = FileChannel # Describimos el destino en HDFS NetcatAgent.sinks.HdfsSink.type = hdfs NetcatAgent.sinks.HdfsSink.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/net_data/ NetcatAgent.sinks.HdfsSink.hdfs.writeFormat = Text NetcatAgent.sinks.HdfsSink.hdfs.fileType = DataStream NetcatAgent.sinks.HdfsSink.channel = FileChannel # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAgent.channels.FileChannel.type = file NetcatAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint Lanzamos al agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat.conf \\ --name NetcatAgent \\ -Dflume.root.logger = INFO,console En una nueva pesta\u00f1a introducimos el siguiente comando y escribimos curl telnet://localhost:44444 Una vez conectados, escribimos varias frases con saltos de l\u00ednea. Por cada vez que pulsamos Enter , nos aparecer\u00e1 un OK . Probando Netcat OK Esto parece que funciona m\u00e1s o menos OK A continuaci\u00f3n, nos vamos al navegador web de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/flume/net_data ) y comprobamos que se ha creado el fichero: Resultado del flujo Netcat-HDFS Caso 4 - Flujos encadenados \u00b6 Es muy com\u00fan definir un pipeline de flujos encadenados, uniendo la salida de un agente a la entrada de otro. Para ello, utilizaremos como enlace un sink - source de tipo Avro . Este dise\u00f1o tambi\u00e9n se conoce como flujo Multi-hop : Encadenando flujos En este caso, vamos a crear un primer agente ( NetcatAvroAgent ) que ingeste datos desde Netcat y los coloque en un sink de tipo Avro . Para ello, creamos el agente netcat-avro.conf : netcat-avro.conf # Nombramos a los componentes del agente NetcatAvroAgent.sources = Netcat NetcatAvroAgent.channels = FileChannel NetcatAvroAgent.sinks = AvroSink # Describimos el origen netcat en localhost:44444 NetcatAvroAgent.sources.Netcat.type = netcat NetcatAvroAgent.sources.Netcat.bind = localhost NetcatAvroAgent.sources.Netcat.port = 44444 # Describimos el destino como Avro en localhost:10003 NetcatAvroAgent.sinks.AvroSink.type = avro NetcatAvroAgent.sinks.AvroSink.hostname = localhost NetcatAvroAgent.sinks.AvroSink.port = 10003 # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAvroAgent.sources.Netcat.channels = FileChannel NetcatAvroAgent.sinks.AvroSink.channel = FileChannel NetcatAvroAgent.channels.FileChannel.type = file NetcatAvroAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAvroAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint A continuaci\u00f3n, creamos un segundo agente ( AvroHdfsAgent ) que utilice como fuente Avro y que almacene los eventos recibidos en HDFS. Para ello, creamos el agente avro-hdfs.conf : avro-hdfs.conf # Nombramos a los componentes del agente AvroHdfsAgent.sources = AvroSource AvroHdfsAgent.channels = MemChannel AvroHdfsAgent.sinks = HdfsSink # Describimos el origen como Avro en localhost:10003 AvroHdfsAgent.sources.AvroSource.type = avro AvroHdfsAgent.sources.AvroSource.bind = localhost AvroHdfsAgent.sources.AvroSource.port = 10003 # Describimos el destino HDFS AvroHdfsAgent.sinks.HdfsSink.type = hdfs AvroHdfsAgent.sinks.HdfsSink.hdfs.path = /user/iabd/flume/avro_data/ AvroHdfsAgent.sinks.HdfsSink.hdfs.fileType = DataStream AvroHdfsAgent.sinks.HdfsSink.hdfs.writeFormat = Text # Unimos el origen y el destino AvroHdfsAgent.sources.AvroSource.channels = MemChannel AvroHdfsAgent.sinks.HdfsSink.channel = MemChannel AvroHdfsAgent.channels.MemChannel.type = memory Primero lanzamos este \u00faltimo agente, para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en nueva pesta\u00f1a, lanzamos el primer agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat-avro.conf \\ --name NetcatAvroAgent \\ -Dflume.root.logger = INFO,console Finalmente, en otro terminal, escribimos mensajes Netcat accediendo a curl telnet://localhost:44444 . Si acced\u00e9is a la carpeta /user/iabd/flume/avro_data en HDFS podremos comprobar c\u00f3mo se van creando archivos que agrupan los mensajes enviados. Caso 5 - Flujo multi-agente \u00b6 Para demostrar como varios agentes pueden conectarse entre s\u00ed, vamos a realizar un caso de uso donde vamos a leer informaci\u00f3n de tres fuentes distintas: una fuente de Netcat con un canal basado en ficheros, otra que realice spooling de una carpeta (vigile una carpeta y cuando haya alg\u00fan archivo, lo ingeste y lo elimine) utilizando un canal en memoria y un tercero que ejecute un comando utilizando tambi\u00e9n un canal en memoria. Como agente de consolidaci\u00f3n que una la informaci\u00f3n de las tres fuentes de datos, vamos a reutilizar el agente AvroHdfsAgent que hemos creado en el caso de uso anterior. Consolidando flujos Para ello, vamos a definir los agentes en siguiente fichero de configuraci\u00f3n multiagent-avro.conf ): multiagent-avro.conf # Nombramos las tres fuentes con sus tres sumideros MultiAgent.sources = Netcat Spooldir Exec MultiAgent.channels = FileChannel MemChannel1 MemChannel2 MultiAgent.sinks = AvroSink1 AvroSink2 AvroSink3 # Describimos el primer agente MultiAgent.sources.Netcat.type = netcat MultiAgent.sources.Netcat.bind = localhost MultiAgent.sources.Netcat.port = 10004 # Describimos el segundo agente MultiAgent.sources.Spooldir.type = spooldir MultiAgent.sources.Spooldir.spoolDir = /home/iabd/flume/spoolDir MultiAgent.sources.Spooldir.deletePolicy = immediate # Describimos el tercer agente MultiAgent.sources.Exec.type = exec MultiAgent.sources.Exec.command = cat /home/iabd/datos/empleados.txt # Describimos los tres destinos como Avro en localhost:10003 MultiAgent.sinks.AvroSink1.type = avro MultiAgent.sinks.AvroSink1.hostname = localhost MultiAgent.sinks.AvroSink1.port = 10003 MultiAgent.sinks.AvroSink2.type = avro MultiAgent.sinks.AvroSink2.hostname = localhost MultiAgent.sinks.AvroSink2.port = 10003 MultiAgent.sinks.AvroSink3.type = avro MultiAgent.sinks.AvroSink3.hostname = localhost MultiAgent.sinks.AvroSink3.port = 10003 # Describimos los canales MultiAgent.channels.FileChannel.type = file MultiAgent.channels.FileChannel.dataDir = /home/iabd/flume/data MultiAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint MultiAgent.channels.MemChannel1.type = memory MultiAgent.channels.MemChannel2.type = memory # Unimos los or\u00edgenes y destinos MultiAgent.sources.Netcat.channels = FileChannel MultiAgent.sources.Spooldir.channels = MemChannel1 MultiAgent.sources.Exec.channels = MemChannel2 MultiAgent.sinks.AvroSink1.channel = FileChannel MultiAgent.sinks.AvroSink2.channel = MemChannel1 MultiAgent.sinks.AvroSink3.channel = MemChannel2 Preparaci\u00f3n Antes de arrancar los agentes, aseg\u00farate de tener creada la carpeta /home/iabd/flume/spoolDir y disponible el recurso /home/iabd/datos/empleados.txt . Igual que en el caso de uso anterior, primero lanzamos el agente consolidador para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en una nueva pesta\u00f1a, lanzamos el multi agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/multiagent-avro.conf \\ --name MultiAgent \\ -Dflume.root.logger = INFO,console Interceptores Podemos utilizar interceptores para modificar o borrar eventos al vuelo a partir del timestamp , nombre del host , uuid , etc... incluso mediante el uso de una expresi\u00f3n regular. Si quieres profundizar en el tema, el siguiente art\u00edculo detalla los diferentes tipos y configuraciones: https://data-flair.training/blogs/flume-interceptors/ En este caso, para poder probarlo, adem\u00e1s de enviar comandos Netstat en curl telnet://localhost:10004 , prueba a colocar un archivo de texto (por ejemplo, un documento CSV) en /home/iabd/flume/spoolDir . Actividades \u00b6 Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Haciendo uso de Sqoop y la base de datos retail_db , importa todos los pedidos de la tabla orders cuyo campo order_status sea COMPLETE . Coloca los datos en user/iabd/sqoop/orders/datos_parquet en formato Parquet, utilizando el tabulador como delimitador de campos y utilizando la compresi\u00f3n Snappy. Deber\u00e1s recuperar 22.899 (\u00bfo 22.902?) registros. Haciendo uso de Sqoop y la base de datos retail_db , importa todos los clientes de la tabla customers cuyo campo state sea CA . Coloca los datos en user/iabd/sqoop/customers/datos_avro en formato Avro, utilizando la compresi\u00f3n bzip2. Deber\u00e1s recuperar las columnas customer_id, customer_fname, customer_lname, customer_state . El resultado contendr\u00e1 2012 registros. Mediante Flume , realiza los caso de uso 3, 4 y 5. FIXME: Revisar claves de Twitter (opcional) Haciendo uso de Flume, recupera informaci\u00f3n de Twitter y almac\u00e9nala en HDFS. Para ello, utiliza el Twitter 1% Firehouse source y el HDFS sink . Para ello, necesitar\u00e9is las claves de desarrollo que ya creamos en las sesiones sobre Nifi. Adjunta una captura de pantalla donde se visualice el contenido de uno de los bloques de HDFS. Cuidado con el espacio de almacenamiento Una vez lances el agente, detenlo a los tres segundos para no llenar de datos HDFS. Referencias \u00b6 P\u00e1gina oficial de Sqoop Sqoop User Guide Sqoop Tutorial en Tutorialspoint P\u00e1gina oficial de Flume Flume User Guide","title":"Sqoop y Flume. Herramientas de ingesta de datos en y desde Hadoop."},{"location":"hadoop/05flume.html#sqoop-flume","text":"Las dos herramientas principales utilizadas para importar/exportar datos en HDFS son Sqoop y Flume, las cuales vamos a estudiar a continuaci\u00f3n.","title":"Sqoop / Flume"},{"location":"hadoop/05flume.html#sqoop","text":"Logo de Apache Sqoop Apache Sqoop ( https://sqoop.apache.org ) es una herramienta dise\u00f1ada para transferir de forma eficiente datos crudos entre un cluster de Hadoop y un almacenamiento estructurado, como una base de datos relacional. Sin continuidad Desde Junio de 2021, el proyecto Sqoop ha dejado de mantenerse como proyecto de Apache y forma parte del \u00e1tico . A\u00fan as\u00ed, creemos conveniente conocer su uso en el estado actual. Gran parte de las funcionalidad que ofrece Sqoop se pueden realizar mediante Nifi o Spark . Un caso t\u00edpico de uso es el de cargar los datos en un data lake (ya sea en HDFS o en S3) con datos que importaremos desde una base de datos, como MariaDB , PostgreSQL o MongoDB . Sqoop utiliza una arquitectura basada en conectores, con soporte para plugins que ofrecen la conectividad a los sistemas externos, como pueden ser Oracle o SqlServer . Internamente, Sqoop utiliza los algoritmos MapReduce para importar y exportar los datos. Por defecto, todos los trabajos Sqoop ejecutan cuatro mapas de trabajo, de manera que los datos se dividen en cuatro nodos de Hadoop. Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos ya tenemos tanto Hadoop como Sqoop instalados, podemos descargar la \u00faltima versi\u00f3n desde http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz . Se recomienda seguir las instrucciones resumidas que tenemos en https://www.tutorialspoint.com/sqoop/sqoop_installation.htm o las de https://riptutorial.com/sqoop . Un par de aspectos que hemos tenido que modificar en nuestra m\u00e1quina virtual son: Copiar el driver de MySQL en $SQOOP_HOME/lib Copiar la librer\u00eda commons-langs-2.6 en $SQOOP_HOME/lib Una vez configurado, podemos comprobar que funciona, por ejemplo, consultando las bases de datos que tenemos en MariaDB (aparecen mensajes de warning por no tener instalados/configurados algunos productos): sqoop list-databases --connect jdbc:mysql://localhost --username=iabd --password=iabd","title":"Sqoop"},{"location":"hadoop/05flume.html#importando-datos","text":"La sintaxis b\u00e1sica de Sqoop para importar datos en HDFS es la siguiente: sqoop import -connect jdbc:mysql://host/nombredb -table <nombreTabla> \\ --username <usuarioMariaDB> --password <passwordMariaDB> -m 2 El \u00fanico par\u00e1metro que conviene explicar es -m 2 , el cual est\u00e1 indicando que utilice dos mappers en paralelo para importar los datos. Si no le indicamos este par\u00e1metro, como hemos comentado antes, Sqoop siempre utilizar\u00e1 cuatro mappers . La importaci\u00f3n se realiza en dos pasos: Sqoop escanea la base de datos y colecta los metadatos de la tabla a importar. Env\u00eda un job y transfiere los datos reales utilizando los metadatos necesarios. De forma paralela, cada uno de los mappers se encarga de cargar en HDFS una parte proporcional de los datos. Arquitectura de trabajo de Sqoop Los datos importados se almacenan en carpetas de HDFS, pudiendo especificar otras carpetas, as\u00ed como los caracteres separadores o de terminaci\u00f3n de registro. Adem\u00e1s, podemos utilizar diferentes formatos, como son Avro, ORC, Parquet, ficheros secuenciales o de tipo texto, para almacenar los datos en HDFS.","title":"Importando datos"},{"location":"hadoop/05flume.html#caso-1-importando-datos-desde-mariadb","text":"En el siguiente caso de uso vamos a importar datos que tenemos en una base de datos de MariaDB a HDFS. Sqoop y las zonas horarias Cuando se lanza Sqoop captura los timestamps de nuestra base de datos origen y las convierte a la hora del sistema servidor por lo que tenemos que especificar en nuestra base de datos la zona horaria. Para realizar estos ajustes simplemente editamos el fichero mysqld.cnf que se encuentra en /etc/mysql/my.cnf/ y a\u00f1adimos la siguiente propiedad para asignarle nuestra zona horaria: [mariabd] default_time_zone = 'Europe/Madrid' Primero, vamos a preparar nuestro entorno. Una vez conectados a MariaDB , creamos una base de datos que contenga una tabla con informaci\u00f3n sobre profesores: create database sqoopCaso1 ; use sqoopCaso1 ; CREATE TABLE profesores ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Insertamos datos en la tabla profesores: INSERT INTO profesores ( nombre , edad , materia ) VALUES ( \"Carlos\" , 24 , \"Matem\u00e1ticas\" ), ( \"Pedro\" , 32 , \"Ingl\u00e9s\" ), ( \"Juan\" , 35 , \"Tecnolog\u00eda\" ), ( \"Jose\" , 48 , \"Matem\u00e1ticas\" ), ( \"Paula\" , 24 , \"Inform\u00e1tica\" ), ( \"Susana\" , 32 , \"Inform\u00e1tica\" ), ( \"Lorena\" , 54 , \"Inform\u00e1tica\" ); A continuaci\u00f3n, arrancamos HDFS y YARN : start-dfs.sh start-yarn.sh Con el comando sqoop list-tables listamos todas las tablas de la base de datos sqoopCaso1 : sqoop list-tables --connect jdbc:mysql://localhost/sqoopCaso1 --username = iabd --password = iabd Y finalmente importamos los datos mediante el comando sqoop import : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_hdfs \\ --fields-terminated-by = ',' --lines-terminated-by '\\n' En la primera l\u00ednea, indicamos que vamos a importar datos desde un conexi\u00f3n JDBC, donde se indica el SGBD ( mysql ), el host ( localhost ) y el nombre de la base de datos ( sqoopCaso1 ). En la l\u00ednea dos, se configuran tanto el usuario como la contrase\u00f1a del usuario ( iabd / iabd ) que se conecta a la base de datos. En la tercera l\u00ednea, indicamos la tabla que vamos a leer ( profesores ) y el driver que utilizamos. En la cuarta l\u00ednea configuramos el destino HDFS donde se van a importar los datos. Finalmente, en la \u00faltima l\u00ednea, indicamos el separador de los campos y el car\u00e1cter para separar las l\u00edneas. Si queremos que en el caso de que ya existe la carpeta de destino la borre previamente, a\u00f1adiremos la opci\u00f3n --delete-target-dir . Unhealthy node Nuestra m\u00e1quina virtual tiene el espacio limitado a 30GB, y es probable que en alg\u00fan momento se llene el disco. Adem\u00e1s de eliminar archivos no necesarios, una opci\u00f3n es configurar YARN mediante el archivo yarn-site.xml y configurar las siguientes propiedades para ser m\u00e1s permisivos con la falta de espacio: <property> <name> yarn.nodemanager.disk-health-checker.min-healthy-disks </name> <value> 0.0 </value> </property> <property> <name> yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage </name> <value> 100.0 </value> </property> El resultado que aparece en consola es: 2021-12-14 17:19:04,684 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 2021-12-14 17:19:04,806 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 2021-12-14 17:19:05,057 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time. 2021-12-14 17:19:05,087 INFO manager.SqlManager: Using default fetchSize of 1000 2021-12-14 17:19:05,087 INFO tool.CodeGenTool: Beginning code generation 2021-12-14 17:19:05,793 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,798 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,877 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-3.3.1 Note: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 2021-12-14 17:19:12,153 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.jar 2021-12-14 17:19:12,235 INFO mapreduce.ImportJobBase: Beginning import of profesores 2021-12-14 17:19:12,240 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address 2021-12-14 17:19:12,706 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 2021-12-14 17:19:12,714 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:14,330 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 2021-12-14 17:19:14,608 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2021-12-14 17:19:16,112 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1639498733738_0001 2021-12-14 17:19:22,016 INFO db.DBInputFormat: Using read commited transaction isolation 2021-12-14 17:19:22,018 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM profesores 2021-12-14 17:19:22,022 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 1 to: 7 2021-12-14 17:19:22,214 INFO mapreduce.JobSubmitter: number of splits:4 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639498733738_0001 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2021-12-14 17:19:23,390 INFO conf.Configuration: resource-types.xml not found 2021-12-14 17:19:23,391 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'. 2021-12-14 17:19:24,073 INFO impl.YarnClientImpl: Submitted application application_1639498733738_0001 2021-12-14 17:19:24,300 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1639498733738_0001/ 2021-12-14 17:19:24,303 INFO mapreduce.Job: Running job: job_1639498733738_0001 2021-12-14 17:19:44,015 INFO mapreduce.Job: Job job_1639498733738_0001 running in uber mode : false 2021-12-14 17:19:44,017 INFO mapreduce.Job: map 0% reduce 0% 2021-12-14 17:20:21,680 INFO mapreduce.Job: map 50% reduce 0% 2021-12-14 17:20:23,707 INFO mapreduce.Job: map 100% reduce 0% 2021-12-14 17:20:24,736 INFO mapreduce.Job: Job job_1639498733738_0001 completed successfully 2021-12-14 17:20:24,960 INFO mapreduce.Job: Counters: 34 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=1125124 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=377 HDFS: Number of bytes written=163 HDFS: Number of read operations=24 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 HDFS: Number of bytes read erasure-coded=0 Job Counters Killed map tasks=1 Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=139377 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=139377 Total vcore-milliseconds taken by all map tasks=139377 Total megabyte-milliseconds taken by all map tasks=142722048 Map-Reduce Framework Map input records=7 Map output records=7 Input split bytes=377 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=1218 CPU time spent (ms)=5350 Physical memory (bytes) snapshot=560439296 Virtual memory (bytes) snapshot=10029588480 Total committed heap usage (bytes)=349175808 Peak Map Physical memory (bytes)=142544896 Peak Map Virtual memory (bytes)=2507415552 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=163 2021-12-14 17:20:24,979 INFO mapreduce.ImportJobBase: Transferred 163 bytes in 70,589 seconds (2,3091 bytes/sec) 2021-12-14 17:20:24,986 INFO mapreduce.ImportJobBase: Retrieved 7 records. Vamos a repasar la salida del log para entender el proceso: En la l\u00ednea 5 vemos como se lanza el generador de c\u00f3digo. En las l\u00edneas 6, 7 y 15 vemos como ejecuta la consulta para obtener todos los datos de profesores. En la l\u00ednea 20 obtiene los valores m\u00ednimo y m\u00e1ximo para calcular como dividir los datos. De las l\u00edneas 29 a la 34 se ejecuta el proceso MapReduce . En el resto se puede observar un resumen estad\u00edstico. Si accedemos al interfaz gr\u00e1fico de YARN (en http://iabd-virtualbox:8088/cluster ) podemos ver c\u00f3mo aparece el proceso como realizado: Estado de YARN tras la importaci\u00f3n Si accedemos al interfaz gr\u00e1fico de Hadoop (recuerda que puedes acceder a \u00e9l mediante http://localhost:9870 ) podremos comprobar en el directorio /user/iabd/sqoop que ha creado el directorio que hemos especificado junto con los siguientes archivos: Contenido de /user/iabd/sqoop/profesores_hdfs Si entramos a ver los datos, podemos visualizar el contenido del primer fragmento que contiene los primeros datos de la tabla: Contenido de part-m-0000","title":"Caso 1 - Importando datos desde MariaDB"},{"location":"hadoop/05flume.html#caso-2-exportando-datos-a-mariadb","text":"Ahora vamos a hacer el paso contrario, desde HDFS vamos a exportar los ficheros a otra tabla. As\u00ed pues, primero vamos a crear la nueva tabla en una nueva base de datos (aunque pod\u00edamos haber reutilizado la base de datos): create database sqoopCaso2 ; use sqoopCaso2 ; CREATE TABLE profesores2 ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Para exportar los datos de HDFS y cargarlos en esta nueva tabla lanzamos la siguiente orden: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --export-dir = /user/iabd/sqoop/profesores_hdfs","title":"Caso 2 - Exportando datos a MariaDB"},{"location":"hadoop/05flume.html#formatos-avro-y-parquet","text":"Sqoop permite trabajar con diferentes formatos, tanto Avro como Parquet . Avro es un formato de almacenamiento basado en filas para Hadoop que se usa ampliamente como formato de serializaci\u00f3n. Recuerda que Avro almacena la estructura en formato JSON y los datos en binario. Parquet a su vez es un formato de almacenamiento binario basado en columnas que puede almacenar estructuras de datos anidados. Avro y Hadoop Para que funcione la serializaci\u00f3n con Avro hay que copiar el fichero .jar que viene en el directorio de Sqoop para Avro como librer\u00eda de Hadoop , mediante el siguiente comando: cp $SQOOP_HOME /lib/avro-1.8.1.jar $HADOOP_HOME /share/hadoop/common/lib/ rm $HADOOP_HOME /share/hadoop/common/lib/avro-1.7.7.jar En nuestra m\u00e1quina virtual este paso ya est\u00e1 realizado. Para importar los datos en formato Avro , a\u00f1adiremos la opci\u00f3n --as-avrodatafile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_avro --as-avrodatafile Si en vez de Avro , queremos importar los datos en formato Parquet cambiamos el \u00faltimo par\u00e1metro por --as-parquetfile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_parquet --as-parquetfile Si queremos comprobar los archivos, podemos acceder via HDFS y la opci\u00f3n -ls : hdfs dfs -ls /user/iabd/sqoop/profesores_avro Obteniendo: Found 5 items -rw-r--r-- 1 iabd supergroup 0 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/_SUCCESS -rw-r--r-- 1 iabd supergroup 568 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00000.avro -rw-r--r-- 1 iabd supergroup 569 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00001.avro -rw-r--r-- 1 iabd supergroup 547 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00002.avro -rw-r--r-- 1 iabd supergroup 574 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00003.avro Si queremos ver el contenido de una de las partes, utilizamos la opci\u00f3n -text : hdfs dfs -text /user/iabd/sqoop/profesores_avro/part-m-00000.avro Obteniendo el esquema y los datos en formato Avro : {\"id\":{\"int\":1},\"nombre\":{\"string\":\"Carlos\"},\"edad\":{\"int\":24},\"materia\":{\"string\":\"Matem\u00e1ticas\"}} {\"id\":{\"int\":2},\"nombre\":{\"string\":\"Pedro\"},\"edad\":{\"int\":32},\"materia\":{\"string\":\"Ingl\u00e9s\"}} Autoevaluaci\u00f3n \u00bfQu\u00e9 sucede si ejectuamos el comando hdfs dfs -tail /user/iabd/sqoop/profesores_avro/part-m-00000.avro ? \u00bfPor qu\u00e9 aparece contenido en binario? En el caso de ficheros Parquet , primero listamos los archivos generados: hdfs dfs -ls /user/iabd/sqoop/profesores_parquet Obteniendo: Found 6 items drwxr-xr-x - iabd supergroup 0 2021-12-15 16:13 /user/iabd/sqoop/profesores_parquet/.metadata drwxr-xr-x - iabd supergroup 0 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/.signals -rw-r--r-- 1 iabd supergroup 1094 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet -rw-r--r-- 1 iabd supergroup 1114 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/1e12aaad-98c6-4508-9c41-e1599e698385.parquet -rw-r--r-- 1 iabd supergroup 1097 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/6a803503-f3e0-4f2a-8546-a337f7f90e73.parquet -rw-r--r-- 1 iabd supergroup 1073 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/eda459b2-1da4-4790-b649-0f2f8b83ab06.parquet Podemos usar las parquet-tools para ver su contenido. Si la instalamos mediante pip install parquet-tools , podremos acceder a ficheros locales y almacenados en S3. Si queremos acceder de forma remota via HDFS, podemos descargar la versi\u00f3n Java y utilizarla mediante hadoop (aunque da problemas entre las versiones de Sqoop y Parquet): hadoop jar parquet-tools-1.11.2.jar head -n5 hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet Si queremos obtener informaci\u00f3n sobre los documentos, usaremos la opci\u00f3n meta : hadoop jar parquet-tools-1.11.2.jar meta hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet M\u00e1s informaci\u00f3n sobre parquet-tools en https://pypi.org/project/parquet-tools/ .","title":"Formatos Avro y Parquet"},{"location":"hadoop/05flume.html#trabajando-con-datos-comprimidos","text":"En un principio, vamos a trabajar siempre con los datos sin comprimir. Cuando tengamos datos que vamos a utilizar durante mucho tiempo (del orden de varios a\u00f1os) es cuando nos plantearemos comprimir los datos. Por defecto, podemos comprimir mediante el formato gzip : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_gzip \\ --compress Si en cambio queremos comprimirlo con formato bzip2 : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_bzip \\ --compress --compression-codec bzip2 Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data. As\u00ed pues, para utilizarlo lo indicaremos mediante el codec snappy : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_snappy \\ --compress --compression-codec snappy","title":"Trabajando con datos comprimidos"},{"location":"hadoop/05flume.html#importando-con-filtros","text":"Adem\u00e1s de poder importar todos los datos de una tabla, podemos filtrar los datos. Por ejemplo, podemos indicar mediante la opci\u00f3n --where el filtro a ejecutar en la consulta: sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_materia_info \\ --where \"materia='Inform\u00e1tica'\" Tambi\u00e9n podemos restringir las columnas que queremos recuperar mediante la opci\u00f3n --columns : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_cols \\ --columns \"nombre,materia\" Finalmente, podemos especificar una consulta con clave de particionado (en este caso, ya no indicamos el nombre de la tabla): sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_query \\ --query \"select * from profesores where edad > 40 AND \\$CONDITIONS\" \\ --split-by \"id\" En la consulta, hemos de a\u00f1adir el token \\$CONDITIONS , el cual Hadoop substituir\u00e1 por la columna por la que realiza el particionado.","title":"Importando con filtros"},{"location":"hadoop/05flume.html#importacion-incremental","text":"Si utilizamos procesos batch , es muy com\u00fan realizar importaciones incrementales tras una carga de datos. Para ello, utilizaremos las opciones --incremental append junto con la columna a comprobar mediante --check-column y el \u00faltimo registro cargado mediante --last-value : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_inc \\ --incremental append \\ --check-column id \\ --last-value 4 Despu\u00e9s de ejecutarlo, si vemos la informaci\u00f3n que nos devuelve, en las \u00faltimas l\u00edneas, podemos copiar los par\u00e1metros que tenemos que utilizar para posteriores importaciones. ... 2021-12-15 19:10:59,348 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments: 2021-12-15 19:10:59,348 INFO tool.ImportTool: --incremental append 2021-12-15 19:10:59,348 INFO tool.ImportTool: --check-column id 2021-12-15 19:10:59,348 INFO tool.ImportTool: --last-value 7 2021-12-15 19:10:59,349 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')","title":"Importaci\u00f3n incremental"},{"location":"hadoop/05flume.html#trabajando-con-hive","text":"Podemos importar los datos en HDFS para que luego puedan ser consultables desde Hive . Para ello hemos de utilizar el par\u00e1metro --hive-import e indicar el nombre de la base de datos mediante --hive-database as\u00ed como la opci\u00f3n de --create-hive-table para que cree la tabla indicada en el par\u00e1metro hive-table . Es importante destacar que ya no ponemos destino con target-dir : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database default \\ --create-hive-table --hive-table profesores_mariadb Para comprobar el resultado, dentro de Hive ejecutaremos el comando: describe formatted profesores_mariadb Para exportar los datos, de forma similar haremos: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --driver = com.mysql.jdbc.Driver \\ --h-catalog-table profesores_mariadb","title":"Trabajando con Hive"},{"location":"hadoop/05flume.html#flume","text":"Logo de Apache Flume All\u00e1 por el a\u00f1o 2010 Cloudera present\u00f3 Flume que posteriormente pas\u00f3 a formar parte de Apache ( https://flume.apache.org/ ) como un software para tratamiento e ingesta de datos masivo. Flume permite crear desarrollos complejos que permiten el tratamiento en streaming de datos masivos. Flume funciona como un buffer entre los productores de datos y el destino final. Al utilizar un buffer, evitamos que un productor sature a un consumidor, sin necesidad de preocuparnos de que alg\u00fan destino est\u00e9 inalcanzable o inoperable (por ejemplo, en el caso de que haya ca\u00eddo HDFS), etc... Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos tambi\u00e9n tenemos instalado Flume , podemos descargar la \u00faltima versi\u00f3n desde http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz . A nivel de configuraci\u00f3n s\u00f3lo hemos definido la variable de entorno $FLUME_HOME que apunta a /opt/flume-1.9.0 .","title":"Flume"},{"location":"hadoop/05flume.html#arquitectura","text":"Su arquitectura es sencilla, y se basa en el uso de agentes que se dividen en tres componentes los cuales podemos configurar: Source (fuente): Fuente de origen de los datos, ya sea Twitter , Kafka , una petici\u00f3n Http , etc... Las fuentes son un componente activo que recibe datos desde otra aplicaci\u00f3n que produce datos (aunque tambi\u00e9n existen fuentes que pueden producir datos por s\u00ed mismos, cuyo objetivo es poder probar ciertos flujos de datos). Las fuentes puedes escuchar uno o m\u00e1s puertos de red para recibir o leer datos del sistema de arhcivos. Cada fuente debe conectar a al menos un canal. Una fuente puede escribir en varios canales, replicando los eventos a todos o algunos canales en base a alg\u00fan criterio. Channel (canal): la v\u00eda por donde se tratar\u00e1n los datos. Un canal es un componente pasivo que almacena los datos como un buffer. Se comportan como colas, donde las fuentes publican y los sumideros consumen los datos. M\u00faltiples fuentes pueden escribir de forma segura en el mismo canal, y m\u00faltiples sumideros pueden leer desde el mismo canal. Sin embargo, cada sumidero s\u00f3lo puede leer de un \u00fanico canal. Si m\u00faltiples sumideros leen del mismo canal, s\u00f3lo uno de ellos leer\u00e1 el dato. Sink (sumidero): persistencia/movimiento de los datos, a ficheros / base de datos. Toma eventos del canal de manera continua leyendo y eliminando los eventos. A continuaci\u00f3n, los transmite hacia el siguiente componente, ya sea a HDFS, Hive, etc... Una vez los datos han llegado al siguiente destino, el sumidero informa al canal mediante un commit transaccional para que elimine dichos eventos del canal. Arquitectura Flume - imagen extra\u00edda de https://www.diegocalvo.es/flume/ Es muy recomendable acceder a la gu\u00eda de usuario oficial para consultar todas las fuentes de datos, canales y sumideros disponibles en la actualidad. A continuaci\u00f3n se nombran algunos de los m\u00e1s destacados: Sources Channels Sinks Avro Source Memory Channel HDFS Sink Thrift Source JDBC Channel Hive Sink Exec Source Kafka Channel Logger Sink JMS Source File Channel Avro Sink Spooling Directory Source Spillable Memory Channel Thrift Sink Twitter 1% firehose Source Pseudo Transaction Channel Kafka Sink Kafka Source File Roll Sink NetCat Source Null Sink Sequence Generator Source HBaseSink Syslog Sources AsyncHBaseSink HTTP Source MorphlineSolrSink Multiport Syslog TCP Source ElasticSearchSink Syslog UDP Source Kite Dataset Sink Flume se complica cuando queremos utilizarlo para obtener datos de manera paralela (o multiplexada) y/o necesitamos crear nuestros propios sumideros o interceptores. Pero por lo general, su uso es sencillo y se trata de una herramienta muy recomendada como ayuda/alternativa a herramientas como Pentaho . Algunas de sus caracter\u00edsticas son: Dise\u00f1o flexible basado en flujos de datos de transmisi\u00f3n. Resistente a fallos y robusto con m\u00faltiples conmutaciones por error y mecanismos de recuperaci\u00f3n. Lleva datos desde origen a destino: incluidos HDFS y HBase .","title":"Arquitectura"},{"location":"hadoop/05flume.html#probando-flume","text":"Por ejemplo, vamos a crear un agente el cual llamaremos ExecLoggerAgent el cual va a ejecutar un comando y mostrar\u00e1 el resultado por el log de Flume . Para ello, creamos la configuraci\u00f3n del agente en el fichero agente.conf (todas las propiedades comenzar\u00e1n con el nombre del agente): agente.conf # Nombramos los componentes del agente ExecLoggerAgent.sources = Exec ExecLoggerAgent.channels = MemChannel ExecLoggerAgent.sinks = LoggerSink # Describimos el tipo de origen ExecLoggerAgent.sources.Exec.type = exec ExecLoggerAgent.sources.Exec.command = ls /home/iabd/ # Describimos el destino ExecLoggerAgent.sinks.LoggerSink.type = logger # Describimos la configuraci\u00f3n del canal ExecLoggerAgent.channels.MemChannel.type = memory ExecLoggerAgent.channels.MemChannel.capacity = 1000 ExecLoggerAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal ExecLoggerAgent.sources.Exec.channels = MemChannel ExecLoggerAgent.sinks.LoggerSink.channel = MemChannel Antes de lanzar el agente Flume, recuerda que debes arrancar tanto Hadoop como YARN , por ejemplo, mediante el comando start-all.sh . A continuaci\u00f3n ya podemos lanzar Flume con el agente mediante el comando (la opci\u00f3n -n sirve para indicar el nombre del agente, y con -f indicamos el nombre del archivo de configuraci\u00f3n): flume-ng agent -n ExecLoggerAgent -f agente.conf","title":"Probando Flume"},{"location":"hadoop/05flume.html#configurando-un-agente","text":"Si te has fijado en el ejemplo anterior, los ficheros de configuraci\u00f3n de los agentes siguen el mismo formato. Para definir un flujo dentro de un agente, necesitamos enlazar las fuentes y los sumideros con un canal. Para ello, listaremos las fuentes, sumideros y canales del agente, y entonces apuntaremos la fuente y el sumidero a un canal. 1 - N - 1 Una fuente puede indicar m\u00faltiples canales, pero un sumidero s\u00f3lo puede indicar un \u00fanico canal. As\u00ed pues, el formato ser\u00e1 similar al siguiente archivo: # Listamos las fuentes, sumideros y canales <Agent>.sources = <Source> <Agent>.sinks = <Sink> <Agent>.channels = <Channel1> <Channel2> # Configuramos los canales de la fuente <Agent>.sources.<Source>.channels = <Channel1> <Channel2> ... # Configuramos el canal para el sumidero <Agent>.sinks.<Sink>.channel = <Channel1> Adem\u00e1s de definir el flujo, es necesario configurar las propiedades de cada fuente, sumidero y canal. Para elo se sigue la misma nomenclatura donde fijamos el tipo de componente (mediante la propiedad type ) y el resto de propiedades espec\u00edficas de cada componente: # Propiedades de las fuentes <Agent>.sources.<Source>.<someProperty> = <someValue> # Propiedades de los canales <Agent>.channel.<Channel>.<someProperty> = <someValue> # Propiedades de los sumideros <Agent>.sources.<Sink>.<someProperty> = <someValue> Para cada tipo de fuente , canal y sumidero es recomendable revisar la documentaci\u00f3n para validar todas las propiedades disponibles.","title":"Configurando un agente"},{"location":"hadoop/05flume.html#caso-3a-almacenando-en-hdfs","text":"En este caso de uso vamos generar datos de forma secuencial y los vamos a ingestar en HDFS . Una buena pr\u00e1ctica es colocar los archivos de configuraci\u00f3n dentro de $FLUME_HOME/conf . As\u00ed pues, vamos a crear el agente SeqGenAgent y almacenar la configuraci\u00f3n en el fichero seqgen.conf : seqgen.conf # Nombramos a los componentes del agente SeqGenAgent.sources = SeqSource SeqGenAgent.channels = MemChannel SeqGenAgent.sinks = HDFS # Describimos el tipo de origen SeqGenAgent.sources.SeqSource.type = seq # Describimos el destino SeqGenAgent.sinks.HDFS.type = hdfs SeqGenAgent.sinks.HDFS.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/seqgen_data/ SeqGenAgent.sinks.HDFS.hdfs.filePrefix = flume-caso3-seqgen SeqGenAgent.sinks.HDFS.hdfs.rollInterval = 0 SeqGenAgent.sinks.HDFS.hdfs.rollCount = 1000 SeqGenAgent.sinks.HDFS.hdfs.fileType = DataStream # Describimos la configuraci\u00f3n del canal SeqGenAgent.channels.MemChannel.type = memory SeqGenAgent.channels.MemChannel.capacity = 1000 SeqGenAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal SeqGenAgent.sources.SeqSource.channels = MemChannel SeqGenAgent.sinks.HDFS.channel = MemChannel Ejecutamos el siguiente comando desde $FLUME_HOME y a los pocos segundo lo paramos mediante CTRL + C para que detenga la generaci\u00f3n de n\u00fameros, ya que si no seguir\u00e1 generando archivos en HDFS: ./bin/flume-ng agent --conf ./conf/ --conf-file conf/seqgen.conf \\ --name SeqGenAgent \\ -Dflume.root.logger = INFO,console Vaciando HDFS Si queremos eliminar los ficheros generados en HDFS, recuerda que puedes realizar un borrado recursivo mediante el comando: hdfs dfs -rm -r /user/iabd/flume Si comprobamos por ejemplo el contenido de la carpeta ( hdfs dfs -ls /user/iabd/flume/seqgen_data ) veremos que se han generado m\u00faltiples archivos: Found 10 items -rw-r--r-- 1 iabd supergroup 1402 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 -rw-r--r-- 1 iabd supergroup 1368 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740934 -rw-r--r-- 1 iabd supergroup 1350 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740935 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740936 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740937 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740938 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740939 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740940 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740941 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740942 Y si comprobamos el contenido del primero ( hdfs dfs -cat /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 ) veremos como contiene la secuencia generada: 0 1 2 3 ...","title":"Caso 3a - Almacenando en HDFS"},{"location":"hadoop/05flume.html#caso-3b-de-netcat-a-hdfs","text":"Ahora vamos a crear otro ejemplo de generaci\u00f3n de informaci\u00f3n, pero esta vez, en vez que utilizar la memoria del servidor como canal, vamos a utilizar el sistema de archivos. Adem\u00e1s, para generar la informaci\u00f3n nos basamos en una fuente Netcat , en la cual debemos especificar un puerto de escucha. Mediante esta fuente, Flume quedar\u00e1 a la escucha en dicho puerto y recibir\u00e1 cada l\u00ednea introducida como un evento individual que transferir\u00e1 al canal especificado. En el mismo directorio $FLUME_HOME\\conf , creamos un nuevo fichero con el nombre netcat.conf y creamos otro agente que se va a encargar de generar informaci\u00f3n: netcat.conf # Nombramos a los componentes del agente NetcatAgent.sources = Netcat NetcatAgent.channels = FileChannel NetcatAgent.sinks = HdfsSink # Describimos el origen netcat en localhost:44444 NetcatAgent.sources.Netcat.type = netcat NetcatAgent.sources.Netcat.bind = localhost NetcatAgent.sources.Netcat.port = 44444 NetcatAgent.sources.Netcat.channels = FileChannel # Describimos el destino en HDFS NetcatAgent.sinks.HdfsSink.type = hdfs NetcatAgent.sinks.HdfsSink.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/net_data/ NetcatAgent.sinks.HdfsSink.hdfs.writeFormat = Text NetcatAgent.sinks.HdfsSink.hdfs.fileType = DataStream NetcatAgent.sinks.HdfsSink.channel = FileChannel # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAgent.channels.FileChannel.type = file NetcatAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint Lanzamos al agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat.conf \\ --name NetcatAgent \\ -Dflume.root.logger = INFO,console En una nueva pesta\u00f1a introducimos el siguiente comando y escribimos curl telnet://localhost:44444 Una vez conectados, escribimos varias frases con saltos de l\u00ednea. Por cada vez que pulsamos Enter , nos aparecer\u00e1 un OK . Probando Netcat OK Esto parece que funciona m\u00e1s o menos OK A continuaci\u00f3n, nos vamos al navegador web de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/flume/net_data ) y comprobamos que se ha creado el fichero: Resultado del flujo Netcat-HDFS","title":"Caso 3b - De Netcat a HDFS"},{"location":"hadoop/05flume.html#caso-4-flujos-encadenados","text":"Es muy com\u00fan definir un pipeline de flujos encadenados, uniendo la salida de un agente a la entrada de otro. Para ello, utilizaremos como enlace un sink - source de tipo Avro . Este dise\u00f1o tambi\u00e9n se conoce como flujo Multi-hop : Encadenando flujos En este caso, vamos a crear un primer agente ( NetcatAvroAgent ) que ingeste datos desde Netcat y los coloque en un sink de tipo Avro . Para ello, creamos el agente netcat-avro.conf : netcat-avro.conf # Nombramos a los componentes del agente NetcatAvroAgent.sources = Netcat NetcatAvroAgent.channels = FileChannel NetcatAvroAgent.sinks = AvroSink # Describimos el origen netcat en localhost:44444 NetcatAvroAgent.sources.Netcat.type = netcat NetcatAvroAgent.sources.Netcat.bind = localhost NetcatAvroAgent.sources.Netcat.port = 44444 # Describimos el destino como Avro en localhost:10003 NetcatAvroAgent.sinks.AvroSink.type = avro NetcatAvroAgent.sinks.AvroSink.hostname = localhost NetcatAvroAgent.sinks.AvroSink.port = 10003 # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAvroAgent.sources.Netcat.channels = FileChannel NetcatAvroAgent.sinks.AvroSink.channel = FileChannel NetcatAvroAgent.channels.FileChannel.type = file NetcatAvroAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAvroAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint A continuaci\u00f3n, creamos un segundo agente ( AvroHdfsAgent ) que utilice como fuente Avro y que almacene los eventos recibidos en HDFS. Para ello, creamos el agente avro-hdfs.conf : avro-hdfs.conf # Nombramos a los componentes del agente AvroHdfsAgent.sources = AvroSource AvroHdfsAgent.channels = MemChannel AvroHdfsAgent.sinks = HdfsSink # Describimos el origen como Avro en localhost:10003 AvroHdfsAgent.sources.AvroSource.type = avro AvroHdfsAgent.sources.AvroSource.bind = localhost AvroHdfsAgent.sources.AvroSource.port = 10003 # Describimos el destino HDFS AvroHdfsAgent.sinks.HdfsSink.type = hdfs AvroHdfsAgent.sinks.HdfsSink.hdfs.path = /user/iabd/flume/avro_data/ AvroHdfsAgent.sinks.HdfsSink.hdfs.fileType = DataStream AvroHdfsAgent.sinks.HdfsSink.hdfs.writeFormat = Text # Unimos el origen y el destino AvroHdfsAgent.sources.AvroSource.channels = MemChannel AvroHdfsAgent.sinks.HdfsSink.channel = MemChannel AvroHdfsAgent.channels.MemChannel.type = memory Primero lanzamos este \u00faltimo agente, para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en nueva pesta\u00f1a, lanzamos el primer agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat-avro.conf \\ --name NetcatAvroAgent \\ -Dflume.root.logger = INFO,console Finalmente, en otro terminal, escribimos mensajes Netcat accediendo a curl telnet://localhost:44444 . Si acced\u00e9is a la carpeta /user/iabd/flume/avro_data en HDFS podremos comprobar c\u00f3mo se van creando archivos que agrupan los mensajes enviados.","title":"Caso 4 - Flujos encadenados"},{"location":"hadoop/05flume.html#caso-5-flujo-multi-agente","text":"Para demostrar como varios agentes pueden conectarse entre s\u00ed, vamos a realizar un caso de uso donde vamos a leer informaci\u00f3n de tres fuentes distintas: una fuente de Netcat con un canal basado en ficheros, otra que realice spooling de una carpeta (vigile una carpeta y cuando haya alg\u00fan archivo, lo ingeste y lo elimine) utilizando un canal en memoria y un tercero que ejecute un comando utilizando tambi\u00e9n un canal en memoria. Como agente de consolidaci\u00f3n que una la informaci\u00f3n de las tres fuentes de datos, vamos a reutilizar el agente AvroHdfsAgent que hemos creado en el caso de uso anterior. Consolidando flujos Para ello, vamos a definir los agentes en siguiente fichero de configuraci\u00f3n multiagent-avro.conf ): multiagent-avro.conf # Nombramos las tres fuentes con sus tres sumideros MultiAgent.sources = Netcat Spooldir Exec MultiAgent.channels = FileChannel MemChannel1 MemChannel2 MultiAgent.sinks = AvroSink1 AvroSink2 AvroSink3 # Describimos el primer agente MultiAgent.sources.Netcat.type = netcat MultiAgent.sources.Netcat.bind = localhost MultiAgent.sources.Netcat.port = 10004 # Describimos el segundo agente MultiAgent.sources.Spooldir.type = spooldir MultiAgent.sources.Spooldir.spoolDir = /home/iabd/flume/spoolDir MultiAgent.sources.Spooldir.deletePolicy = immediate # Describimos el tercer agente MultiAgent.sources.Exec.type = exec MultiAgent.sources.Exec.command = cat /home/iabd/datos/empleados.txt # Describimos los tres destinos como Avro en localhost:10003 MultiAgent.sinks.AvroSink1.type = avro MultiAgent.sinks.AvroSink1.hostname = localhost MultiAgent.sinks.AvroSink1.port = 10003 MultiAgent.sinks.AvroSink2.type = avro MultiAgent.sinks.AvroSink2.hostname = localhost MultiAgent.sinks.AvroSink2.port = 10003 MultiAgent.sinks.AvroSink3.type = avro MultiAgent.sinks.AvroSink3.hostname = localhost MultiAgent.sinks.AvroSink3.port = 10003 # Describimos los canales MultiAgent.channels.FileChannel.type = file MultiAgent.channels.FileChannel.dataDir = /home/iabd/flume/data MultiAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint MultiAgent.channels.MemChannel1.type = memory MultiAgent.channels.MemChannel2.type = memory # Unimos los or\u00edgenes y destinos MultiAgent.sources.Netcat.channels = FileChannel MultiAgent.sources.Spooldir.channels = MemChannel1 MultiAgent.sources.Exec.channels = MemChannel2 MultiAgent.sinks.AvroSink1.channel = FileChannel MultiAgent.sinks.AvroSink2.channel = MemChannel1 MultiAgent.sinks.AvroSink3.channel = MemChannel2 Preparaci\u00f3n Antes de arrancar los agentes, aseg\u00farate de tener creada la carpeta /home/iabd/flume/spoolDir y disponible el recurso /home/iabd/datos/empleados.txt . Igual que en el caso de uso anterior, primero lanzamos el agente consolidador para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en una nueva pesta\u00f1a, lanzamos el multi agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/multiagent-avro.conf \\ --name MultiAgent \\ -Dflume.root.logger = INFO,console Interceptores Podemos utilizar interceptores para modificar o borrar eventos al vuelo a partir del timestamp , nombre del host , uuid , etc... incluso mediante el uso de una expresi\u00f3n regular. Si quieres profundizar en el tema, el siguiente art\u00edculo detalla los diferentes tipos y configuraciones: https://data-flair.training/blogs/flume-interceptors/ En este caso, para poder probarlo, adem\u00e1s de enviar comandos Netstat en curl telnet://localhost:10004 , prueba a colocar un archivo de texto (por ejemplo, un documento CSV) en /home/iabd/flume/spoolDir .","title":"Caso 5 - Flujo multi-agente"},{"location":"hadoop/05flume.html#actividades","text":"Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Haciendo uso de Sqoop y la base de datos retail_db , importa todos los pedidos de la tabla orders cuyo campo order_status sea COMPLETE . Coloca los datos en user/iabd/sqoop/orders/datos_parquet en formato Parquet, utilizando el tabulador como delimitador de campos y utilizando la compresi\u00f3n Snappy. Deber\u00e1s recuperar 22.899 (\u00bfo 22.902?) registros. Haciendo uso de Sqoop y la base de datos retail_db , importa todos los clientes de la tabla customers cuyo campo state sea CA . Coloca los datos en user/iabd/sqoop/customers/datos_avro en formato Avro, utilizando la compresi\u00f3n bzip2. Deber\u00e1s recuperar las columnas customer_id, customer_fname, customer_lname, customer_state . El resultado contendr\u00e1 2012 registros. Mediante Flume , realiza los caso de uso 3, 4 y 5. FIXME: Revisar claves de Twitter (opcional) Haciendo uso de Flume, recupera informaci\u00f3n de Twitter y almac\u00e9nala en HDFS. Para ello, utiliza el Twitter 1% Firehouse source y el HDFS sink . Para ello, necesitar\u00e9is las claves de desarrollo que ya creamos en las sesiones sobre Nifi. Adjunta una captura de pantalla donde se visualice el contenido de uno de los bloques de HDFS. Cuidado con el espacio de almacenamiento Una vez lances el agente, detenlo a los tres segundos para no llenar de datos HDFS.","title":"Actividades"},{"location":"hadoop/05flume.html#referencias","text":"P\u00e1gina oficial de Sqoop Sqoop User Guide Sqoop Tutorial en Tutorialspoint P\u00e1gina oficial de Flume Flume User Guide","title":"Referencias"},{"location":"hadoop/06hive.html","text":"Hive \u00b6 Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un cl\u00faster de Hadoop . Permite leer, escribir y gestionar grandes datasets (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop . Logo de Apache Hive El proyecto lo inici\u00f3 Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os. Hive y Hadoop \u00b6 Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop , Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduce u otras tecnolog\u00edas. Aunque en principio estaba dise\u00f1ado para el procesamiento batch , ahora se integra con frameworks en streaming como Tez y Spark . Ecosistema Hadoop Caracter\u00edsticas \u00b6 Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema , y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase . Para interactuar con Hive utilizaremos HiveQL , el cual es un dialecto de SQL (recuerda que SQL no es sensible a las may\u00fasculas, excepto en la comparaci\u00f3n de cadenas). Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n podemos personalizar el procesamiento de consultas creando un esquema de tabla acorde con nuestros datos, pero sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON o cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive se ejecuta en varios datanodes en paralelo, con varios trabajos MapReduce asociados. Estas operaciones son de tipo batch , por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets. Ventajas \u00b6 Las ventajas de utilizar Hive son: Reduce la complejidad de la programaci\u00f3n MapReduce al usar HiveQL como lenguaje de consulta. Est\u00e1 orientado a aplicaciones de tipo Data Warehouse , con datos est\u00e1ticos, poco cambiantes y sin requisitos de tiempos de respuesta r\u00e1pidos. Permite a los usuarios despreocuparse de en qu\u00e9 formato y d\u00f3nde se almacenan los datos. Incorpora Beeline : una herramienta por l\u00ednea de comandos para realizar consultas con HiveQL . En cambio, Hive no es la mejor opci\u00f3n para consultas en tiempo real o de tipo transaccional. Adem\u00e1s, no est\u00e1 dise\u00f1ado para usarse con actualizaciones de valores al nivel de registro, y el soporte de SQL es limitado. Alternativas \u00b6 Una de las alternativas m\u00e1s populares es Cloudera Impala , el cual utiliza un demonio dedicado en cada datanode del cl\u00faster, de manera que hay un coordinador que reenv\u00eda a cada datanode la consulta a realizar y luego se encarga de unir los datos en el resultado final. Impala utiliza el metastore de Hive y soporta la mayor\u00eda de construcciones de Hive , con lo que la migraci\u00f3n de un sistema a otro es sencilla. Otras alternativas open source son : Presto de Facebook y Apache Drill , con arquitecturas muy similares a Impala . Spark SQL : utiliza Spark como motor de ejecuci\u00f3n y permite utilizar consultas SQL embebidas. La estudiaremos en el \u00faltimo bloque del curso. Pig Apache Pig es una herramienta que abstrae el acceso a MapReduce de forma similar a como lo realiza Hive , pero en vez de SQL, utiliza su propio lenguaje de scripting ( PigLatin ) para expresar los flujos de datos. Actualmente ha perdido uso en detrimento de Hive / Impala y de Spark . Ten\u00e9is una peque\u00f1a introducci\u00f3n en https://www.analyticsvidhya.com/blog/2021/08/an-introduction-to-apache-pig-for-absolute-beginners/ . Componentes \u00b6 A continuaci\u00f3n podemos ver un gr\u00e1fico que relaciona los diferentes componentes de Hive y define su arquitectura: Arquitectura de Apache Hive Hive Server \u00b6 HiveServer 2 (HS2) es la \u00faltima versi\u00f3n del servicio. Se compone de una interfaz que permite a clientes externos ejecutar consultas contra Apache Hive y obtener los resultados. Est\u00e1 basado en Thrift RPC y soporta clientes concurrentes. Para arrancar el servidor, ejecutaremos el comando hiveserver2 . A este servidor nos conectaremos mediante la herramienta Beeline (Beeline CLI) con el comando beeline . Hive Metastore \u00b6 Es el repositorio central para los metatados de Hive , y se almacena en una base de datos relacional como MySQL , PostgreSQL o Apache Derby (embebida). Mantiene los metadatos, las tablas y sus tipos mediante Hive DDL ( Data Definition Language ). Adem\u00e1s, el sistema se puede configurar para que tambi\u00e9n almacene estad\u00edsticas de las operaciones y registros de autorizaci\u00f3n para optimizar las consultas. En las \u00faltimas versiones de Hive , este componente se puede desplegar de forma remota e independiente, para no compartir la misma JVM con HiveServer . Dentro del metastore podemos encontrar el Hive Catalog ( HCatalog ), que permite acceder a sus metadatos, actuando como una API. Al poder desplegarse de forma aislada e independiente, permite que otras aplicaciones hagan uso del schema sin tener que desplegar el motor de consultas de Hive. As\u00ed pues, al Metastore podremos acceder mediante HiveCLI , o a trav\u00e9s del Hive Server mediante una conexi\u00f3n remota mediante Beeline . Beeline \u00b6 Hive incorpora Beeline , el cual act\u00faa como un cliente basado en JDBC para hacer consultas por l\u00ednea de comandos contra el Hive Server , sin necesitar las dependencias de Hive . Por otro lado, tambi\u00e9n podemos utilizar Hive CLI , un cliente basado en Apache Thrift , que usa los mismos drivers que Hive . Apache Tez Hive 3 deja de soportar MapReduce . Apache Tez lo reemplaza como el motor de ejecuci\u00f3n por defecto, de manera que mejora el rendimiento y se ejecuta sobre Hadoop Yarn , que encola y planifica los trabajos en el cl\u00faster. Adem\u00e1s de Tez , Hive tambi\u00e9n puede utilizar Apache Spark como motor de ejecuci\u00f3n. Para indicar que queremos ejecutar Tez como motor de ejecuci\u00f3n, ejecutar\u00edamos el siguiente comando: SET hive.execution.engine=tez; En nuestro caso no tenemos Tez instalado en la m\u00e1quina virtual, quedando fuera del alcance del presente curso. Tipos de datos \u00b6 Los tipos de datos que podemos emplear en Hive son muy similares a los que se utilizan en el DDL de SQL. Los tipos simples m\u00e1s comunes son STRING e INT , aunque podemos utilizar otros tipos como TINYINT , BIGINT , DOUBLE , DATE , TIMESTAMP , etc... Para realizar una conversi\u00f3n explicita de tipos, por ejemplo de un tipo texto a uno num\u00e9rico, hay que utilizar la funci\u00f3n CAST : select CAST ( '1' as INT ) from tablaPruebas ; Respecto a los tipos compuestos, tenemos tres tipos: arrays mediante el tipo ARRAY , para agrupar elementos del mismo tipo: [\"manzana\", \"pera\", \"naranja] . mapas mediante el tipo MAP , para definir parejas de clave-valor: {1: \"manzana\", 2: \"pera\"} estructuras mediante el tipo STRUCT , para definir estructuras con propiedades: {\"fruta\": \"manzana\", \"cantidad\": 1, \"tipo\": \"postre\"} . Instalaci\u00f3n y configuraci\u00f3n \u00b6 M\u00e1quina virtual Los siguientes pasos no son necesarios ya que nuestra m\u00e1quina virtual ya tiene Hive instalado y configurado correctamente. Si quieres hacer tu propia instalaci\u00f3n sigue los siguientes pasos de la documentaci\u00f3n oficial. Una vez instalado, vamos a configurarlo. Para ello, debemos crear los ficheros de configuraci\u00f3n a partir de las plantilla que ofrece Hive . Para ello, desde la carpeta $HIVE_HOME/conf , ejecutaremos los siguientes comandos: cp hive-default.xml.template hive-site.xml cp hive-env.sh.template hive-env.sh cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties cp hive-log4j2.properties.template hive-log4j2.properties Modificamos el fichero hive.env.sh para incluir dos variables de entorno con las rutas de Hadoop y la configuraci\u00f3n de Hive hive.env.sh export HADOOP_HOME = /opt/hadoop-3.3.1 export HIVE_CONF_DIR = /opt/hive-3.1.2/conf Para que funcione la ingesta de datos en Hive mediante Sqoop , necesitamos a\u00f1adir una librer\u00eda a Sqoop : cp $HIVE_HOME /lib/hive-common-3.1.2.jar $SQOOP_HOME /lib Preparamos HDFS para crear la estructura de archivos: hdfs dfs -mkdir /tmp hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /tmp hdfs dfs -chmod g+w /user/hive/warehouse Para el metastore , como en nuestra m\u00e1quina virtual tenemos un servidor de MariaDB corriendo, vamos a reutilizarlo. La mayor\u00eda de ejemplos que hay en internet y la diferente bibliograf\u00eda, utilizan DerbyDB como almac\u00e9n (ya que no requiere una instalaci\u00f3n extra). As\u00ed pues, creamos el almac\u00e9n mediante: schematool -dbType mysql -initSchema Modificamos el fichero de configuraci\u00f3n hive-site.xml y configuramos : hive-site.xml <!-- nuevas propiedades --> <property> <name> system:java.io.tmpdir </name> <value> /tmp/hive/java </value> </property> <property> <name> system:user.name </name> <value> ${user.name} </value> </property> <property> <name> datanucleus.schema.autoCreateAll </name> <value> true </value> </property> <!-- propiedades existentes a modificar --> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> com.mysql.jdbc.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> iabd </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> iabd </value> </property> Hola Mundo \u00b6 Si entramos a nuestro $HIVE_HOME podemos comprobar con tenemos las siguientes herramientas: hive : Herramienta cliente beeline : Otra herramienta cliente hiserver2 : Nos permite arrancar el servidor de Hive schematool : Nos permite trabajar contra la base de datos de metadatos (Metastore) Una vez arrancado Hadoop y YARN , vamos a arrancar Hive mediante el cliente local: hive Y una vez dentro, podemos comprobar las bases de datos existentes para ver que todo se configur\u00f3 correctamente show databases ; Si quisi\u00e9ramos ejecutar un script, podemos hacerlo desde el propio comando hive con la opci\u00f3n -f : hive -f script.sql Adem\u00e1s, tenemos la opci\u00f3n de pasar una consulta desde la propia l\u00ednea de comandos mediante la opci\u00f3n -e : hive -e ' select * from tablaEjemplo ` Acceso remoto \u00b6 HiveServer2 (desde Hive 0.11) tiene su propio cliente conocido como Beeline . En entornos reales, el cliente Hive est\u00e1 en desuso a favor de Beeline , por la falta de m\u00faltiples usuarios, seguridad y otras caracter\u00edsticas de HiveServer2. Arrancamos HiveServer2 y Beeline en dos pesta\u00f1as diferentes mediante los comandos hiveserver2 y beeline . Una vez dentro de Beeline , nos conectamos al servidor: !connect jdbc:hive2://iabd-virtualbox:10000 Al conectarnos, tras introducir iabd como usuario y contrase\u00f1a, obtendremos un interfaz similar al siguiente: Beeline version 3.1.2 by Apache Hive beeline> !connect jdbc:hive2://iabd-virtualbox:10000 Connecting to jdbc:hive2://iabd-virtualbox:10000 Enter username for jdbc:hive2://iabd-virtualbox:10000: iabd Enter password for jdbc:hive2://iabd-virtualbox:10000: **** Connected to: Apache Hive (version 3.1.2) Driver: Hive JDBC (version 3.1.2) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://iabd-virtualbox:10000> Dentro de Beeline , en cualquier momento podemos ejecutar el comando help que nos mostrar\u00e1 todos los comandos disponibles. Si nos fijamos, adem\u00e1s de las comandos del cliente hive, tenemos los comandos beeline que empiezan por el s\u00edmbolo de exclamaci\u00f3n ! : 0: jdbc:hive2://iabd-virtualbox:10000> help !addlocaldriverjar Add driver jar file in the beeline client side. !addlocaldrivername Add driver name that needs to be supported in the beeline client side. !all Execute the specified SQL against all the current connections !autocommit Set autocommit mode on or off !batch Start or execute a batch of statements ... Otra forma de trabajar, para arrancar en el mismo proceso Beeline y HiveServer2 para pruebas/desarrollo y tener una experiencia similar al cliente Hive accediendo de forma local, podemos ejecutar el siguiente comando: beeline -u jdbc:hive2:// Mediante la interfaz gr\u00e1fica de Hive Server UI a la cual podemos acceder mediante http://localhost:10002 podemos monitorizar los procesos ejecutados por HiveServer2 : Monitorizaci\u00f3n mediante Hive Server UI Caso de uso 1: Creaci\u00f3n y borrado de tablas \u00b6 Para este caso de uso, vamos a utilizar la base de datos retail_db que ya utilizamos en las actividades de la sesi\u00f3n anterior. Para empezar, vamos a cargar en HDFS los datos de los clientes que contiene la tabla customer . Mediante Sqoop , ejecutamos el siguiente comando: sqoop import --connect \"jdbc:mysql://localhost/retail_db\" \\ --username iabd --password iabd \\ --table customers --target-dir /user/iabd/hive/customer \\ --fields-terminated-by '|' --delete-target-dir \\ --columns \"customer_id,customer_fname,customer_lname,customer_city\" Una vez nos hemos conectado con el cliente hive o mediante beeline , creamos una base de datos llamada iabd : create database iabd ; Nos conectamos a la base de datos que acabamos de crear: use iabd ; default Si olvidamos el comando use , se utilizar\u00e1 la base de datos default , la cual reside en /user/hive/warehouse como ra\u00edz en HDFS. A continuaci\u00f3n, vamos a crear una tabla que almacene el identificador, nombre, apellido y ciudad de los clientes (como puedes observar, la sintaxis es similar a SQL): CREATE TABLE customers ( custId INT , fName STRING , lName STRING , city STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/user/iabd/hive/customer' ; Y ya podemos realizar algunas consultas: select * from customers limit 5 ; select count ( * ) from customers ; En ocasiones necesitamos almacenar la salida de una consulta Hive en una nueva tabla. Las definiciones de las columnas de la nueva tabla se deriva de las columnas recuperadas en la consulta. Para ello, usaremos el comando create table-as select : CREATE TABLE customers_new as SELECT * from customers ; En el caso de la consulta falle por alg\u00fan motivo, la tabla no se crear\u00eda. Otra posibilidad es crear una tabla con la misma estructura que otra ya existente (pero sin datos): CREATE TABLE customers2 LIKE customers ; En cualquier momento podemos obtener informaci\u00f3n de la tabla: describe customers_new ; describe formatted customers_new ; Si empleamos la forma larga, obtendremos mucha m\u00e1s informaci\u00f3n. Por ejemplo, si nos fijamos, vemos que la localizaci\u00f3n de la nueva tabla ya no es /user/iabd/hive/customer sino hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/customers_new . Esto se debe a que en vez de crear una tabla enlazada a un recurso de HDFS ya existente, ha creado una copia de los datos en el propio almac\u00e9n de Hive (hemos pasado de una tabla externa a una interna). Igual que las creamos, las podemos eliminar: drop table customers_new ; drop table customers2 ; Si ejecutamos el comando !tables (o show tables en el cliente hive ) veremos que ya no aparecen dichas tablas. En el caso de que queramos eliminar una base de datos, de la misma manera que en SQL, ejecutar\u00edamos el comando drop database iabd; . Caso de uso 2: Insertando datos \u00b6 Para insertar datos en las tablas de Hive lo podemos hacer de varias formas: Cargando los datos mediante sentencias LOAD DATA . Insertando los datos mediante sentencias INSERT . Cargando los datos directamente mediante Sqoop o alguna herramienta similar. Cargando datos \u00b6 Para cargar datos se utiliza la sentencia LOAD DATA . Si quisi\u00e9ramos volver a cargar los datos desde HDFS utilizaremos: LOAD DATA INPATH '/user/iabd/hive/customer' overwrite into table customers ; Si en cambio vamos a cargar los datos desde un archivo local a nuestro sistema de archivos a\u00f1adiremos LOCAL : LOAD DATA LOCAL INPATH '/home/iabd/datos' overwrite into table customers ; Insertando datos \u00b6 Aunque podemos insertar datos de forma at\u00f3mica (es decir, registro a registro mediante INSERT INTO TABLE ... VALUES ), realmente las inserciones que se realizan en Hive se hacen a partir de los datos de otras tablas mediante el comando insert-select a modo de ETL: INSERT OVERWRITE TABLE destino SELECT col1 , col2 FROM fuente ; Mediante la opci\u00f3n OVERWRITE , en cada ejecuci\u00f3n se vac\u00eda la tabla y se vuelve a rellenar. Si no lo indicamos o utilizamos INTO , los datos se a\u00f1adir\u00edan a los ya existentes. Si necesitamos insertar datos en m\u00faltiples tablas a la vez lo haremos mediante el comando from-insert : FROM fuente INSERT OVERWRITE TABLE destino1 SELECT col1 , col2 INSERT OVERWRITE TABLE destino2 SELECT col1 , col3 Por ejemplo, vamos a crear un par de tablas con la misma estructura de clientes, pero para almacenar los clientes de determinadas ciudades: CREATE TABLE customers_brooklyn LIKE customers ; CREATE TABLE customers_caguas LIKE customers ; Y a continuaci\u00f3n rellenamos ambas tablas con sus clientes; FROM customers INSERT OVERWRITE TABLE customers_brooklyn SELECT custId , fName , lName , city WHERE city = \"Brooklyn\" INSERT OVERWRITE TABLE customers_caguas SELECT custId , fName , lName , city WHERE city = \"Caguas\" ; Ingestando datos \u00b6 Tal como vimos en la sesi\u00f3n anterior , podemos ingestar los datos en Hive haciendo uso de Sqoop (tambi\u00e9n lo podemos hacer con Flume o Nifi ). Por ejemplo, vamos a ingestar los datos de la tabla orders de la base de datos MariaDB que tenemos instalada en nuestra m\u00e1quina virtual. En el comando de Sqoop le indicamos que dentro de Hive lo ingeste en la base de datos iabd y que cree una tabla llamada orders : sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = orders --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table orders Una vez realizada la ingesta, podemos comprobar que los datos est\u00e1n dentro de Hive (en una tabla interna/gestionada): hdfs dfs -ls /user/hive/warehouse/iabd.db/orders Y si entramos a hive , podemos consultar sus datos: select * from orders limit 10 ; Extrayendo datos insertados \u00b6 Combinando los comandos de HQL y HDFS podemos extraer datos a ficheros locales o remotos: # A\u00f1adiendo contenido local hive -e \"use iabd; select * from customers\" >> prueba1 # Sobrescribiendo contenido local hive -e \"use iabd; select * from customers\" > prueba2 # A\u00f1adiendo contenido HDFS hive -e \"use iabd; select * from customers\" | hdfs dfs --appendToFile /tmp/prueba3 # Sobrescribiendo contenido hive -e \"use iabd; select * from customers\" | hdfs dfs --put -f /tmp/prueba4 Si indicamos la propiedad set hive.cli.print.header=true antes de la consulta, tambi\u00e9n nos mostrar\u00e1 el encabezado de las columnas. Esto puede ser \u00fatil si queremos generar un csv con el resultado de una consulta: hive -e 'use iabd; set hive.cli.print.header=true; select * from customers' | \\ sed 's/[\\t]/,/g' > fichero.csv \u00bfY usar INSERT LOCAL ? Mediante INSERT LOCAL podemos escribir el resultado de una consulta en nuestro sistema de archivos, fuera de HDFS. El problema es que si hay muchos datos crear\u00e1 m\u00faltiples ficheros y necesitaremos concatenarlos para tener un \u00fanico resultado: insert overwrite local directory '/home/iabd/datos' row format delimited field terminated by ',' select * from customers ; Caso de uso 3: Consultas con join \u00b6 En este caso de uso vamos a trabajar con los datos de clientes que hemos cargado en los dos casos anteriores, tanto en customers como en orders . Si queremos relacionar los datos de ambas tablas, tenemos que hacer un join entre la clave ajena de orders ( order_customer_id ) y la clave primaria de customers ( custid ): hive > describe customers ; OK custid int fname string lname string city string Time taken : 0 . 426 seconds , Fetched : 4 row ( s ) hive > describe orders ; OK order_id int order_date string order_customer_id int order_status string Time taken : 0 . 276 seconds , Fetched : 4 row ( s ) Para ello, para obtener la ciudad de cada pedido, podemos ejecutar la consulta: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Outer join \u00b6 De la misma manera que en cualquier SGBD, podemos realizar un outer join , tanto left como right o full . Por ejemplo, vamos a obtener para cada cliente, cuantos pedidos ha realizado: select c . custid , count ( order_id ) from customers c join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Si queremos que salgan todos los clientes, independientemente de que tengan pedidos, deberemos realizar un left outer join : select c . custid , count ( order_id ) from customers c left outer join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Semi-joins \u00b6 Si quisi\u00e9ramos obtener las ciudades de los clientes que han realizado pedidos podr\u00edamos realizar la siguiente consulta: select distinct c . city from customers c where c . custid in ( select order_customer_id from orders ); Mediante un semi-join podemos obtener el mismo resultado: select distinct city from customers c left semi join orders o on ( c . custid = o . order_customer_id ) Hay que tener en cuenta la restricci\u00f3n que las columnas de la tabla de la derecha s\u00f3lo pueden aparecer en la clausula on , nunca en la expresi\u00f3n select . Map joins \u00b6 Consideramos la consulta inicial de join: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Si una tabla es suficientemente peque\u00f1a para caber en memoria, tal como nos ocurre con nuestros datos, Hive puede cargarla en memoria para realizar el join en cada uno de los mappers . Esto se conoce como un map join . El job que ejecuta la consulta no tiene reducers , con lo que esta consulta no funcionar\u00e1 para un right o right outer join , ya que la ausencias de coincidencias s\u00f3lo se puede detectar en los pasos de agregaci\u00f3n ( reduce ). En el caso de utilizar map joins con tablas organizadas en buckets , la sintaxis es la misma, s\u00f3lo siendo necesario activarlo mediante la propiedad hive.optimize.bucketmapjoin : SET hive.optimize.bucketmapjoin = true ; Comandos \u00b6 Mediante los casos de uso realizados hasta ahora, hemos podido observar c\u00f3mo para interactuar con Hive se utilizan comandos similares a SQL. Es conveniente consultar la siguiente cheatsheet : http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf Adem\u00e1s Hive viene con un conjunto de funciones predefinidas para tratamiento de cadenas, fechas, funciones estad\u00edsticas, condicionales, etc... las cuales puedes consultar en la documentaci\u00f3n oficial . Mediante el comando show functions podemos obtener una lista de las funciones. Si queremos m\u00e1s informaci\u00f3n sobre una determinada funci\u00f3n utilizaremos el comando describe function nombreFuncion : hive > describe function length ; length ( str | binary ) - Returns the length of str or number of bytes in binary data Caso de uso 4: Tabla interna \u00b6 Hive permite crear tablas de dos tipos: tabla interna o gestionada: Hive gestiona la estructura y el almacenamiento de los datos. Para ello, crea los datos en HDFS. Al borrar la tabla de Hive , se borra la informaci\u00f3n de HDFS. tabla externa : Hive define la estructura de los datos en el metastore , pero los datos ya residen previamente en HDFS. Al borrar la tabla de Hive , no se eliminan los datos de HDFS. Se emplea cuando compartimos datos almacenados en HDFS entre diferentes herramientas. En este caso de uso, vamos a centrarnos en una tabla interna. Supongamos el siguiente fichero con datos de empleados: empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Podemos observar como se utiliza | como separador de campos. Analizando los datos, vemos que tenemos los siguientes campos: Nombre Centros de trabajo (array con las ciudades) Sexo y edad Destreza y puntuaci\u00f3n Departamento y cargo Creamos la siguiente tabla interna en Hive mediante el siguiente comando: CREATE TABLE IF NOT EXISTS empleados_interna ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT 'Esto es una tabla interna' ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' ; La sintaxis es muy similar a SQL, destacando las siguientes opciones: ROW FORMAT DELIMITED : cada registro ocupa una l\u00ednea FIELDS TERMINATED BY '|' : define el | como separador de campos COLLECTION ITEMS TERMINATED BY ',' : define la coma como separador de los arrays / estructuras MAP KEYS TERMINATED BY ':' : define los dos puntos como separador utilizado en los mapas. Si queremos comprobar la estructura de la tabla mediante el comando show create table empleados_interna veremos las opciones que hemos indicado: + ----------------------------------------------------+ | createtab_stmt | + ----------------------------------------------------+ | CREATE TABLE ` empleados_interna ` ( | | ` name ` string , | | ` work_place ` array < string > , | | ` sex_age ` struct < sex : string , age : int > , | | ` skills_score ` map < string , int > , | | ` depart_title ` map < string , array < string >> ) | | COMMENT 'Esto es una tabla interna' | | ROW FORMAT SERDE | | 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' | | WITH SERDEPROPERTIES ( | | 'collection.delim' = ',' , | | 'field.delim' = '|' , | | 'mapkey.delim' = ':' , | | 'serialization.format' = '|' ) | | STORED AS INPUTFORMAT | | 'org.apache.hadoop.mapred.TextInputFormat' | | OUTPUTFORMAT | | 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' | | LOCATION | | 'hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/empleados_interna' | | TBLPROPERTIES ( | | 'bucketing_version' = '2' , | | 'transient_lastDdlTime' = '1647432129' ) | + ----------------------------------------------------+ A continuaci\u00f3n, vamos a cargar los datos del fichero empleados.txt , el cual colocaremos en nuestra carpeta de Descargas : LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_interna ; Comprobamos que los datos se han cargado correctamente: select * from empleados_interna ; Y obtenemos: +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | empleados_interna.name | empleados_interna.work_place | empleados_interna.sex_age | empleados_interna.skills_score | empleados_interna.depart_title | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | Michael | [\"Montreal\",\"Toronto\"] | {\"sex\":\"Male\",\"age\":30} | {\"DB\":80} | {\"Product\":[\"Developer\",\"Lead\"]} | | Will | [\"Montreal\"] | {\"sex\":\"Male\",\"age\":35} | {\"Perl\":85} | {\"Product\":[\"Lead\"],\"Test\":[\"Lead\"]} | | Shelley | [\"New York\"] | {\"sex\":\"Female\",\"age\":27} | {\"Python\":80} | {\"Test\":[\"Lead\"],\"COE\":[\"Architect\"]} | | Lucy | [\"Vancouver\"] | {\"sex\":\"Female\",\"age\":57} | {\"Sales\":89,\"HR\":94} | {\"Sales\":[\"Lead\"]} | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ Y si nos abrimos otra pesta\u00f1a, mediante HDFS, comprobamos que tenemos los datos: hdfs dfs -ls /user/hive/warehouse/curso.db/empleados_interna hdfs dfs -cat /user/hive/warehouse/curso.db/empleados_interna/empleados.txt Y obtenemos: Michael|Montreal,Toronto|Male,30|DB:80|Product:DeveloperLead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Consultando datos compuestos \u00b6 Si nos fijamos bien en la tabla, tenemos tres columnas con diferentes datos compuestos. work_place , con un ARRAY<string> sex_age con una STRUCT<sex:string,age:int> skills_score con un MAP<string,int> , depart_title con un MAP<STRING,ARRAY<STRING>> Si queremos obtener los datos del array , podemos realizar las siguientes consultas: Todos los elementos Elementos individuales Cantidad de elementos Explode array Todos los lugares de trabajo: select name , work_place from empleados_interna ; Resultado: +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [ \"Montreal\" , \"Toronto\" ] | | Will | [ \"Montreal\" ] | | Shelley | [ \"New York\" ] | | Lucy | [ \"Vancouver\" ] | +----------+-------------------------+ Los dos primeros puestos de trabajo: select work_place [ 0 ] as lugar1 , work_place [ 1 ] as lugar2 from empleados_interna ; Resultado: +------------+----------+ | lugar1 | lugar2 | +------------+----------+ | Montreal | Toronto | | Montreal | NULL | | New York | NULL | | Vancouver | NULL | +------------+----------+ Cantidad de lugares de trabajo: select size ( work_place ) as cantLugares from empleados_interna ; Resultado: +--------------+ | cantlugares | +--------------+ | 2 | | 1 | | 1 | | 1 | +--------------+ Empleados y lugares de trabajo mostrados uno por fila: select name , lugar from empleados_interna lateral view explode ( work_place ) e2 as lugar ; Para ello hemos creado una vista lateral y con la funci\u00f3n explode desenrollamos el array. Resultado: +----------+------------+ | name | lugar | +----------+------------+ | Michael | Montreal | | Michael | Toronto | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+------------+ En el caso de la estructura con el sexo y la edad podemos realizar las siguientes consultas Todos los elementos Elementos individuales Todas las estructuras de sexo/edad: select sex_age from empleados_interna ; Resultado: +----------------------------+ | sex_age | +----------------------------+ | { \"sex\" : \"Male\" , \"age\" : 30 } | | { \"sex\" : \"Male\" , \"age\" : 35 } | | { \"sex\" : \"Female\" , \"age\" : 27 } | | { \"sex\" : \"Female\" , \"age\" : 57 } | +----------------------------+ Sexo y edad por separado select sex_age . sex as sexo , sex_age . age as edad from empleados_interna ; Resultado: +---------+-------+ | sexo | edad | +---------+-------+ | Male | 30 | | Male | 35 | | Female | 27 | | Female | 57 | +---------+-------+ Respecto al mapa con las habilidades y sus puntuaciones: Todos los elementos Elementos individuales Claves y valores Todas las habilidades como un mapa: select skills_score from empleados_interna ; Resultado: +-----------------------+ | skills_score | +-----------------------+ | { \"DB\" : 80 } | | { \"Perl\" : 85 } | | { \"Python\" : 80 } | | { \"Sales\" : 89 , \"HR\" : 94 } | +-----------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , skills_score [ \"DB\" ] as db , skills_score [ \"Perl\" ] as perl , skills_score [ \"Python\" ] as python , skills_score [ \"Sales\" ] as ventas , skills_score [ \"HR\" ] as hr from empleados_interna ; Resultado: +----------+-------+-------+---------+---------+-------+ | name | db | perl | python | ventas | hr | +----------+-------+-------+---------+---------+-------+ | Michael | 80 | NULL | NULL | NULL | NULL | | Will | NULL | 85 | NULL | NULL | NULL | | Shelley | NULL | NULL | 80 | NULL | NULL | | Lucy | NULL | NULL | NULL | 89 | 94 | +----------+-------+-------+---------+---------+-------+ Claves y valores de las habilidades: select name , map_keys ( skills_score ) as claves , map_values ( skills_score ) as valores from empleados_interna ; Resultado: +----------+-----------------+----------+ | name | claves | valores | +----------+-----------------+----------+ | Michael | [ \"DB\" ] | [ 80 ] | | Will | [ \"Perl\" ] | [ 85 ] | | Shelley | [ \"Python\" ] | [ 80 ] | | Lucy | [ \"Sales\" , \"HR\" ] | [ 89 , 94 ] | +----------+-----------------+----------+ Y finalmente, con el mapa de departamentos que contiene un array: Todos los elementos Elementos individuales Primer elemento de los elementos individuales Toda la informaci\u00f3n sobre los departamentos: select depart_title from empleados_interna ; Resultado: +----------------------------------------+ | depart_title | +----------------------------------------+ | { \"Product\" : [ \"Developer\" , \"Lead\" ]} | | { \"Product\" : [ \"Lead\" ], \"Test\" : [ \"Lead\" ]} | | { \"Test\" : [ \"Lead\" ], \"COE\" : [ \"Architect\" ]} | | { \"Sales\" : [ \"Lead\" ]} | +----------------------------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , depart_title [ \"Product\" ] as product , depart_title [ \"Test\" ] as test , depart_title [ \"COE\" ] as coe , depart_title [ \"Sales\" ] as sales from empleados_interna ; Resultado: +----------+-----------------------+-----------+----------------+-----------+ | name | product | test | coe | sales | +----------+-----------------------+-----------+----------------+-----------+ | Michael | [ \"Developer\" , \"Lead\" ] | NULL | NULL | NULL | | Will | [ \"Lead\" ] | [ \"Lead\" ] | NULL | NULL | | Shelley | NULL | [ \"Lead\" ] | [ \"Architect\" ] | NULL | | Lucy | NULL | NULL | NULL | [ \"Lead\" ] | +----------+-----------------------+-----------+----------------+-----------+ Primera habilidad de producto y pruebas de cada empleado: select name , depart_title [ \"Product\" ][ 0 ] as product0 , depart_title [ \"Test\" ][ 0 ] as test0 from empleados_interna ; Resultado: +----------+-------------+--------+ | name | product0 | test0 | +----------+-------------+--------+ | Michael | Developer | NULL | | Will | Lead | Lead | | Shelley | NULL | Lead | | Lucy | NULL | NULL | +----------+-------------+--------+ Caso de uso 5: Tabla externa \u00b6 En este caso de uso vamos a repetir la misma estructura de la tabla del caso anterior, pero en esta ocasi\u00f3n en una tabla externa. De esta manera, al borrar la tabla de Hive , no se borra la informaci\u00f3n de HDFS. Para ello, \u00fanicamente hemos de a\u00f1adir la palabra EXTERNAL a la instrucci\u00f3n CREATE TABLE y la clausula LOCATION para indicar la ruta de HDFS donde se encuentran los datos: CREATE EXTERNAL TABLE IF NOT EXISTS empleados_externa ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT \"Esto es una tabla externa\" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LOCATION \"/user/iabd/hive/empleados_externa\" ; Realizamos la misma carga que en el caso anterior: LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_externa ; Si hacemos una consulta sobre la tabla para ver que est\u00e1n todos los campos obtendremos la misma informaci\u00f3n que antes: SELECT * FROM empleados_externa ; Interna o externa Como normal general, si todo nuestro procesamiento lo hacemos mediante Hive , es m\u00e1s c\u00f3modo utilizar tablas internas. Si no es as\u00ed, y otras herramientas acceden al mismo dataset, es mejor utilizar tablas externas. Un patr\u00f3n de uso muy com\u00fan es utilizar una tabla externa para acceder al dataset inicial almacenado en HDFS (creado por otro proceso), y posteriormente crear una transformaci\u00f3n en Hive para mover los datos a una tabla interna. Estructuras de datos en Hive \u00b6 Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: tablas, particiones y buckets . Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Acabamos de ver en el apartado anterior que Hive permite crear tablas externas, similares a las tablas en una base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS. Particiones \u00b6 Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento cuando utilizamos consultas que filtran los datos mediante la cl\u00e1usula where . Por ejemplo, si estamos almacenado ficheros de log (tanto la l\u00ednea del log como su timestamp ), podemos pensar en agrupar por fecha los diferentes ficheros. Podr\u00edamos a\u00f1adir otra partici\u00f3n para tambi\u00e9n dividirlos por pa\u00edses: CREATE TABLE logs ( ts BIGINT , linea STRING ) PARTITIONED BY ( fecha STRING , pais STRING ); Por ejemplo, para cargar los datos en una partici\u00f3n: LOAD DATA LOCAL INPATH 'input/hive/particiones/log1' INTO TABLE logs PARTITION ( fecha = '2022-01-01' , pais = 'ES' ); A nivel del sistema de fichero, las particiones se traducen en subdirectorios dentro de la carpeta de la tabla. Por ejemplo, tras insertar varios ficheros de logs, podr\u00edamos tener una estructura similar a: /user/hive/warehouse/logs \u251c\u2500\u2500 fecha=2022-01-01/ \u2502 \u251c\u2500\u2500 pais=ES/ \u2502 \u2502 \u251c\u2500\u2500 log1 \u2502 \u2502 \u2514\u2500\u2500 log2 \u2502 \u2514\u2500\u2500 pais=US/ \u2502 \u2514\u2500\u2500 log3 \u2514\u2500\u2500 fecha=2022-01-02/ \u251c\u2500\u2500 pais=ES/ \u2502 \u2514\u2500\u2500 log4 \u2514\u2500\u2500 pais=US/ \u251c\u2500\u2500 log5 \u2514\u2500\u2500 log6 Para averiguar las particiones en Hive , utilizaremos el comando SHOW PARTITIONS : hive> SHOW PARTITIONS logs; fecha=2022-01-01/pais=ES fecha=2022-01-01/pais=US fecha=2022-01-02/pais=ES fecha=2022-01-02/pais=US Hay que tener en cuenta que la definici\u00f3n de columnas de la clausula PARTITIONED BY forman parte de las columnas de la tabla, y se conocen como columnas de partici\u00f3n . Sin embargo, los ficheros de datos no contienen valores para esas columnas, ya que se deriva el nombre del subdirectorio. Podemos utilizar las columnas de partici\u00f3n en las consultas igual que una columna ordinaria. Hive traduce la consulta en la navegaci\u00f3n adecuada para s\u00f3lo escanear las particiones relevantes. Por ejemplo, la siguiente consulta solo escanear\u00e1 los ficheros log1 , log2 y log4 : SELECT ts , fecha , linea FROM logs WHERE pais = 'ES' ; Moviendo datos a una tabla particionada Si queremos mover datos de una tabla ordinaria a una particionada (vaciando los datos de la partici\u00f3n existente): INSERT OVERWRITE TABLE logs PARTITION ( dt = '2022-01-01' ) SELECT col1 , col2 FROM fuente ; Otra posibilidad es utilizar un particionado din\u00e1mico , de manera que las particiones se crean de forma relativa a los datos: INSERT OVERWRITE TABLE logs PARTITION ( dt ) SELECT col1 , col2 FROM fuente ; Para ello, previamente hay que habilitarlo (por defecto est\u00e1 deshabilitado para evitar la creaci\u00f3n de m\u00faltiples particiones sin querer) y configurar el modo no estricto para que no nos obligue a indicar al menos una partici\u00f3n est\u00e1tica: set hive.exec.dynamic.partition = true set hive.exec.dynamic.partition.mode = nonstrict ; Buckets \u00b6 Otro concepto importante en Hive son los buckets . Son particiones hasheadas por una columna/clave, en las que los datos se distribuyen en funci\u00f3n de su valor hash . Existen dos razones por las cuales queramos organizar las tablas (o particiones) en buckets . La primera es para conseguir consultas m\u00e1s eficientes, ya que imponen una estructura extra en las tablas. Los buckets pueden acelerar las operaciones de tipo join si las claves de bucketing y de join coinciden, ya que una clave ajena busca \u00fanicamente en el bucket adecuado de la clave primaria. Debido a los beneficios de los buckets/particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. Para indicar que nuestras tablas utilicen buckets , hemos de emplear la clausula CLUSTERED BY para indicar la columnas y el n\u00famero de buckets (se recomienda que la cantidad de buckets sea potencia de 2): CREATE TABLE usuarios_bucketed ( id INT , nombre STRING ) CLUSTERED BY ( id ) INTO 4 BUCKETS ; Bucketing est\u00e1 muy relacionado con el proceso de carga de datos. Para cargar los datos en una tabla con buckets , debemos bien indicar el n\u00famero m\u00e1ximo de reducers para que coincida con el n\u00famero de buckets , o habilitar el bucketing (esta es la recomendada): set map . reduce . tasks = 4 ; set hive . enforce . bucketing = true ; -- mejor as\u00ed Una vez creada la tabla, se rellena con los datos que tenemos en otra tabla: INSERT OVERWRITE TABLE usuarios_bucketed SELECT * FROM usuarios ; F\u00edsicamente, cada bucket es un fichero de la carpeta con la tabla (o partici\u00f3n). El nombre del fichero no es importante, pero el bucket n es el fichero n\u00famero n . Por ejemplo, si miramos el contenido de la tabla en HDFS tendremos: hive> dfs -ls /user/hive/warehouse/usuarios_bucketed ; 000000_0 000001_0 000002_0 000003_0 El segundo motivo es para obtener un sampling de forma m\u00e1s eficiente. Al trabajar con grandes datasets, normalmente obtenemos una peque\u00f1a fracci\u00f3n del dataset para comprender o refinar los datos. Por ejemplo, podemos obtener los datos de \u00fanicamente uno de los buckets : SELECT * FROM usuarios_bucketed TABLESAMPLE ( BUCKET 1 OUT OF 4 ON id ); Mediante el id del usuario determinamos el bucket (el cual se utiliza para realizar el hash del valor y ubicarlo dentro de uno de los buckets ), de manera que cada bucket contendr\u00e1 de manera eficiente un conjunto aleatorio de usuarios. Resumen \u00b6 A continuaci\u00f3n mostramos en una tabla puntos a favor y en contra de utilizar estas estructuras Particionado + Particionado - Bucketing + Bucketing - Distribuye la carga de ejecuci\u00f3n horizontalmente. Existe la posibilidad de crear demasiadas particiones que contienen muy poco datos Proporciona una respuesta de consulta m\u00e1s r\u00e1pida, al acceder a porciones. El n\u00famero de buckets se define durante la creaci\u00f3n de la tabla -> Los programadores deben cargar manualmente un volumen equilibrado de datos. En la partici\u00f3n tiene lugar la ejecuci\u00f3n m\u00e1s r\u00e1pida de consultas con el volumen de datos bajo. Por ejemplo, la poblaci\u00f3n de b\u00fasqueda de la Ciudad del Vaticano devuelve muy r\u00e1pido en lugar de buscar la poblaci\u00f3n mundial completa. La partici\u00f3n es eficaz para datos de bajo volumen. Pero algunas consultas como agrupar por un gran volumen de datos tardan mucho en ejecutarse. Por ejemplo, agrupar las consultas del mes de Enero tardar\u00e1 m\u00e1s que los viernes de Enero. Al utilizar vol\u00famenes similares de datos en cada partici\u00f3n, los map joins ser\u00e1n m\u00e1s r\u00e1pidos. Supongamos que $HDFS_HIVE contiene la ruta con la ra\u00edz de las tablas internas de Hive, en nuestro caso /user/hive/warehouse . Respecto al nivel de estructura y representaci\u00f3n en carpetas de HDFS tendr\u00edamos: ENTIDAD EJEMPLO UBICACI\u00d3N base de datos iabd $HDFS_HIVE/iabd.db tabla T $HDFS_HIVE/iabd.db/T partici\u00f3n fecha='01012022' $HDFS_HIVE/iabd.db/T/fecha=01012022 bucket columna id $HDFS_HIVE/iabd.db/T/fecha=01012022/000000_0 Caso de uso 6: Particionado y Bucketing \u00b6 A continuaci\u00f3n, vamos a coger los datos de las productos y las categor\u00edas de la base de datos retail_db , y colocarlos en una estructura de Hive particionada y que utilice bucketing . La estructura de las tablas es la siguiente: Relaci\u00f3n entre productos y categor\u00edas El primer paso es traernos los datos de MariaDB a tablas internas haciendo uso de Sqoop : Tabla categories Tabla products sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = categories --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table categories sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = products --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table products El siguiente paso que vamos a realizar es crear en Hive una tabla con el c\u00f3digo del producto, su nombre, el nombre de la categor\u00eda y el precio del producto. Estos datos los vamos a particionar por categor\u00eda y clusterizado en 8 buckets: CREATE TABLE IF NOT EXISTS productos ( id INT , nombre STRING , precio DOUBLE ) PARTITIONED BY ( categoria STRING ) CLUSTERED BY ( id ) INTO 8 BUCKETS ; Y cargamos los datos con una consulta que realice un join de las tablas categories y products con particionado din\u00e1mico (recuerda activarlo mediante set hive.exec.dynamic.partition.mode=nonstrict; ): INSERT OVERWRITE TABLE productos PARTITION ( categoria ) SELECT p . product_id as id , p . product_name as nombre , p . product_price as precio , c . category_name as categoria FROM products p join categories c on ( p . product_category_id = c . category_id ); Si queremos comprobar como se han creado las particiones y los buckets, desde un terminal podemos acceder a HDFS y mostrar su contenido: hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos/categoria = Accessories Si volvemos a Hive , ahora podemos consultar los datos: select * from productos limit 5 ; Y vemos c\u00f3mo aparecen 5 elementos que pertenecen a la primera partici\u00f3n. Si quisi\u00e9ramos, por ejemplo, 10 elementos de particiones diferentes deber\u00edamos ordenarlos de manera aleatoria: select * from productos order by rand () limit 10 ; A continuaci\u00f3n vamos a realizar diversas consultas utilizando las funciones ventana que soporta Hive . M\u00e1s informaci\u00f3n en https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics . Consultas con enteros que cuentan/ordenan \u00b6 Consultas sobre categor\u00edas Las siguientes consultas las vamos a realizar sobre s\u00f3lo dos categor\u00edas para acotar los resultados obtenidos. Adem\u00e1s, hemos recortado el nombre del producto a 20 caracteres para facilitar la legibilidad de los resultados. Las funciones rank y dense_rank permite obtener la posici\u00f3n que ocupan los datos. Se diferencia en que rank cuenta los elementos repetidos/empatados, mientras que dense_rank no. Por ejemplo, vamos a obtener la posici\u00f3n que ocupan los productos respecto al precio agrupados por su categor\u00eda: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , rank () over ( partition by categoria order by precio desc ) as rank , dense_rank () over ( partition by categoria order by precio desc ) as denseRank from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+-------+------------+ | nombre | categoria | precio | rank | denserank | +-----------------------+--------------------+----------+-------+------------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 5 | 5 | | Diamondback Adult So | Basketball | 299.98 | 7 | 6 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | 7 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | 8 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | 9 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | 10 | | Easton S1 Youth Bat | Basketball | 179.97 | 11 | 10 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | 11 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | 12 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | 13 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | 14 | | Elevation Training M | Basketball | 79.99 | 17 | 15 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | 16 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | 17 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | 18 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | 19 | | adidas Brazuca 2014 | Basketball | 29.99 | 21 | 19 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | 20 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | 21 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 2 | 2 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | 3 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | 4 | +-----------------------+--------------------+----------+-------+------------+ La funci\u00f3n row_number permite numerar los resultados: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , row_number () over ( partition by categoria order by precio desc ) as numfila from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | numfila | +-----------------------+--------------------+----------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 6 | | Diamondback Adult So | Basketball | 299.98 | 7 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | | Easton S1 Youth Bat | Basketball | 179.97 | 12 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | | Elevation Training M | Basketball | 79.99 | 17 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | | adidas Brazuca 2014 | Basketball | 29.99 | 22 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 3 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | +-----------------------+--------------------+----------+----------+ Consultas por posici\u00f3n \u00b6 A continuaci\u00f3n vamos a ver las funciones lead y lag . Estas funciones se encargan de obtener el valor posterior y anterior respecto a un valor. select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , lead ( precio ) over ( partition by categoria order by precio desc ) as sig , lag ( precio ) over ( partition by categoria order by precio desc ) as ant from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+---------+----------+ | nombre | categoria | precio | sig | ant | +-----------------------+--------------------+----------+---------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 999.99 | NULL | | SOLE E25 Elliptical | Basketball | 999.99 | 349.98 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 309.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 299.99 | 349.98 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | 309.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.98 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 249.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 209.99 | 299.98 | | Fitness Gear 300 lb | Basketball | 209.99 | 199.99 | 249.97 | | Quik Shade Summit SX | Basketball | 199.99 | 179.97 | 209.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | 199.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 159.99 | 179.97 | | adidas Brazuca 2014 | Basketball | 159.99 | 149.99 | 179.97 | | Quest 12' x 12' Dome | Basketball | 149.99 | 99.95 | 159.99 | | Fitbit Flex Wireless | Basketball | 99.95 | 99.0 | 149.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 79.99 | 99.95 | | Elevation Training M | Basketball | 79.99 | 69.99 | 99.0 | | MAC Sports Collapsib | Basketball | 69.99 | 59.98 | 79.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 39.99 | 69.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 29.99 | 59.98 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 29.99 | 39.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 28.0 | 29.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 21.99 | 29.99 | | Nike Women's Pro Vic | Basketball | 21.99 | NULL | 28.0 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 169.99 | NULL | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 119.99 | 169.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 0.0 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | NULL | 119.99 | +-----------------------+--------------------+----------+---------+----------+ Consultas de agregaci\u00f3n \u00b6 Las funciones de agregaci\u00f3n que ya conocemos como count , sum , min y max tambi\u00e9n las podemos aplicar sobre particiones de datos y as\u00ed poder mostrar los datos agregados para cada elemento: select substr ( nombre , 1 , 20 ) as nombre , categoria , count ( precio ) over ( partition by categoria ) as cantidad , min ( precio ) over ( partition by categoria ) as menor , max ( precio ) over ( partition by categoria ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+-----------+--------+----------+ | nombre | categoria | cantidad | menor | mayor | +-----------------------+--------------------+-----------+--------+----------+ | Fitbit Flex Wireless | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Re | Basketball | 24 | 21.99 | 1799.99 | | Nike+ Fuelband SE | Basketball | 24 | 21.99 | 1799.99 | | Elevation Training M | Basketball | 24 | 21.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Girls' C | Basketball | 24 | 21.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | Easton Mako Youth Ba | Basketball | 24 | 21.99 | 1799.99 | | SOLE E25 Elliptical | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 24 | 21.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 24 | 21.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 24 | 21.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Boys' In | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult So | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Cor | Basketball | 24 | 21.99 | 1799.99 | | Quik Shade Summit SX | Basketball | 24 | 21.99 | 1799.99 | | Quest 12' x 12' Dome | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Vic | Basketball | 24 | 21.99 | 1799.99 | | Cleveland Golf Class | Bike & Skate Shop | 5 | 0.0 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | +-----------------------+--------------------+-----------+--------+----------+ Las consultas que hemos visto en este caso de uso tambi\u00e9n se conocen como funciones ventana , ya que se ejecutan sobre un subconjunto de los datos. La ventana viene dada por la partici\u00f3n o por la posici\u00f3n una vez ordenados los datos. Los posibles valores son: rows between current row and unbounded following : desde la fila actual hasta el final de la ventana/partici\u00f3n. rows between current row and N following : desde la fila actual hasta los N siguientes. rows between unbounded preceding and current row : desde el inicio de la ventana hasta la fila actual. rows between unbounded preceding and N following : desde el inicio de la ventana hasta los N siguientes. rows between unbounded preceding and unbounded following : desde el inicio de la ventana hasta el final de la ventana (caso por defecto) rows between N preceding and M following : desde N filas anteriores hasta M filas siguientes. Por ejemplo, para obtener el m\u00e1ximo precio desde la fila actual hasta el resto de la partici\u00f3n: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between current row and unbounded following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 1799.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 1799.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 1799.99 | | Elevation Training M | Basketball | 79.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 1799.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 1799.99 | | Diamondback Girls' C | Basketball | 299.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 1799.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 1799.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 309.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 69.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 299.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.98 | | Nike Women's Pro Cor | Basketball | 28.0 | 199.99 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 149.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 21.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 169.99 | +-----------------------+--------------------+----------+----------+ Si queremos comparar el precio y quedarnos con el mayor respecto al anterior y el posterior podr\u00edamos realizar la siguiente consulta: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between 1 preceding and 1 following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 99.95 | | adidas Brazuca 2014 | Basketball | 39.99 | 209.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 349.98 | | Diamondback Adult Re | Basketball | 349.98 | 349.98 | | Nike+ Fuelband SE | Basketball | 99.0 | 349.98 | | Elevation Training M | Basketball | 79.99 | 179.97 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | | adidas Brazuca 2014 | Basketball | 29.99 | 299.99 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 999.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 999.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 309.99 | | MAC Sports Collapsib | Basketball | 69.99 | 159.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 1799.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 299.98 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 199.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 149.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 169.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 179.99 | +-----------------------+--------------------+----------+----------+ Referencias \u00b6 P\u00e1gina oficial de Hive Apache Hive Essentials - Second Edition de Dayong Du. Tutorial de Hive de TutorialsPoint. Actividades \u00b6 Realiza los casos de uso del 1 al 5. En la entrega debes adjuntar un una captura de pantalla donde se vea la ejecuci\u00f3n de las diferentes instrucciones. (opcional) Realiza el caso de uso 6. (opcional) A partir de la base de datos retail_db , importa las tablas orders y order_items , en las cuales puedes obtener la cantidad de productos que contiene un pedido. Utilizando todas las tablas que ya hemos importado en los casos anteriores, crea una tabla externa en Hive llamada pedidos utilizando 8 buckets con el c\u00f3digo del cliente, que contenga: C\u00f3digo y fecha del pedido. Precio del pedido (sumando las l\u00edneas de pedido). C\u00f3digo, nombre y apellidos del cliente. Adjunta scripts y capturas de: la importaci\u00f3n, creaci\u00f3n y carga de datos de las tablas que necesites. la definici\u00f3n de la tabla: describe formatted pedidos; contenido de HDFS que demuestre la creaci\u00f3n de los buckets . (opcional) Investiga la creaci\u00f3n de vistas en Hive y crea una vista con los datos de los clientes y sus pedidos siempre y cuando superen los 200$.","title":"Apache Hive. Acceso a HDFS con un interfaz similar a tablas relacionales mediante SQL."},{"location":"hadoop/06hive.html#hive","text":"Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un cl\u00faster de Hadoop . Permite leer, escribir y gestionar grandes datasets (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop . Logo de Apache Hive El proyecto lo inici\u00f3 Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os.","title":"Hive"},{"location":"hadoop/06hive.html#hive-y-hadoop","text":"Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop , Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduce u otras tecnolog\u00edas. Aunque en principio estaba dise\u00f1ado para el procesamiento batch , ahora se integra con frameworks en streaming como Tez y Spark . Ecosistema Hadoop","title":"Hive y Hadoop"},{"location":"hadoop/06hive.html#caracteristicas","text":"Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema , y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase . Para interactuar con Hive utilizaremos HiveQL , el cual es un dialecto de SQL (recuerda que SQL no es sensible a las may\u00fasculas, excepto en la comparaci\u00f3n de cadenas). Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n podemos personalizar el procesamiento de consultas creando un esquema de tabla acorde con nuestros datos, pero sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON o cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive se ejecuta en varios datanodes en paralelo, con varios trabajos MapReduce asociados. Estas operaciones son de tipo batch , por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets.","title":"Caracter\u00edsticas"},{"location":"hadoop/06hive.html#componentes","text":"A continuaci\u00f3n podemos ver un gr\u00e1fico que relaciona los diferentes componentes de Hive y define su arquitectura: Arquitectura de Apache Hive","title":"Componentes"},{"location":"hadoop/06hive.html#tipos-de-datos","text":"Los tipos de datos que podemos emplear en Hive son muy similares a los que se utilizan en el DDL de SQL. Los tipos simples m\u00e1s comunes son STRING e INT , aunque podemos utilizar otros tipos como TINYINT , BIGINT , DOUBLE , DATE , TIMESTAMP , etc... Para realizar una conversi\u00f3n explicita de tipos, por ejemplo de un tipo texto a uno num\u00e9rico, hay que utilizar la funci\u00f3n CAST : select CAST ( '1' as INT ) from tablaPruebas ; Respecto a los tipos compuestos, tenemos tres tipos: arrays mediante el tipo ARRAY , para agrupar elementos del mismo tipo: [\"manzana\", \"pera\", \"naranja] . mapas mediante el tipo MAP , para definir parejas de clave-valor: {1: \"manzana\", 2: \"pera\"} estructuras mediante el tipo STRUCT , para definir estructuras con propiedades: {\"fruta\": \"manzana\", \"cantidad\": 1, \"tipo\": \"postre\"} .","title":"Tipos de datos"},{"location":"hadoop/06hive.html#instalacion-y-configuracion","text":"M\u00e1quina virtual Los siguientes pasos no son necesarios ya que nuestra m\u00e1quina virtual ya tiene Hive instalado y configurado correctamente. Si quieres hacer tu propia instalaci\u00f3n sigue los siguientes pasos de la documentaci\u00f3n oficial. Una vez instalado, vamos a configurarlo. Para ello, debemos crear los ficheros de configuraci\u00f3n a partir de las plantilla que ofrece Hive . Para ello, desde la carpeta $HIVE_HOME/conf , ejecutaremos los siguientes comandos: cp hive-default.xml.template hive-site.xml cp hive-env.sh.template hive-env.sh cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties cp hive-log4j2.properties.template hive-log4j2.properties Modificamos el fichero hive.env.sh para incluir dos variables de entorno con las rutas de Hadoop y la configuraci\u00f3n de Hive hive.env.sh export HADOOP_HOME = /opt/hadoop-3.3.1 export HIVE_CONF_DIR = /opt/hive-3.1.2/conf Para que funcione la ingesta de datos en Hive mediante Sqoop , necesitamos a\u00f1adir una librer\u00eda a Sqoop : cp $HIVE_HOME /lib/hive-common-3.1.2.jar $SQOOP_HOME /lib Preparamos HDFS para crear la estructura de archivos: hdfs dfs -mkdir /tmp hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /tmp hdfs dfs -chmod g+w /user/hive/warehouse Para el metastore , como en nuestra m\u00e1quina virtual tenemos un servidor de MariaDB corriendo, vamos a reutilizarlo. La mayor\u00eda de ejemplos que hay en internet y la diferente bibliograf\u00eda, utilizan DerbyDB como almac\u00e9n (ya que no requiere una instalaci\u00f3n extra). As\u00ed pues, creamos el almac\u00e9n mediante: schematool -dbType mysql -initSchema Modificamos el fichero de configuraci\u00f3n hive-site.xml y configuramos : hive-site.xml <!-- nuevas propiedades --> <property> <name> system:java.io.tmpdir </name> <value> /tmp/hive/java </value> </property> <property> <name> system:user.name </name> <value> ${user.name} </value> </property> <property> <name> datanucleus.schema.autoCreateAll </name> <value> true </value> </property> <!-- propiedades existentes a modificar --> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> com.mysql.jdbc.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> iabd </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> iabd </value> </property>","title":"Instalaci\u00f3n y configuraci\u00f3n"},{"location":"hadoop/06hive.html#hola-mundo","text":"Si entramos a nuestro $HIVE_HOME podemos comprobar con tenemos las siguientes herramientas: hive : Herramienta cliente beeline : Otra herramienta cliente hiserver2 : Nos permite arrancar el servidor de Hive schematool : Nos permite trabajar contra la base de datos de metadatos (Metastore) Una vez arrancado Hadoop y YARN , vamos a arrancar Hive mediante el cliente local: hive Y una vez dentro, podemos comprobar las bases de datos existentes para ver que todo se configur\u00f3 correctamente show databases ; Si quisi\u00e9ramos ejecutar un script, podemos hacerlo desde el propio comando hive con la opci\u00f3n -f : hive -f script.sql Adem\u00e1s, tenemos la opci\u00f3n de pasar una consulta desde la propia l\u00ednea de comandos mediante la opci\u00f3n -e : hive -e ' select * from tablaEjemplo `","title":"Hola Mundo"},{"location":"hadoop/06hive.html#acceso-remoto","text":"HiveServer2 (desde Hive 0.11) tiene su propio cliente conocido como Beeline . En entornos reales, el cliente Hive est\u00e1 en desuso a favor de Beeline , por la falta de m\u00faltiples usuarios, seguridad y otras caracter\u00edsticas de HiveServer2. Arrancamos HiveServer2 y Beeline en dos pesta\u00f1as diferentes mediante los comandos hiveserver2 y beeline . Una vez dentro de Beeline , nos conectamos al servidor: !connect jdbc:hive2://iabd-virtualbox:10000 Al conectarnos, tras introducir iabd como usuario y contrase\u00f1a, obtendremos un interfaz similar al siguiente: Beeline version 3.1.2 by Apache Hive beeline> !connect jdbc:hive2://iabd-virtualbox:10000 Connecting to jdbc:hive2://iabd-virtualbox:10000 Enter username for jdbc:hive2://iabd-virtualbox:10000: iabd Enter password for jdbc:hive2://iabd-virtualbox:10000: **** Connected to: Apache Hive (version 3.1.2) Driver: Hive JDBC (version 3.1.2) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://iabd-virtualbox:10000> Dentro de Beeline , en cualquier momento podemos ejecutar el comando help que nos mostrar\u00e1 todos los comandos disponibles. Si nos fijamos, adem\u00e1s de las comandos del cliente hive, tenemos los comandos beeline que empiezan por el s\u00edmbolo de exclamaci\u00f3n ! : 0: jdbc:hive2://iabd-virtualbox:10000> help !addlocaldriverjar Add driver jar file in the beeline client side. !addlocaldrivername Add driver name that needs to be supported in the beeline client side. !all Execute the specified SQL against all the current connections !autocommit Set autocommit mode on or off !batch Start or execute a batch of statements ... Otra forma de trabajar, para arrancar en el mismo proceso Beeline y HiveServer2 para pruebas/desarrollo y tener una experiencia similar al cliente Hive accediendo de forma local, podemos ejecutar el siguiente comando: beeline -u jdbc:hive2:// Mediante la interfaz gr\u00e1fica de Hive Server UI a la cual podemos acceder mediante http://localhost:10002 podemos monitorizar los procesos ejecutados por HiveServer2 : Monitorizaci\u00f3n mediante Hive Server UI","title":"Acceso remoto"},{"location":"hadoop/06hive.html#caso-de-uso-1-creacion-y-borrado-de-tablas","text":"Para este caso de uso, vamos a utilizar la base de datos retail_db que ya utilizamos en las actividades de la sesi\u00f3n anterior. Para empezar, vamos a cargar en HDFS los datos de los clientes que contiene la tabla customer . Mediante Sqoop , ejecutamos el siguiente comando: sqoop import --connect \"jdbc:mysql://localhost/retail_db\" \\ --username iabd --password iabd \\ --table customers --target-dir /user/iabd/hive/customer \\ --fields-terminated-by '|' --delete-target-dir \\ --columns \"customer_id,customer_fname,customer_lname,customer_city\" Una vez nos hemos conectado con el cliente hive o mediante beeline , creamos una base de datos llamada iabd : create database iabd ; Nos conectamos a la base de datos que acabamos de crear: use iabd ; default Si olvidamos el comando use , se utilizar\u00e1 la base de datos default , la cual reside en /user/hive/warehouse como ra\u00edz en HDFS. A continuaci\u00f3n, vamos a crear una tabla que almacene el identificador, nombre, apellido y ciudad de los clientes (como puedes observar, la sintaxis es similar a SQL): CREATE TABLE customers ( custId INT , fName STRING , lName STRING , city STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/user/iabd/hive/customer' ; Y ya podemos realizar algunas consultas: select * from customers limit 5 ; select count ( * ) from customers ; En ocasiones necesitamos almacenar la salida de una consulta Hive en una nueva tabla. Las definiciones de las columnas de la nueva tabla se deriva de las columnas recuperadas en la consulta. Para ello, usaremos el comando create table-as select : CREATE TABLE customers_new as SELECT * from customers ; En el caso de la consulta falle por alg\u00fan motivo, la tabla no se crear\u00eda. Otra posibilidad es crear una tabla con la misma estructura que otra ya existente (pero sin datos): CREATE TABLE customers2 LIKE customers ; En cualquier momento podemos obtener informaci\u00f3n de la tabla: describe customers_new ; describe formatted customers_new ; Si empleamos la forma larga, obtendremos mucha m\u00e1s informaci\u00f3n. Por ejemplo, si nos fijamos, vemos que la localizaci\u00f3n de la nueva tabla ya no es /user/iabd/hive/customer sino hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/customers_new . Esto se debe a que en vez de crear una tabla enlazada a un recurso de HDFS ya existente, ha creado una copia de los datos en el propio almac\u00e9n de Hive (hemos pasado de una tabla externa a una interna). Igual que las creamos, las podemos eliminar: drop table customers_new ; drop table customers2 ; Si ejecutamos el comando !tables (o show tables en el cliente hive ) veremos que ya no aparecen dichas tablas. En el caso de que queramos eliminar una base de datos, de la misma manera que en SQL, ejecutar\u00edamos el comando drop database iabd; .","title":"Caso de uso 1: Creaci\u00f3n y borrado de tablas"},{"location":"hadoop/06hive.html#caso-de-uso-2-insertando-datos","text":"Para insertar datos en las tablas de Hive lo podemos hacer de varias formas: Cargando los datos mediante sentencias LOAD DATA . Insertando los datos mediante sentencias INSERT . Cargando los datos directamente mediante Sqoop o alguna herramienta similar.","title":"Caso de uso 2: Insertando datos"},{"location":"hadoop/06hive.html#cargando-datos","text":"Para cargar datos se utiliza la sentencia LOAD DATA . Si quisi\u00e9ramos volver a cargar los datos desde HDFS utilizaremos: LOAD DATA INPATH '/user/iabd/hive/customer' overwrite into table customers ; Si en cambio vamos a cargar los datos desde un archivo local a nuestro sistema de archivos a\u00f1adiremos LOCAL : LOAD DATA LOCAL INPATH '/home/iabd/datos' overwrite into table customers ;","title":"Cargando datos"},{"location":"hadoop/06hive.html#insertando-datos","text":"Aunque podemos insertar datos de forma at\u00f3mica (es decir, registro a registro mediante INSERT INTO TABLE ... VALUES ), realmente las inserciones que se realizan en Hive se hacen a partir de los datos de otras tablas mediante el comando insert-select a modo de ETL: INSERT OVERWRITE TABLE destino SELECT col1 , col2 FROM fuente ; Mediante la opci\u00f3n OVERWRITE , en cada ejecuci\u00f3n se vac\u00eda la tabla y se vuelve a rellenar. Si no lo indicamos o utilizamos INTO , los datos se a\u00f1adir\u00edan a los ya existentes. Si necesitamos insertar datos en m\u00faltiples tablas a la vez lo haremos mediante el comando from-insert : FROM fuente INSERT OVERWRITE TABLE destino1 SELECT col1 , col2 INSERT OVERWRITE TABLE destino2 SELECT col1 , col3 Por ejemplo, vamos a crear un par de tablas con la misma estructura de clientes, pero para almacenar los clientes de determinadas ciudades: CREATE TABLE customers_brooklyn LIKE customers ; CREATE TABLE customers_caguas LIKE customers ; Y a continuaci\u00f3n rellenamos ambas tablas con sus clientes; FROM customers INSERT OVERWRITE TABLE customers_brooklyn SELECT custId , fName , lName , city WHERE city = \"Brooklyn\" INSERT OVERWRITE TABLE customers_caguas SELECT custId , fName , lName , city WHERE city = \"Caguas\" ;","title":"Insertando datos"},{"location":"hadoop/06hive.html#ingestando-datos","text":"Tal como vimos en la sesi\u00f3n anterior , podemos ingestar los datos en Hive haciendo uso de Sqoop (tambi\u00e9n lo podemos hacer con Flume o Nifi ). Por ejemplo, vamos a ingestar los datos de la tabla orders de la base de datos MariaDB que tenemos instalada en nuestra m\u00e1quina virtual. En el comando de Sqoop le indicamos que dentro de Hive lo ingeste en la base de datos iabd y que cree una tabla llamada orders : sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = orders --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table orders Una vez realizada la ingesta, podemos comprobar que los datos est\u00e1n dentro de Hive (en una tabla interna/gestionada): hdfs dfs -ls /user/hive/warehouse/iabd.db/orders Y si entramos a hive , podemos consultar sus datos: select * from orders limit 10 ;","title":"Ingestando datos"},{"location":"hadoop/06hive.html#extrayendo-datos-insertados","text":"Combinando los comandos de HQL y HDFS podemos extraer datos a ficheros locales o remotos: # A\u00f1adiendo contenido local hive -e \"use iabd; select * from customers\" >> prueba1 # Sobrescribiendo contenido local hive -e \"use iabd; select * from customers\" > prueba2 # A\u00f1adiendo contenido HDFS hive -e \"use iabd; select * from customers\" | hdfs dfs --appendToFile /tmp/prueba3 # Sobrescribiendo contenido hive -e \"use iabd; select * from customers\" | hdfs dfs --put -f /tmp/prueba4 Si indicamos la propiedad set hive.cli.print.header=true antes de la consulta, tambi\u00e9n nos mostrar\u00e1 el encabezado de las columnas. Esto puede ser \u00fatil si queremos generar un csv con el resultado de una consulta: hive -e 'use iabd; set hive.cli.print.header=true; select * from customers' | \\ sed 's/[\\t]/,/g' > fichero.csv \u00bfY usar INSERT LOCAL ? Mediante INSERT LOCAL podemos escribir el resultado de una consulta en nuestro sistema de archivos, fuera de HDFS. El problema es que si hay muchos datos crear\u00e1 m\u00faltiples ficheros y necesitaremos concatenarlos para tener un \u00fanico resultado: insert overwrite local directory '/home/iabd/datos' row format delimited field terminated by ',' select * from customers ;","title":"Extrayendo datos insertados"},{"location":"hadoop/06hive.html#caso-de-uso-3-consultas-con-join","text":"En este caso de uso vamos a trabajar con los datos de clientes que hemos cargado en los dos casos anteriores, tanto en customers como en orders . Si queremos relacionar los datos de ambas tablas, tenemos que hacer un join entre la clave ajena de orders ( order_customer_id ) y la clave primaria de customers ( custid ): hive > describe customers ; OK custid int fname string lname string city string Time taken : 0 . 426 seconds , Fetched : 4 row ( s ) hive > describe orders ; OK order_id int order_date string order_customer_id int order_status string Time taken : 0 . 276 seconds , Fetched : 4 row ( s ) Para ello, para obtener la ciudad de cada pedido, podemos ejecutar la consulta: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid );","title":"Caso de uso 3: Consultas con join"},{"location":"hadoop/06hive.html#outer-join","text":"De la misma manera que en cualquier SGBD, podemos realizar un outer join , tanto left como right o full . Por ejemplo, vamos a obtener para cada cliente, cuantos pedidos ha realizado: select c . custid , count ( order_id ) from customers c join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Si queremos que salgan todos los clientes, independientemente de que tengan pedidos, deberemos realizar un left outer join : select c . custid , count ( order_id ) from customers c left outer join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ;","title":"Outer join"},{"location":"hadoop/06hive.html#semi-joins","text":"Si quisi\u00e9ramos obtener las ciudades de los clientes que han realizado pedidos podr\u00edamos realizar la siguiente consulta: select distinct c . city from customers c where c . custid in ( select order_customer_id from orders ); Mediante un semi-join podemos obtener el mismo resultado: select distinct city from customers c left semi join orders o on ( c . custid = o . order_customer_id ) Hay que tener en cuenta la restricci\u00f3n que las columnas de la tabla de la derecha s\u00f3lo pueden aparecer en la clausula on , nunca en la expresi\u00f3n select .","title":"Semi-joins"},{"location":"hadoop/06hive.html#map-joins","text":"Consideramos la consulta inicial de join: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Si una tabla es suficientemente peque\u00f1a para caber en memoria, tal como nos ocurre con nuestros datos, Hive puede cargarla en memoria para realizar el join en cada uno de los mappers . Esto se conoce como un map join . El job que ejecuta la consulta no tiene reducers , con lo que esta consulta no funcionar\u00e1 para un right o right outer join , ya que la ausencias de coincidencias s\u00f3lo se puede detectar en los pasos de agregaci\u00f3n ( reduce ). En el caso de utilizar map joins con tablas organizadas en buckets , la sintaxis es la misma, s\u00f3lo siendo necesario activarlo mediante la propiedad hive.optimize.bucketmapjoin : SET hive.optimize.bucketmapjoin = true ;","title":"Map joins"},{"location":"hadoop/06hive.html#comandos","text":"Mediante los casos de uso realizados hasta ahora, hemos podido observar c\u00f3mo para interactuar con Hive se utilizan comandos similares a SQL. Es conveniente consultar la siguiente cheatsheet : http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf Adem\u00e1s Hive viene con un conjunto de funciones predefinidas para tratamiento de cadenas, fechas, funciones estad\u00edsticas, condicionales, etc... las cuales puedes consultar en la documentaci\u00f3n oficial . Mediante el comando show functions podemos obtener una lista de las funciones. Si queremos m\u00e1s informaci\u00f3n sobre una determinada funci\u00f3n utilizaremos el comando describe function nombreFuncion : hive > describe function length ; length ( str | binary ) - Returns the length of str or number of bytes in binary data","title":"Comandos"},{"location":"hadoop/06hive.html#caso-de-uso-4-tabla-interna","text":"Hive permite crear tablas de dos tipos: tabla interna o gestionada: Hive gestiona la estructura y el almacenamiento de los datos. Para ello, crea los datos en HDFS. Al borrar la tabla de Hive , se borra la informaci\u00f3n de HDFS. tabla externa : Hive define la estructura de los datos en el metastore , pero los datos ya residen previamente en HDFS. Al borrar la tabla de Hive , no se eliminan los datos de HDFS. Se emplea cuando compartimos datos almacenados en HDFS entre diferentes herramientas. En este caso de uso, vamos a centrarnos en una tabla interna. Supongamos el siguiente fichero con datos de empleados: empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Podemos observar como se utiliza | como separador de campos. Analizando los datos, vemos que tenemos los siguientes campos: Nombre Centros de trabajo (array con las ciudades) Sexo y edad Destreza y puntuaci\u00f3n Departamento y cargo Creamos la siguiente tabla interna en Hive mediante el siguiente comando: CREATE TABLE IF NOT EXISTS empleados_interna ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT 'Esto es una tabla interna' ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' ; La sintaxis es muy similar a SQL, destacando las siguientes opciones: ROW FORMAT DELIMITED : cada registro ocupa una l\u00ednea FIELDS TERMINATED BY '|' : define el | como separador de campos COLLECTION ITEMS TERMINATED BY ',' : define la coma como separador de los arrays / estructuras MAP KEYS TERMINATED BY ':' : define los dos puntos como separador utilizado en los mapas. Si queremos comprobar la estructura de la tabla mediante el comando show create table empleados_interna veremos las opciones que hemos indicado: + ----------------------------------------------------+ | createtab_stmt | + ----------------------------------------------------+ | CREATE TABLE ` empleados_interna ` ( | | ` name ` string , | | ` work_place ` array < string > , | | ` sex_age ` struct < sex : string , age : int > , | | ` skills_score ` map < string , int > , | | ` depart_title ` map < string , array < string >> ) | | COMMENT 'Esto es una tabla interna' | | ROW FORMAT SERDE | | 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' | | WITH SERDEPROPERTIES ( | | 'collection.delim' = ',' , | | 'field.delim' = '|' , | | 'mapkey.delim' = ':' , | | 'serialization.format' = '|' ) | | STORED AS INPUTFORMAT | | 'org.apache.hadoop.mapred.TextInputFormat' | | OUTPUTFORMAT | | 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' | | LOCATION | | 'hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/empleados_interna' | | TBLPROPERTIES ( | | 'bucketing_version' = '2' , | | 'transient_lastDdlTime' = '1647432129' ) | + ----------------------------------------------------+ A continuaci\u00f3n, vamos a cargar los datos del fichero empleados.txt , el cual colocaremos en nuestra carpeta de Descargas : LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_interna ; Comprobamos que los datos se han cargado correctamente: select * from empleados_interna ; Y obtenemos: +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | empleados_interna.name | empleados_interna.work_place | empleados_interna.sex_age | empleados_interna.skills_score | empleados_interna.depart_title | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | Michael | [\"Montreal\",\"Toronto\"] | {\"sex\":\"Male\",\"age\":30} | {\"DB\":80} | {\"Product\":[\"Developer\",\"Lead\"]} | | Will | [\"Montreal\"] | {\"sex\":\"Male\",\"age\":35} | {\"Perl\":85} | {\"Product\":[\"Lead\"],\"Test\":[\"Lead\"]} | | Shelley | [\"New York\"] | {\"sex\":\"Female\",\"age\":27} | {\"Python\":80} | {\"Test\":[\"Lead\"],\"COE\":[\"Architect\"]} | | Lucy | [\"Vancouver\"] | {\"sex\":\"Female\",\"age\":57} | {\"Sales\":89,\"HR\":94} | {\"Sales\":[\"Lead\"]} | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ Y si nos abrimos otra pesta\u00f1a, mediante HDFS, comprobamos que tenemos los datos: hdfs dfs -ls /user/hive/warehouse/curso.db/empleados_interna hdfs dfs -cat /user/hive/warehouse/curso.db/empleados_interna/empleados.txt Y obtenemos: Michael|Montreal,Toronto|Male,30|DB:80|Product:DeveloperLead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead","title":"Caso de uso 4: Tabla interna"},{"location":"hadoop/06hive.html#consultando-datos-compuestos","text":"Si nos fijamos bien en la tabla, tenemos tres columnas con diferentes datos compuestos. work_place , con un ARRAY<string> sex_age con una STRUCT<sex:string,age:int> skills_score con un MAP<string,int> , depart_title con un MAP<STRING,ARRAY<STRING>> Si queremos obtener los datos del array , podemos realizar las siguientes consultas: Todos los elementos Elementos individuales Cantidad de elementos Explode array Todos los lugares de trabajo: select name , work_place from empleados_interna ; Resultado: +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [ \"Montreal\" , \"Toronto\" ] | | Will | [ \"Montreal\" ] | | Shelley | [ \"New York\" ] | | Lucy | [ \"Vancouver\" ] | +----------+-------------------------+ Los dos primeros puestos de trabajo: select work_place [ 0 ] as lugar1 , work_place [ 1 ] as lugar2 from empleados_interna ; Resultado: +------------+----------+ | lugar1 | lugar2 | +------------+----------+ | Montreal | Toronto | | Montreal | NULL | | New York | NULL | | Vancouver | NULL | +------------+----------+ Cantidad de lugares de trabajo: select size ( work_place ) as cantLugares from empleados_interna ; Resultado: +--------------+ | cantlugares | +--------------+ | 2 | | 1 | | 1 | | 1 | +--------------+ Empleados y lugares de trabajo mostrados uno por fila: select name , lugar from empleados_interna lateral view explode ( work_place ) e2 as lugar ; Para ello hemos creado una vista lateral y con la funci\u00f3n explode desenrollamos el array. Resultado: +----------+------------+ | name | lugar | +----------+------------+ | Michael | Montreal | | Michael | Toronto | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+------------+ En el caso de la estructura con el sexo y la edad podemos realizar las siguientes consultas Todos los elementos Elementos individuales Todas las estructuras de sexo/edad: select sex_age from empleados_interna ; Resultado: +----------------------------+ | sex_age | +----------------------------+ | { \"sex\" : \"Male\" , \"age\" : 30 } | | { \"sex\" : \"Male\" , \"age\" : 35 } | | { \"sex\" : \"Female\" , \"age\" : 27 } | | { \"sex\" : \"Female\" , \"age\" : 57 } | +----------------------------+ Sexo y edad por separado select sex_age . sex as sexo , sex_age . age as edad from empleados_interna ; Resultado: +---------+-------+ | sexo | edad | +---------+-------+ | Male | 30 | | Male | 35 | | Female | 27 | | Female | 57 | +---------+-------+ Respecto al mapa con las habilidades y sus puntuaciones: Todos los elementos Elementos individuales Claves y valores Todas las habilidades como un mapa: select skills_score from empleados_interna ; Resultado: +-----------------------+ | skills_score | +-----------------------+ | { \"DB\" : 80 } | | { \"Perl\" : 85 } | | { \"Python\" : 80 } | | { \"Sales\" : 89 , \"HR\" : 94 } | +-----------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , skills_score [ \"DB\" ] as db , skills_score [ \"Perl\" ] as perl , skills_score [ \"Python\" ] as python , skills_score [ \"Sales\" ] as ventas , skills_score [ \"HR\" ] as hr from empleados_interna ; Resultado: +----------+-------+-------+---------+---------+-------+ | name | db | perl | python | ventas | hr | +----------+-------+-------+---------+---------+-------+ | Michael | 80 | NULL | NULL | NULL | NULL | | Will | NULL | 85 | NULL | NULL | NULL | | Shelley | NULL | NULL | 80 | NULL | NULL | | Lucy | NULL | NULL | NULL | 89 | 94 | +----------+-------+-------+---------+---------+-------+ Claves y valores de las habilidades: select name , map_keys ( skills_score ) as claves , map_values ( skills_score ) as valores from empleados_interna ; Resultado: +----------+-----------------+----------+ | name | claves | valores | +----------+-----------------+----------+ | Michael | [ \"DB\" ] | [ 80 ] | | Will | [ \"Perl\" ] | [ 85 ] | | Shelley | [ \"Python\" ] | [ 80 ] | | Lucy | [ \"Sales\" , \"HR\" ] | [ 89 , 94 ] | +----------+-----------------+----------+ Y finalmente, con el mapa de departamentos que contiene un array: Todos los elementos Elementos individuales Primer elemento de los elementos individuales Toda la informaci\u00f3n sobre los departamentos: select depart_title from empleados_interna ; Resultado: +----------------------------------------+ | depart_title | +----------------------------------------+ | { \"Product\" : [ \"Developer\" , \"Lead\" ]} | | { \"Product\" : [ \"Lead\" ], \"Test\" : [ \"Lead\" ]} | | { \"Test\" : [ \"Lead\" ], \"COE\" : [ \"Architect\" ]} | | { \"Sales\" : [ \"Lead\" ]} | +----------------------------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , depart_title [ \"Product\" ] as product , depart_title [ \"Test\" ] as test , depart_title [ \"COE\" ] as coe , depart_title [ \"Sales\" ] as sales from empleados_interna ; Resultado: +----------+-----------------------+-----------+----------------+-----------+ | name | product | test | coe | sales | +----------+-----------------------+-----------+----------------+-----------+ | Michael | [ \"Developer\" , \"Lead\" ] | NULL | NULL | NULL | | Will | [ \"Lead\" ] | [ \"Lead\" ] | NULL | NULL | | Shelley | NULL | [ \"Lead\" ] | [ \"Architect\" ] | NULL | | Lucy | NULL | NULL | NULL | [ \"Lead\" ] | +----------+-----------------------+-----------+----------------+-----------+ Primera habilidad de producto y pruebas de cada empleado: select name , depart_title [ \"Product\" ][ 0 ] as product0 , depart_title [ \"Test\" ][ 0 ] as test0 from empleados_interna ; Resultado: +----------+-------------+--------+ | name | product0 | test0 | +----------+-------------+--------+ | Michael | Developer | NULL | | Will | Lead | Lead | | Shelley | NULL | Lead | | Lucy | NULL | NULL | +----------+-------------+--------+","title":"Consultando datos compuestos"},{"location":"hadoop/06hive.html#caso-de-uso-5-tabla-externa","text":"En este caso de uso vamos a repetir la misma estructura de la tabla del caso anterior, pero en esta ocasi\u00f3n en una tabla externa. De esta manera, al borrar la tabla de Hive , no se borra la informaci\u00f3n de HDFS. Para ello, \u00fanicamente hemos de a\u00f1adir la palabra EXTERNAL a la instrucci\u00f3n CREATE TABLE y la clausula LOCATION para indicar la ruta de HDFS donde se encuentran los datos: CREATE EXTERNAL TABLE IF NOT EXISTS empleados_externa ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT \"Esto es una tabla externa\" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LOCATION \"/user/iabd/hive/empleados_externa\" ; Realizamos la misma carga que en el caso anterior: LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_externa ; Si hacemos una consulta sobre la tabla para ver que est\u00e1n todos los campos obtendremos la misma informaci\u00f3n que antes: SELECT * FROM empleados_externa ; Interna o externa Como normal general, si todo nuestro procesamiento lo hacemos mediante Hive , es m\u00e1s c\u00f3modo utilizar tablas internas. Si no es as\u00ed, y otras herramientas acceden al mismo dataset, es mejor utilizar tablas externas. Un patr\u00f3n de uso muy com\u00fan es utilizar una tabla externa para acceder al dataset inicial almacenado en HDFS (creado por otro proceso), y posteriormente crear una transformaci\u00f3n en Hive para mover los datos a una tabla interna.","title":"Caso de uso 5: Tabla externa"},{"location":"hadoop/06hive.html#estructuras-de-datos-en-hive","text":"Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: tablas, particiones y buckets . Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Acabamos de ver en el apartado anterior que Hive permite crear tablas externas, similares a las tablas en una base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS.","title":"Estructuras de datos en Hive"},{"location":"hadoop/06hive.html#particiones","text":"Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento cuando utilizamos consultas que filtran los datos mediante la cl\u00e1usula where . Por ejemplo, si estamos almacenado ficheros de log (tanto la l\u00ednea del log como su timestamp ), podemos pensar en agrupar por fecha los diferentes ficheros. Podr\u00edamos a\u00f1adir otra partici\u00f3n para tambi\u00e9n dividirlos por pa\u00edses: CREATE TABLE logs ( ts BIGINT , linea STRING ) PARTITIONED BY ( fecha STRING , pais STRING ); Por ejemplo, para cargar los datos en una partici\u00f3n: LOAD DATA LOCAL INPATH 'input/hive/particiones/log1' INTO TABLE logs PARTITION ( fecha = '2022-01-01' , pais = 'ES' ); A nivel del sistema de fichero, las particiones se traducen en subdirectorios dentro de la carpeta de la tabla. Por ejemplo, tras insertar varios ficheros de logs, podr\u00edamos tener una estructura similar a: /user/hive/warehouse/logs \u251c\u2500\u2500 fecha=2022-01-01/ \u2502 \u251c\u2500\u2500 pais=ES/ \u2502 \u2502 \u251c\u2500\u2500 log1 \u2502 \u2502 \u2514\u2500\u2500 log2 \u2502 \u2514\u2500\u2500 pais=US/ \u2502 \u2514\u2500\u2500 log3 \u2514\u2500\u2500 fecha=2022-01-02/ \u251c\u2500\u2500 pais=ES/ \u2502 \u2514\u2500\u2500 log4 \u2514\u2500\u2500 pais=US/ \u251c\u2500\u2500 log5 \u2514\u2500\u2500 log6 Para averiguar las particiones en Hive , utilizaremos el comando SHOW PARTITIONS : hive> SHOW PARTITIONS logs; fecha=2022-01-01/pais=ES fecha=2022-01-01/pais=US fecha=2022-01-02/pais=ES fecha=2022-01-02/pais=US Hay que tener en cuenta que la definici\u00f3n de columnas de la clausula PARTITIONED BY forman parte de las columnas de la tabla, y se conocen como columnas de partici\u00f3n . Sin embargo, los ficheros de datos no contienen valores para esas columnas, ya que se deriva el nombre del subdirectorio. Podemos utilizar las columnas de partici\u00f3n en las consultas igual que una columna ordinaria. Hive traduce la consulta en la navegaci\u00f3n adecuada para s\u00f3lo escanear las particiones relevantes. Por ejemplo, la siguiente consulta solo escanear\u00e1 los ficheros log1 , log2 y log4 : SELECT ts , fecha , linea FROM logs WHERE pais = 'ES' ; Moviendo datos a una tabla particionada Si queremos mover datos de una tabla ordinaria a una particionada (vaciando los datos de la partici\u00f3n existente): INSERT OVERWRITE TABLE logs PARTITION ( dt = '2022-01-01' ) SELECT col1 , col2 FROM fuente ; Otra posibilidad es utilizar un particionado din\u00e1mico , de manera que las particiones se crean de forma relativa a los datos: INSERT OVERWRITE TABLE logs PARTITION ( dt ) SELECT col1 , col2 FROM fuente ; Para ello, previamente hay que habilitarlo (por defecto est\u00e1 deshabilitado para evitar la creaci\u00f3n de m\u00faltiples particiones sin querer) y configurar el modo no estricto para que no nos obligue a indicar al menos una partici\u00f3n est\u00e1tica: set hive.exec.dynamic.partition = true set hive.exec.dynamic.partition.mode = nonstrict ;","title":"Particiones"},{"location":"hadoop/06hive.html#buckets","text":"Otro concepto importante en Hive son los buckets . Son particiones hasheadas por una columna/clave, en las que los datos se distribuyen en funci\u00f3n de su valor hash . Existen dos razones por las cuales queramos organizar las tablas (o particiones) en buckets . La primera es para conseguir consultas m\u00e1s eficientes, ya que imponen una estructura extra en las tablas. Los buckets pueden acelerar las operaciones de tipo join si las claves de bucketing y de join coinciden, ya que una clave ajena busca \u00fanicamente en el bucket adecuado de la clave primaria. Debido a los beneficios de los buckets/particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. Para indicar que nuestras tablas utilicen buckets , hemos de emplear la clausula CLUSTERED BY para indicar la columnas y el n\u00famero de buckets (se recomienda que la cantidad de buckets sea potencia de 2): CREATE TABLE usuarios_bucketed ( id INT , nombre STRING ) CLUSTERED BY ( id ) INTO 4 BUCKETS ; Bucketing est\u00e1 muy relacionado con el proceso de carga de datos. Para cargar los datos en una tabla con buckets , debemos bien indicar el n\u00famero m\u00e1ximo de reducers para que coincida con el n\u00famero de buckets , o habilitar el bucketing (esta es la recomendada): set map . reduce . tasks = 4 ; set hive . enforce . bucketing = true ; -- mejor as\u00ed Una vez creada la tabla, se rellena con los datos que tenemos en otra tabla: INSERT OVERWRITE TABLE usuarios_bucketed SELECT * FROM usuarios ; F\u00edsicamente, cada bucket es un fichero de la carpeta con la tabla (o partici\u00f3n). El nombre del fichero no es importante, pero el bucket n es el fichero n\u00famero n . Por ejemplo, si miramos el contenido de la tabla en HDFS tendremos: hive> dfs -ls /user/hive/warehouse/usuarios_bucketed ; 000000_0 000001_0 000002_0 000003_0 El segundo motivo es para obtener un sampling de forma m\u00e1s eficiente. Al trabajar con grandes datasets, normalmente obtenemos una peque\u00f1a fracci\u00f3n del dataset para comprender o refinar los datos. Por ejemplo, podemos obtener los datos de \u00fanicamente uno de los buckets : SELECT * FROM usuarios_bucketed TABLESAMPLE ( BUCKET 1 OUT OF 4 ON id ); Mediante el id del usuario determinamos el bucket (el cual se utiliza para realizar el hash del valor y ubicarlo dentro de uno de los buckets ), de manera que cada bucket contendr\u00e1 de manera eficiente un conjunto aleatorio de usuarios.","title":"Buckets"},{"location":"hadoop/06hive.html#resumen","text":"A continuaci\u00f3n mostramos en una tabla puntos a favor y en contra de utilizar estas estructuras Particionado + Particionado - Bucketing + Bucketing - Distribuye la carga de ejecuci\u00f3n horizontalmente. Existe la posibilidad de crear demasiadas particiones que contienen muy poco datos Proporciona una respuesta de consulta m\u00e1s r\u00e1pida, al acceder a porciones. El n\u00famero de buckets se define durante la creaci\u00f3n de la tabla -> Los programadores deben cargar manualmente un volumen equilibrado de datos. En la partici\u00f3n tiene lugar la ejecuci\u00f3n m\u00e1s r\u00e1pida de consultas con el volumen de datos bajo. Por ejemplo, la poblaci\u00f3n de b\u00fasqueda de la Ciudad del Vaticano devuelve muy r\u00e1pido en lugar de buscar la poblaci\u00f3n mundial completa. La partici\u00f3n es eficaz para datos de bajo volumen. Pero algunas consultas como agrupar por un gran volumen de datos tardan mucho en ejecutarse. Por ejemplo, agrupar las consultas del mes de Enero tardar\u00e1 m\u00e1s que los viernes de Enero. Al utilizar vol\u00famenes similares de datos en cada partici\u00f3n, los map joins ser\u00e1n m\u00e1s r\u00e1pidos. Supongamos que $HDFS_HIVE contiene la ruta con la ra\u00edz de las tablas internas de Hive, en nuestro caso /user/hive/warehouse . Respecto al nivel de estructura y representaci\u00f3n en carpetas de HDFS tendr\u00edamos: ENTIDAD EJEMPLO UBICACI\u00d3N base de datos iabd $HDFS_HIVE/iabd.db tabla T $HDFS_HIVE/iabd.db/T partici\u00f3n fecha='01012022' $HDFS_HIVE/iabd.db/T/fecha=01012022 bucket columna id $HDFS_HIVE/iabd.db/T/fecha=01012022/000000_0","title":"Resumen"},{"location":"hadoop/06hive.html#caso-de-uso-6-particionado-y-bucketing","text":"A continuaci\u00f3n, vamos a coger los datos de las productos y las categor\u00edas de la base de datos retail_db , y colocarlos en una estructura de Hive particionada y que utilice bucketing . La estructura de las tablas es la siguiente: Relaci\u00f3n entre productos y categor\u00edas El primer paso es traernos los datos de MariaDB a tablas internas haciendo uso de Sqoop : Tabla categories Tabla products sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = categories --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table categories sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = products --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table products El siguiente paso que vamos a realizar es crear en Hive una tabla con el c\u00f3digo del producto, su nombre, el nombre de la categor\u00eda y el precio del producto. Estos datos los vamos a particionar por categor\u00eda y clusterizado en 8 buckets: CREATE TABLE IF NOT EXISTS productos ( id INT , nombre STRING , precio DOUBLE ) PARTITIONED BY ( categoria STRING ) CLUSTERED BY ( id ) INTO 8 BUCKETS ; Y cargamos los datos con una consulta que realice un join de las tablas categories y products con particionado din\u00e1mico (recuerda activarlo mediante set hive.exec.dynamic.partition.mode=nonstrict; ): INSERT OVERWRITE TABLE productos PARTITION ( categoria ) SELECT p . product_id as id , p . product_name as nombre , p . product_price as precio , c . category_name as categoria FROM products p join categories c on ( p . product_category_id = c . category_id ); Si queremos comprobar como se han creado las particiones y los buckets, desde un terminal podemos acceder a HDFS y mostrar su contenido: hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos/categoria = Accessories Si volvemos a Hive , ahora podemos consultar los datos: select * from productos limit 5 ; Y vemos c\u00f3mo aparecen 5 elementos que pertenecen a la primera partici\u00f3n. Si quisi\u00e9ramos, por ejemplo, 10 elementos de particiones diferentes deber\u00edamos ordenarlos de manera aleatoria: select * from productos order by rand () limit 10 ; A continuaci\u00f3n vamos a realizar diversas consultas utilizando las funciones ventana que soporta Hive . M\u00e1s informaci\u00f3n en https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics .","title":"Caso de uso 6: Particionado y Bucketing"},{"location":"hadoop/06hive.html#consultas-con-enteros-que-cuentanordenan","text":"Consultas sobre categor\u00edas Las siguientes consultas las vamos a realizar sobre s\u00f3lo dos categor\u00edas para acotar los resultados obtenidos. Adem\u00e1s, hemos recortado el nombre del producto a 20 caracteres para facilitar la legibilidad de los resultados. Las funciones rank y dense_rank permite obtener la posici\u00f3n que ocupan los datos. Se diferencia en que rank cuenta los elementos repetidos/empatados, mientras que dense_rank no. Por ejemplo, vamos a obtener la posici\u00f3n que ocupan los productos respecto al precio agrupados por su categor\u00eda: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , rank () over ( partition by categoria order by precio desc ) as rank , dense_rank () over ( partition by categoria order by precio desc ) as denseRank from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+-------+------------+ | nombre | categoria | precio | rank | denserank | +-----------------------+--------------------+----------+-------+------------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 5 | 5 | | Diamondback Adult So | Basketball | 299.98 | 7 | 6 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | 7 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | 8 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | 9 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | 10 | | Easton S1 Youth Bat | Basketball | 179.97 | 11 | 10 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | 11 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | 12 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | 13 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | 14 | | Elevation Training M | Basketball | 79.99 | 17 | 15 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | 16 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | 17 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | 18 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | 19 | | adidas Brazuca 2014 | Basketball | 29.99 | 21 | 19 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | 20 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | 21 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 2 | 2 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | 3 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | 4 | +-----------------------+--------------------+----------+-------+------------+ La funci\u00f3n row_number permite numerar los resultados: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , row_number () over ( partition by categoria order by precio desc ) as numfila from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | numfila | +-----------------------+--------------------+----------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 6 | | Diamondback Adult So | Basketball | 299.98 | 7 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | | Easton S1 Youth Bat | Basketball | 179.97 | 12 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | | Elevation Training M | Basketball | 79.99 | 17 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | | adidas Brazuca 2014 | Basketball | 29.99 | 22 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 3 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | +-----------------------+--------------------+----------+----------+","title":"Consultas con enteros que cuentan/ordenan"},{"location":"hadoop/06hive.html#consultas-por-posicion","text":"A continuaci\u00f3n vamos a ver las funciones lead y lag . Estas funciones se encargan de obtener el valor posterior y anterior respecto a un valor. select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , lead ( precio ) over ( partition by categoria order by precio desc ) as sig , lag ( precio ) over ( partition by categoria order by precio desc ) as ant from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+---------+----------+ | nombre | categoria | precio | sig | ant | +-----------------------+--------------------+----------+---------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 999.99 | NULL | | SOLE E25 Elliptical | Basketball | 999.99 | 349.98 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 309.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 299.99 | 349.98 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | 309.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.98 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 249.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 209.99 | 299.98 | | Fitness Gear 300 lb | Basketball | 209.99 | 199.99 | 249.97 | | Quik Shade Summit SX | Basketball | 199.99 | 179.97 | 209.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | 199.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 159.99 | 179.97 | | adidas Brazuca 2014 | Basketball | 159.99 | 149.99 | 179.97 | | Quest 12' x 12' Dome | Basketball | 149.99 | 99.95 | 159.99 | | Fitbit Flex Wireless | Basketball | 99.95 | 99.0 | 149.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 79.99 | 99.95 | | Elevation Training M | Basketball | 79.99 | 69.99 | 99.0 | | MAC Sports Collapsib | Basketball | 69.99 | 59.98 | 79.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 39.99 | 69.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 29.99 | 59.98 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 29.99 | 39.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 28.0 | 29.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 21.99 | 29.99 | | Nike Women's Pro Vic | Basketball | 21.99 | NULL | 28.0 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 169.99 | NULL | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 119.99 | 169.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 0.0 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | NULL | 119.99 | +-----------------------+--------------------+----------+---------+----------+","title":"Consultas por posici\u00f3n"},{"location":"hadoop/06hive.html#consultas-de-agregacion","text":"Las funciones de agregaci\u00f3n que ya conocemos como count , sum , min y max tambi\u00e9n las podemos aplicar sobre particiones de datos y as\u00ed poder mostrar los datos agregados para cada elemento: select substr ( nombre , 1 , 20 ) as nombre , categoria , count ( precio ) over ( partition by categoria ) as cantidad , min ( precio ) over ( partition by categoria ) as menor , max ( precio ) over ( partition by categoria ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+-----------+--------+----------+ | nombre | categoria | cantidad | menor | mayor | +-----------------------+--------------------+-----------+--------+----------+ | Fitbit Flex Wireless | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Re | Basketball | 24 | 21.99 | 1799.99 | | Nike+ Fuelband SE | Basketball | 24 | 21.99 | 1799.99 | | Elevation Training M | Basketball | 24 | 21.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Girls' C | Basketball | 24 | 21.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | Easton Mako Youth Ba | Basketball | 24 | 21.99 | 1799.99 | | SOLE E25 Elliptical | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 24 | 21.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 24 | 21.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 24 | 21.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Boys' In | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult So | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Cor | Basketball | 24 | 21.99 | 1799.99 | | Quik Shade Summit SX | Basketball | 24 | 21.99 | 1799.99 | | Quest 12' x 12' Dome | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Vic | Basketball | 24 | 21.99 | 1799.99 | | Cleveland Golf Class | Bike & Skate Shop | 5 | 0.0 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | +-----------------------+--------------------+-----------+--------+----------+ Las consultas que hemos visto en este caso de uso tambi\u00e9n se conocen como funciones ventana , ya que se ejecutan sobre un subconjunto de los datos. La ventana viene dada por la partici\u00f3n o por la posici\u00f3n una vez ordenados los datos. Los posibles valores son: rows between current row and unbounded following : desde la fila actual hasta el final de la ventana/partici\u00f3n. rows between current row and N following : desde la fila actual hasta los N siguientes. rows between unbounded preceding and current row : desde el inicio de la ventana hasta la fila actual. rows between unbounded preceding and N following : desde el inicio de la ventana hasta los N siguientes. rows between unbounded preceding and unbounded following : desde el inicio de la ventana hasta el final de la ventana (caso por defecto) rows between N preceding and M following : desde N filas anteriores hasta M filas siguientes. Por ejemplo, para obtener el m\u00e1ximo precio desde la fila actual hasta el resto de la partici\u00f3n: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between current row and unbounded following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 1799.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 1799.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 1799.99 | | Elevation Training M | Basketball | 79.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 1799.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 1799.99 | | Diamondback Girls' C | Basketball | 299.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 1799.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 1799.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 309.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 69.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 299.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.98 | | Nike Women's Pro Cor | Basketball | 28.0 | 199.99 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 149.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 21.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 169.99 | +-----------------------+--------------------+----------+----------+ Si queremos comparar el precio y quedarnos con el mayor respecto al anterior y el posterior podr\u00edamos realizar la siguiente consulta: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between 1 preceding and 1 following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 99.95 | | adidas Brazuca 2014 | Basketball | 39.99 | 209.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 349.98 | | Diamondback Adult Re | Basketball | 349.98 | 349.98 | | Nike+ Fuelband SE | Basketball | 99.0 | 349.98 | | Elevation Training M | Basketball | 79.99 | 179.97 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | | adidas Brazuca 2014 | Basketball | 29.99 | 299.99 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 999.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 999.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 309.99 | | MAC Sports Collapsib | Basketball | 69.99 | 159.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 1799.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 299.98 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 199.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 149.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 169.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 179.99 | +-----------------------+--------------------+----------+----------+","title":"Consultas de agregaci\u00f3n"},{"location":"hadoop/06hive.html#referencias","text":"P\u00e1gina oficial de Hive Apache Hive Essentials - Second Edition de Dayong Du. Tutorial de Hive de TutorialsPoint.","title":"Referencias"},{"location":"hadoop/06hive.html#actividades","text":"Realiza los casos de uso del 1 al 5. En la entrega debes adjuntar un una captura de pantalla donde se vea la ejecuci\u00f3n de las diferentes instrucciones. (opcional) Realiza el caso de uso 6. (opcional) A partir de la base de datos retail_db , importa las tablas orders y order_items , en las cuales puedes obtener la cantidad de productos que contiene un pedido. Utilizando todas las tablas que ya hemos importado en los casos anteriores, crea una tabla externa en Hive llamada pedidos utilizando 8 buckets con el c\u00f3digo del cliente, que contenga: C\u00f3digo y fecha del pedido. Precio del pedido (sumando las l\u00edneas de pedido). C\u00f3digo, nombre y apellidos del cliente. Adjunta scripts y capturas de: la importaci\u00f3n, creaci\u00f3n y carga de datos de las tablas que necesites. la definici\u00f3n de la tabla: describe formatted pedidos; contenido de HDFS que demuestre la creaci\u00f3n de los buckets . (opcional) Investiga la creaci\u00f3n de vistas en Hive y crea una vista con los datos de los clientes y sus pedidos siempre y cuando superen los 200$.","title":"Actividades"},{"location":"sa/index.html","text":"Unidad de Trabajo 2.- Sistemas de almacenamiento \u00b6 Resultados de aprendizaje \u00b6 RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Planificaci\u00f3n (20h) \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Almacenamiento de datos / NoSQL Mi\u00e9rcoles 26 Oct 1p + 1o 19.- MongoDB Mi\u00e9rcoles 26 Oct Mi\u00e9rcoles 2 Nov 3p + 3o 21.- Modelado documental Lunes 7 Nov 2p + 2o 25.- Agregaciones en MongoDB Mi\u00e9rcoles 9 Nov 2p + 2o 28.- Replicaci\u00f3n y Particionado Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Unidad de Trabajo 2.- Sistemas de almacenamiento"},{"location":"sa/index.html#unidad-de-trabajo-2-sistemas-de-almacenamiento","text":"","title":"Unidad de Trabajo 2.- Sistemas de almacenamiento"},{"location":"sa/index.html#resultados-de-aprendizaje","text":"RA5074.1 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. RA5074.3 Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. RA5075.1 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. RA5075.2 Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. RA5075.3 Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos.","title":"Resultados de aprendizaje"},{"location":"sa/index.html#planificacion-20h","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 18.- Almacenamiento de datos / NoSQL Mi\u00e9rcoles 26 Oct 1p + 1o 19.- MongoDB Mi\u00e9rcoles 26 Oct Mi\u00e9rcoles 2 Nov 3p + 3o 21.- Modelado documental Lunes 7 Nov 2p + 2o 25.- Agregaciones en MongoDB Mi\u00e9rcoles 9 Nov 2p + 2o 28.- Replicaci\u00f3n y Particionado Lunes 14 Nov 2p + 2o 30.- MongoDB y Python Mi\u00e9rcoles 16 Nov 2p + 2o","title":"Planificaci\u00f3n (20h)"},{"location":"sa/01nosql.html","text":"Almacenamiento de Datos \u00b6 Se puede decir que estamos en la tercera plataforma del almacenamiento de datos. La primera lleg\u00f3 con los primeros computadores y se materializ\u00f3 en las bases de datos jer\u00e1rquicas y en red, as\u00ed como en el almacenamiento ISAM. La segunda vino de la mano de internet y las arquitecturas cliente-servidor, lo que dio lugar a las bases de datos relacionales. La tercera se ve motivada por el big data, los dispositivos m\u00f3viles, las arquitecturas cloud, las redes de IoT y las tecnolog\u00edas/redes sociales. Es tal el volumen de datos que se genera que aparecen nuevos paradigmas como NoSQL, NewSQL y las plataformas de Big Data. En esta sesi\u00f3n nos vamos a centrar en NoSQL. NoSQL aparece como una necesidad debida al creciente volumen de datos sobre usuarios, objetos y productos que las empresas tienen que almacenar, as\u00ed como la frecuencia con la que se accede a los datos. Los SGDB relacionales existentes no fueron dise\u00f1ados teniendo en cuenta la escalabilidad ni la flexibilidad necesaria por las frecuentes modificaciones que necesitan las aplicaciones modernas; tampoco aprovechan que el almacenamiento a d\u00eda de hoy es muy barato, ni el nivel de procesamiento que alcanzan las m\u00e1quinas actuales. Motivaci\u00f3n de NoSQL La soluci\u00f3n es el despliegue de las aplicaciones y sus datos en cl\u00fasteres de servidores, distribuyendo el procesamiento en m\u00faltiples m\u00e1quinas . No Solo SQL \u00b6 Si definimos NoSQL formalmente, podemos decir que se trata de un conjunto de tecnolog\u00edas que permiten el procesamiento r\u00e1pido y eficiente de conjuntos de datos dando la mayor importancia al rendimiento, la fiabilidad y la agilidad . Si nos basamos en el acr\u00f3nimo, el t\u00e9rmino se refiere a cualquier almac\u00e9n de datos que no sigue un modelo relacional, los datos no son relacionales y por tanto no utilizan SQL como lenguaje de consulta. Otra aceptaci\u00f3n implica que el No hace referencia a not only , es decir, que los sistemas NoSQL se centran en sistemas complementarios a los SGBD relacionales, que fijan sus prioridades en la escalabilidad y la disponibilidad en contra de la atomicidad y consistencia de los datos. ACID Las bases de datos relacionales cumplen las caracter\u00edsticas ACID para ofrecer transaccionalidad sobre los datos: A tomicidad: las transacciones implican que se realizan todas las operaciones o no se realiza ninguna. C onsistencia: la base de datos asegura que los datos pasan de un estado v\u00e1lido o otro tambi\u00e9n. I solation (Aislamiento): Una transacci\u00f3n no afecta a otras transacciones, de manera que la modificaci\u00f3n de un registro / documento no es visible por otras lecturas. D urabilidad: La escritura de los datos asegura que una vez finalizada una operaci\u00f3n, los datos no se perder\u00e1n. Los diferentes tipos de bases de datos NoSQL existentes se pueden agrupar en cuatro categor\u00edas: Clave-Valor : Los almacenes clave-valor son las bases de datos NoSQL m\u00e1s simples. Cada elemento de la base de datos se almacena con un nombre de atributo (o clave) junto a su valor. Los almacenes m\u00e1s conocidos son Redis , Riak y AWS DynamoDB . Algunos almacenes, como es el caso de Redis, permiten que cada valor tenga un tipo (por ejemplo, integer) lo cual a\u00f1ade funcionalidad extra. Documentales : Cada clave se asocia a una estructura compleja que se conoce como documento. Este puede contener diferentes pares clave-valor, o pares de clave-array o incluso documentos anidados, como en un documento JSON. Los ejemplos m\u00e1s conocidos son MongoDB y CouchDB . Grafos : Los almacenes de grafos se usan para almacenar informaci\u00f3n sobre redes, como pueden ser conexiones sociales. Los ejemplos m\u00e1s conocidos son Neo4J , AWS Neptune y ArangoDB . Basados en columnas : Los almacenes basados en columnas como BigTable , Cassandra y HBase est\u00e1n optimizados para consultas sobre grandes conjuntos de datos, y almacenan los datos como columnas en vez de como filas. Sistemas NoSQL Caracter\u00edsticas \u00b6 Si nos centramos en sus beneficios y los comparamos con las base de datos relacionales, las bases de datos NoSQL son m\u00e1s escalables, ofrecen un rendimiento mayor y sus modelos de datos resuelven varios problemas que no se plantearon al definir el modelo relacional: Grandes vol\u00famenes de datos estructurados, semi-estructurados y sin estructurar. Casi todas las implementaciones NoSQL ofrecen alg\u00fan tipo de representaci\u00f3n para datos sin esquema, lo que permite comenzar con una estructura y con el paso del tiempo, a\u00f1adir nuevos campos, ya sean sencillos o anidados a datos ya existentes. Sprints \u00e1giles, iteraciones r\u00e1pidas y frecuentes commits / pushes de c\u00f3digo, al emplear una sintaxis sencilla para la realizaci\u00f3n de consultas y la posibilidad de tener un modelo que vaya creciendo al mismo ritmo que el desarrollo. Arquitectura eficiente y escalable dise\u00f1ada para trabajar con clusters en vez de una arquitectura monol\u00edtica y costosa. Las soluciones NoSQL soportan la escalabilidad de un modo transparente para el desarrollador. Una caracter\u00edstica adicional que comparten los sistemas NoSQL es que ofrecen un mecanismo de cach\u00e9 de datos integrado (en los sistemas relacionales se pueden configurar de manera externa), de manera que se pueden configurar los sistemas para que los datos se mantengan en memoria y se persistan de manera peri\u00f3dica. El uso de una cach\u00e9 conlleva que la consistencia de los datos no sea completa y podamos tener una consistencia eventual. Esquema din\u00e1micos \u00b6 Las bases de datos relacionales requieren definir los esquemas antes de a\u00f1adir los datos. Una base de datos SQL necesita saber de antemano los datos que vamos a almacenar; por ejemplo, si nos centramos en los datos de un cliente, ser\u00edan el nombre, apellidos, n\u00famero de tel\u00e9fono, etc\u2026\u200b Esto casa bastante mal con los enfoques de desarrollo \u00e1gil, ya que cada vez que a\u00f1adimos nuevas funcionalidades, el esquema de la base de datos suele cambiar. De modo que si a mitad de desarrollo decidimos almacenar los productos favoritos de un cliente del cual guard\u00e1bamos su direcci\u00f3n y n\u00fameros de tel\u00e9fono, tendr\u00edamos que a\u00f1adir una nueva columna a la base de datos y migrar la base de datos entera a un nuevo esquema. Si la base de datos es grande, conlleva un proceso lento que implica parar el sistema durante un tiempo considerable. Si frecuentemente cambiamos los datos que la aplicaci\u00f3n almacena (al usar un desarrollo iterativo), tambi\u00e9n tendremos per\u00edodos frecuentes de inactividad del sistema. As\u00ed pues, no hay un modo efectivo mediante una base de datos relacional, de almacenar los datos que est\u00e1n desestructurados o que no se conocen de antemano. Las bases de datos NoSQL se construyen para permitir la inserci\u00f3n de datos sin un esquema predefinido. Esto facilita la modificaci\u00f3n de la aplicaci\u00f3n en tiempo real, sin preocuparse por interrupciones de servicio. Aunque no tengamos un esquema al guardar la informaci\u00f3n, s\u00ed que podemos definir esquemas de lectura ( schema-on-read ) para comprobar que la informaci\u00f3n almacenada tiene el formato que espera cargar cada aplicaci\u00f3n. De este modo se consigue un desarrollo m\u00e1s r\u00e1pido, integraci\u00f3n de c\u00f3digo m\u00e1s robusto y menos tiempo empleado en la administraci\u00f3n de la base de datos. Particionado \u00b6 Dado el modo en el que se estructuran las bases de datos relacionales, normalmente escalan verticalmente - un \u00fanico servidor que almacena toda la base de datos para asegurar la disponibilidad continua de los datos. Esto se traduce en costes que se incrementan r\u00e1pidamente, con un l\u00edmites definidos por el propio hardware, y en un peque\u00f1o n\u00famero de puntos cr\u00edticos de fallo dentro de la infraestructura de datos. La soluci\u00f3n es escalar horizontalmente, a\u00f1adiendo nuevos servidores en vez de concentrarse en incrementar la capacidad de un \u00fanico servidor, lo que permite tratar con conjuntos de datos m\u00e1s grandes de lo que ser\u00eda capaz cualquier m\u00e1quina por s\u00ed sola. Este escalado horizontal se conoce como Sharding o Particionado. El particionado no es \u00fanico de las bases de datos NoSQL. Las bases de datos relacionales tambi\u00e9n lo soportan. Si en un sistema relacional queremos particionar los datos, podemos distinguir entre particionado: Horizontal : diferentes filas en diferentes particiones. Vertical : diferentes columnas en particiones distintas. Particionado de los datos - digitalocean.com En el caso de las bases de datos NoSQL, el particionado depende del modelo de la base de datos: Los almacenes clave-valor y las bases de datos documentales normalmente se particionan horizontalmente. Las bases de datos basados en columnas se pueden particionar horizontal o verticalmente. Escalar horizontalmente una base de datos relacional entre muchas instancias de servidores se puede conseguir pero normalmente conlleva el uso de SANs ( Storage Area Networks ) y otras triqui\u00f1uelas para hacer que el hardware act\u00fae como un \u00fanico servidor. Como los sistemas SQL no ofrecen esta prestaci\u00f3n de forma nativa, los equipos de desarrollo se las tienen que ingeniar para conseguir desplegar m\u00faltiples bases de datos relacionales en varias m\u00e1quinas. Para ello: Los datos se almacenan en cada instancia de base de datos de manera aut\u00f3noma El c\u00f3digo de aplicaci\u00f3n se desarrolla para distribuir los datos y las consultas y agregar los resultados de los datos a trav\u00e9s de todas las instancias de bases de datos Se debe desarrollar c\u00f3digo adicional para gestionar los fallos sobre los recursos, para realizar joins entre diferentes bases de datos, balancear los datos y/o replicarlos, etc\u2026\u200b Adem\u00e1s, muchos beneficios de las bases de datos como la integridad transaccional se ven comprometidos o incluso eliminados al emplear un escalado horizontal. Auto-sharding \u00b6 Por contra, las bases de datos NoSQL normalmente soportan auto-sharding , lo que implica que de manera nativa y autom\u00e1ticamente se dividen los datos entre un n\u00famero arbitrario de servidores, sin que la aplicaci\u00f3n sea consciente de la composici\u00f3n del pool de servidores. Los datos y las consultas se balancean entre los servidores. El particionado se realiza mediante un m\u00e9todo consistente, como puede ser: Por rangos de su id: por ejemplo \"los usuarios del 1 al mill\u00f3n est\u00e1n en la partici\u00f3n 1\" o \"los usuarios cuyo nombre va de la A a la E\" en una partici\u00f3n, en otra de la M a la Q, y de la R a la Z en la tercera. Particionado por rango - digitalocean.com Por listas : dividiendo los datos por la categor\u00eda del dato, es decir, en el caso de datos sobre libros, las novelas en una partici\u00f3n, las recetas de cocina en otra, etc.. Mediante un funci\u00f3n hash , la cual devuelve un valor para un elemento que determine a que partici\u00f3n pertenece. Particionado por hash - digitalocean.com Cuando particionar \u00b6 El motivo para particionar los datos se debe a: limitaciones de almacenamiento: los datos no caben en un \u00fanico servidor, tanto a nivel de disco como de memoria RAM. rendimiento: al balancear la carga entre particiones las escrituras ser\u00e1n m\u00e1s r\u00e1pidas que al centrarlas en un \u00fanico servidor. disponibilidad: si un servidor esta ocupado, otro servidor puede devolver los datos. La carga de los servidores se reduce. No particionaremos los datos cuando la cantidad sea peque\u00f1a, ya que el hecho de distribuir los datos conlleva unos costes que pueden no compensar con un volumen de datos insuficiente. Tampoco esperaremos a particionar cuando tengamos much\u00edsimos datos, ya que el proceso de particionado puede provocar sobrecarga del sistema. La nube facilita de manera considerable este escalado, mediante proveedores como AWS o Azure los cuales ofrecen virtualmente una capacidad ilimitada bajo demanda, y despreocup\u00e1ndose de todas las tareas necesarias para la administraci\u00f3n de la base de datos. Los desarrolladores ya no necesitamos construir plataformas complejas para nuestras aplicaciones, de modo que nos podemos centrar en escribir c\u00f3digo de aplicaci\u00f3n. Una granja de servidores con commodity hardware puede ofrecer el mismo procesamiento y capacidad de almacenamiento que un \u00fanico servidor de alto rendimiento por mucho menos coste. Replicaci\u00f3n \u00b6 La replicaci\u00f3n mantiene copias id\u00e9nticas de los datos en m\u00faltiples servidores, lo que facilita que las aplicaciones siempre funcionen y los datos se mantengan seguros, incluso si alguno de los servidores sufre alg\u00fan problema. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n soportan la replicaci\u00f3n autom\u00e1tica, lo que implica una alta disponibilidad y recuperaci\u00f3n frente a desastres sin la necesidad de aplicaciones de terceros encargadas de ello. Desde el punto de vista del desarrollador, el entorno de almacenamiento es virtual y ajeno al c\u00f3digo de aplicaci\u00f3n. Existen dos formas de realiza la replicaci\u00f3n: Maestro-esclavo / Primario-secundario \u00b6 Todas las escrituras se realizan en el nodo principal y despu\u00e9s se replican a los nodos secundarios. El nodo primario es un SPOF ( single point of failure ). Replicaci\u00f3n primario-secundario Par-a-par ( peer-to-peer ) \u00b6 Todos los nodos tienen el mismo nivel jer\u00e1rquico, de manera que todos admiten escrituras. Al poder haber escrituras simult\u00e1neas sobre el mismo datos en diferentes nodos, pueden darse inconsistencia en los datos. Replicaci\u00f3n Peer-to-peer La replicaci\u00f3n de los datos se utiliza para alcanzar: escalabilidad , incrementando el rendimiento al poder distribuir las consultas en diferentes nodos, y mejorar la redundancia al permitir que cada nodo tenga una copia de los datos. disponibilidad , ofreciendo tolerancia a fallos de hardware o corrupci\u00f3n de la base de datos. Al replicar los datos vamos a poder tener una copia de la base de datos, dar soporte a un servidor de datos agregados, o tener nodos a modo de copias de seguridad que pueden tomar el control en caso de fallo. aislamiento (la i en ACID - isolation ), entendido como la propiedad que define cuando y c\u00f3mo al realizar cambios en un nodo se propagan al resto de nodos. Si replicamos los datos podemos crear copias sincronizadas para separar procesos de la base de datos de producci\u00f3n, pudiendo ejecutar informes, anal\u00edtica de datos o copias de seguridad en nodos secundarios de modo que no tenga un impacto negativo en el nodo principal, as\u00ed como ofrecer un sistema sencillo para separar el entorno de producci\u00f3n del de preproducci\u00f3n. Replicaci\u00f3n vs particionado No hay que confundir la replicaci\u00f3n (copia de los datos en varias m\u00e1quinas) con el particionado (cada m\u00e1quina tiene un subconjunto de los datos). El entorno m\u00e1s seguro y con mejor rendimiento es aquel que tiene los datos particionados y replicados (cada m\u00e1quina que tiene un subconjunto de los datos est\u00e1 replicada en 2 o m\u00e1s). Replicaci\u00f3n y particionado - codingexplained.com/figcaption> Implantando NoSQL \u00b6 Normalmente, las empresas empezar\u00e1n con una prueba de baja escalabilidad de una base de datos NoSQL, de modo que les permita comprender la tecnolog\u00eda asumiendo muy poco riesgo. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n son open-source, y por tanto se pueden probar sin ning\u00fan coste extra. Al tener unos ciclos de desarrollo m\u00e1s r\u00e1pidos, las empresas pueden innovar con mayor velocidad y mejorar la experiencia de sus cliente a un menor coste. Elegir la base de datos correcta para el proyecto es un tema importante. Se deben considerar las diferentes alternativas a las infraestructuras legacy teniendo en cuenta varios factores: la escalabilidad o el rendimiento m\u00e1s all\u00e1 de las capacidades del sistema existente. identificar alternativas viables respecto al software propietario. incrementar la velocidad y agilidad del proceso de desarrollo. As\u00ed pues, al elegir un base de datos hemos de tener en cuenta las siguientes dimensiones: modelo de datos: A elegir entre un modelo documental, basado en columnas, de grafos o mediante clave-valor. modelo de consultas: Dependiendo de la aplicaci\u00f3n, puede ser aceptable un modelo de consultas que s\u00f3lo accede a los registros por su clave primaria. En cambio, otras aplicaciones pueden necesitar consultar por diferentes valores de cada registro. Adem\u00e1s, si la aplicaci\u00f3n necesita modificar los registros, la base de datos necesita consultar los datos por un \u00edndice secundario. modelo de consistencia: Los sistemas NoSQL normalmente mantienen m\u00faltiples copias de los datos para ofrecer disponibilidad y escalabilidad al sistema, lo que define la consistencia del mismo. Los sistemas NoSQL tienden a ser consistentes o eventualmente consistentes. APIs: No existe un est\u00e1ndar para interactuar con los sistemas NoSQL. Cada sistema presenta diferentes dise\u00f1os y capacidades para los equipos de desarrollo. La madurez de un API puede suponer una inversi\u00f3n en tiempo y dinero a la hora de desarrollar y mantener el sistema NoSQL. soporte comercial y de la comunidad: Los usuarios deben considerar la salud de la compa\u00f1ia o de los proyectos al evaluar una base de datos. El producto debe evolucionar y mantenerse para introducir nuevas prestaciones y corregir fallos. Una base de datos con una comunidad fuerte de usuarios: permite encontrar y contratar desarrolladores con destrezas en el producto. facilita encontrar informaci\u00f3n, documentaci\u00f3n y ejemplos de c\u00f3digo. ayuda a las empresas a retener el talento. favorece que otras empresas de software integren sus productos y participen en el ecosistema de la base de datos. Casos de uso \u00b6 Una vez conocemos los diferentes sistemas y qu\u00e9 elementos puede hacer que nos decidamos por una soluci\u00f3n u otra, conviene repasar los casos de uso m\u00e1s comunes: si vamos a crear una aplicaci\u00f3n web cuyo campos sean personalizables, usaremos una soluci\u00f3n documental. como una capa de cach\u00e9, mediante un almac\u00e9n clave-valor. para almacenar archivos binarios sin preocuparse de la gesti\u00f3n de permisos del sistema de archivos, y poder realizar consultas sobre sus metadatos, ya sea mediante una soluci\u00f3n documental o un almac\u00e9n clave-valor. para almacenar un enorme volumen de datos, donde la consistencia no es lo m\u00e1s importante, pero si la disponibilidad y su capacidad de ser distribuida, mediante una soluci\u00f3n documental o basada en columnas. Modelos de Datos \u00b6 La principal clasificaci\u00f3n de los sistemas de bases de datos NoSQL se realiza respecto a los diferentes modelos de datos: Documental \u00b6 Mientras las bases de datos relacionales almacenan los datos en filas y columnas, las bases de datos documentales emplean documentos. Estos documentos utilizan una estructura JSON, ofreciendo un modo natural e intuitivo para modelar datos de manera similar a la orientaci\u00f3n a objetos, donde cada documento es un objeto. Representaci\u00f3n de un documento Los documentos se agrupan en colecciones o bases de datos, dependiendo del sistema, lo que permite agrupar documentos. Los documentos contienen uno o m\u00e1s campos, donde cada campo contiene un valor con un tipo, ya sea cadena, entero, flotante, fecha, binario o array u otro documento. En vez de extender los datos entre m\u00faltiples columnas y tablas, cada registro y sus datos asociados se almacenan de manera unida en un \u00fanico documento. Esto simplifica el acceso a los datos y reduce (y en ocasiones elimina) la necesidad de joins y transacciones complejas. Dicho de otra manera, en las bases de datos documentales, los datos que van juntos y se emplean juntos, se almacenan juntos. Caracter\u00edsticas \u00b6 En una base de datos documental, la noci\u00f3n de esquema es din\u00e1mico: cada documento puede contener diferentes campos. Esta flexibilidad puede ser \u00fatil para modelar datos desestructurados y polim\u00f3rficos, lo que facilita la evoluci\u00f3n del desarrollo al permitir a\u00f1adir nuevos campos de manera din\u00e1mica. Perfectamente podemos tener dos documentos que pertenecen a la misma colecci\u00f3n, pero con atributos diferentes. Por ejemplo, un primer documento puede ser el siguiente: { \"_id\" : \"BW001\" , \"nombre\" : \"Bruce\" , \"apellido\" : \"Wayne\" , \"edad\" : 35 , \"salario\" : 10000000 } Mientras que un segundo documento dentro de la misma colecci\u00f3n podr\u00eda ser: { \"_id\" : \"JK1\" , \"nombre\" : \"Joker\" , \"edad\" : 34 , \"salario\" : 5000000 , \"direccion\" : { // (1)! \"calle\" : \"Asilo Arkham\" , \"ciudad\" : \"Gotham\" }, \"proyectos\" : [ // (2)! \"desintoxicacion-virus\" , \"top-secret-007\" ] } Un objeto o subdocumento permite agrupar informaci\u00f3n similar a una relaci\u00f3n 1:1 de un modelo relacional. De esta manera, no necesitamos una tabla Direccion . Un array puede contener valores o documentos, de manera que podr\u00edamos tener un array de documentos, permitiendo agrupar informaci\u00f3n similar a una relaci\u00f3n 1:N de un modelo relacional. De esta manera, no necesitamos una tabla Proyectos . Normalmente, cada documento contiene un elemento clave, sobre el cual se puede obtener un documento de manera un\u00edvoca. De todos modos, las bases de datos documentales ofrecen un completo mecanismo de consultas, posibilitando obtener informaci\u00f3n por cualquier campo del documento. Algunos productos ofrecen opciones de indexado para optimizar las consultas, como pueden ser \u00edndices compuestos, dispersos, con tiempo de vida (TTL), \u00fanicos, de texto o geoespaciales. Adem\u00e1s, estos sistemas ofrecen productos que permiten analizar los datos, mediante funciones de agregaci\u00f3n o implementaci\u00f3n de MapReduce. Respecto a la modificaciones, los documentos se pueden actualizar en una \u00fanica sentencia, sin necesidad de dar rodeos para elegir los datos a modificar. Casos de uso \u00b6 Las bases de datos documentales sirven para prop\u00f3sito general, v\u00e1lidos para un amplio abanico de aplicaciones gracias a la flexibilidad que ofrece el modelo de datos, lo que permite consultar cualquier campo y modelar de manera natural de manera similar a la programaci\u00f3n orientada a objetos. Entre los casos de \u00e9xito de estos sistemas cabe destacar: Sistemas de flujo de eventos: entre diferentes aplicaciones dentro de una empresa Gestores de Contenido, plataformas de Blogging: al almacenar los documentos mediante JSON, facilita la estructura de datos para guardar los comentarios, registros de usuarios, etc\u2026\u200b Anal\u00edticas Web, datos en Tiempo Real: al permitir modificar partes de un documento, e insertar nuevos atributos a un documento cuando se necesita una nueva m\u00e9trica Aplicaciones eCommerce: conforme las aplicaciones crecen, el esquema tambi\u00e9n lo hace Si nos centramos en aquellos casos donde no conviene este tipo de sistemas podemos destacar: Sistemas operacionales con transacciones complejas. Sistemas con consultas agregadas que modifican su estructura. Si los criterios de las consultas no paran de cambiar, acabaremos normalizando los datos. Los productos m\u00e1s destacados son: MongoDB : http://www.mongodb.com . Esta base de datos la vamos a estudiar en profundidad en esta unidad de trabajo. CouchDB : http://couchdb.apache.org Clave-Valor \u00b6 Un almac\u00e9n clave-valor es una simple tabla hash donde todos los accesos a la base de datos se realizan a trav\u00e9s de la clave primaria. Desde una perspectiva de modelo de datos, los almacenes de clave-valor son los m\u00e1s b\u00e1sicos. Su funcionamiento es similar a tener una tabla relacional con dos columnas, por ejemplo id y nombre , siendo id la columna utilizada como clave y nombre como valor. Mientras que en una base de datos en el campo nombre s\u00f3lo podemos almacenar datos de tipo cadena o num\u00e9rico, en un almac\u00e9n clave-valor, el valor puede ser de un dato simple o un objeto. Cuando una aplicaci\u00f3n accede mediante la clave y el valor, se almacenan el par de elementos. Si la clave ya existe, el valor se modifica. Representaci\u00f3n de un almac\u00e9n clave-valor El cliente puede tanto obtener el valor por la clave, asignar un valor a una clave o eliminar una clave del almac\u00e9n. El valor, sin embargo, es opaco al sistema, el cual no sabe que hay dentro de \u00e9l, ya que los datos s\u00f3lo se pueden consultar por la clave, lo cual puede ser un inconveniente. As\u00ed pues, la aplicaci\u00f3n es responsable de saber qu\u00e9 hay almacenado en cada valor. Por ejemplo, Riak utiliza el concepto de bucket (cubo) como una manera de agrupar claves, de manera similar a una tabla. Por ejemplo, Riak permite interactuar con la base de datos mediante peticiones HTTP: curl -v -X PUT <http://localhost:8091/riak/heroes/ace> -H \"Content-Type: application/json\" -d { \"nombre\" : \"Batman\" , \"color\" : \"Negro\" } Algunos almacenes clave-valor, como puede ser Redis , permiten almacenar datos con cualquier estructura, como por ejemplos listas, conjuntos, hashes y pueden realizar operaciones como intersecci\u00f3n, uni\u00f3n, diferencia y rango. Comandos Redis Python SET nombre \"Bruce Wayne\" // String HSET heroe nombre \"Batman\" // Hash \u2013 set HSET heroe color \"Negro\" SADD \"heroe:amigos\" \"Robin\" \"Alfred\" // Set \u2013 create/update import redis r = redis . Redis () r . mset ({ \"Croatia\" : \"Zagreb\" , \"Bahamas\" : \"Nassau\" }) r . get ( \"Bahamas\" ) # b'Nassau' Estas prestaciones hacen que Redis se extrapole a \u00e1mbitos ajenos a un almac\u00e9n clave-valor. Otra caracter\u00edstica que ofrecen algunos almacenes es que permiten crear un segundo nivel de consulta o incluso definir m\u00e1s de una clave para un mismo objeto. Como los almacenes clave-valor siempre utilizan accesos por clave primaria, de manera general tienen un gran rendimiento y son f\u00e1cilmente escalables. Si queremos que su rendimiento sea m\u00e1ximo, pueden configurarse para que mantengan la informaci\u00f3n en memoria y que se serialice de manera peri\u00f3dica, a costa de tener una consistencia eventual de los datos. Casos de uso \u00b6 Este modelo es muy \u00fatil para representar datos desestructurados o polim\u00f3rficos, ya que no fuerzan ning\u00fan esquema m\u00e1s all\u00e1 de los pares de clave-valor. Entre los casos de uso de estos almacenes podemos destacar el almacenaje de: Informaci\u00f3n sobre la sesi\u00f3n de navegaci\u00f3n ( sessionid ) Perfiles de usuario, preferencias Datos del carrito de la compra Cachear datos Todas estas operaciones van a asociada a operaciones de recuperaci\u00f3n, modificaci\u00f3n o inserci\u00f3n de los datos de una sola vez, de ah\u00ed su elecci\u00f3n. En cambio, no conviene utilizar estos almacenes cuando queremos realizar: Relaciones entre datos Transacciones entre varias operaciones Consultas por los datos del valor Operaciones con conjuntos de claves Los almacenes m\u00e1s empleados son: Riak : https://riak.com Redis : http://redis.io AWS DynamoDB : http://aws.amazon.com/dynamodb Voldemort : http://www.project-voldemort.com/voldemort implementaci\u00f3n open-source de Amazon DynamoDB Basado en columnas \u00b6 Las bases de datos relacionales utilizan la fila como unidad de almacenamiento, lo que permite un buen rendimiento de escritura. Sin embargo, cuando las escrituras son ocasionales y es m\u00e1s comun tener que leer unas pocas columnas de muchas filas a la vez, es mejor utilizar como unidad de almacenamiento un grupos de columnas. Es decir, lo que hacemos es girar el modelo 90 grados, de manera que los registros se almacenan en columnas en vez de hacerlo por filas. Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que permite codificarlos/comprimirlos, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas Autoevaluaci\u00f3n Si tenemos que a\u00f1adir un nuevo registro \u00bfQu\u00e9 modelo ser\u00e1 m\u00e1s eficiente? Sin embargo, a medida que se incrementa la utilizaci\u00f3n de an\u00e1lisis de datos en memoria, con soluciones como Spark , los beneficios relativos de la base de datos columnares comparados con los de las bases de datos orientadas a filas pueden llegar a ser menos importantes. Representaci\u00f3n \u00b6 Un modelo basado en columnas se representa como una estructura agregada de dos niveles. El primer nivel formado por un almac\u00e9n clave-valor, siendo la clave el identificador de la fila, y el valor un nuevo mapa con los datos agregados de la fila (familias de columnas). Los valores de este segundo nivel son las columnas. De este modo, podemos acceder a los datos de un fila, o a una determinada columna: Representaci\u00f3n de un almac\u00e9n basado en columnas BigTable Los modelos de datos basados en columnas se basan en la implementaci\u00f3n de Google de la tecnolog\u00eda BigTable ( http://research.google.com/archive/bigtable.html ), la cual consiste en columnas separadas y sin esquema, a modo de mapa de dos niveles. As\u00ed pues, los almacenes basados en columnas utilizan un mapa ordenado multi-dimensional y distribuido para almacenar los datos. Est\u00e1n pensados para que cada fila tenga una gran n\u00famero de columnas (del orden del mill\u00f3n), almacenando las diferentes versiones que tenga una fila (pudiendo almacenar del orden de miles de millones de filas). Familias de columnas \u00b6 Una columna consiste en un pareja name - value , donde el nombre hace de clave. Adem\u00e1s, contiene un atributo timestamp para poder expirar datos y resolver conflictos de escritura. Un ejemplo de columna podr\u00eda ser: { na me : \"nombre\" , value : \"Bruce\" , t imes ta mp : 12345667890 } Una fila es una colecci\u00f3n de columnas agrupadas a una clave. { { na me : \"nombre\" , value : \"Bruce\" , t imes ta mp : 12345667890 }, { na me : \"nombre\" , value : \"Clark\" , t imes ta mp : 12345667891 }, { na me : \"nombre\" , value : \"Barbara\" , t imes ta mp : 12345667892 } } Si agrupamos filas similares tendremos una familia de columnas : // familia de columnas { // fila \"tim-gordon\" : { n ombre : \"Tim\" , apellido : \"Gordon\" , ul t imaVisi ta : \"2015/12/12\" } // fila \"bruce-wayne\" : { n ombre : \"Bruce\" , apellido : \"Wayne\" , lugar : \"Gotham\" } } Con este ejemplo, podemos ver como las diferentes filas de la misma tabla (familia de columnas) no tienen por que compartir el mismo conjunto de columnas. Adem\u00e1s, las columnas se pueden anidar dentro de otras formando super-columnas , donde el valor es un nuevo mapa de columnas. { na me : \"libro:978-84-16152-08-7\" , value : { au t or : \"Grant Morrison\" , t i tul o : \"Batman - Asilo Arkham\" , isb n : \"978-84-16152-08-7\" } } Cuando se utilizan super columnas para crear familias de columnas tendremos una familia de super columnas. En resumen, las bases de datos basadas en columnas, almacenan los datos en familias de columnas como filas, las cuales tienen muchas columnas asociadas al identificador de una fila. Las familias de columnas son grupos de datos relacionados, a las cuales normalmente se accede de manera conjunta. Operaciones \u00b6 A la hora de consultar los datos, \u00e9stos se pueden obtener por la clave primaria de la familia. As\u00ed pues, podemos obtener toda una familia, o la columna de una familia: // Mediante Cassandra GET Clientes [ 'bruce-wayne' ]; // familia GET Clientes [ 'bruce-wayne' ][ 'lugar' ]; // columna Algunos productos ofrecen un soporte limitado para \u00edndices secundarios, pero con restricciones. Por ejemplo, Cassandra ofrece el lenguaje CQL similar a SQL pero sin joins, ni subconsultas donde las restricciones de where son sencillas: SELECT * FROM Clientes SELECT nombre , email FROM Clientes SELECT nombre , email FROM Clientes WHERE lugar = 'Gotham' Las actualizaciones se realizan en dos pasos: primero encontrar el registro y segundo modificarlo. En estos sistemas, una modificaci\u00f3n puede suponer una reescritura completa del registro independientemente que hayan cambiado unos pocos bytes del mismo. Casos de uso \u00b6 Las bases de datos columnares se han empleado durante d\u00e9cadas ofreciendo beneficios a las aplicaciones de negocio modernas, como la anal\u00edtica de datos, business intelligence y data warehousing . Son multiprop\u00f3sito, aunque su uso se centra en el mercado del big data, la anal\u00edtica de datos, cubos multidimensionales OLAP y/o almacenar metadatos y realizar anal\u00edtica en tiempo real. Adem\u00e1s de poder comprimir los datos, los datos est\u00e1n auto-indexados, lo que implica que utiliza menos espacio en disco, y acelera la ejecuci\u00f3n de consultas agregadas entre m\u00faltiples tablas que implica el uso de joins. En cambio, no se recomienda su uso en aplicaciones de procesamiento transaccional (OLTP), ya que las bases de datos relacionales gestionan mejor el procesamiento concurrente y el aislamiento de las operaciones. Los productos m\u00e1s destacados son: HBase : http://hbase.apache.org , el cual se basa en Hadoop - http://hadoop.apache.org Cassandra : http://cassandra.apache.org Amazon Redshift : https://aws.amazon.com/es/redshift/ Grafos \u00b6 Las bases de datos de grafos almacenan entidades y las relaciones entre estas entidades. Las entidades se conocen como nodos, los cuales tienen propiedades. Cada nodo es similar a una instancia de un objeto. Las relaciones, tambi\u00e9n conocidas como v\u00e9rtices, a su vez tienen propiedades, y su sentido es importante. Representaci\u00f3n de un grafo Los nodos se organizan mediante relaciones que facilitan encontrar patrones de informaci\u00f3n existente entre los nodos. Este tipo de organizaci\u00f3n permite almacenar los datos una vez e interpretar los datos de diferentes maneras dependiendo de sus relaciones. Los nodos son entidades que tienen propiedades, tales como el nombre. Por ejemplo, en el gr\u00e1fico cada nodo tiene una propiedad name . Tambi\u00e9n podemos ver que las relaciones tienen tipos, como label , since , etc\u2026\u200b Estas propiedades permiten organizar los nodos. Las relaciones pueden tener m\u00faltiples propiedades, y adem\u00e1s tienen direcci\u00f3n, con lo cual si queremos incluir bidireccionalidad tenemos que a\u00f1adir dos relaciones en sentidos opuestos. Tanto los nodos como las relaciones tienen un atributo id que los identifica. Por ejemplo, podemos comenzar a crear el grafo anterior mediante Neo4J de la siguiente manera: Node alice = graphDb . createNode (); alice . setProperty ( \"name\" , \"Alice\" ); Node bob = graphDb . createNode (); bob . setProperty ( \"name\" , \"Bob\" ); alice . createRelationshipTo ( bob , FRIEND ); bob . createRelationshipTo ( alice , FRIEND ); Los nodos permiten tener diferentes tipos de relaciones entre ellos y as\u00ed representar relaciones entre las entidades del dominio, y tener relaciones secundarias para caracter\u00edsticas como categor\u00eda, camino, \u00e1rboles de tiempo, listas enlazas para acceso ordenado, etc\u2026\u200b Al no existir un l\u00edmite en el n\u00famero ni en el tipo de relaciones que puede tener un nodo, todas se pueden representar en la misma base de datos. Traversing \u00b6 Una vez tenemos creado un grafo de nodos y relaciones, podemos consultar el grafo de muchas maneras; por ejemplo \" obtener todos los nodos que son miembros del grupo de ajedrez y que tienen m\u00e1s de 20 a\u00f1os \". Realizar una consulta se conoce como hacer un traversing (recorrido) del mismo. Un ejemplo de traversing mediante Neo4J ser\u00eda: Node ajedrez = nodeIndex . get ( \"name\" , \"chess\" ). getSingle (); allRelationships = ajedrez . getRelationships ( Direction . INCOMING ); Una ventaja a destacar de las bases de datos basadas en grafos es que podemos cambiar los requisitos de traversing sin tener que cambiar los nodos o sus relaciones. En las bases de datos de grafos, recorrer las relaciones es muy r\u00e1pido, ya que no se calculan en tiempo de consulta, sino que se persisten como una relaci\u00f3n, y por tanto no hay que hacer ning\u00fan c\u00e1lculo. En cambio, en una base de datos relacional, para crear una estructura de grafo se realiza para una relaci\u00f3n sencilla ( \u00bfQuien es mi jefe?\" ). Para poder a\u00f1adir otras relaciones necesitamos muchos cambios en el esquema y trasladar datos entre tablas. Adem\u00e1s, necesitamos de antemano saber qu\u00e9 consultas queremos realizar para modelar las tablas y las relaciones acorde a las consultas. As\u00ed pues, estos sistemas ofrecen modelos ricos de consultas donde se pueden investigar las relaciones simples y complejas entre los nodos para obtener informaci\u00f3n directa e indirecta de los datos del sistemas. Los tipos de an\u00e1lisis que se realizan sobre estos sistema se ci\u00f1en a los tipos de relaci\u00f3n existente entre los datos. Casos de uso \u00b6 Mientras que el modelo de grafos no es muy intuitivo y tiene una importante curva de aprendizaje, se puede usar en un gran n\u00famero de aplicaciones. Su principal atractivo es que facilitan almacenar las relaciones entre entidades de una aplicaci\u00f3n, como por ejemplo en una red social, o las intersecciones existentes entre carreteras. Es decir, se emplean para almacenar datos que se representan como nodos interconectados. Por lo tanto, los casos de uso son: Datos conectados: redes sociales con diferentes tipos de conexiones entre los usuarios. Enrutamiento, entrega o servicios basados en la posici\u00f3n: si las relaciones almacenan la distancia entre los nodos, podemos realizar consultas sobre lugares cercanos, trayecto m\u00e1s corto, etc\u2026\u200b Motores de recomendaciones: de compras, de lugares visitados, etc\u2026\u200b En cambio, no se recomienda su uso cuando necesitemos modificar todos o un subconjunto de entidades, ya que modificar una propiedad en todos los nodos es una operaci\u00f3n compleja. Los productos m\u00e1s destacados son: Neo4j : http://neo4j.com ArangoDB : https://www.arangodb.com/ Apache Giraph : https://giraph.apache.org/ Amazon Neptune : https://aws.amazon.com/es/neptune/ Consistencia \u00b6 En un sistema consistente, las escrituras de una aplicaci\u00f3n son visibles en siguientes consultas. Con una consistencia eventual, las escrituras no son visibles inmediatamente. Por ejemplo, en un sistema de control de stock, si el sistema es consistente, cada consulta obtendr\u00e1 el estado real del inventario, mientras que si tiene consistencia eventual, puede que no sea el estado real en un momento concreto pero terminar\u00e1 si\u00e9ndolo en breve. Sistemas consistentes \u00b6 Cada aplicaci\u00f3n tiene diferentes requisitos para la consistencia de los datos. Para muchas aplicaciones, es imprescindible que los datos sean consistentes en todo momento. Como los equipos de desarrollo han estado trabajo con un modelo de datos relacional durante d\u00e9cadas, este enfoque parece natural. Sin embargo, en otras ocasiones, la consistencia eventual es un traspi\u00e9s aceptable si conlleva una mayor flexibilidad en la disponibilidad del sistema. Las bases de datos documentales y de grafos pueden ser consistentes o eventualmente consistentes. Por ejemplo, MongoDB ofrece un consistencia configurable. De manera predeterminada, los datos son consistentes, de modo que todas las escrituras y lecturas se realizan sobre la copia principal de los datos. Pero como opci\u00f3n, las consultas de lectura, se pueden realizar con las copias secundarias donde los datos tendr\u00e1n consistencia eventual. La elecci\u00f3n de la consistencia se realiza a nivel de consulta. Sistemas de consistencia eventual \u00b6 Con los sistemas eventualmente consistentes, hay un per\u00edodo de tiempo en el que todas las copias de los datos no est\u00e1n sincronizados. Esto puede ser aceptable para aplicaciones de s\u00f3lo-lectura y almacenes de datos que no cambian frecuentemente, como los archivos hist\u00f3ricos. Dentro del mismo saco podemos meter las aplicaciones con alta tasa de escritura donde las lecturas sean poco frecuentes, como un archivo de log. Un claro ejemplo de sistema eventualmente consistente es el servicio DNS, donde tras registrar un dominio, puede tardar varios d\u00edas en propagar los datos a trav\u00e9s de Internet, pero siempre est\u00e1n disponibles aunque contenga una versi\u00f3n antigua de los datos. Respecto a las bases de datos NoSQL, los almacenes de clave-valor y los basados en columnas son sistemas eventualmente consistentes. Estos tienen que soportar conflictos en las actualizaciones de registros individuales. Como las escrituras se pueden aplicar a cualquier copia de los datos, puede ocurrir, y no ser\u00eda muy extra\u00f1o, que hubiese un conflicto de escritura. Algunos sistemas como Riak utilizan vectores de reloj para determinar el orden de los eventos y asegurar que la operaci\u00f3n m\u00e1s reciente gana en caso de un conflicto. Otros sistemas como CouchDB , retienen todos los valores conflictivos y permiten al usuario resolver el conflicto. Otro enfoque seguido por Cassandra sencillamente asume que el valor m\u00e1s grande es el correcto. Por estos motivos, las escrituras tienden a comportarse bien en sistemas eventualmente consistentes, pero las actualizaciones pueden conllevar sacrificios que complican la aplicaci\u00f3n. Teorema de CAP \u00b6 Propuesto por Eric Brewer en el a\u00f1o 2000, prueba que podemos crear una base de datos distribuida que elija dos de las siguientes tres caracter\u00edsticas: C onsistencia: las escrituras son at\u00f3micas y todas las peticiones posteriores obtienen el nuevo valor, independientemente del lugar de la petici\u00f3n. Disponibilidad ( A vailable ): la base de datos devolver\u00e1 siempre un valor. En la pr\u00e1ctica significa que no hay downtime . Tolerancia a P articiones: el sistema funcionar\u00e1 incluso si la comunicaci\u00f3n con un servidor se interrumpe de manera temporal (para ello, ha de dividir los datos entre diferentes nodos). Es decir, implica que se pueden recibir lecturas desde unos nodos que no contienen informaci\u00f3n escrita en otros. En otras palabras, podemos crear un sistema de base de datos que sea consistente y tolerante a particiones (CP), un sistema que sea disponible y tolerante a particiones (AP), o un sistema que sea consistente y disponible (CA). Pero no es posible crear una base de datos distribuida que sea consistente, disponible y tolerante a particiones al mismo tiempo. Teorema de CAP El teorema CAP es \u00fatil cuando consideramos el sistema de base de datos que necesitamos, ya que nos permite decidir cual de las tres caracter\u00edsticas vamos a descartar. La elecci\u00f3n realmente se centra entre la disponibilidad y la consistencia, ya que la tolerancia a particiones es una decisi\u00f3n de arquitectura (sea o no distribuida). Aunque el teorema dicte que si en un sistema distribuido elegimos disponibilidad no podemos tener consistencia, todav\u00eda podemos obtener consistencia eventual. Es decir, cada nodo siempre estar\u00e1 disponible para servir peticiones, aunque estos nodos no puedan asegurar que la informaci\u00f3n que contienen sea consistente (pero si bastante precisa), en alg\u00fan momento lo ser\u00e1. Algunas bases de datos tolerantes a particiones se pueden ajustar para ser m\u00e1s o menos consistentes o disponible a nivel de petici\u00f3n. Por ejemplo, Riak trabaja de esta manera, permitiendo a los clientes decidir en tiempo de petici\u00f3n qu\u00e9 nivel de consistencia necesitan. Clasificaci\u00f3n seg\u00fan CAP \u00b6 El siguiente gr\u00e1fico muestra c\u00f3mo dependiendo de estos atributos podemos clasificar los sistemas NoSQL: Clasificaci\u00f3n seg\u00fan CAP As\u00ed pues, las bases de datos NoSQL se clasifican en: CP : Consistente y tolerantes a particiones. Tanto MongoDB como HBase son CP, ya que dentro de una partici\u00f3n pueden no estar disponibles para responder una determinada consulta (por ejemplo, evitando lecturas en los nodos secundarios), aunque son tolerantes a fallos porque cualquier nodo secundario se puede convertir en principal y asumir el rol del nodo ca\u00eddo. AP : Disponibles y tolerantes a particiones. DynamoDB permite replicar los datos entre sus nodos aunque no garantiza la consistencia en ninguno de los sus servidores. CA : Consistentes y disponible. Aqu\u00ed es donde situar\u00edamos a los SGDB relacionales. Por ejemplo, PostreSQL es CA (aunque ofrece un producto complementario para dar soporte al particionado, como PgCluster), ya que no distribuyen los datos y por tanto la partici\u00f3n no es una restricci\u00f3n. Lo bueno es que la gran mayor\u00eda de sistemas permiten configurarse para cambiar su tipo CAP, lo que permite que MongoDB pase de CP a AP, o CouchDB de AP a CP. BASE \u00b6 De forma an\u00e1loga al modelo transaccional ACID para las bases de datos relacionales que dan soporte a la transaccionalidad ofreciendo en todo momento un sistema consistente, las bases de datos distribuidas siguen el modelo transaccional BASE, el cual se centra en la alta disponibilidad y significa: B\u00e1sicamente disponible ( B asically A vailable ): la base de datos siempre responde a las solicitudes recibidas, ya sea con una respuesta exitosa o con un error, a\u00fan en el caso de que el sistema soporte la tolerancia a particiones (de manera que caiga alg\u00fan nodo o no est\u00e9 accesible por problemas de la red). Esto puedo implicar lecturas desde nodos que no han recibido la \u00faltima escritura, por lo que el resultado puede no ser consistente. Estado blando ( S oft State ): la base de datos puede encontrarse en un estado inconsistente cuando se produce una lectura, de modo que es posible realizar dos veces la misma lectura y obtener dos resultados distintos a pesar de que no haya habido ninguna escritura entre ambas operaciones, sino que la escritura se hab\u00eda realizado antes en el tiempo y no se hab\u00eda persistido hasta ese momento. Consistencia eventual ( E ventual consistency ): tras cada escritura, la consistencia de la base de datos s\u00f3lo se alcanza una vez el cambio ha sido propagado a todos los nodos. Durante el tiempo que tarda en producirse la consistencia, observamos un estado blando de la base de datos. Una base de datos que sigue el modelo transaccional BASE prefiere la disponibilidad antes que la consistencia (es decir, desde el punto de vista del teorema CAP es AP). Referencias \u00b6 Next Generation Databases : NoSQL, NewSQL, and Big Data NoSQL Distilled : A Brief Guide to the Emerging World of Polyglot Persistence Row vs Column Oriented Databases Understanding Database Sharding Actividades \u00b6 ( RA5075.2 / CE5.2a / 2p) Contesta a las siguientes preguntas: \u00bfQu\u00e9 significa el prefijo No del acr\u00f3nimo NoSQL ? \u00bfUn sistema puede soportar al mismo tiempo replicaci\u00f3n y particionado? Para los siguientes supuestos, indica qu\u00e9 modelo de datos emplear\u00edas y justifica tu respuesta: Wiki sobre de personajes de c\u00f3mics. Informaci\u00f3n acad\u00e9mica de un pa\u00eds (centros, alumnos, profesores, asignaturas, calificaciones, \u2026\u200b) Investiga en qu\u00e9 consiste la persistencia pol\u00edglota . Clasifica las siguientes bases de datos seg\u00fan el teorema de CAP en CA, CP o AP: BigTable, Cassandra, CouchDB, DynamoDB, HBase, MongoDB, Redis, Riak, Voldemort . ( RA5075.2 / CE5.2a / 1p) Crea una presentaci\u00f3n de 5-6 diapositivas donde expliques en qu\u00e9 consiste el movimiento NewSQL , su relaci\u00f3n con NoSQL y qu\u00e9 ofrecen bases de datos como CockroachDB y/o VoltDB .","title":"S18.- Almacenamiento de datos. NoSQL"},{"location":"sa/01nosql.html#almacenamiento-de-datos","text":"Se puede decir que estamos en la tercera plataforma del almacenamiento de datos. La primera lleg\u00f3 con los primeros computadores y se materializ\u00f3 en las bases de datos jer\u00e1rquicas y en red, as\u00ed como en el almacenamiento ISAM. La segunda vino de la mano de internet y las arquitecturas cliente-servidor, lo que dio lugar a las bases de datos relacionales. La tercera se ve motivada por el big data, los dispositivos m\u00f3viles, las arquitecturas cloud, las redes de IoT y las tecnolog\u00edas/redes sociales. Es tal el volumen de datos que se genera que aparecen nuevos paradigmas como NoSQL, NewSQL y las plataformas de Big Data. En esta sesi\u00f3n nos vamos a centrar en NoSQL. NoSQL aparece como una necesidad debida al creciente volumen de datos sobre usuarios, objetos y productos que las empresas tienen que almacenar, as\u00ed como la frecuencia con la que se accede a los datos. Los SGDB relacionales existentes no fueron dise\u00f1ados teniendo en cuenta la escalabilidad ni la flexibilidad necesaria por las frecuentes modificaciones que necesitan las aplicaciones modernas; tampoco aprovechan que el almacenamiento a d\u00eda de hoy es muy barato, ni el nivel de procesamiento que alcanzan las m\u00e1quinas actuales. Motivaci\u00f3n de NoSQL La soluci\u00f3n es el despliegue de las aplicaciones y sus datos en cl\u00fasteres de servidores, distribuyendo el procesamiento en m\u00faltiples m\u00e1quinas .","title":"Almacenamiento de Datos"},{"location":"sa/01nosql.html#no-solo-sql","text":"Si definimos NoSQL formalmente, podemos decir que se trata de un conjunto de tecnolog\u00edas que permiten el procesamiento r\u00e1pido y eficiente de conjuntos de datos dando la mayor importancia al rendimiento, la fiabilidad y la agilidad . Si nos basamos en el acr\u00f3nimo, el t\u00e9rmino se refiere a cualquier almac\u00e9n de datos que no sigue un modelo relacional, los datos no son relacionales y por tanto no utilizan SQL como lenguaje de consulta. Otra aceptaci\u00f3n implica que el No hace referencia a not only , es decir, que los sistemas NoSQL se centran en sistemas complementarios a los SGBD relacionales, que fijan sus prioridades en la escalabilidad y la disponibilidad en contra de la atomicidad y consistencia de los datos. ACID Las bases de datos relacionales cumplen las caracter\u00edsticas ACID para ofrecer transaccionalidad sobre los datos: A tomicidad: las transacciones implican que se realizan todas las operaciones o no se realiza ninguna. C onsistencia: la base de datos asegura que los datos pasan de un estado v\u00e1lido o otro tambi\u00e9n. I solation (Aislamiento): Una transacci\u00f3n no afecta a otras transacciones, de manera que la modificaci\u00f3n de un registro / documento no es visible por otras lecturas. D urabilidad: La escritura de los datos asegura que una vez finalizada una operaci\u00f3n, los datos no se perder\u00e1n. Los diferentes tipos de bases de datos NoSQL existentes se pueden agrupar en cuatro categor\u00edas: Clave-Valor : Los almacenes clave-valor son las bases de datos NoSQL m\u00e1s simples. Cada elemento de la base de datos se almacena con un nombre de atributo (o clave) junto a su valor. Los almacenes m\u00e1s conocidos son Redis , Riak y AWS DynamoDB . Algunos almacenes, como es el caso de Redis, permiten que cada valor tenga un tipo (por ejemplo, integer) lo cual a\u00f1ade funcionalidad extra. Documentales : Cada clave se asocia a una estructura compleja que se conoce como documento. Este puede contener diferentes pares clave-valor, o pares de clave-array o incluso documentos anidados, como en un documento JSON. Los ejemplos m\u00e1s conocidos son MongoDB y CouchDB . Grafos : Los almacenes de grafos se usan para almacenar informaci\u00f3n sobre redes, como pueden ser conexiones sociales. Los ejemplos m\u00e1s conocidos son Neo4J , AWS Neptune y ArangoDB . Basados en columnas : Los almacenes basados en columnas como BigTable , Cassandra y HBase est\u00e1n optimizados para consultas sobre grandes conjuntos de datos, y almacenan los datos como columnas en vez de como filas. Sistemas NoSQL","title":"No Solo SQL"},{"location":"sa/01nosql.html#caracteristicas","text":"Si nos centramos en sus beneficios y los comparamos con las base de datos relacionales, las bases de datos NoSQL son m\u00e1s escalables, ofrecen un rendimiento mayor y sus modelos de datos resuelven varios problemas que no se plantearon al definir el modelo relacional: Grandes vol\u00famenes de datos estructurados, semi-estructurados y sin estructurar. Casi todas las implementaciones NoSQL ofrecen alg\u00fan tipo de representaci\u00f3n para datos sin esquema, lo que permite comenzar con una estructura y con el paso del tiempo, a\u00f1adir nuevos campos, ya sean sencillos o anidados a datos ya existentes. Sprints \u00e1giles, iteraciones r\u00e1pidas y frecuentes commits / pushes de c\u00f3digo, al emplear una sintaxis sencilla para la realizaci\u00f3n de consultas y la posibilidad de tener un modelo que vaya creciendo al mismo ritmo que el desarrollo. Arquitectura eficiente y escalable dise\u00f1ada para trabajar con clusters en vez de una arquitectura monol\u00edtica y costosa. Las soluciones NoSQL soportan la escalabilidad de un modo transparente para el desarrollador. Una caracter\u00edstica adicional que comparten los sistemas NoSQL es que ofrecen un mecanismo de cach\u00e9 de datos integrado (en los sistemas relacionales se pueden configurar de manera externa), de manera que se pueden configurar los sistemas para que los datos se mantengan en memoria y se persistan de manera peri\u00f3dica. El uso de una cach\u00e9 conlleva que la consistencia de los datos no sea completa y podamos tener una consistencia eventual.","title":"Caracter\u00edsticas"},{"location":"sa/01nosql.html#esquema-dinamicos","text":"Las bases de datos relacionales requieren definir los esquemas antes de a\u00f1adir los datos. Una base de datos SQL necesita saber de antemano los datos que vamos a almacenar; por ejemplo, si nos centramos en los datos de un cliente, ser\u00edan el nombre, apellidos, n\u00famero de tel\u00e9fono, etc\u2026\u200b Esto casa bastante mal con los enfoques de desarrollo \u00e1gil, ya que cada vez que a\u00f1adimos nuevas funcionalidades, el esquema de la base de datos suele cambiar. De modo que si a mitad de desarrollo decidimos almacenar los productos favoritos de un cliente del cual guard\u00e1bamos su direcci\u00f3n y n\u00fameros de tel\u00e9fono, tendr\u00edamos que a\u00f1adir una nueva columna a la base de datos y migrar la base de datos entera a un nuevo esquema. Si la base de datos es grande, conlleva un proceso lento que implica parar el sistema durante un tiempo considerable. Si frecuentemente cambiamos los datos que la aplicaci\u00f3n almacena (al usar un desarrollo iterativo), tambi\u00e9n tendremos per\u00edodos frecuentes de inactividad del sistema. As\u00ed pues, no hay un modo efectivo mediante una base de datos relacional, de almacenar los datos que est\u00e1n desestructurados o que no se conocen de antemano. Las bases de datos NoSQL se construyen para permitir la inserci\u00f3n de datos sin un esquema predefinido. Esto facilita la modificaci\u00f3n de la aplicaci\u00f3n en tiempo real, sin preocuparse por interrupciones de servicio. Aunque no tengamos un esquema al guardar la informaci\u00f3n, s\u00ed que podemos definir esquemas de lectura ( schema-on-read ) para comprobar que la informaci\u00f3n almacenada tiene el formato que espera cargar cada aplicaci\u00f3n. De este modo se consigue un desarrollo m\u00e1s r\u00e1pido, integraci\u00f3n de c\u00f3digo m\u00e1s robusto y menos tiempo empleado en la administraci\u00f3n de la base de datos.","title":"Esquema din\u00e1micos"},{"location":"sa/01nosql.html#particionado","text":"Dado el modo en el que se estructuran las bases de datos relacionales, normalmente escalan verticalmente - un \u00fanico servidor que almacena toda la base de datos para asegurar la disponibilidad continua de los datos. Esto se traduce en costes que se incrementan r\u00e1pidamente, con un l\u00edmites definidos por el propio hardware, y en un peque\u00f1o n\u00famero de puntos cr\u00edticos de fallo dentro de la infraestructura de datos. La soluci\u00f3n es escalar horizontalmente, a\u00f1adiendo nuevos servidores en vez de concentrarse en incrementar la capacidad de un \u00fanico servidor, lo que permite tratar con conjuntos de datos m\u00e1s grandes de lo que ser\u00eda capaz cualquier m\u00e1quina por s\u00ed sola. Este escalado horizontal se conoce como Sharding o Particionado. El particionado no es \u00fanico de las bases de datos NoSQL. Las bases de datos relacionales tambi\u00e9n lo soportan. Si en un sistema relacional queremos particionar los datos, podemos distinguir entre particionado: Horizontal : diferentes filas en diferentes particiones. Vertical : diferentes columnas en particiones distintas. Particionado de los datos - digitalocean.com En el caso de las bases de datos NoSQL, el particionado depende del modelo de la base de datos: Los almacenes clave-valor y las bases de datos documentales normalmente se particionan horizontalmente. Las bases de datos basados en columnas se pueden particionar horizontal o verticalmente. Escalar horizontalmente una base de datos relacional entre muchas instancias de servidores se puede conseguir pero normalmente conlleva el uso de SANs ( Storage Area Networks ) y otras triqui\u00f1uelas para hacer que el hardware act\u00fae como un \u00fanico servidor. Como los sistemas SQL no ofrecen esta prestaci\u00f3n de forma nativa, los equipos de desarrollo se las tienen que ingeniar para conseguir desplegar m\u00faltiples bases de datos relacionales en varias m\u00e1quinas. Para ello: Los datos se almacenan en cada instancia de base de datos de manera aut\u00f3noma El c\u00f3digo de aplicaci\u00f3n se desarrolla para distribuir los datos y las consultas y agregar los resultados de los datos a trav\u00e9s de todas las instancias de bases de datos Se debe desarrollar c\u00f3digo adicional para gestionar los fallos sobre los recursos, para realizar joins entre diferentes bases de datos, balancear los datos y/o replicarlos, etc\u2026\u200b Adem\u00e1s, muchos beneficios de las bases de datos como la integridad transaccional se ven comprometidos o incluso eliminados al emplear un escalado horizontal.","title":"Particionado"},{"location":"sa/01nosql.html#replicacion","text":"La replicaci\u00f3n mantiene copias id\u00e9nticas de los datos en m\u00faltiples servidores, lo que facilita que las aplicaciones siempre funcionen y los datos se mantengan seguros, incluso si alguno de los servidores sufre alg\u00fan problema. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n soportan la replicaci\u00f3n autom\u00e1tica, lo que implica una alta disponibilidad y recuperaci\u00f3n frente a desastres sin la necesidad de aplicaciones de terceros encargadas de ello. Desde el punto de vista del desarrollador, el entorno de almacenamiento es virtual y ajeno al c\u00f3digo de aplicaci\u00f3n. Existen dos formas de realiza la replicaci\u00f3n:","title":"Replicaci\u00f3n"},{"location":"sa/01nosql.html#implantando-nosql","text":"Normalmente, las empresas empezar\u00e1n con una prueba de baja escalabilidad de una base de datos NoSQL, de modo que les permita comprender la tecnolog\u00eda asumiendo muy poco riesgo. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n son open-source, y por tanto se pueden probar sin ning\u00fan coste extra. Al tener unos ciclos de desarrollo m\u00e1s r\u00e1pidos, las empresas pueden innovar con mayor velocidad y mejorar la experiencia de sus cliente a un menor coste. Elegir la base de datos correcta para el proyecto es un tema importante. Se deben considerar las diferentes alternativas a las infraestructuras legacy teniendo en cuenta varios factores: la escalabilidad o el rendimiento m\u00e1s all\u00e1 de las capacidades del sistema existente. identificar alternativas viables respecto al software propietario. incrementar la velocidad y agilidad del proceso de desarrollo. As\u00ed pues, al elegir un base de datos hemos de tener en cuenta las siguientes dimensiones: modelo de datos: A elegir entre un modelo documental, basado en columnas, de grafos o mediante clave-valor. modelo de consultas: Dependiendo de la aplicaci\u00f3n, puede ser aceptable un modelo de consultas que s\u00f3lo accede a los registros por su clave primaria. En cambio, otras aplicaciones pueden necesitar consultar por diferentes valores de cada registro. Adem\u00e1s, si la aplicaci\u00f3n necesita modificar los registros, la base de datos necesita consultar los datos por un \u00edndice secundario. modelo de consistencia: Los sistemas NoSQL normalmente mantienen m\u00faltiples copias de los datos para ofrecer disponibilidad y escalabilidad al sistema, lo que define la consistencia del mismo. Los sistemas NoSQL tienden a ser consistentes o eventualmente consistentes. APIs: No existe un est\u00e1ndar para interactuar con los sistemas NoSQL. Cada sistema presenta diferentes dise\u00f1os y capacidades para los equipos de desarrollo. La madurez de un API puede suponer una inversi\u00f3n en tiempo y dinero a la hora de desarrollar y mantener el sistema NoSQL. soporte comercial y de la comunidad: Los usuarios deben considerar la salud de la compa\u00f1ia o de los proyectos al evaluar una base de datos. El producto debe evolucionar y mantenerse para introducir nuevas prestaciones y corregir fallos. Una base de datos con una comunidad fuerte de usuarios: permite encontrar y contratar desarrolladores con destrezas en el producto. facilita encontrar informaci\u00f3n, documentaci\u00f3n y ejemplos de c\u00f3digo. ayuda a las empresas a retener el talento. favorece que otras empresas de software integren sus productos y participen en el ecosistema de la base de datos.","title":"Implantando NoSQL"},{"location":"sa/01nosql.html#modelos-de-datos","text":"La principal clasificaci\u00f3n de los sistemas de bases de datos NoSQL se realiza respecto a los diferentes modelos de datos:","title":"Modelos de Datos"},{"location":"sa/01nosql.html#documental","text":"Mientras las bases de datos relacionales almacenan los datos en filas y columnas, las bases de datos documentales emplean documentos. Estos documentos utilizan una estructura JSON, ofreciendo un modo natural e intuitivo para modelar datos de manera similar a la orientaci\u00f3n a objetos, donde cada documento es un objeto. Representaci\u00f3n de un documento Los documentos se agrupan en colecciones o bases de datos, dependiendo del sistema, lo que permite agrupar documentos. Los documentos contienen uno o m\u00e1s campos, donde cada campo contiene un valor con un tipo, ya sea cadena, entero, flotante, fecha, binario o array u otro documento. En vez de extender los datos entre m\u00faltiples columnas y tablas, cada registro y sus datos asociados se almacenan de manera unida en un \u00fanico documento. Esto simplifica el acceso a los datos y reduce (y en ocasiones elimina) la necesidad de joins y transacciones complejas. Dicho de otra manera, en las bases de datos documentales, los datos que van juntos y se emplean juntos, se almacenan juntos.","title":"Documental"},{"location":"sa/01nosql.html#clave-valor","text":"Un almac\u00e9n clave-valor es una simple tabla hash donde todos los accesos a la base de datos se realizan a trav\u00e9s de la clave primaria. Desde una perspectiva de modelo de datos, los almacenes de clave-valor son los m\u00e1s b\u00e1sicos. Su funcionamiento es similar a tener una tabla relacional con dos columnas, por ejemplo id y nombre , siendo id la columna utilizada como clave y nombre como valor. Mientras que en una base de datos en el campo nombre s\u00f3lo podemos almacenar datos de tipo cadena o num\u00e9rico, en un almac\u00e9n clave-valor, el valor puede ser de un dato simple o un objeto. Cuando una aplicaci\u00f3n accede mediante la clave y el valor, se almacenan el par de elementos. Si la clave ya existe, el valor se modifica. Representaci\u00f3n de un almac\u00e9n clave-valor El cliente puede tanto obtener el valor por la clave, asignar un valor a una clave o eliminar una clave del almac\u00e9n. El valor, sin embargo, es opaco al sistema, el cual no sabe que hay dentro de \u00e9l, ya que los datos s\u00f3lo se pueden consultar por la clave, lo cual puede ser un inconveniente. As\u00ed pues, la aplicaci\u00f3n es responsable de saber qu\u00e9 hay almacenado en cada valor. Por ejemplo, Riak utiliza el concepto de bucket (cubo) como una manera de agrupar claves, de manera similar a una tabla. Por ejemplo, Riak permite interactuar con la base de datos mediante peticiones HTTP: curl -v -X PUT <http://localhost:8091/riak/heroes/ace> -H \"Content-Type: application/json\" -d { \"nombre\" : \"Batman\" , \"color\" : \"Negro\" } Algunos almacenes clave-valor, como puede ser Redis , permiten almacenar datos con cualquier estructura, como por ejemplos listas, conjuntos, hashes y pueden realizar operaciones como intersecci\u00f3n, uni\u00f3n, diferencia y rango. Comandos Redis Python SET nombre \"Bruce Wayne\" // String HSET heroe nombre \"Batman\" // Hash \u2013 set HSET heroe color \"Negro\" SADD \"heroe:amigos\" \"Robin\" \"Alfred\" // Set \u2013 create/update import redis r = redis . Redis () r . mset ({ \"Croatia\" : \"Zagreb\" , \"Bahamas\" : \"Nassau\" }) r . get ( \"Bahamas\" ) # b'Nassau' Estas prestaciones hacen que Redis se extrapole a \u00e1mbitos ajenos a un almac\u00e9n clave-valor. Otra caracter\u00edstica que ofrecen algunos almacenes es que permiten crear un segundo nivel de consulta o incluso definir m\u00e1s de una clave para un mismo objeto. Como los almacenes clave-valor siempre utilizan accesos por clave primaria, de manera general tienen un gran rendimiento y son f\u00e1cilmente escalables. Si queremos que su rendimiento sea m\u00e1ximo, pueden configurarse para que mantengan la informaci\u00f3n en memoria y que se serialice de manera peri\u00f3dica, a costa de tener una consistencia eventual de los datos.","title":"Clave-Valor"},{"location":"sa/01nosql.html#basado-en-columnas","text":"Las bases de datos relacionales utilizan la fila como unidad de almacenamiento, lo que permite un buen rendimiento de escritura. Sin embargo, cuando las escrituras son ocasionales y es m\u00e1s comun tener que leer unas pocas columnas de muchas filas a la vez, es mejor utilizar como unidad de almacenamiento un grupos de columnas. Es decir, lo que hacemos es girar el modelo 90 grados, de manera que los registros se almacenan en columnas en vez de hacerlo por filas. Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnar los datos del mismo tipo se agrupan, lo que permite codificarlos/comprimirlos, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas Autoevaluaci\u00f3n Si tenemos que a\u00f1adir un nuevo registro \u00bfQu\u00e9 modelo ser\u00e1 m\u00e1s eficiente? Sin embargo, a medida que se incrementa la utilizaci\u00f3n de an\u00e1lisis de datos en memoria, con soluciones como Spark , los beneficios relativos de la base de datos columnares comparados con los de las bases de datos orientadas a filas pueden llegar a ser menos importantes.","title":"Basado en columnas"},{"location":"sa/01nosql.html#grafos","text":"Las bases de datos de grafos almacenan entidades y las relaciones entre estas entidades. Las entidades se conocen como nodos, los cuales tienen propiedades. Cada nodo es similar a una instancia de un objeto. Las relaciones, tambi\u00e9n conocidas como v\u00e9rtices, a su vez tienen propiedades, y su sentido es importante. Representaci\u00f3n de un grafo Los nodos se organizan mediante relaciones que facilitan encontrar patrones de informaci\u00f3n existente entre los nodos. Este tipo de organizaci\u00f3n permite almacenar los datos una vez e interpretar los datos de diferentes maneras dependiendo de sus relaciones. Los nodos son entidades que tienen propiedades, tales como el nombre. Por ejemplo, en el gr\u00e1fico cada nodo tiene una propiedad name . Tambi\u00e9n podemos ver que las relaciones tienen tipos, como label , since , etc\u2026\u200b Estas propiedades permiten organizar los nodos. Las relaciones pueden tener m\u00faltiples propiedades, y adem\u00e1s tienen direcci\u00f3n, con lo cual si queremos incluir bidireccionalidad tenemos que a\u00f1adir dos relaciones en sentidos opuestos. Tanto los nodos como las relaciones tienen un atributo id que los identifica. Por ejemplo, podemos comenzar a crear el grafo anterior mediante Neo4J de la siguiente manera: Node alice = graphDb . createNode (); alice . setProperty ( \"name\" , \"Alice\" ); Node bob = graphDb . createNode (); bob . setProperty ( \"name\" , \"Bob\" ); alice . createRelationshipTo ( bob , FRIEND ); bob . createRelationshipTo ( alice , FRIEND ); Los nodos permiten tener diferentes tipos de relaciones entre ellos y as\u00ed representar relaciones entre las entidades del dominio, y tener relaciones secundarias para caracter\u00edsticas como categor\u00eda, camino, \u00e1rboles de tiempo, listas enlazas para acceso ordenado, etc\u2026\u200b Al no existir un l\u00edmite en el n\u00famero ni en el tipo de relaciones que puede tener un nodo, todas se pueden representar en la misma base de datos.","title":"Grafos"},{"location":"sa/01nosql.html#consistencia","text":"En un sistema consistente, las escrituras de una aplicaci\u00f3n son visibles en siguientes consultas. Con una consistencia eventual, las escrituras no son visibles inmediatamente. Por ejemplo, en un sistema de control de stock, si el sistema es consistente, cada consulta obtendr\u00e1 el estado real del inventario, mientras que si tiene consistencia eventual, puede que no sea el estado real en un momento concreto pero terminar\u00e1 si\u00e9ndolo en breve.","title":"Consistencia"},{"location":"sa/01nosql.html#sistemas-consistentes","text":"Cada aplicaci\u00f3n tiene diferentes requisitos para la consistencia de los datos. Para muchas aplicaciones, es imprescindible que los datos sean consistentes en todo momento. Como los equipos de desarrollo han estado trabajo con un modelo de datos relacional durante d\u00e9cadas, este enfoque parece natural. Sin embargo, en otras ocasiones, la consistencia eventual es un traspi\u00e9s aceptable si conlleva una mayor flexibilidad en la disponibilidad del sistema. Las bases de datos documentales y de grafos pueden ser consistentes o eventualmente consistentes. Por ejemplo, MongoDB ofrece un consistencia configurable. De manera predeterminada, los datos son consistentes, de modo que todas las escrituras y lecturas se realizan sobre la copia principal de los datos. Pero como opci\u00f3n, las consultas de lectura, se pueden realizar con las copias secundarias donde los datos tendr\u00e1n consistencia eventual. La elecci\u00f3n de la consistencia se realiza a nivel de consulta.","title":"Sistemas consistentes"},{"location":"sa/01nosql.html#sistemas-de-consistencia-eventual","text":"Con los sistemas eventualmente consistentes, hay un per\u00edodo de tiempo en el que todas las copias de los datos no est\u00e1n sincronizados. Esto puede ser aceptable para aplicaciones de s\u00f3lo-lectura y almacenes de datos que no cambian frecuentemente, como los archivos hist\u00f3ricos. Dentro del mismo saco podemos meter las aplicaciones con alta tasa de escritura donde las lecturas sean poco frecuentes, como un archivo de log. Un claro ejemplo de sistema eventualmente consistente es el servicio DNS, donde tras registrar un dominio, puede tardar varios d\u00edas en propagar los datos a trav\u00e9s de Internet, pero siempre est\u00e1n disponibles aunque contenga una versi\u00f3n antigua de los datos. Respecto a las bases de datos NoSQL, los almacenes de clave-valor y los basados en columnas son sistemas eventualmente consistentes. Estos tienen que soportar conflictos en las actualizaciones de registros individuales. Como las escrituras se pueden aplicar a cualquier copia de los datos, puede ocurrir, y no ser\u00eda muy extra\u00f1o, que hubiese un conflicto de escritura. Algunos sistemas como Riak utilizan vectores de reloj para determinar el orden de los eventos y asegurar que la operaci\u00f3n m\u00e1s reciente gana en caso de un conflicto. Otros sistemas como CouchDB , retienen todos los valores conflictivos y permiten al usuario resolver el conflicto. Otro enfoque seguido por Cassandra sencillamente asume que el valor m\u00e1s grande es el correcto. Por estos motivos, las escrituras tienden a comportarse bien en sistemas eventualmente consistentes, pero las actualizaciones pueden conllevar sacrificios que complican la aplicaci\u00f3n.","title":"Sistemas de consistencia eventual"},{"location":"sa/01nosql.html#teorema-de-cap","text":"Propuesto por Eric Brewer en el a\u00f1o 2000, prueba que podemos crear una base de datos distribuida que elija dos de las siguientes tres caracter\u00edsticas: C onsistencia: las escrituras son at\u00f3micas y todas las peticiones posteriores obtienen el nuevo valor, independientemente del lugar de la petici\u00f3n. Disponibilidad ( A vailable ): la base de datos devolver\u00e1 siempre un valor. En la pr\u00e1ctica significa que no hay downtime . Tolerancia a P articiones: el sistema funcionar\u00e1 incluso si la comunicaci\u00f3n con un servidor se interrumpe de manera temporal (para ello, ha de dividir los datos entre diferentes nodos). Es decir, implica que se pueden recibir lecturas desde unos nodos que no contienen informaci\u00f3n escrita en otros. En otras palabras, podemos crear un sistema de base de datos que sea consistente y tolerante a particiones (CP), un sistema que sea disponible y tolerante a particiones (AP), o un sistema que sea consistente y disponible (CA). Pero no es posible crear una base de datos distribuida que sea consistente, disponible y tolerante a particiones al mismo tiempo. Teorema de CAP El teorema CAP es \u00fatil cuando consideramos el sistema de base de datos que necesitamos, ya que nos permite decidir cual de las tres caracter\u00edsticas vamos a descartar. La elecci\u00f3n realmente se centra entre la disponibilidad y la consistencia, ya que la tolerancia a particiones es una decisi\u00f3n de arquitectura (sea o no distribuida). Aunque el teorema dicte que si en un sistema distribuido elegimos disponibilidad no podemos tener consistencia, todav\u00eda podemos obtener consistencia eventual. Es decir, cada nodo siempre estar\u00e1 disponible para servir peticiones, aunque estos nodos no puedan asegurar que la informaci\u00f3n que contienen sea consistente (pero si bastante precisa), en alg\u00fan momento lo ser\u00e1. Algunas bases de datos tolerantes a particiones se pueden ajustar para ser m\u00e1s o menos consistentes o disponible a nivel de petici\u00f3n. Por ejemplo, Riak trabaja de esta manera, permitiendo a los clientes decidir en tiempo de petici\u00f3n qu\u00e9 nivel de consistencia necesitan.","title":"Teorema de CAP"},{"location":"sa/01nosql.html#clasificacion-segun-cap","text":"El siguiente gr\u00e1fico muestra c\u00f3mo dependiendo de estos atributos podemos clasificar los sistemas NoSQL: Clasificaci\u00f3n seg\u00fan CAP As\u00ed pues, las bases de datos NoSQL se clasifican en: CP : Consistente y tolerantes a particiones. Tanto MongoDB como HBase son CP, ya que dentro de una partici\u00f3n pueden no estar disponibles para responder una determinada consulta (por ejemplo, evitando lecturas en los nodos secundarios), aunque son tolerantes a fallos porque cualquier nodo secundario se puede convertir en principal y asumir el rol del nodo ca\u00eddo. AP : Disponibles y tolerantes a particiones. DynamoDB permite replicar los datos entre sus nodos aunque no garantiza la consistencia en ninguno de los sus servidores. CA : Consistentes y disponible. Aqu\u00ed es donde situar\u00edamos a los SGDB relacionales. Por ejemplo, PostreSQL es CA (aunque ofrece un producto complementario para dar soporte al particionado, como PgCluster), ya que no distribuyen los datos y por tanto la partici\u00f3n no es una restricci\u00f3n. Lo bueno es que la gran mayor\u00eda de sistemas permiten configurarse para cambiar su tipo CAP, lo que permite que MongoDB pase de CP a AP, o CouchDB de AP a CP.","title":"Clasificaci\u00f3n seg\u00fan CAP"},{"location":"sa/01nosql.html#base","text":"De forma an\u00e1loga al modelo transaccional ACID para las bases de datos relacionales que dan soporte a la transaccionalidad ofreciendo en todo momento un sistema consistente, las bases de datos distribuidas siguen el modelo transaccional BASE, el cual se centra en la alta disponibilidad y significa: B\u00e1sicamente disponible ( B asically A vailable ): la base de datos siempre responde a las solicitudes recibidas, ya sea con una respuesta exitosa o con un error, a\u00fan en el caso de que el sistema soporte la tolerancia a particiones (de manera que caiga alg\u00fan nodo o no est\u00e9 accesible por problemas de la red). Esto puedo implicar lecturas desde nodos que no han recibido la \u00faltima escritura, por lo que el resultado puede no ser consistente. Estado blando ( S oft State ): la base de datos puede encontrarse en un estado inconsistente cuando se produce una lectura, de modo que es posible realizar dos veces la misma lectura y obtener dos resultados distintos a pesar de que no haya habido ninguna escritura entre ambas operaciones, sino que la escritura se hab\u00eda realizado antes en el tiempo y no se hab\u00eda persistido hasta ese momento. Consistencia eventual ( E ventual consistency ): tras cada escritura, la consistencia de la base de datos s\u00f3lo se alcanza una vez el cambio ha sido propagado a todos los nodos. Durante el tiempo que tarda en producirse la consistencia, observamos un estado blando de la base de datos. Una base de datos que sigue el modelo transaccional BASE prefiere la disponibilidad antes que la consistencia (es decir, desde el punto de vista del teorema CAP es AP).","title":"BASE"},{"location":"sa/01nosql.html#referencias","text":"Next Generation Databases : NoSQL, NewSQL, and Big Data NoSQL Distilled : A Brief Guide to the Emerging World of Polyglot Persistence Row vs Column Oriented Databases Understanding Database Sharding","title":"Referencias"},{"location":"sa/01nosql.html#actividades","text":"( RA5075.2 / CE5.2a / 2p) Contesta a las siguientes preguntas: \u00bfQu\u00e9 significa el prefijo No del acr\u00f3nimo NoSQL ? \u00bfUn sistema puede soportar al mismo tiempo replicaci\u00f3n y particionado? Para los siguientes supuestos, indica qu\u00e9 modelo de datos emplear\u00edas y justifica tu respuesta: Wiki sobre de personajes de c\u00f3mics. Informaci\u00f3n acad\u00e9mica de un pa\u00eds (centros, alumnos, profesores, asignaturas, calificaciones, \u2026\u200b) Investiga en qu\u00e9 consiste la persistencia pol\u00edglota . Clasifica las siguientes bases de datos seg\u00fan el teorema de CAP en CA, CP o AP: BigTable, Cassandra, CouchDB, DynamoDB, HBase, MongoDB, Redis, Riak, Voldemort . ( RA5075.2 / CE5.2a / 1p) Crea una presentaci\u00f3n de 5-6 diapositivas donde expliques en qu\u00e9 consiste el movimiento NewSQL , su relaci\u00f3n con NoSQL y qu\u00e9 ofrecen bases de datos como CockroachDB y/o VoltDB .","title":"Actividades"},{"location":"sa/02mongo.html","text":"MongoDB ( http://www.mongodb.com ) es una de las bases de datos NoSQL m\u00e1s conocidas. Sigue un modelo de datos documental, donde los documentos se basan en JSON. huMONGOus Como curiosidad, su nombre viene de la palabra inglesa humongous , que significa gigantesco/enorme. MongoDB destaca porque: Soporta esquemas din\u00e1micos: diferentes documentos de una misma colecci\u00f3n pueden tener atributos diferentes. Aunque inicialmente ten\u00eda un soporte limitado de joins , desde la versi\u00f3n 5.2 se pueden realizar incluso entre colecciones particionadas. Soporte de transacciones s\u00f3lo a nivel de aplicaci\u00f3n. Lo que en un RDMS puede suponer m\u00faltiples operaciones, con MongoDB se puede hacer en una sola operaci\u00f3n al insertar/actualizar todo un documento de una sola vez, pero si queremos crear una transacci\u00f3n entre dos documentos, la gesti\u00f3n la debe realizar el driver. Conceptos \u00b6 Hay una serie de conceptos que conviene conocer antes de entrar en detalle: MongoDB tienen el mismo concepto de base de datos que un RDMS . Dentro de una instancia de MongoDB podemos tener 0 o m\u00e1s bases de datos, actuando cada una como un contenedor de alto nivel. Una base de datos tendr\u00e1 0 o m\u00e1s colecciones. Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un RDMS . MongoDB ofrece diferentes tipos de colecciones, desde las normales cuyo tama\u00f1o crece conforme lo hace el n\u00famero de documentos, como las colecciones capped , las cuales tienen un tama\u00f1o predefinido y que pueden contener una cierta cantidad de informaci\u00f3n que se sustituir\u00e1 por nueva cuando se llene. Las colecciones contienen 0 o m\u00e1s documentos , por lo que es similar a una fila o registro de un RDMS . Cada documento contiene 0 o m\u00e1s atributos, compuestos de parejas clave/valor . Cada uno de estos documentos no sigue ning\u00fan esquema, por lo que dos documentos de una misma colecci\u00f3n pueden contener todos los atributos diferentes entre s\u00ed. Elementos de MongoDB As\u00ed pues, tenemos que una base de datos va a contener varias colecciones, donde cada colecci\u00f3n contendr\u00e1 un conjunto de documentos: Modelo de MongoDB Adem\u00e1s, MongoDB soporta \u00edndices , igual que cualquier RDMS , para acelerar la b\u00fasqueda de datos. Al realizar cualquier consulta, se devuelve un cursor , con el cual podemos hacer cosas tales como contar, ordenar, limitar o saltar documentos. BSON \u00b6 Mediante JavaScript podemos crear objetos que se representan con JSON. Internamente, MongoDB almacena los documentos mediante BSON ( Binary JSON ). Podemos consultar la especificaci\u00f3n en http://BSONSpec.org Especificaci\u00f3n BSON BSON representa un superset de JSON ya que: Permite almacenar datos en binario Incluye un conjunto de tipos de datos no incluidos en JSON, como pueden ser ObjectId , Date o BinData . Podemos consultar todos los tipos que soporta un objeto BSON en http://docs.mongodb.org/manual/reference/bson-types/ Un ejemplo de un objeto BSON podr\u00eda ser: var yo = { n ombre : \"Aitor\" , apellidos : \"Medrano\" , fna c : ne w Da te ( \"Oct 3, 1977\" ) , hobbies : [ \"programaci\u00f3n\" , \"videojuegos\" , \"baloncesto\" ], casado : true , hijos : 2 , co nta c t o : { t wi tter : \"@aitormedrano\" , email : \"a.medrano@edu.gva.es\" }, fe chaCreacio n : ne w Times ta mp() } Los documentos BSON tienen las siguientes restricciones: No pueden tener un tama\u00f1o superior a 16 MB. El atributo _id queda reservado para la clave primaria. Desde MongoDB 5.0 los nombres de los campos pueden empezar por $ y/o contener el . , aunque en la medida de lo posible, es recomendable evitar su uso. Adem\u00e1s MongoDB: No asegura que el orden de los campos se respete. Es sensible a los tipos de los datos Es sensible a las may\u00fasculas. Por lo que estos documentos son distintos: { \"edad\" : \"18\" } { \"edad\" : 18 } { \"Edad\" : 18 } Si queremos validar si un documento JSON es v\u00e1lido, podemos usar http://jsonlint.com/ . Hemos de tener en cuenta que s\u00f3lo valida JSON y no BSON, por tanto nos dar\u00e1 errores en los tipos de datos propios de BSON. Puesta en marcha \u00b6 En la actualidad, MongoDB se comercializa mediante tres productos: Mongo Atlas , como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB. MongoDB Community Edition , versi\u00f3n gratuita para trabajar on-premise, con versiones para Windows, MacOS y Linux. MongoDB Enterprise Advanced , versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada. Instalaci\u00f3n \u00b6 Desde https://www.mongodb.com/try/download/community podemos descargar la versi\u00f3n Community acorde a nuestro sistema operativo. Independientemente de nuestro sistema operativo, por defecto, el demonio se lanza sobre el puerto 27017. Una vez instalado, si accedemos a http://localhost:27017 podremos ver que nos indica c\u00f3mo estamos intentando acceder mediante HTTP a MongoDB mediante el puerto reservado al driver nativo. Acceso al puerto 27017 En vez de instalarlo como un servicio en nuestra m\u00e1quina, a d\u00eda de hoy, es mucho m\u00e1s c\u00f3modo hacer uso de contenedores Docker o utilizar una soluci\u00f3n cloud . Docker \u00b6 Para lanzar el contenedor de Docker al que llamaremos iadb-mongo mediante el siguiente comando: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo MongoDB y procesadores AVX Si tenemos un procesador sin soporte para AVX, necesitamos instalar una versi\u00f3n inferior a la 5.0. As\u00ed pues, podemos indicar la versi\u00f3n 4.4: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo:4.4 A continuaci\u00f3n vamos a descargar el conjunto de datos sampledata.archive.gz que ofrece MongoDB a modo de prueba, el cual vamos a emplear a lo largo de las diferentes sesiones. Volvemos al terminal de nuestro sistema y copiamos los datos desde nuestro sistema a la carpeta /tmp del contenedor: docker cp sampledata.archive.gz iabd-mongo:/tmp Posteriormente abrimos un terminal dentro de nuestro contenedor (o mediante Attach Shell en VSCode ): docker exec -it iabd-mongo bash Y finalmente, restauramos los datos mediante mongorestore : mongorestore --gzip --archive = /tmp/sampledata.archive.gz Una vez cargados, nos informar\u00e1 que se han restaurado 433281 documentos. Mongo Atlas \u00b6 Y si preferimos una soluci\u00f3n cloud , disponemos de Mongo Atlas , que nos ofrece de manera gratuita un cluster compartido de servidores con 3 nodos y 512 MB para datos. Si queremos una soluci\u00f3n serverless o un servidor dedicado, ya tendremos que pasar por caja . Registro en Mongo Atlas Para comenzar a utilizar Mongo Atlas el primer paso es registrarnos y completar un cuestionario sobre nuestro uso. Tras ello: Creamos el cluster de despliegue. En nuestro caso, hemos realizado el despliegue en AWS en la regi\u00f3n de Paris ( eu-west-3 ) y dejado el nombre por defecto, Cluster 0 . Elecci\u00f3n del cluster Creamos un usuario/contrase\u00f1a para autenticar nuestra conexi\u00f3n. En nuestro caso, hemos creado el usuario iabd con la contrase\u00f1a iabdiabd (despu\u00e9s la podemos modificar desde el men\u00fa Security -> Database Access ): Configuraci\u00f3n del usuario En la misma pantalla, indicamos que permitimos las conexiones desde todas las direcciones IP (esta decisi\u00f3n s\u00f3lo la tomamos por comodidad, para poder conectarnos desde casa y el centro) mediante la IP 0.0.0.0 (despu\u00e9s podemos modificar la configuraci\u00f3n desde el men\u00fa Security -> Network Access ). Una vez realizados los dos pasos anteriores, comenzar\u00e1 la creaci\u00f3n del cluster, la cual puede tardar de 2 a 3 minutos. Dashboard del cluster A continuaci\u00f3n, cargaremos los datos de ejemplo. Para ello, en el men\u00fa con los tres puntos ( ... ), elegiremos la opci\u00f3n Load Sample Dataset . Una vez haya finalizado, podremos ver los datos cargados pulsando sobre el bot\u00f3n Browse Collections : Colecciones con los datos de prueba Conexi\u00f3n segura Mediante srv se establece una conexi\u00f3n segura Finalmente, para obtener la cadena de conexi\u00f3n, desde el dashboard del cluster con la opci\u00f3n Connect o desde la pesta\u00f1a Cmd Line Tools del propio cluster, podremos obtener la cadena de conexi\u00f3n , que tendr\u00e1 un formato similar a : mongodb+srv://usuario:password@host/basededatos A continuaci\u00f3n, vamos a conocer las diferentes herramientas que nos ofrece MongoDB para posteriormente estudiar todas las operaciones que podemos realizar. mongosh \u00b6 Tras arrancar el demonio mongod (el cual se lanza autom\u00e1ticamente mediante Docker o con el cluster de Mongo Atlas ) llega el momento de acceder mediante el cliente mongosh (en versiones anteriores el comando utilizado era mongo ), el cual funciona igual que un shell, de modo que con la fecha hacia arriba visualizaremos el \u00faltimo comando. El cliente utiliza JavaScript como lenguaje de interacci\u00f3n con la base de datos. Si nos conectamos desde Docker, no necesitamos instalarlo. Primero nos conectamos al contenedor: docker exec -it iabd-mongo bash Al conectar con mongosh si no le indicamos nada se conectar\u00e1 por defecto a la base de datos test de localhost . Si queremos conectarnos a una base de datos concreta, por ejemplo a sample_training , la pasaremos como par\u00e1metro: root @ 3 ad17b675fb1 : /# mongosh sample_training Current Mongosh Log ID : 6316498 f30f8283fedcfabc2 Connecting to : mongodb : //127.0.0.1:27017/sample_training?directConnection=true&serverSelectionTimeoutMS=2000 Using MongoDB : 5.0.4 Using Mongosh : 1.1.2 Si queremos ver las bases de datos que existen ejecutaremos el comando show dbs : sample_training> show dbs ; admin 41 kB config 73 .7 kB local 73 .7 kB sample_airbnb 55 .1 MB sample_analytics 9 .9 MB sample_geospatial 999 kB sample_mflix 48 .5 MB sample_restaurants 6 .2 MB sample_supplies 991 kB sample_training 43 .4 MB sample_weatherdata 2 .49 MB Si nos quisi\u00e9ramos conectar a nuestro cluster de Mongo Atlas utilizaremos la cadena de conexi\u00f3n tras el comando mongosh : root@3ad17b675fb1:/# mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/test Current Mongosh Log ID: 63164ac26030844c1576f8b4 Connecting to: mongodb+srv://<credentials>@cluster0.dfaz5er.mongodb.net/test Using MongoDB: 5 .0.11 Using Mongosh: 1 .1.2 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ Atlas atlas-4wikkb-shard-0 [ primary ] test> Uso externo Si no queremos tener que conectarnos al contenedor o vamos a trabajar con un servidor remoto, podemos instalar \u00fanicamente el shell desde https://www.mongodb.com/try/download/shell MongoDB Database Tools \u00b6 Adem\u00e1s del propio servidor de MongoDB y el cliente para conectarse a \u00e9l, MongoDB ofrece un conjunto de herramientas para interactuar con las bases de datos, permitiendo crear y restaurar copias de seguridad. Si estamos interesados en introducir o exportar una colecci\u00f3n de datos mediante JSON, podemos emplear los comandos mongoimport y mongoexport : mongoimport -d nombreBaseDatos -c coleccion \u2013-file nombreFichero.json mongoexport -d nombreBaseDatos -c coleccion nombreFichero.json Estas herramientas interact\u00faan con datos JSON y no sobre toda la base de datos. Un caso particular y muy com\u00fan es importar datos que se encuentran en formato CSV/TSV. Para ello, emplearemos el par\u00e1metro --type csv : mongoimport --type tsv -d test -c poblacion --headerline --drop poblacionEspanya2013.tsv En vez de realizar un export , es m\u00e1s conveniente realizar un backup en binario mediante mongodump , el cual genera ficheros BSON. Estos archivos posteriormente se restauran mediante mongorestore : mongodump -d nombreBaseDatos nombreFichero.bson mongorestore -d nombreBaseDatos nombreFichero.bson Autoevaluaci\u00f3n Intenta exportar los datos de la base de datos sample_training desde MongoAtlas. Veras que ha creado una carpeta que contiene dos archivos \u00bfCu\u00e1les son? \u00bfQu\u00e9 contiene cada uno de ellos y cual es su formato? Si necesitamos transformar un fichero BSON a JSON (de binario a texto), tenemos el comando bsondump : bsondump file.bson > file.json Info M\u00e1s informaci\u00f3n sobre copias de seguridad en https://www.mongodb.com/docs/manual/core/backups/ . Para poder trabajar con MongoDB desde cualquier aplicaci\u00f3n necesitamos un driver. MongoDB ofrece drivers oficiales para casi todos los lenguajes de programaci\u00f3n actuales. En la sesi\u00f3n 28 de 'MongoDB y Python' trabajaremos con PyMongo . Monitorizaci\u00f3n Tanto mongostat como mongotop permiten visualizar el estado del servidor MongoDB, as\u00ed como algunas estad\u00edsticas sobre su rendimiento. Si trabajamos con MongoAtlas estas herramientas est\u00e1n integradas en las diferentes herramientas de monitorizaci\u00f3n de la plataforma. En versiones anteriores, una herramienta de terceros bastante utilizada era RoboMongo / Robo3T / Studio3T el cual extiende el shell y ofrece un IDE m\u00e1s amigable. A d\u00edas de hoy, MongoDB tiene su propio IDE conocido como Mongo Compass . MongoDB Compass \u00b6 En el curso nos vamos a centrar en el uso del shell y la conectividad de MongoDB mediante Python, pero no est\u00e1 de m\u00e1s conocer las herramientas visuales que facilitan el trabajo con MongoDB en el d\u00eda a d\u00eda. Una de ellas es MongoDB Compass , que facilita la exploraci\u00f3n y manipulaci\u00f3n de los datos. De una manera flexible e intuitiva, Compass ofrece visualizaciones detalladas de los esquemas, m\u00e9tricas de rendimiento en tiempo real as\u00ed como herramientas para la creaci\u00f3n de consultas. Existen tres versiones de Compass, una completa con todas las caracter\u00edsticas, una de s\u00f3lo lectura sin posibilidad de insertar, modificar o eliminar datos (perfecta para anal\u00edtica de datos) y una \u00faltima versi\u00f3n isolated que solo permite la conexi\u00f3n a una instancia local. Una vez descargada e instalada la versi\u00f3n que nos interesa, tras crear la conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), veremos en el men\u00fa de la izquierda un resumen del cluster, as\u00ed como las consultas que vayamos almacenando y las diferentes bases de datos almacenadas: GUI de Mongo Compass Si seleccionamos una base de datos concreta, y de ella, una colecci\u00f3n en el men\u00fa de la izquierda, en el panel central tendremos una visualizaci\u00f3n de los datos contenidos, as\u00ed como opciones para ver su esquema, realizar consultas agregadas, editar los \u00edndices, etc... Adem\u00e1s, podremos realizar consultas sobre los datos: Opciones desde una colecci\u00f3n mongosh en Compass Si te fijas, en la barra inferior podemos desplegar un panel para interactuar mediante comandos como lo har\u00edamos desde mongosh . MongoDB for VSCode \u00b6 Tambi\u00e9n podemos utilizar la extensi\u00f3n que lleva VSCode para trabajar con MongoDB . Tras su instalaci\u00f3n creamos una conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), y una vez conectados, podremos recorrer las colecciones con los datos as\u00ed como utilizar un playground para interactuar de manera similar al shell: Uso de la extensi\u00f3n de VSCode Hola MongoDB \u00b6 Pues una vez que ya nos hemos conectado a MongoDB mediante mongosh , vamos a empezar a interactuar con los datos. En cualquier momento podemos cambiar la base de datos activa mediante use nombreBaseDatos . Si la base de datos no existiese, MongoDB crear\u00e1 dicha base de datos. Esto es una verdad a medias, ya que la base de datos realmente se crea al insertar datos dentro de alguna colecci\u00f3n. As\u00ed pues, vamos a crear nuestra base de datos iabd : use iabd Una vez creada, podemos crear nuestra primera colecci\u00f3n, que llamaremos people , e insertaremos un persona con nuestros datos personales mediante el m\u00e9todo insertOne , al que le pasamos un objeto JSON: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Tras ejecutar el comando, veremos que nos devuelve un objeto JSON con su ACK y el identificador del documento insertado: { ack n owledged : true , i nserte dId : Objec t Id( \"6316fc938cc2bc168bfed066\" ) } Una vez insertada, s\u00f3lo nos queda realizar una consulta para recuperar los datos y comprobar que todo funciona correctamente mediante el m\u00e9todo findOne : db . people . findOne () Lo que nos dar\u00e1 como resultado un objeto JSON que contiene un atributo _id con el mismo identificador mostrado anteriormente, adem\u00e1s de los que le a\u00f1adimos al insertar la persona: { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' } Como podemos observar, todas las instrucciones van a seguir el patr\u00f3n de db.nombreColeccion.operacion() . Trabajando con el shell \u00b6 Antes de entrar en detalles en las instrucciones necesarias para realizar las operaciones CRUD, veamos algunos comandos que nos ser\u00e1n muy \u00fatiles al interactuar con el shell: Comando Funci\u00f3n show dbs Muestra el nombre de las bases de datos show collections Muestra el nombre de las colecciones db Muestra el nombre de la base de datos que estamos utilizando db.dropDatabase() Elimina la base de datos actual db.help() Muestra los comandos disponibles db.version() Muestra la versi\u00f3n actual del servidor En el resto de la sesi\u00f3n vamos a hacer un uso intenso del shell de MongoDB. Por ejemplo, si nos basamos en el objeto definido en el apartado de BSON, podemos ejecutar las siguientes instrucciones: > db . people . insertOne ( yo ) // (1) < { acknowledged : true , insertedId : ObjectId ( \"631704a042aae0893122f2d6\" ) } > db . people . find () // (2) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) } ] > yo . profesion = \"Profesor\" < Profesor > db . people . insertOne ( yo ) // (3) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) }, { _id : ObjectId ( \"6317056d42aae0893122f2d7\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662453101 , i : 1 }), profesion : 'Profesor' } ] > db . people . countDocuments () // (4) < 3 Si queremos insertar un documento en una colecci\u00f3n, hemos de utilizar el m\u00e9todo insertOne pas\u00e1ndole como par\u00e1metro el documento que queremos insertar, ya sea a partir de una variable o el propio documento en s\u00ed. find recupera todos los documentos de la colecci\u00f3n Modificamos nuestro documento y los volvemos a insertar. Realmente va a crear un nuevo documento, y no se va a quejar de que ya exista, porque nuestro documento no contiene ning\u00fan atributo identificador, por lo que considera que se trata de una nueva persona. Obtenemos la cantidad de documentos de la colecci\u00f3n mediante countDocuments . Con este ejemplo, hemos podido observar como los documentos de una misma colecci\u00f3n no tienen por qu\u00e9 tener el mismo esquema, ni hemos necesitado definirlo expl\u00edcitamente antes de insertar datos. As\u00ed pues, el esquema se ir\u00e1 generando y actualizando conforme se inserten documentos. M\u00e1s adelante veremos que podemos definir un esquema para validar que los datos que insertamos cumplan restricciones de tipos de datos o elementos que obligatoriamente deben estar rellenados. Empleando JavaScript \u00b6 Ya hemos comentado que el shell utiliza JavaScript como lenguaje de interacci\u00f3n, por lo que podemos almacenar los comandos en un script externo y ejecutarlo mediante load() : load ( \"scripts/misDatos.js\" ); load ( \"/data/db/scripts/misDatos.js\" ); Si hacemos una referencia relativa, lo hace respecto a la ruta desde la cual se ejecuta el shell mongosh . Otra manera de lanzar un script es hacerlo desde la l\u00ednea de comandos, pas\u00e1ndole como segundo par\u00e1metro el script a ejecutar: mongosh iabd misDatos.js Si el c\u00f3digo a ejecutar no necesita almacenarse en un script externo, el propio shell permite introducir instrucciones en varias l\u00edneas: > for ( var i = 0 ; i < 10 ; i ++ ) { ... db . espias . insertOne ({ \"nombre\" : \"James Bond \" + i , \"agente\" : \"00\" + i }); ... } < { acknowledged : true , insertedId : ObjectId ( \"63171d2142aae0893122f2e1\" ) } > db . espias . find () < [ { _id : ObjectId ( \"63171d2142aae0893122f2d8\" ), nombre : 'James Bond 0' , agente : '000' }, { _id : ObjectId ( \"63171d2142aae0893122f2d9\" ), nombre : 'James Bond 1' , agente : '001' }, ... { _id : ObjectId ( \"63171d2142aae0893122f2e1\" ), nombre : 'James Bond 9' , agente : '009' } ] ObjectId \u00b6 En MongoDB , el atributo _id es \u00fanico dentro de la colecci\u00f3n, y hace la funci\u00f3n de clave primaria. Se le asocia un ObjectId , el cual es un tipo BSON de 12 bytes que se crea mediante: el timestamp actual (4 bytes) un valor aleatorio y \u00fanico por m\u00e1quina y proceso (5 bytes) un contador inicializado a n\u00famero aleatorio (3 bytes). Este objeto lo crea el driver y no MongoDB , por lo cual no deberemos considerar que siguen un orden concreto, ya que clientes diferentes pueden tener timestamps desincronizados. Lo que s\u00ed que podemos obtener a partir del ObjectId es la fecha de creaci\u00f3n del documento, mediante el m\u00e9todo getTimestamp() del atributo _id . Obteniendo la fecha de creaci\u00f3n de un documento > db.people.findOne () ._id < ObjectId ( \"6316fc938cc2bc168bfed066\" ) > db.people.findOne () ._id.getTimestamp () < ISODate ( \"2022-09-06T07:53:55.000Z\" ) Este identificador es global, \u00fanico e inmutable. Esto es, no habr\u00e1 dos repetidos y una vez un documento tiene un _id , \u00e9ste no se puede modificar. Si en la definici\u00f3n del objeto a insertar no ponemos el atributo identificador, MongoDB crear\u00e1 uno de manera autom\u00e1tica. Si lo ponemos nosotros de manera expl\u00edcita, MongoDB no a\u00f1adir\u00e1 ning\u00fan ObjectId . Eso s\u00ed, debemos asegurarnos que sea \u00fanico (podemos usar n\u00fameros, cadenas, etc\u2026\u200b). Por lo tanto, podemos asignar un identificador al insertar: > db . people . insert ({ _id : 4 , nombre : \"Marina\" , edad : 14 }) < { acknowledged : true , insertedIds : { '0' : 4 } } Tipos de datos Cuidado con los tipos, ya que no es lo mismo insertar un atributo con edad:14 (se considera el campo como entero) que con edad:\"14\" , ya que considera el campo como texto. O tambi\u00e9n, si queremos podemos hacer que el _id de un documento sea un documento en s\u00ed, y no un entero, para ello, al insertarlo, podemos asignarle un objeto JSON al atributo identificador: > db . people . insertOne ({ _id : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' }, ciudad : 'Elx' }) < { acknowledged : true , insertedId : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' } } Recuperando datos \u00b6 Para recuperar los datos de una colecci\u00f3n o un documento en concreto usaremos el m\u00e9todo find() : > db . people . find () < { _id : ObjectId ( \"6316fc1597eb703de2add36e\" ), nombre : 'Aitor' , edad : 45 , profesion : 'Profesor' } { _id : ObjectId ( \"6317048697eb703de2add36f\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : 1977 - 10 - 02 T23 : 00 : 00 .000 Z , hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452870 , i : 3 }) } El m\u00e9todo find() sobre una colecci\u00f3n devuelve un cursor a los datos obtenidos, el cual se queda abierto con el servidor y que se cierra autom\u00e1ticamente a los 30 minutos de inactividad o al finalizar su recorrido. Si hay muchos resultados, la consola nos mostrar\u00e1 un subconjunto de los datos (20). Si queremos seguir obteniendo resultados, solo tenemos que introducir it , para que contin\u00fae iterando el cursor. En cambio, si s\u00f3lo queremos recuperar un documento hemos de utilizar findOne() : > db.people. f i n dO ne () > { _id : Objec t Id( \"6316fc1597eb703de2add36e\" ) , n ombre : 'Ai t or' , edad : 45 , pro fes io n : 'Pro fes or' } Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de 10.000 documentos sobre los viajes realizados por los usuarios de una empresa de alquiler de bicicletas, los cuales han sido extra\u00eddos de https://ride.citibikenyc.com/system-data . Esta colecci\u00f3n ( trips ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de viaje ser\u00eda: > use sample_training < 'switched to db sample_training' > db . trips . findOne () < { _id : ObjectId ( \"572bb8222b288919b68abf5b\" ), tripduration : 889 , 'start station id' : 268 , 'start station name' : 'Howard St & Centre St' , 'end station id' : 3002 , 'end station name' : 'South End Ave & Liberty St' , bikeid : 22794 , usertype : 'Subscriber' , 'birth year' : 1961 , 'start station location' : { type : 'Point' , coordinates : [ - 73.99973337 , 40.71910537 ] }, 'end station location' : { type : 'Point' , coordinates : [ - 74.015756 , 40.711512 ] }, 'start time' : 2016 - 01 - 01 T00 : 01 : 06 .000 Z , 'stop time' : 2016 - 01 - 01 T00 : 15 : 56.000 Z } Criterios en consultas \u00b6 Al hacer una consulta, si queremos obtener datos mediante m\u00e1s de un criterio, en el primer par\u00e1metro del find podemos pasar un objeto JSON con los campos a cumplir (condici\u00f3n Y). Consulta Resultado db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) { _id : Objec t Id( \"572bb8222b288919b68ad197\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 23724 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 45.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 48.000 Z } { _id : Objec t Id( \"572bb8222b288919b68ad191\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 17075 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 38.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 42.000 Z } Consejo de Rendimiento Las consultas disyuntivas, es decir, con varios criterios u operador $and , deben filtrar el conjunto m\u00e1s peque\u00f1o cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar documentos que cumplen los criterios A, B y C. Digamos que el criterio A lo cumplen 40.000 documentos, el B lo hacen 9.000 y el C s\u00f3lo 200. Si filtramos A, luego B, y finalmente C, el conjunto que trabaja cada criterio es muy grande. Restringiendo consultas AND En cambio, si hacemos una consulta que primero empiece por el criterio m\u00e1s restrictivo, el resultado con lo que se intersecciona el siguiente criterio es menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas AND de menor a mayor MongoDB tambi\u00e9n ofrece operadores l\u00f3gicos para los campos num\u00e9ricos: Comparador Operador menor que ( < ) $lt menor o igual que ( \u2264 ) $lte mayor que ( > ) $gt mayor o igual que ( \u2265 ) $gte Estos operadores se pueden utilizar de forma simult\u00e1nea sobre un mismo campo o sobre diferentes campos, sobre campos anidados o que forman parte de un array, y se colocan como un nuevo documento en el valor del campo a filtrar, compuesto del operador y del valor a comparar mediante la siguiente sintaxis: db . < coleccion > . find ({ < campo >: { < operador >: < valor > } }) Por ejemplo, para recuperar los viajes que han durado menos de 5 minutos o comprendidos entre 3 y 5 minutos (el campo almacena el tiempo en segundos), podemos hacer: db . trips . find ({ \"tripduration\" : { $lt : 300 } }) db . trips . find ({ \"tripduration\" : { $gt : 180 , $lte : 300 } }) Para los campos de texto, adem\u00e1s de la comparaci\u00f3n directa, podemos usar el operador $ne para obtener los documentos cuyo campos no tienen un determinado valor ( not equal ). As\u00ed pues, podemos usarlo para averiguar todos los trayectos realizados por usuarios que no son subscriptores ( Subscriber ): db . trips . find ({ \"usertype\" : { $ne : \"Subscriber\" } }) Por supuesto, podemos tener diferentes operadores en campos distintos. Por ejemplo, si queremos ver los viajes de menos de un minuto y medio realizado por usuarios que no son subscriptores har\u00edamos: db . trips . find ({ \"tripduration\" : { $lt : 90 }, \"usertype\" : { $ne : \"Subscriber\" } }) Case sensitive Las comparaciones de cadenas se realizan siguiendo el orden UTF8, similar a ASCII, con lo cual no es lo mismo buscar un rango entre may\u00fasculas que min\u00fasculas. Con cierto parecido a la condici\u00f3n de valor no nulo de las BBDD relacionales y teniendo en cuenta que la libertad de esquema puede provocar que un documento tenga unos campos determinados y otro no lo tenga, podemos utilizar el operador $exists si queremos averiguar si un campo existe (y por tanto tiene alg\u00fan valor). db . people . find ({ \"edad\" : { $exists : true }}) Polimorfismo Mucho cuidado al usar polimorfismo y almacenar en un mismo campo un entero y una cadena, ya que al hacer comparaciones para recuperar datos, no vamos a poder mezclar cadenas con valores num\u00e9ricos. Se considera un antipatr\u00f3n el mezclar tipos de datos en un campo. Pese a que ciertos operadores contengan su correspondiente operador negado, MongoDB ofrece el operador $not . \u00c9ste puede utilizarse conjuntamente con otros operadores para negar el resultado de los documentos obtenidos. Por ejemplo, si queremos obtener todas las personas cuya edad no sea m\u00faltiplo de 5, podr\u00edamos hacerlo as\u00ed: db . people . find ({ edad : { $not : { $mod : [ 5 , 0 ]}}}) Expresiones regulares \u00b6 Finalmente, si queremos realizar consultas sobre partes de un campo de texto, hemos de emplear expresiones regulares. Para ello, tenemos el operador $regexp o, de manera m\u00e1s sencilla, indicando como valor la expresi\u00f3n regular a cumplir: Por ejemplo, para buscar la cantidad de viajes que salen da alguna estaci\u00f3n cuyo nombre contenga Central Park podemos hacer: db . trips . find ({ \"start station name\" : /Central Park/ }). count () db . trips . find ({ \"start station name\" : /central park/i }). count () db . trips . find ({ \"start station name\" : { $regex : /central park/i }}). count () B\u00fasquedas sobre textos Si vamos a realizar b\u00fasquedas intensivas sobre texto, desde MongoDB han creado un producto espec\u00edfico dentro del ecosistema de Mongo Atlas el cual ofrece un mejor rendimiento y mayor funcionalidad que el uso de expresiones regulares, conocido con Mongo Atlas Search . Si usamos una soluci\u00f3n on-premise , mediante \u00edndices de texto y el operator $text podemos realizar b\u00fasquedas. Operador $expr \u00b6 El operador $expr es un operador de consulta expresiva que permite utilizar expresiones de agregaci\u00f3n dentro de las consultas. Permite utilizar variables y sentencias condicionales, asi como comparar campos dentro de un documento. As\u00ed pues, si queremos comparar valores entre dos campos, podemos hacerlo mediante $expr referenciando a los campos anteponiendo un dolar ( $ ) delante del campo, de manera que si queremos obtener los viajes que comienzan y finalizan en la misma estaci\u00f3n podemos hacer: db . trips . find ({ \"$expr\" : { \"$eq\" : [ \"$end station id\" , \"$start station id\" ]}}) Al poner el $ delante de un campo, en vez de referenciar al campo, lo que hace es referenciar a su valor, por lo que $end station id est\u00e1 referenciando al valor del campo end station id . Otros operadores El operador $type permite recuperar documentos que dependan del tipo de campo que contiene. El operador $where permite introducir una expresi\u00f3n JavaScript . Proyecci\u00f3n de campos \u00b6 Las consultas realizadas hasta ahora devuelven los documentos completos. Si queremos que devuelva un campo o varios campos en concreto, hemos de pasar un segundo par\u00e1metro de tipo JSON con aquellos campos que deseamos mostrar con el valor true o 1 . Destacar que si no se indica nada, por defecto siempre mostrar\u00e1 el campo _id > db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }, { tripduration : 1 }) < { _id : ObjectId ( \"572bb8222b288919b68ad191\" ), tripduration : 1143 } Por lo tanto, si queremos que no se muestre el _id , lo podremos a false o 0 : > db. tr ips. f i n d( { 's tart s tat io n id' : 405 , 'e n d s tat io n id' : 146 }, { tr ipdura t io n : 1 , _id : 0 } ) < { tr ipdura t io n : 1143 } No mezcles Al hacer una proyecci\u00f3n, no podemos mezclar campos que se vean ( 1 ) con los que no ( 0 ). Es decir, hemos de hacer algo similar a: db . < coleccion > . find ({ < consulta > }, { < campo1 >: 1 , < campo2 >: 1 }) db . < coleccion > . find ({ < consulta > }, { < campo1 >: 0 , < campo2 >: 0 }) As\u00ed pues, s\u00f3lo se mezclar\u00e1 la visibilidad de los campos cuando queramos ocultar el _id . Condiciones compuestas con Y / O \u00b6 Para usar la conjunci\u00f3n o la disyunci\u00f3n, tenemos los operadores $and y $or . Son operadores prefijo, de modo que se ponen antes de las subconsultas que se van a evaluar. Estos operadores trabajan con arrays, donde cada uno de los elementos es un documento con la condici\u00f3n a evaluar, de modo que se realiza la uni\u00f3n entre estas condiciones, aplicando la l\u00f3gica asociada a AND y a OR. db . trips . find ({ $or : [{ 'start station id' : 405 }, { 'end station id' : 146 }] }) db . trips . find ({ $or : [{ \"tripduration\" : { $lte : 70 }}, { \"tripduration\" : { $gte : 3600 }}] }) Realmente el operador $and no se suele usar porque podemos anidar en la consulta dos criterios, al poner uno dentro del otro. As\u00ed pues, estas dos consultas hacen lo mismo: db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) db . trips . find ({ $and : [ { 'start station id' : 405 }, { 'end station id' : 146 } ] }) Consejo de Rendimiento Las consultas conjuntivas, es decir, con varios criterios excluyentes u operador $or , deben filtrar el conjunto m\u00e1s grande cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar los mismos documentos que cumplen los criterios A (40.000 documentos), B (9.000 documentos) y C (200 documentos). Si filtramos C, luego B, y finalmente A, el conjunto de documentos que tiene que comprobar MongoDB es muy grande. Restringiendo consultas OR de menor a mayor En cambio, si hacemos una consulta que primero empiece por el criterio menos restrictivo, el conjunto de documentos sobre el cual va a tener que comprobar siguientes criterios va a ser menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas OR de mayor a menor Tambi\u00e9n podemos utilizar el operado $nor , que no es m\u00e1s que la negaci\u00f3n de $or y que obtendr\u00e1 aquellos documentos que no cumplan ninguna de las condiciones. Autoevaluaci\u00f3n Que obtendr\u00edamos al ejecutar la siguiente consulta: db . trips . find ({ \"tripduration\" : { $lte : 65 }, $nor : [ { usertype : \"Customer\" }, { \"birth year\" : 1989 } ] }) Finalmente, si queremos indicar mediante un array los diferentes valores que puede cumplir un campo, podemos utilizar el operador $in : db . trips . find ({ \"birth year\" : { $in : [ 1977 , 1980 ]} }) Por supuesto, tambi\u00e9n existe su negaci\u00f3n mediante $nin . Condiciones sobre objetos anidados \u00b6 Si queremos acceder a campos de subdocumentos, siguiendo la sintaxis de JSON, se utiliza la notaci\u00f3n punto. Esta notaci\u00f3n permite acceder al campo de un documento anidado, da igual el nivel en el que est\u00e9 y su orden respecto al resto de campos. Preparando los ejemplos Para los siguientes ejemplos sobre documentos anidados y arrays, vamos a utilizar una colecci\u00f3n de 500 documentos sobre mensajes de un blog. Esta colecci\u00f3n ( posts ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de mensaje ser\u00eda: > use sample_training 'switched to db sample_training' > db . posts . findOne () < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc3f9\" ), body : 'Amendment I\\n<p>Congress shall make ....\"\\n<p>\\n' , permalink : 'aRjNnLZkJkTyspAIoRGe' , author : 'machine' , title : 'Bill of Rights' , tags : 'santa' , 'xylophone' , 'math' , 'dream' , 'action' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'HvizfYVx@pKvLaagH.com' , author : 'Santiago Dollins' }, { body : 'Lorem ipsum dolor sit...' , email : 'WpOUCpdD@hccdxJvT.com' , author : 'Jaclyn Morado' }, { body : 'Lorem ipsum dolor sit amet...' , email : 'OgDzHfFN@cWsDtCtx.com' , author : 'Houston Valenti' }], date : 2012 - 11 - 20 T05 : 05 : 15.231 Z } Para acceder al autor de los comentarios de un mensaje usar\u00edamos la propiedad comments.author . Por ejemplo, para averiguar los mensajes titulados Bill of Rights y que tienen alg\u00fan comentario creado por Santiago Dollins har\u00edamos: db . posts . find ({ title : 'Bill of Rights' , \"comments.author\" : 'Santiago Dollins' }) Consultas sobre arrays \u00b6 Si trabajamos con arrays, vamos a poder consultar el contenido de una posici\u00f3n del mismo tal como si fuera un campo normal, siempre que sea un campo de primer nivel, es decir, no sea un documento embebido dentro de un array. Si queremos filtrar teniendo en cuenta el n\u00famero de ocurrencias del array, podemos utilizar: $all para filtrar ocurrencias que tienen todos los valores del array, es decir, los valores pasados a la consulta ser\u00e1n un subconjunto del resultado. Puede que devuelva los mismos, o un array con m\u00e1s campos (el orden no importa) $in , igual que SQL, para obtener las ocurrencias que cumple con alguno de los valores pasados (similar a usar $or sobre un conjunto de valores de un mismo campo). Si queremos su negaci\u00f3n, usaremos $nin , para obtener los documentos que no cumplen ninguno de los valores. Por ejemplo, si queremos obtener los mensajes que contenga las etiquetas dream y action tendr\u00edamos: db . posts . find ( { tags : { $all : [ \"dream\" , \"action\" ]}} ) En cambio, si queremos los mensajes que contengan alguna de esas etiquetas har\u00edamos: db . posts . find ( { tags : { $in : [ \"dream\" , \"action\" ]}} ) Si el array contiene documentos y queremos filtrar la consulta sobre los campos de los documentos del array, tenemos que utilizar $elemMatch , de manera que obtengamos aquellos que al menos encuentre un elemento que cumpla el criterio. As\u00ed pues, si queremos recuperar los mensajes que tienen un comentario cuyo autor sea Santiago Dollins har\u00edamos: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Criterio con notaci\u00f3n punto En el ejemplo anterior, si s\u00f3lo hubi\u00e9ramos tenido un campo para el filtrado, podr\u00edamos haber utilizado la notaci\u00f3n punto comments.author . Si s\u00f3lo queremos los comentarios escritos por un determinado autor, adem\u00e1s de en el filtrado, hemos de indicarlo en la proyecci\u00f3n: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}}, { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Si lo que nos interesa es la cantidad de elementos que contiene un array, emplearemos el operador $size . Por ejemplo, para obtener los mensajes que tienen 10 etiquetas har\u00edamos: db . posts . find ( { tags : { $size : 10 }} ) Finalmente, a la hora de proyectar los datos, si no estamos interesados en todos los valores de un campo que es un array, podemos restringir el resultado mediante el operador $slice . As\u00ed pues, si quisi\u00e9ramos obtener los mensajes titulados US Constitution y que de esos mensajes, mostrara s\u00f3lo tres etiquetas y dos comentarios, har\u00edamos: > db . posts . find ( { title : \"US Constitution\" }, { comments : { $slice : 2 }, tags : { $slice : 3 }} ) < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc416\" ), body : 'We the People ...' , permalink : 'NhWDUNColpvxFjovsgqU' , author : 'machine' , title : 'US Constitution' , tags : [ 'engineer' , 'granddaughter' , 'sundial' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'ftRlVMZN@auLhwhlj.com' , author : 'Leonida Lafond' }, { body : 'Lorem ipsum dolor sit...' , email : 'dsoLAdFS@VGBBuDVs.com' , author : 'Nobuko Linzey' } ], date : 2012 - 11 - 20 T05 : 05 : 15.276 Z } Conjunto de valores \u00b6 Igual que en SQL, a partir de un colecci\u00f3n, si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct : > db . trips . distinct ( 'usertype' ) < [ 'Customer' , 'Subscriber' ] Si queremos filtrar los datos sobre los que se obtienen los valores, le pasaremos un segundo par\u00e1metro con el criterio a aplicar: > db . trips . distinct ( 'usertype' , { \"birth year\" : { $gt : 1990 } } ) < [ 'Subscriber' ] Cursores \u00b6 Al hacer una consulta en el shell se devuelve un cursor. Este cursor lo podemos guardar en un variable, y partir de ah\u00ed trabajar con \u00e9l como har\u00edamos mediante cualquier lenguaje de programaci\u00f3n. Si cur es la variable que referencia al cursor, podremos utilizar los siguientes m\u00e9todos: M\u00e9todo Uso Lugar de ejecuci\u00f3n cur.hasNext() true / false para saber si quedan elementos Cliente cur.next() Pasa al siguiente documento Cliente cur.limit(cantidad) Restringe el n\u00famero de resultados a cantidad Servidor cur.sort({campo:1}) Ordena los datos por campo: 1 ascendente o -1 o descendente Servidor cur.skip(cantidad) Permite saltar cantidad elementos con el cursor Servidor La consulta no se ejecuta hasta que el cursor comprueba o pasa al siguiente documento ( next / hasNext ), por ello que tanto limit como sort (ambos modifican el cursor) s\u00f3lo se pueden realizar antes de recorrer cualquier elemento del cursor. Como tras realizar una consulta con find , realmente se devuelve un cursor, un uso muy habitual es encadenar una operaci\u00f3n de find con sort y/o limit para ordenar el resultado por uno o m\u00e1s campos y posteriormente limitar el n\u00famero de documentos a devolver. As\u00ed pues, si quisi\u00e9ramos obtener los tres viajes que m\u00e1s han durado, podr\u00edamos hacerlo as\u00ed: db . trips . find (). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Tambi\u00e9n podemos filtrar previamente a ordenar y limitar: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Finalmente, podemos paginar utilizando el m\u00e9todo skip , para mostrar viajes de 10 en 10 a partir de la tercera p\u00e1gina, podr\u00edamos hacer algo as\u00ed: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 10 ). skip ( 20 ) Autoevaluaci\u00f3n A partir de la colecci\u00f3n trips , escribe un consulta que recupere los viajes realizados por subscriptores ordenados descendentemente por su duraci\u00f3n y que obtenga los documentos de 15 al 20. Contando Documentos \u00b6 Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo countDocuments . Por ejemplo: > db . trips . countDocuments ({ \"birth year\" : 1977 }) < 186 > db . trips . countDocuments ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 116 count Desde la versi\u00f3n 4.0, los m\u00e9todos count a nivel de colecci\u00f3n y de cursor est\u00e1n caducados ( deprecated ), y no se recomienda su utilizaci\u00f3n. A\u00fan as\u00ed, es muy com\u00fan utilizarlo como m\u00e9todo de un cursor: db . trips . find ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}). count () Cuando tenemos much\u00edsimos datos, si no necesitamos exactitud pero queremos un valor estimado el cual tarde menos en conseguirse (utiliza los metadatos de las colecciones), podemos usar estimatedDocumentCount > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 }) < 10.000 > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 10.000 Modificando documentos \u00b6 Preparando un persona Para este apartado, vamos a insertar dos veces la misma persona sobre la cual realizaremos las modificaciones: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Para actualizar (y fusionar datos), se utilizan los m\u00e9todos updateOne / updateMany dependiendo de cuantos documentos queremos que modifique. Ambos m\u00e9todos requieren 2 par\u00e1metros: el primero es la consulta para averiguar sobre qu\u00e9 documentos, y en el segundo par\u00e1metro, los campos a modificar utilizando los operadores de actualizaci\u00f3n: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $set : { nombre : \"Marina Medrano\" , salario : 123456 }}) Al realizar la modificaci\u00f3n, el shell nos devolver\u00e1 informaci\u00f3n sobre cuantos documentos ha encontrado, modificado y m\u00e1s informaci\u00f3n: { ack n owledged : true , i nserte dId : null , ma t chedCou nt : 1 , modi f iedCou nt : 1 , upser te dCou nt : 0 } Como hay m\u00e1s de una persona con el mismo nombre, al haber utilizado updateOne s\u00f3lo modificar\u00e1 el primer documento que ha encontrado. \u00a1Cuidado! En versiones antiguas de MongoDB, adem\u00e1s de utilizar los operadores de actualizaci\u00f3n, pod\u00edamos pasarle como par\u00e1metro un documento, de manera que MongoDB realizaba un reemplazo de los campos, es decir, si en el origen hab\u00eda 100 campos y en la operaci\u00f3n de modificaci\u00f3n s\u00f3lo pon\u00edamos 2, el resultado \u00fanicamente contendr\u00eda 2 campos. Es por ello, que ahora es obligatorio utilizar los operadores. Si cuando vamos a actualizar, en el criterio de selecci\u00f3n no encuentra el documento sobre el que hacer los cambios, no se realiza ninguna acci\u00f3n. Si quisi\u00e9ramos que en el caso de no encontrar nada insertase un nuevo documento, acci\u00f3n conocida como upsert ( update + insert ), hay que pasarle un tercer par\u00e1metro al m\u00e9todo con el objeto {upsert:true} . Si encuentra el documento, lo modificar\u00e1, pero si no, crear\u00e1 uno nuevo: db . people . updateOne ({ nombre : \"Andreu Medrano\" }, { $set : { name : \"Andreu Medrano\" , twitter : \"@andreumedrano\" }}, { upsert : true }) Operadores de actualizaci\u00f3n \u00b6 MongoDB ofrece un conjunto de operadores para simplificar la modificaci\u00f3n de campos. El operador m\u00e1s utilizado es el operador $set , el cual admite los campos que se van a modificar. Si el campo no existe, lo crear\u00e1. Por ejemplo, para modificar el salario har\u00edamos: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $se t :{ salario : 1000000 }} ) Mediante $inc podemos incrementar el valor de una variable: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $i n c :{ salario : 1000 }} ) Para eliminar un campo de un documento, usaremos el operador $unset . De este modo, para eliminar el campo twitter de una persona har\u00edamos: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $unset : { twitter : '' }}) Otros operadores que podemos utilizar son $mul , $min , $max y $currentDate . Podemos consultar todos los operadores disponibles en https://www.mongodb.com/docs/manual/reference/operator/update/ Autoevaluaci\u00f3n Tras realizar la siguiente operaci\u00f3n sobre una colecci\u00f3n vac\u00eda: db . people . updateOne ({ nombre : 'yo' }, { '$set' : { 'hobbies' : [ 'gaming' , 'sofing' ]}}, { upsert : true } ); \u00bfCu\u00e1l es el estado de la colecci\u00f3n? Finalmente, un caso particular de las actualizaciones es la posibilidad de renombrar un campo mediante el operador $rename : db . people . updateMany ( { _id : 1 }, { $rename : { 'nickname' : 'alias' , 'cell' : 'movil' }}) Podemos consultar todas las opciones de configuraci\u00f3n de una actualizaci\u00f3n en https://www.mongodb.com/docs/manual/reference/method/db.collection.update/ . Control de la concurrencia \u00b6 Cuando se hace una actualizaci\u00f3n m\u00faltiple, MongoDB no realiza la operaci\u00f3n de manera at\u00f3mica (a no ser que utilicemos transacciones desde el driver), lo que provoca que se puedan producir pausas ( pause yielding ). Cada documento en s\u00ed es at\u00f3mico, por lo que ninguno se va a quedar a la mitad. MongoDB ofrece el m\u00e9todo findAndModify para encontrar y modificar un documento de manera at\u00f3mica, y as\u00ed evitar que, entre la b\u00fasqueda y la modificaci\u00f3n, el estado del documento se vea afectado. Adem\u00e1s, devuelve el documento modificado. Un caso de uso muy com\u00fan es para contadores y casos similares. db . people . findAndModify ({ query : { nombre : \"Marina Medrano\" }, update : { $inc : { salario : 100 , edad :- 30 }}, new : true }) Por defecto, el documento devuelto ser\u00e1 el resultado que ha encontrado con la consulta. Si queremos que nos devuelva el documento modificado con los cambios deseados, necesitamos utilizar el par\u00e1metro new a true . Si no lo indicamos o lo ponemos a false , tendremos el comportamiento por defecto. Actualizaciones sobre Arrays \u00b6 Para trabajar con arrays necesitamos nuevos operadores que nos permitan tanto introducir como eliminar elementos de una manera m\u00e1s sencilla que sustituir todos los elementos del array. Los operadores que podemos emplear para trabajar con arrays son: Operador Prop\u00f3sito $push A\u00f1ade uno o varios elementos $addToSet A\u00f1ade un elemento sin duplicados $pull Elimina un elemento $pullAll Elimina varios elementos $pop Elimina el primer o el \u00faltimo Preparando los ejemplos Para trabajar con los arrays, vamos a suponer que tenemos una colecci\u00f3n de enlaces donde vamos a almacenar un documento por cada site, con un atributo tags con etiquetas sobre el enlace en cuesti\u00f3n db . enlaces . insertOne ({ titulo : \"www.google.es\" , tags : [ \"mapas\" , \"videos\" ]}) De modo que tendr\u00edamos el siguiente documento: { _id : Objec t Id( \"633c60e8ac452ac9d7f9fe74\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' ] } A\u00f1adiendo elementos \u00b6 Si queremos a\u00f1adir uno o varios elementos, usaremos el operador $push . Cuando queremos a\u00f1adir varios elementos a la vez, mediante el operador $each le pasamos un array con los datos: db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $push : { tags : \"blog\" }} ) db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $push : { tags : { $each : [ \"calendario\" , \"email\" , \"mapas\" ]}}}) Al hacer estar modificaci\u00f3n, el resultado del documento ser\u00eda: { _id : Objec t Id( \"633c61b5ac452ac9d7f9fe75\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' , 'blog' , 'cale n dario' , 'email' , 'mapas' ] } Al utilizar $push no se tiene en cuenta lo que contiene el array, por tanto, si un elemento ya existe, se repetir\u00e1 y tendremos duplicados. Si queremos evitar los duplicados, usaremos $addToSet : db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $addToSet : { tags : \"buscador\" }}) Si queremos a\u00f1adir m\u00e1s de un campo a la vez sin duplicados, debemos anidar el operador $each igual que hemos hecho antes: db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $addToSet : { tags : { $each : [ \"drive\" , \"traductor\" ]}}}) En cambio, si queremos eliminar elementos de un array, usaremos el operador $pull : db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $pull : { tags : \"traductor\" }}) Similar al caso anterior, con $pullAll , eliminaremos varios elementos de una sola vez: db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $pullAll : { tags : [ \"calendario\" , \"email\" ]}}) Otra manera de eliminar elementos del array es mediante $pop , el cual elimina el primero ( -1 ) o el \u00faltimo ( 1 ) elemento del array: db . enlaces . updateMany ({ titulo : \"www.google.es\" }, { $pop : { tags :- 1 }}) Operador posicional \u00b6 Por \u00faltimo, tenemos el operador posicional , el cual se expresa con el s\u00edmbolo $ y nos permite modificar el elemento que ocupa una determinada posici\u00f3n del array sin saber exactamente cual es esa posici\u00f3n. Supongamos que tenemos las calificaciones de los estudiantes (colecci\u00f3n students ) en un documento con una estructura similar a la siguiente: { \"_id\" : 1 , \"notas\" : [ 80 , 85 , 90 ] } y queremos cambiar la calificaci\u00f3n de 80 por 82. Mediante el operador posicional haremos: db . students . updateOne ( { _id : 1 , notas : 80 }, { $set : { \"notas.$\" : 82 } } ) De manera similar, si queremos modificar parte de un documento el cual forma parte de un array, debemos usar la notaci\u00f3n punto tras el $ . Por ejemplo, supongamos que tenemos estas calificaciones de un determinado alumno, las cuales forman parte de un objeto dentro de un array: { \"_id\" : 4 , \"notas\" : [ { n o ta : 80 , media : 75 }, { n o ta : 85 , media : 90 }, { n o ta : 90 , media : 85 } ] } Podemos observar como tenemos cada calificaci\u00f3n como parte de un objeto dentro de un array. Si queremos cambiar el valor de media a 89 de la calificaci\u00f3n cuya nota es 85 , haremos: db . students . updateOne ( { _id : 4 , \"notas.nota\" : 85 }, { $set : { \"notas.$.media\" : 89 } } ) Es decir, el $ referencia al documento que ha cumplido el filtro de b\u00fasqueda. M\u00e1s operadores posicionales Adem\u00e1s del operador posicional $, tenemos disponible el operador posicional $[] que indica que afecta a todos los elementos del array, y el operador posicional $[identificador] que identifica que elementos del array cumplen una condici\u00f3n para su filtrado. Podemos consultar toda la documentaci\u00f3n disponible sobre estos operadores en http://docs.mongodb.org/manual/reference/operator/update-array/ Borrando documentos \u00b6 Para borrar, usaremos los m\u00e9todo deleteOne o deleteMany , los cuales funcionan de manera similar a findOne y find . Si no pasamos ning\u00fan par\u00e1metro, deleteOne borrar\u00e1 el primer documento, o en el caso de deleteMany toda la colecci\u00f3n documento a documento. Si le pasamos un par\u00e1metro, \u00e9ste ser\u00e1 el criterio de selecci\u00f3n de documentos a eliminar. db . people . deleteOne ({ nombre : \"Marina Medrano\" }) Al eliminar un documento, no podemos olvidar que cualquier referencia al documento que exista en la base de datos seguir\u00e1 existiendo. Por este motivo, manualmente tambi\u00e9n hay que eliminar o modificar esas referencias. Si queremos borrar toda la colecci\u00f3n, es m\u00e1s eficiente usar el m\u00e9todo drop , ya que tambi\u00e9n elimina los \u00edndices. db . people . drop () Eliminar un campo Recordad que eliminar un determinado campo de un documento no se considera un operaci\u00f3n de borrado, sino una actualizaci\u00f3n mediante el operador $unset . Referencias \u00b6 Manual de MongoDB Cheatsheet oficial Comparaci\u00f3n entre SQL y MongoDB Cursos gratuitos de Mongo University Consultas solucionadas sobre la colecci\u00f3n sample_restaurants.restaurants en w3resource Actividades \u00b6 ( RA5075.2 / CE5.2b / 1p) Crea un cluster en MongoAtlas , carga los datos de ejemplo y adjunta capturas de pantalla de: Dashboard del cluster Bases de datos / colecciones creadas A continuaci\u00f3n, con\u00e9ctate mediante MongoDB Compass y adjunta una captura de pantalla tras conectar con el cl\u00faster. ( RA5075.1 / CE5.1d / 2p) Haciendo uso de mongosh , escribe los comandos necesarios para: Obtener las bases de datos creadas. Sobre la base de datos sample_training y la colecci\u00f3n zips averigua: Cuantos documentos hay en la ciudad de SAN DIEGO . Cuantos documentos tienen menos de 100 personas (campo pop ). Obt\u00e9n los estados de la ciudad de SAN DIEGO (Soluci\u00f3n: [ 'CA', 'TX' ] ). Cual es el c\u00f3digo postal de la ciudad de ALLEN que no tiene habitantes (s\u00f3lo recupera el zip , no nos interesa ning\u00fan otro campo, ni el _id ). Listado con los 5 c\u00f3digos postales m\u00e1s poblados (muestra los documentos completos). Cantidad de documentos que no tienen menos de 5.000 habitantes ni m\u00e1s de 1.000.000 (debes utilizar el operador $nor ). Cuantos documentos tienen m\u00e1s habitantes que su propio c\u00f3digo postal (campo zip ). Sobre la colecci\u00f3n posts averigua: Cuantos mensajes tienen las etiquetas restaurant o moon . Los comentarios que ha escrito el usuario Salena Olmos . Recupera los mensajes que en body contengan la palabra earth , y devuelve el t\u00edtulo, 3 comentarios y 5 etiquetas. ( RA5075.1 / CE5.1d / 2p) Escribe los comandos necesarios para realizar las siguientes operaciones sobre la colecci\u00f3n zips : Crea una entrada con los siguientes datos: { ci t y : 'ELX' , zip : ' 03206 ' , loc : { x : 38.265500 , y : -0.698459 }, pop : 230224 , s tate : 'Espa\u00f1a' } Crea una entrada con los datos del c\u00f3digo postal donde vives (si es el mismo c\u00f3digo postal, crea uno diferente). Modifica la poblaci\u00f3n de tu c\u00f3digo postal a 1.000.000 . Incrementa la poblaci\u00f3n de todas los documentos de Espa\u00f1a en 666 personas. A\u00f1ade un campo prov a ambos documentos con valor Alicante . Modifica los documentos de Espa\u00f1a y a\u00f1ade un atributo tags que contenga un array vac\u00edo. Modifica todos los documentos de la provincia de Alicante y a\u00f1ade al atributo tags el valor sun . Modifica el valor de sun de tu c\u00f3digo postal y sustit\u00fayelo por house . Renombra en los documentos de la provincia de Alicante el atributo prov por provincia Elimina las coordenadas del zip 03206 . Elimina tu entrada.","title":"S19.- MongoDB"},{"location":"sa/02mongo.html#conceptos","text":"Hay una serie de conceptos que conviene conocer antes de entrar en detalle: MongoDB tienen el mismo concepto de base de datos que un RDMS . Dentro de una instancia de MongoDB podemos tener 0 o m\u00e1s bases de datos, actuando cada una como un contenedor de alto nivel. Una base de datos tendr\u00e1 0 o m\u00e1s colecciones. Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un RDMS . MongoDB ofrece diferentes tipos de colecciones, desde las normales cuyo tama\u00f1o crece conforme lo hace el n\u00famero de documentos, como las colecciones capped , las cuales tienen un tama\u00f1o predefinido y que pueden contener una cierta cantidad de informaci\u00f3n que se sustituir\u00e1 por nueva cuando se llene. Las colecciones contienen 0 o m\u00e1s documentos , por lo que es similar a una fila o registro de un RDMS . Cada documento contiene 0 o m\u00e1s atributos, compuestos de parejas clave/valor . Cada uno de estos documentos no sigue ning\u00fan esquema, por lo que dos documentos de una misma colecci\u00f3n pueden contener todos los atributos diferentes entre s\u00ed. Elementos de MongoDB As\u00ed pues, tenemos que una base de datos va a contener varias colecciones, donde cada colecci\u00f3n contendr\u00e1 un conjunto de documentos: Modelo de MongoDB Adem\u00e1s, MongoDB soporta \u00edndices , igual que cualquier RDMS , para acelerar la b\u00fasqueda de datos. Al realizar cualquier consulta, se devuelve un cursor , con el cual podemos hacer cosas tales como contar, ordenar, limitar o saltar documentos.","title":"Conceptos"},{"location":"sa/02mongo.html#bson","text":"Mediante JavaScript podemos crear objetos que se representan con JSON. Internamente, MongoDB almacena los documentos mediante BSON ( Binary JSON ). Podemos consultar la especificaci\u00f3n en http://BSONSpec.org Especificaci\u00f3n BSON BSON representa un superset de JSON ya que: Permite almacenar datos en binario Incluye un conjunto de tipos de datos no incluidos en JSON, como pueden ser ObjectId , Date o BinData . Podemos consultar todos los tipos que soporta un objeto BSON en http://docs.mongodb.org/manual/reference/bson-types/ Un ejemplo de un objeto BSON podr\u00eda ser: var yo = { n ombre : \"Aitor\" , apellidos : \"Medrano\" , fna c : ne w Da te ( \"Oct 3, 1977\" ) , hobbies : [ \"programaci\u00f3n\" , \"videojuegos\" , \"baloncesto\" ], casado : true , hijos : 2 , co nta c t o : { t wi tter : \"@aitormedrano\" , email : \"a.medrano@edu.gva.es\" }, fe chaCreacio n : ne w Times ta mp() } Los documentos BSON tienen las siguientes restricciones: No pueden tener un tama\u00f1o superior a 16 MB. El atributo _id queda reservado para la clave primaria. Desde MongoDB 5.0 los nombres de los campos pueden empezar por $ y/o contener el . , aunque en la medida de lo posible, es recomendable evitar su uso. Adem\u00e1s MongoDB: No asegura que el orden de los campos se respete. Es sensible a los tipos de los datos Es sensible a las may\u00fasculas. Por lo que estos documentos son distintos: { \"edad\" : \"18\" } { \"edad\" : 18 } { \"Edad\" : 18 } Si queremos validar si un documento JSON es v\u00e1lido, podemos usar http://jsonlint.com/ . Hemos de tener en cuenta que s\u00f3lo valida JSON y no BSON, por tanto nos dar\u00e1 errores en los tipos de datos propios de BSON.","title":"BSON"},{"location":"sa/02mongo.html#puesta-en-marcha","text":"En la actualidad, MongoDB se comercializa mediante tres productos: Mongo Atlas , como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB. MongoDB Community Edition , versi\u00f3n gratuita para trabajar on-premise, con versiones para Windows, MacOS y Linux. MongoDB Enterprise Advanced , versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada.","title":"Puesta en marcha"},{"location":"sa/02mongo.html#instalacion","text":"Desde https://www.mongodb.com/try/download/community podemos descargar la versi\u00f3n Community acorde a nuestro sistema operativo. Independientemente de nuestro sistema operativo, por defecto, el demonio se lanza sobre el puerto 27017. Una vez instalado, si accedemos a http://localhost:27017 podremos ver que nos indica c\u00f3mo estamos intentando acceder mediante HTTP a MongoDB mediante el puerto reservado al driver nativo. Acceso al puerto 27017 En vez de instalarlo como un servicio en nuestra m\u00e1quina, a d\u00eda de hoy, es mucho m\u00e1s c\u00f3modo hacer uso de contenedores Docker o utilizar una soluci\u00f3n cloud .","title":"Instalaci\u00f3n"},{"location":"sa/02mongo.html#docker","text":"Para lanzar el contenedor de Docker al que llamaremos iadb-mongo mediante el siguiente comando: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo MongoDB y procesadores AVX Si tenemos un procesador sin soporte para AVX, necesitamos instalar una versi\u00f3n inferior a la 5.0. As\u00ed pues, podemos indicar la versi\u00f3n 4.4: docker run -p 127 .0.0.1:27017:27017 --name iabd-mongo -d mongo:4.4 A continuaci\u00f3n vamos a descargar el conjunto de datos sampledata.archive.gz que ofrece MongoDB a modo de prueba, el cual vamos a emplear a lo largo de las diferentes sesiones. Volvemos al terminal de nuestro sistema y copiamos los datos desde nuestro sistema a la carpeta /tmp del contenedor: docker cp sampledata.archive.gz iabd-mongo:/tmp Posteriormente abrimos un terminal dentro de nuestro contenedor (o mediante Attach Shell en VSCode ): docker exec -it iabd-mongo bash Y finalmente, restauramos los datos mediante mongorestore : mongorestore --gzip --archive = /tmp/sampledata.archive.gz Una vez cargados, nos informar\u00e1 que se han restaurado 433281 documentos.","title":"Docker"},{"location":"sa/02mongo.html#mongo-atlas","text":"Y si preferimos una soluci\u00f3n cloud , disponemos de Mongo Atlas , que nos ofrece de manera gratuita un cluster compartido de servidores con 3 nodos y 512 MB para datos. Si queremos una soluci\u00f3n serverless o un servidor dedicado, ya tendremos que pasar por caja . Registro en Mongo Atlas Para comenzar a utilizar Mongo Atlas el primer paso es registrarnos y completar un cuestionario sobre nuestro uso. Tras ello: Creamos el cluster de despliegue. En nuestro caso, hemos realizado el despliegue en AWS en la regi\u00f3n de Paris ( eu-west-3 ) y dejado el nombre por defecto, Cluster 0 . Elecci\u00f3n del cluster Creamos un usuario/contrase\u00f1a para autenticar nuestra conexi\u00f3n. En nuestro caso, hemos creado el usuario iabd con la contrase\u00f1a iabdiabd (despu\u00e9s la podemos modificar desde el men\u00fa Security -> Database Access ): Configuraci\u00f3n del usuario En la misma pantalla, indicamos que permitimos las conexiones desde todas las direcciones IP (esta decisi\u00f3n s\u00f3lo la tomamos por comodidad, para poder conectarnos desde casa y el centro) mediante la IP 0.0.0.0 (despu\u00e9s podemos modificar la configuraci\u00f3n desde el men\u00fa Security -> Network Access ). Una vez realizados los dos pasos anteriores, comenzar\u00e1 la creaci\u00f3n del cluster, la cual puede tardar de 2 a 3 minutos. Dashboard del cluster A continuaci\u00f3n, cargaremos los datos de ejemplo. Para ello, en el men\u00fa con los tres puntos ( ... ), elegiremos la opci\u00f3n Load Sample Dataset . Una vez haya finalizado, podremos ver los datos cargados pulsando sobre el bot\u00f3n Browse Collections : Colecciones con los datos de prueba Conexi\u00f3n segura Mediante srv se establece una conexi\u00f3n segura Finalmente, para obtener la cadena de conexi\u00f3n, desde el dashboard del cluster con la opci\u00f3n Connect o desde la pesta\u00f1a Cmd Line Tools del propio cluster, podremos obtener la cadena de conexi\u00f3n , que tendr\u00e1 un formato similar a : mongodb+srv://usuario:password@host/basededatos A continuaci\u00f3n, vamos a conocer las diferentes herramientas que nos ofrece MongoDB para posteriormente estudiar todas las operaciones que podemos realizar.","title":"Mongo Atlas"},{"location":"sa/02mongo.html#mongosh","text":"Tras arrancar el demonio mongod (el cual se lanza autom\u00e1ticamente mediante Docker o con el cluster de Mongo Atlas ) llega el momento de acceder mediante el cliente mongosh (en versiones anteriores el comando utilizado era mongo ), el cual funciona igual que un shell, de modo que con la fecha hacia arriba visualizaremos el \u00faltimo comando. El cliente utiliza JavaScript como lenguaje de interacci\u00f3n con la base de datos. Si nos conectamos desde Docker, no necesitamos instalarlo. Primero nos conectamos al contenedor: docker exec -it iabd-mongo bash Al conectar con mongosh si no le indicamos nada se conectar\u00e1 por defecto a la base de datos test de localhost . Si queremos conectarnos a una base de datos concreta, por ejemplo a sample_training , la pasaremos como par\u00e1metro: root @ 3 ad17b675fb1 : /# mongosh sample_training Current Mongosh Log ID : 6316498 f30f8283fedcfabc2 Connecting to : mongodb : //127.0.0.1:27017/sample_training?directConnection=true&serverSelectionTimeoutMS=2000 Using MongoDB : 5.0.4 Using Mongosh : 1.1.2 Si queremos ver las bases de datos que existen ejecutaremos el comando show dbs : sample_training> show dbs ; admin 41 kB config 73 .7 kB local 73 .7 kB sample_airbnb 55 .1 MB sample_analytics 9 .9 MB sample_geospatial 999 kB sample_mflix 48 .5 MB sample_restaurants 6 .2 MB sample_supplies 991 kB sample_training 43 .4 MB sample_weatherdata 2 .49 MB Si nos quisi\u00e9ramos conectar a nuestro cluster de Mongo Atlas utilizaremos la cadena de conexi\u00f3n tras el comando mongosh : root@3ad17b675fb1:/# mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/test Current Mongosh Log ID: 63164ac26030844c1576f8b4 Connecting to: mongodb+srv://<credentials>@cluster0.dfaz5er.mongodb.net/test Using MongoDB: 5 .0.11 Using Mongosh: 1 .1.2 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ Atlas atlas-4wikkb-shard-0 [ primary ] test> Uso externo Si no queremos tener que conectarnos al contenedor o vamos a trabajar con un servidor remoto, podemos instalar \u00fanicamente el shell desde https://www.mongodb.com/try/download/shell","title":"mongosh"},{"location":"sa/02mongo.html#mongodb-database-tools","text":"Adem\u00e1s del propio servidor de MongoDB y el cliente para conectarse a \u00e9l, MongoDB ofrece un conjunto de herramientas para interactuar con las bases de datos, permitiendo crear y restaurar copias de seguridad. Si estamos interesados en introducir o exportar una colecci\u00f3n de datos mediante JSON, podemos emplear los comandos mongoimport y mongoexport : mongoimport -d nombreBaseDatos -c coleccion \u2013-file nombreFichero.json mongoexport -d nombreBaseDatos -c coleccion nombreFichero.json Estas herramientas interact\u00faan con datos JSON y no sobre toda la base de datos. Un caso particular y muy com\u00fan es importar datos que se encuentran en formato CSV/TSV. Para ello, emplearemos el par\u00e1metro --type csv : mongoimport --type tsv -d test -c poblacion --headerline --drop poblacionEspanya2013.tsv En vez de realizar un export , es m\u00e1s conveniente realizar un backup en binario mediante mongodump , el cual genera ficheros BSON. Estos archivos posteriormente se restauran mediante mongorestore : mongodump -d nombreBaseDatos nombreFichero.bson mongorestore -d nombreBaseDatos nombreFichero.bson Autoevaluaci\u00f3n Intenta exportar los datos de la base de datos sample_training desde MongoAtlas. Veras que ha creado una carpeta que contiene dos archivos \u00bfCu\u00e1les son? \u00bfQu\u00e9 contiene cada uno de ellos y cual es su formato? Si necesitamos transformar un fichero BSON a JSON (de binario a texto), tenemos el comando bsondump : bsondump file.bson > file.json Info M\u00e1s informaci\u00f3n sobre copias de seguridad en https://www.mongodb.com/docs/manual/core/backups/ . Para poder trabajar con MongoDB desde cualquier aplicaci\u00f3n necesitamos un driver. MongoDB ofrece drivers oficiales para casi todos los lenguajes de programaci\u00f3n actuales. En la sesi\u00f3n 28 de 'MongoDB y Python' trabajaremos con PyMongo . Monitorizaci\u00f3n Tanto mongostat como mongotop permiten visualizar el estado del servidor MongoDB, as\u00ed como algunas estad\u00edsticas sobre su rendimiento. Si trabajamos con MongoAtlas estas herramientas est\u00e1n integradas en las diferentes herramientas de monitorizaci\u00f3n de la plataforma. En versiones anteriores, una herramienta de terceros bastante utilizada era RoboMongo / Robo3T / Studio3T el cual extiende el shell y ofrece un IDE m\u00e1s amigable. A d\u00edas de hoy, MongoDB tiene su propio IDE conocido como Mongo Compass .","title":"MongoDB Database Tools"},{"location":"sa/02mongo.html#mongodb-compass","text":"En el curso nos vamos a centrar en el uso del shell y la conectividad de MongoDB mediante Python, pero no est\u00e1 de m\u00e1s conocer las herramientas visuales que facilitan el trabajo con MongoDB en el d\u00eda a d\u00eda. Una de ellas es MongoDB Compass , que facilita la exploraci\u00f3n y manipulaci\u00f3n de los datos. De una manera flexible e intuitiva, Compass ofrece visualizaciones detalladas de los esquemas, m\u00e9tricas de rendimiento en tiempo real as\u00ed como herramientas para la creaci\u00f3n de consultas. Existen tres versiones de Compass, una completa con todas las caracter\u00edsticas, una de s\u00f3lo lectura sin posibilidad de insertar, modificar o eliminar datos (perfecta para anal\u00edtica de datos) y una \u00faltima versi\u00f3n isolated que solo permite la conexi\u00f3n a una instancia local. Una vez descargada e instalada la versi\u00f3n que nos interesa, tras crear la conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), veremos en el men\u00fa de la izquierda un resumen del cluster, as\u00ed como las consultas que vayamos almacenando y las diferentes bases de datos almacenadas: GUI de Mongo Compass Si seleccionamos una base de datos concreta, y de ella, una colecci\u00f3n en el men\u00fa de la izquierda, en el panel central tendremos una visualizaci\u00f3n de los datos contenidos, as\u00ed como opciones para ver su esquema, realizar consultas agregadas, editar los \u00edndices, etc... Adem\u00e1s, podremos realizar consultas sobre los datos: Opciones desde una colecci\u00f3n mongosh en Compass Si te fijas, en la barra inferior podemos desplegar un panel para interactuar mediante comandos como lo har\u00edamos desde mongosh .","title":"MongoDB Compass"},{"location":"sa/02mongo.html#mongodb-for-vscode","text":"Tambi\u00e9n podemos utilizar la extensi\u00f3n que lleva VSCode para trabajar con MongoDB . Tras su instalaci\u00f3n creamos una conexi\u00f3n a partir de la cadena de conexi\u00f3n (similar a mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/test ), y una vez conectados, podremos recorrer las colecciones con los datos as\u00ed como utilizar un playground para interactuar de manera similar al shell: Uso de la extensi\u00f3n de VSCode","title":"MongoDB for VSCode"},{"location":"sa/02mongo.html#hola-mongodb","text":"Pues una vez que ya nos hemos conectado a MongoDB mediante mongosh , vamos a empezar a interactuar con los datos. En cualquier momento podemos cambiar la base de datos activa mediante use nombreBaseDatos . Si la base de datos no existiese, MongoDB crear\u00e1 dicha base de datos. Esto es una verdad a medias, ya que la base de datos realmente se crea al insertar datos dentro de alguna colecci\u00f3n. As\u00ed pues, vamos a crear nuestra base de datos iabd : use iabd Una vez creada, podemos crear nuestra primera colecci\u00f3n, que llamaremos people , e insertaremos un persona con nuestros datos personales mediante el m\u00e9todo insertOne , al que le pasamos un objeto JSON: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Tras ejecutar el comando, veremos que nos devuelve un objeto JSON con su ACK y el identificador del documento insertado: { ack n owledged : true , i nserte dId : Objec t Id( \"6316fc938cc2bc168bfed066\" ) } Una vez insertada, s\u00f3lo nos queda realizar una consulta para recuperar los datos y comprobar que todo funciona correctamente mediante el m\u00e9todo findOne : db . people . findOne () Lo que nos dar\u00e1 como resultado un objeto JSON que contiene un atributo _id con el mismo identificador mostrado anteriormente, adem\u00e1s de los que le a\u00f1adimos al insertar la persona: { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' } Como podemos observar, todas las instrucciones van a seguir el patr\u00f3n de db.nombreColeccion.operacion() .","title":"Hola MongoDB"},{"location":"sa/02mongo.html#trabajando-con-el-shell","text":"Antes de entrar en detalles en las instrucciones necesarias para realizar las operaciones CRUD, veamos algunos comandos que nos ser\u00e1n muy \u00fatiles al interactuar con el shell: Comando Funci\u00f3n show dbs Muestra el nombre de las bases de datos show collections Muestra el nombre de las colecciones db Muestra el nombre de la base de datos que estamos utilizando db.dropDatabase() Elimina la base de datos actual db.help() Muestra los comandos disponibles db.version() Muestra la versi\u00f3n actual del servidor En el resto de la sesi\u00f3n vamos a hacer un uso intenso del shell de MongoDB. Por ejemplo, si nos basamos en el objeto definido en el apartado de BSON, podemos ejecutar las siguientes instrucciones: > db . people . insertOne ( yo ) // (1) < { acknowledged : true , insertedId : ObjectId ( \"631704a042aae0893122f2d6\" ) } > db . people . find () // (2) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) } ] > yo . profesion = \"Profesor\" < Profesor > db . people . insertOne ( yo ) // (3) < [ { _id : ObjectId ( \"6316fc938cc2bc168bfed066\" ), nombre : 'Aitor Medrano' , edad : 45 , profesion : 'Profesor' }, { _id : ObjectId ( \"631704a042aae0893122f2d6\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452896 , i : 1 }) }, { _id : ObjectId ( \"6317056d42aae0893122f2d7\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : ISODate ( \"1977-10-03T00:00:00.000Z\" ), hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662453101 , i : 1 }), profesion : 'Profesor' } ] > db . people . countDocuments () // (4) < 3 Si queremos insertar un documento en una colecci\u00f3n, hemos de utilizar el m\u00e9todo insertOne pas\u00e1ndole como par\u00e1metro el documento que queremos insertar, ya sea a partir de una variable o el propio documento en s\u00ed. find recupera todos los documentos de la colecci\u00f3n Modificamos nuestro documento y los volvemos a insertar. Realmente va a crear un nuevo documento, y no se va a quejar de que ya exista, porque nuestro documento no contiene ning\u00fan atributo identificador, por lo que considera que se trata de una nueva persona. Obtenemos la cantidad de documentos de la colecci\u00f3n mediante countDocuments . Con este ejemplo, hemos podido observar como los documentos de una misma colecci\u00f3n no tienen por qu\u00e9 tener el mismo esquema, ni hemos necesitado definirlo expl\u00edcitamente antes de insertar datos. As\u00ed pues, el esquema se ir\u00e1 generando y actualizando conforme se inserten documentos. M\u00e1s adelante veremos que podemos definir un esquema para validar que los datos que insertamos cumplan restricciones de tipos de datos o elementos que obligatoriamente deben estar rellenados.","title":"Trabajando con el shell"},{"location":"sa/02mongo.html#empleando-javascript","text":"Ya hemos comentado que el shell utiliza JavaScript como lenguaje de interacci\u00f3n, por lo que podemos almacenar los comandos en un script externo y ejecutarlo mediante load() : load ( \"scripts/misDatos.js\" ); load ( \"/data/db/scripts/misDatos.js\" ); Si hacemos una referencia relativa, lo hace respecto a la ruta desde la cual se ejecuta el shell mongosh . Otra manera de lanzar un script es hacerlo desde la l\u00ednea de comandos, pas\u00e1ndole como segundo par\u00e1metro el script a ejecutar: mongosh iabd misDatos.js Si el c\u00f3digo a ejecutar no necesita almacenarse en un script externo, el propio shell permite introducir instrucciones en varias l\u00edneas: > for ( var i = 0 ; i < 10 ; i ++ ) { ... db . espias . insertOne ({ \"nombre\" : \"James Bond \" + i , \"agente\" : \"00\" + i }); ... } < { acknowledged : true , insertedId : ObjectId ( \"63171d2142aae0893122f2e1\" ) } > db . espias . find () < [ { _id : ObjectId ( \"63171d2142aae0893122f2d8\" ), nombre : 'James Bond 0' , agente : '000' }, { _id : ObjectId ( \"63171d2142aae0893122f2d9\" ), nombre : 'James Bond 1' , agente : '001' }, ... { _id : ObjectId ( \"63171d2142aae0893122f2e1\" ), nombre : 'James Bond 9' , agente : '009' } ]","title":"Empleando JavaScript"},{"location":"sa/02mongo.html#objectid","text":"En MongoDB , el atributo _id es \u00fanico dentro de la colecci\u00f3n, y hace la funci\u00f3n de clave primaria. Se le asocia un ObjectId , el cual es un tipo BSON de 12 bytes que se crea mediante: el timestamp actual (4 bytes) un valor aleatorio y \u00fanico por m\u00e1quina y proceso (5 bytes) un contador inicializado a n\u00famero aleatorio (3 bytes). Este objeto lo crea el driver y no MongoDB , por lo cual no deberemos considerar que siguen un orden concreto, ya que clientes diferentes pueden tener timestamps desincronizados. Lo que s\u00ed que podemos obtener a partir del ObjectId es la fecha de creaci\u00f3n del documento, mediante el m\u00e9todo getTimestamp() del atributo _id . Obteniendo la fecha de creaci\u00f3n de un documento > db.people.findOne () ._id < ObjectId ( \"6316fc938cc2bc168bfed066\" ) > db.people.findOne () ._id.getTimestamp () < ISODate ( \"2022-09-06T07:53:55.000Z\" ) Este identificador es global, \u00fanico e inmutable. Esto es, no habr\u00e1 dos repetidos y una vez un documento tiene un _id , \u00e9ste no se puede modificar. Si en la definici\u00f3n del objeto a insertar no ponemos el atributo identificador, MongoDB crear\u00e1 uno de manera autom\u00e1tica. Si lo ponemos nosotros de manera expl\u00edcita, MongoDB no a\u00f1adir\u00e1 ning\u00fan ObjectId . Eso s\u00ed, debemos asegurarnos que sea \u00fanico (podemos usar n\u00fameros, cadenas, etc\u2026\u200b). Por lo tanto, podemos asignar un identificador al insertar: > db . people . insert ({ _id : 4 , nombre : \"Marina\" , edad : 14 }) < { acknowledged : true , insertedIds : { '0' : 4 } } Tipos de datos Cuidado con los tipos, ya que no es lo mismo insertar un atributo con edad:14 (se considera el campo como entero) que con edad:\"14\" , ya que considera el campo como texto. O tambi\u00e9n, si queremos podemos hacer que el _id de un documento sea un documento en s\u00ed, y no un entero, para ello, al insertarlo, podemos asignarle un objeto JSON al atributo identificador: > db . people . insertOne ({ _id : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' }, ciudad : 'Elx' }) < { acknowledged : true , insertedId : { nombre : 'Aitor' , apellidos : 'Medrano' , twitter : '@aitormedrano' } }","title":"ObjectId"},{"location":"sa/02mongo.html#recuperando-datos","text":"Para recuperar los datos de una colecci\u00f3n o un documento en concreto usaremos el m\u00e9todo find() : > db . people . find () < { _id : ObjectId ( \"6316fc1597eb703de2add36e\" ), nombre : 'Aitor' , edad : 45 , profesion : 'Profesor' } { _id : ObjectId ( \"6317048697eb703de2add36f\" ), nombre : 'Aitor' , apellidos : 'Medrano' , fnac : 1977 - 10 - 02 T23 : 00 : 00 .000 Z , hobbies : [ 'programaci\u00f3n' , 'videojuegos' , 'baloncesto' ], casado : true , hijos : 2 , contacto : { twitter : '@aitormedrano' , email : 'a.medrano@edu.gva.es' }, fechaCreacion : Timestamp ({ t : 1662452870 , i : 3 }) } El m\u00e9todo find() sobre una colecci\u00f3n devuelve un cursor a los datos obtenidos, el cual se queda abierto con el servidor y que se cierra autom\u00e1ticamente a los 30 minutos de inactividad o al finalizar su recorrido. Si hay muchos resultados, la consola nos mostrar\u00e1 un subconjunto de los datos (20). Si queremos seguir obteniendo resultados, solo tenemos que introducir it , para que contin\u00fae iterando el cursor. En cambio, si s\u00f3lo queremos recuperar un documento hemos de utilizar findOne() : > db.people. f i n dO ne () > { _id : Objec t Id( \"6316fc1597eb703de2add36e\" ) , n ombre : 'Ai t or' , edad : 45 , pro fes io n : 'Pro fes or' } Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de 10.000 documentos sobre los viajes realizados por los usuarios de una empresa de alquiler de bicicletas, los cuales han sido extra\u00eddos de https://ride.citibikenyc.com/system-data . Esta colecci\u00f3n ( trips ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de viaje ser\u00eda: > use sample_training < 'switched to db sample_training' > db . trips . findOne () < { _id : ObjectId ( \"572bb8222b288919b68abf5b\" ), tripduration : 889 , 'start station id' : 268 , 'start station name' : 'Howard St & Centre St' , 'end station id' : 3002 , 'end station name' : 'South End Ave & Liberty St' , bikeid : 22794 , usertype : 'Subscriber' , 'birth year' : 1961 , 'start station location' : { type : 'Point' , coordinates : [ - 73.99973337 , 40.71910537 ] }, 'end station location' : { type : 'Point' , coordinates : [ - 74.015756 , 40.711512 ] }, 'start time' : 2016 - 01 - 01 T00 : 01 : 06 .000 Z , 'stop time' : 2016 - 01 - 01 T00 : 15 : 56.000 Z }","title":"Recuperando datos"},{"location":"sa/02mongo.html#criterios-en-consultas","text":"Al hacer una consulta, si queremos obtener datos mediante m\u00e1s de un criterio, en el primer par\u00e1metro del find podemos pasar un objeto JSON con los campos a cumplir (condici\u00f3n Y). Consulta Resultado db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) { _id : Objec t Id( \"572bb8222b288919b68ad197\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 23724 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 45.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 48.000 Z } { _id : Objec t Id( \"572bb8222b288919b68ad191\" ) , tr ipdura t io n : 1143 , 's tart s tat io n id' : 405 , 's tart s tat io n na me' : 'Washi n g t o n S t & Ga nse voor t S t ' , 'e n d s tat io n id' : 146 , 'e n d s tat io n na me' : 'Hudso n S t & Reade S t ' , bikeid : 17075 , user t ype : 'Cus t omer' , 'bir t h year' : '' , 's tart s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.008119 , 40.739323 ] }, 'e n d s tat io n loca t io n ' : { t ype : 'Poi nt ' , coordi nates : [ -74.0091059 , 40.71625008 ] }, 's tart t ime' : 2016-01-01 T 13 : 37 : 38.000 Z , 's t op t ime' : 2016-01-01 T 13 : 56 : 42.000 Z } Consejo de Rendimiento Las consultas disyuntivas, es decir, con varios criterios u operador $and , deben filtrar el conjunto m\u00e1s peque\u00f1o cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar documentos que cumplen los criterios A, B y C. Digamos que el criterio A lo cumplen 40.000 documentos, el B lo hacen 9.000 y el C s\u00f3lo 200. Si filtramos A, luego B, y finalmente C, el conjunto que trabaja cada criterio es muy grande. Restringiendo consultas AND En cambio, si hacemos una consulta que primero empiece por el criterio m\u00e1s restrictivo, el resultado con lo que se intersecciona el siguiente criterio es menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas AND de menor a mayor MongoDB tambi\u00e9n ofrece operadores l\u00f3gicos para los campos num\u00e9ricos: Comparador Operador menor que ( < ) $lt menor o igual que ( \u2264 ) $lte mayor que ( > ) $gt mayor o igual que ( \u2265 ) $gte Estos operadores se pueden utilizar de forma simult\u00e1nea sobre un mismo campo o sobre diferentes campos, sobre campos anidados o que forman parte de un array, y se colocan como un nuevo documento en el valor del campo a filtrar, compuesto del operador y del valor a comparar mediante la siguiente sintaxis: db . < coleccion > . find ({ < campo >: { < operador >: < valor > } }) Por ejemplo, para recuperar los viajes que han durado menos de 5 minutos o comprendidos entre 3 y 5 minutos (el campo almacena el tiempo en segundos), podemos hacer: db . trips . find ({ \"tripduration\" : { $lt : 300 } }) db . trips . find ({ \"tripduration\" : { $gt : 180 , $lte : 300 } }) Para los campos de texto, adem\u00e1s de la comparaci\u00f3n directa, podemos usar el operador $ne para obtener los documentos cuyo campos no tienen un determinado valor ( not equal ). As\u00ed pues, podemos usarlo para averiguar todos los trayectos realizados por usuarios que no son subscriptores ( Subscriber ): db . trips . find ({ \"usertype\" : { $ne : \"Subscriber\" } }) Por supuesto, podemos tener diferentes operadores en campos distintos. Por ejemplo, si queremos ver los viajes de menos de un minuto y medio realizado por usuarios que no son subscriptores har\u00edamos: db . trips . find ({ \"tripduration\" : { $lt : 90 }, \"usertype\" : { $ne : \"Subscriber\" } }) Case sensitive Las comparaciones de cadenas se realizan siguiendo el orden UTF8, similar a ASCII, con lo cual no es lo mismo buscar un rango entre may\u00fasculas que min\u00fasculas. Con cierto parecido a la condici\u00f3n de valor no nulo de las BBDD relacionales y teniendo en cuenta que la libertad de esquema puede provocar que un documento tenga unos campos determinados y otro no lo tenga, podemos utilizar el operador $exists si queremos averiguar si un campo existe (y por tanto tiene alg\u00fan valor). db . people . find ({ \"edad\" : { $exists : true }}) Polimorfismo Mucho cuidado al usar polimorfismo y almacenar en un mismo campo un entero y una cadena, ya que al hacer comparaciones para recuperar datos, no vamos a poder mezclar cadenas con valores num\u00e9ricos. Se considera un antipatr\u00f3n el mezclar tipos de datos en un campo. Pese a que ciertos operadores contengan su correspondiente operador negado, MongoDB ofrece el operador $not . \u00c9ste puede utilizarse conjuntamente con otros operadores para negar el resultado de los documentos obtenidos. Por ejemplo, si queremos obtener todas las personas cuya edad no sea m\u00faltiplo de 5, podr\u00edamos hacerlo as\u00ed: db . people . find ({ edad : { $not : { $mod : [ 5 , 0 ]}}})","title":"Criterios en consultas"},{"location":"sa/02mongo.html#proyeccion-de-campos","text":"Las consultas realizadas hasta ahora devuelven los documentos completos. Si queremos que devuelva un campo o varios campos en concreto, hemos de pasar un segundo par\u00e1metro de tipo JSON con aquellos campos que deseamos mostrar con el valor true o 1 . Destacar que si no se indica nada, por defecto siempre mostrar\u00e1 el campo _id > db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }, { tripduration : 1 }) < { _id : ObjectId ( \"572bb8222b288919b68ad191\" ), tripduration : 1143 } Por lo tanto, si queremos que no se muestre el _id , lo podremos a false o 0 : > db. tr ips. f i n d( { 's tart s tat io n id' : 405 , 'e n d s tat io n id' : 146 }, { tr ipdura t io n : 1 , _id : 0 } ) < { tr ipdura t io n : 1143 } No mezcles Al hacer una proyecci\u00f3n, no podemos mezclar campos que se vean ( 1 ) con los que no ( 0 ). Es decir, hemos de hacer algo similar a: db . < coleccion > . find ({ < consulta > }, { < campo1 >: 1 , < campo2 >: 1 }) db . < coleccion > . find ({ < consulta > }, { < campo1 >: 0 , < campo2 >: 0 }) As\u00ed pues, s\u00f3lo se mezclar\u00e1 la visibilidad de los campos cuando queramos ocultar el _id .","title":"Proyecci\u00f3n de campos"},{"location":"sa/02mongo.html#condiciones-compuestas-con-y-o","text":"Para usar la conjunci\u00f3n o la disyunci\u00f3n, tenemos los operadores $and y $or . Son operadores prefijo, de modo que se ponen antes de las subconsultas que se van a evaluar. Estos operadores trabajan con arrays, donde cada uno de los elementos es un documento con la condici\u00f3n a evaluar, de modo que se realiza la uni\u00f3n entre estas condiciones, aplicando la l\u00f3gica asociada a AND y a OR. db . trips . find ({ $or : [{ 'start station id' : 405 }, { 'end station id' : 146 }] }) db . trips . find ({ $or : [{ \"tripduration\" : { $lte : 70 }}, { \"tripduration\" : { $gte : 3600 }}] }) Realmente el operador $and no se suele usar porque podemos anidar en la consulta dos criterios, al poner uno dentro del otro. As\u00ed pues, estas dos consultas hacen lo mismo: db . trips . find ({ 'start station id' : 405 , 'end station id' : 146 }) db . trips . find ({ $and : [ { 'start station id' : 405 }, { 'end station id' : 146 } ] }) Consejo de Rendimiento Las consultas conjuntivas, es decir, con varios criterios excluyentes u operador $or , deben filtrar el conjunto m\u00e1s grande cuanto m\u00e1s pronto posible. Supongamos que vamos a consultar los mismos documentos que cumplen los criterios A (40.000 documentos), B (9.000 documentos) y C (200 documentos). Si filtramos C, luego B, y finalmente A, el conjunto de documentos que tiene que comprobar MongoDB es muy grande. Restringiendo consultas OR de menor a mayor En cambio, si hacemos una consulta que primero empiece por el criterio menos restrictivo, el conjunto de documentos sobre el cual va a tener que comprobar siguientes criterios va a ser menor, y por tanto, se realizar\u00e1 m\u00e1s r\u00e1pido. Restringiendo consultas OR de mayor a menor Tambi\u00e9n podemos utilizar el operado $nor , que no es m\u00e1s que la negaci\u00f3n de $or y que obtendr\u00e1 aquellos documentos que no cumplan ninguna de las condiciones. Autoevaluaci\u00f3n Que obtendr\u00edamos al ejecutar la siguiente consulta: db . trips . find ({ \"tripduration\" : { $lte : 65 }, $nor : [ { usertype : \"Customer\" }, { \"birth year\" : 1989 } ] }) Finalmente, si queremos indicar mediante un array los diferentes valores que puede cumplir un campo, podemos utilizar el operador $in : db . trips . find ({ \"birth year\" : { $in : [ 1977 , 1980 ]} }) Por supuesto, tambi\u00e9n existe su negaci\u00f3n mediante $nin .","title":"Condiciones compuestas con Y / O"},{"location":"sa/02mongo.html#condiciones-sobre-objetos-anidados","text":"Si queremos acceder a campos de subdocumentos, siguiendo la sintaxis de JSON, se utiliza la notaci\u00f3n punto. Esta notaci\u00f3n permite acceder al campo de un documento anidado, da igual el nivel en el que est\u00e9 y su orden respecto al resto de campos. Preparando los ejemplos Para los siguientes ejemplos sobre documentos anidados y arrays, vamos a utilizar una colecci\u00f3n de 500 documentos sobre mensajes de un blog. Esta colecci\u00f3n ( posts ) est\u00e1 cargada tanto en el cluster de MongoAtlas como en el contenedor de Docker (si has seguido las instrucciones) de la base de datos sample_training . Un ejemplo de mensaje ser\u00eda: > use sample_training 'switched to db sample_training' > db . posts . findOne () < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc3f9\" ), body : 'Amendment I\\n<p>Congress shall make ....\"\\n<p>\\n' , permalink : 'aRjNnLZkJkTyspAIoRGe' , author : 'machine' , title : 'Bill of Rights' , tags : 'santa' , 'xylophone' , 'math' , 'dream' , 'action' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'HvizfYVx@pKvLaagH.com' , author : 'Santiago Dollins' }, { body : 'Lorem ipsum dolor sit...' , email : 'WpOUCpdD@hccdxJvT.com' , author : 'Jaclyn Morado' }, { body : 'Lorem ipsum dolor sit amet...' , email : 'OgDzHfFN@cWsDtCtx.com' , author : 'Houston Valenti' }], date : 2012 - 11 - 20 T05 : 05 : 15.231 Z } Para acceder al autor de los comentarios de un mensaje usar\u00edamos la propiedad comments.author . Por ejemplo, para averiguar los mensajes titulados Bill of Rights y que tienen alg\u00fan comentario creado por Santiago Dollins har\u00edamos: db . posts . find ({ title : 'Bill of Rights' , \"comments.author\" : 'Santiago Dollins' })","title":"Condiciones sobre objetos anidados"},{"location":"sa/02mongo.html#consultas-sobre-arrays","text":"Si trabajamos con arrays, vamos a poder consultar el contenido de una posici\u00f3n del mismo tal como si fuera un campo normal, siempre que sea un campo de primer nivel, es decir, no sea un documento embebido dentro de un array. Si queremos filtrar teniendo en cuenta el n\u00famero de ocurrencias del array, podemos utilizar: $all para filtrar ocurrencias que tienen todos los valores del array, es decir, los valores pasados a la consulta ser\u00e1n un subconjunto del resultado. Puede que devuelva los mismos, o un array con m\u00e1s campos (el orden no importa) $in , igual que SQL, para obtener las ocurrencias que cumple con alguno de los valores pasados (similar a usar $or sobre un conjunto de valores de un mismo campo). Si queremos su negaci\u00f3n, usaremos $nin , para obtener los documentos que no cumplen ninguno de los valores. Por ejemplo, si queremos obtener los mensajes que contenga las etiquetas dream y action tendr\u00edamos: db . posts . find ( { tags : { $all : [ \"dream\" , \"action\" ]}} ) En cambio, si queremos los mensajes que contengan alguna de esas etiquetas har\u00edamos: db . posts . find ( { tags : { $in : [ \"dream\" , \"action\" ]}} ) Si el array contiene documentos y queremos filtrar la consulta sobre los campos de los documentos del array, tenemos que utilizar $elemMatch , de manera que obtengamos aquellos que al menos encuentre un elemento que cumpla el criterio. As\u00ed pues, si queremos recuperar los mensajes que tienen un comentario cuyo autor sea Santiago Dollins har\u00edamos: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Criterio con notaci\u00f3n punto En el ejemplo anterior, si s\u00f3lo hubi\u00e9ramos tenido un campo para el filtrado, podr\u00edamos haber utilizado la notaci\u00f3n punto comments.author . Si s\u00f3lo queremos los comentarios escritos por un determinado autor, adem\u00e1s de en el filtrado, hemos de indicarlo en la proyecci\u00f3n: db . posts . find ( { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}}, { comments : { $elemMatch : { author : \"Santiago Dollins\" , email : \"xnZKyvWD@jHfVKtUh.com\" }}} ) Si lo que nos interesa es la cantidad de elementos que contiene un array, emplearemos el operador $size . Por ejemplo, para obtener los mensajes que tienen 10 etiquetas har\u00edamos: db . posts . find ( { tags : { $size : 10 }} ) Finalmente, a la hora de proyectar los datos, si no estamos interesados en todos los valores de un campo que es un array, podemos restringir el resultado mediante el operador $slice . As\u00ed pues, si quisi\u00e9ramos obtener los mensajes titulados US Constitution y que de esos mensajes, mostrara s\u00f3lo tres etiquetas y dos comentarios, har\u00edamos: > db . posts . find ( { title : \"US Constitution\" }, { comments : { $slice : 2 }, tags : { $slice : 3 }} ) < { _id : ObjectId ( \"50ab0f8bbcf1bfe2536dc416\" ), body : 'We the People ...' , permalink : 'NhWDUNColpvxFjovsgqU' , author : 'machine' , title : 'US Constitution' , tags : [ 'engineer' , 'granddaughter' , 'sundial' ], comments : [ { body : 'Lorem ipsum dolor ...' , email : 'ftRlVMZN@auLhwhlj.com' , author : 'Leonida Lafond' }, { body : 'Lorem ipsum dolor sit...' , email : 'dsoLAdFS@VGBBuDVs.com' , author : 'Nobuko Linzey' } ], date : 2012 - 11 - 20 T05 : 05 : 15.276 Z }","title":"Consultas sobre arrays"},{"location":"sa/02mongo.html#conjunto-de-valores","text":"Igual que en SQL, a partir de un colecci\u00f3n, si queremos obtener todos los diferentes valores que existen en un campo, utilizaremos el m\u00e9todo distinct : > db . trips . distinct ( 'usertype' ) < [ 'Customer' , 'Subscriber' ] Si queremos filtrar los datos sobre los que se obtienen los valores, le pasaremos un segundo par\u00e1metro con el criterio a aplicar: > db . trips . distinct ( 'usertype' , { \"birth year\" : { $gt : 1990 } } ) < [ 'Subscriber' ]","title":"Conjunto de valores"},{"location":"sa/02mongo.html#cursores","text":"Al hacer una consulta en el shell se devuelve un cursor. Este cursor lo podemos guardar en un variable, y partir de ah\u00ed trabajar con \u00e9l como har\u00edamos mediante cualquier lenguaje de programaci\u00f3n. Si cur es la variable que referencia al cursor, podremos utilizar los siguientes m\u00e9todos: M\u00e9todo Uso Lugar de ejecuci\u00f3n cur.hasNext() true / false para saber si quedan elementos Cliente cur.next() Pasa al siguiente documento Cliente cur.limit(cantidad) Restringe el n\u00famero de resultados a cantidad Servidor cur.sort({campo:1}) Ordena los datos por campo: 1 ascendente o -1 o descendente Servidor cur.skip(cantidad) Permite saltar cantidad elementos con el cursor Servidor La consulta no se ejecuta hasta que el cursor comprueba o pasa al siguiente documento ( next / hasNext ), por ello que tanto limit como sort (ambos modifican el cursor) s\u00f3lo se pueden realizar antes de recorrer cualquier elemento del cursor. Como tras realizar una consulta con find , realmente se devuelve un cursor, un uso muy habitual es encadenar una operaci\u00f3n de find con sort y/o limit para ordenar el resultado por uno o m\u00e1s campos y posteriormente limitar el n\u00famero de documentos a devolver. As\u00ed pues, si quisi\u00e9ramos obtener los tres viajes que m\u00e1s han durado, podr\u00edamos hacerlo as\u00ed: db . trips . find (). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Tambi\u00e9n podemos filtrar previamente a ordenar y limitar: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 3 ) Finalmente, podemos paginar utilizando el m\u00e9todo skip , para mostrar viajes de 10 en 10 a partir de la tercera p\u00e1gina, podr\u00edamos hacer algo as\u00ed: db . trips . find ({ usertype : \"Customer\" }). sort ({ \"tripduration\" :- 1 }). limit ( 10 ). skip ( 20 ) Autoevaluaci\u00f3n A partir de la colecci\u00f3n trips , escribe un consulta que recupere los viajes realizados por subscriptores ordenados descendentemente por su duraci\u00f3n y que obtenga los documentos de 15 al 20.","title":"Cursores"},{"location":"sa/02mongo.html#contando-documentos","text":"Para contar el n\u00famero de documentos, en vez de find usaremos el m\u00e9todo countDocuments . Por ejemplo: > db . trips . countDocuments ({ \"birth year\" : 1977 }) < 186 > db . trips . countDocuments ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 116 count Desde la versi\u00f3n 4.0, los m\u00e9todos count a nivel de colecci\u00f3n y de cursor est\u00e1n caducados ( deprecated ), y no se recomienda su utilizaci\u00f3n. A\u00fan as\u00ed, es muy com\u00fan utilizarlo como m\u00e9todo de un cursor: db . trips . find ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}). count () Cuando tenemos much\u00edsimos datos, si no necesitamos exactitud pero queremos un valor estimado el cual tarde menos en conseguirse (utiliza los metadatos de las colecciones), podemos usar estimatedDocumentCount > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 }) < 10.000 > db . trips . estimatedDocumentCount ({ \"birth year\" : 1977 , \"tripduration\" : { $lt : 600 }}) < 10.000","title":"Contando Documentos"},{"location":"sa/02mongo.html#modificando-documentos","text":"Preparando un persona Para este apartado, vamos a insertar dos veces la misma persona sobre la cual realizaremos las modificaciones: db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) db . people . insertOne ({ nombre : \"Aitor Medrano\" , edad : 45 , profesion : \"Profesor\" }) Para actualizar (y fusionar datos), se utilizan los m\u00e9todos updateOne / updateMany dependiendo de cuantos documentos queremos que modifique. Ambos m\u00e9todos requieren 2 par\u00e1metros: el primero es la consulta para averiguar sobre qu\u00e9 documentos, y en el segundo par\u00e1metro, los campos a modificar utilizando los operadores de actualizaci\u00f3n: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $set : { nombre : \"Marina Medrano\" , salario : 123456 }}) Al realizar la modificaci\u00f3n, el shell nos devolver\u00e1 informaci\u00f3n sobre cuantos documentos ha encontrado, modificado y m\u00e1s informaci\u00f3n: { ack n owledged : true , i nserte dId : null , ma t chedCou nt : 1 , modi f iedCou nt : 1 , upser te dCou nt : 0 } Como hay m\u00e1s de una persona con el mismo nombre, al haber utilizado updateOne s\u00f3lo modificar\u00e1 el primer documento que ha encontrado. \u00a1Cuidado! En versiones antiguas de MongoDB, adem\u00e1s de utilizar los operadores de actualizaci\u00f3n, pod\u00edamos pasarle como par\u00e1metro un documento, de manera que MongoDB realizaba un reemplazo de los campos, es decir, si en el origen hab\u00eda 100 campos y en la operaci\u00f3n de modificaci\u00f3n s\u00f3lo pon\u00edamos 2, el resultado \u00fanicamente contendr\u00eda 2 campos. Es por ello, que ahora es obligatorio utilizar los operadores. Si cuando vamos a actualizar, en el criterio de selecci\u00f3n no encuentra el documento sobre el que hacer los cambios, no se realiza ninguna acci\u00f3n. Si quisi\u00e9ramos que en el caso de no encontrar nada insertase un nuevo documento, acci\u00f3n conocida como upsert ( update + insert ), hay que pasarle un tercer par\u00e1metro al m\u00e9todo con el objeto {upsert:true} . Si encuentra el documento, lo modificar\u00e1, pero si no, crear\u00e1 uno nuevo: db . people . updateOne ({ nombre : \"Andreu Medrano\" }, { $set : { name : \"Andreu Medrano\" , twitter : \"@andreumedrano\" }}, { upsert : true })","title":"Modificando documentos"},{"location":"sa/02mongo.html#operadores-de-actualizacion","text":"MongoDB ofrece un conjunto de operadores para simplificar la modificaci\u00f3n de campos. El operador m\u00e1s utilizado es el operador $set , el cual admite los campos que se van a modificar. Si el campo no existe, lo crear\u00e1. Por ejemplo, para modificar el salario har\u00edamos: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $se t :{ salario : 1000000 }} ) Mediante $inc podemos incrementar el valor de una variable: db.people.upda te O ne ( { n ombre : \"Aitor Medrano\" }, { $i n c :{ salario : 1000 }} ) Para eliminar un campo de un documento, usaremos el operador $unset . De este modo, para eliminar el campo twitter de una persona har\u00edamos: db . people . updateOne ({ nombre : \"Aitor Medrano\" }, { $unset : { twitter : '' }}) Otros operadores que podemos utilizar son $mul , $min , $max y $currentDate . Podemos consultar todos los operadores disponibles en https://www.mongodb.com/docs/manual/reference/operator/update/ Autoevaluaci\u00f3n Tras realizar la siguiente operaci\u00f3n sobre una colecci\u00f3n vac\u00eda: db . people . updateOne ({ nombre : 'yo' }, { '$set' : { 'hobbies' : [ 'gaming' , 'sofing' ]}}, { upsert : true } ); \u00bfCu\u00e1l es el estado de la colecci\u00f3n? Finalmente, un caso particular de las actualizaciones es la posibilidad de renombrar un campo mediante el operador $rename : db . people . updateMany ( { _id : 1 }, { $rename : { 'nickname' : 'alias' , 'cell' : 'movil' }}) Podemos consultar todas las opciones de configuraci\u00f3n de una actualizaci\u00f3n en https://www.mongodb.com/docs/manual/reference/method/db.collection.update/ .","title":"Operadores de actualizaci\u00f3n"},{"location":"sa/02mongo.html#control-de-la-concurrencia","text":"Cuando se hace una actualizaci\u00f3n m\u00faltiple, MongoDB no realiza la operaci\u00f3n de manera at\u00f3mica (a no ser que utilicemos transacciones desde el driver), lo que provoca que se puedan producir pausas ( pause yielding ). Cada documento en s\u00ed es at\u00f3mico, por lo que ninguno se va a quedar a la mitad. MongoDB ofrece el m\u00e9todo findAndModify para encontrar y modificar un documento de manera at\u00f3mica, y as\u00ed evitar que, entre la b\u00fasqueda y la modificaci\u00f3n, el estado del documento se vea afectado. Adem\u00e1s, devuelve el documento modificado. Un caso de uso muy com\u00fan es para contadores y casos similares. db . people . findAndModify ({ query : { nombre : \"Marina Medrano\" }, update : { $inc : { salario : 100 , edad :- 30 }}, new : true }) Por defecto, el documento devuelto ser\u00e1 el resultado que ha encontrado con la consulta. Si queremos que nos devuelva el documento modificado con los cambios deseados, necesitamos utilizar el par\u00e1metro new a true . Si no lo indicamos o lo ponemos a false , tendremos el comportamiento por defecto.","title":"Control de la concurrencia"},{"location":"sa/02mongo.html#actualizaciones-sobre-arrays","text":"Para trabajar con arrays necesitamos nuevos operadores que nos permitan tanto introducir como eliminar elementos de una manera m\u00e1s sencilla que sustituir todos los elementos del array. Los operadores que podemos emplear para trabajar con arrays son: Operador Prop\u00f3sito $push A\u00f1ade uno o varios elementos $addToSet A\u00f1ade un elemento sin duplicados $pull Elimina un elemento $pullAll Elimina varios elementos $pop Elimina el primer o el \u00faltimo Preparando los ejemplos Para trabajar con los arrays, vamos a suponer que tenemos una colecci\u00f3n de enlaces donde vamos a almacenar un documento por cada site, con un atributo tags con etiquetas sobre el enlace en cuesti\u00f3n db . enlaces . insertOne ({ titulo : \"www.google.es\" , tags : [ \"mapas\" , \"videos\" ]}) De modo que tendr\u00edamos el siguiente documento: { _id : Objec t Id( \"633c60e8ac452ac9d7f9fe74\" ) , t i tul o : 'www.google.es' , ta gs : [ 'mapas' , 'videos' ] }","title":"Actualizaciones sobre Arrays"},{"location":"sa/02mongo.html#borrando-documentos","text":"Para borrar, usaremos los m\u00e9todo deleteOne o deleteMany , los cuales funcionan de manera similar a findOne y find . Si no pasamos ning\u00fan par\u00e1metro, deleteOne borrar\u00e1 el primer documento, o en el caso de deleteMany toda la colecci\u00f3n documento a documento. Si le pasamos un par\u00e1metro, \u00e9ste ser\u00e1 el criterio de selecci\u00f3n de documentos a eliminar. db . people . deleteOne ({ nombre : \"Marina Medrano\" }) Al eliminar un documento, no podemos olvidar que cualquier referencia al documento que exista en la base de datos seguir\u00e1 existiendo. Por este motivo, manualmente tambi\u00e9n hay que eliminar o modificar esas referencias. Si queremos borrar toda la colecci\u00f3n, es m\u00e1s eficiente usar el m\u00e9todo drop , ya que tambi\u00e9n elimina los \u00edndices. db . people . drop () Eliminar un campo Recordad que eliminar un determinado campo de un documento no se considera un operaci\u00f3n de borrado, sino una actualizaci\u00f3n mediante el operador $unset .","title":"Borrando documentos"},{"location":"sa/02mongo.html#referencias","text":"Manual de MongoDB Cheatsheet oficial Comparaci\u00f3n entre SQL y MongoDB Cursos gratuitos de Mongo University Consultas solucionadas sobre la colecci\u00f3n sample_restaurants.restaurants en w3resource","title":"Referencias"},{"location":"sa/02mongo.html#actividades","text":"( RA5075.2 / CE5.2b / 1p) Crea un cluster en MongoAtlas , carga los datos de ejemplo y adjunta capturas de pantalla de: Dashboard del cluster Bases de datos / colecciones creadas A continuaci\u00f3n, con\u00e9ctate mediante MongoDB Compass y adjunta una captura de pantalla tras conectar con el cl\u00faster. ( RA5075.1 / CE5.1d / 2p) Haciendo uso de mongosh , escribe los comandos necesarios para: Obtener las bases de datos creadas. Sobre la base de datos sample_training y la colecci\u00f3n zips averigua: Cuantos documentos hay en la ciudad de SAN DIEGO . Cuantos documentos tienen menos de 100 personas (campo pop ). Obt\u00e9n los estados de la ciudad de SAN DIEGO (Soluci\u00f3n: [ 'CA', 'TX' ] ). Cual es el c\u00f3digo postal de la ciudad de ALLEN que no tiene habitantes (s\u00f3lo recupera el zip , no nos interesa ning\u00fan otro campo, ni el _id ). Listado con los 5 c\u00f3digos postales m\u00e1s poblados (muestra los documentos completos). Cantidad de documentos que no tienen menos de 5.000 habitantes ni m\u00e1s de 1.000.000 (debes utilizar el operador $nor ). Cuantos documentos tienen m\u00e1s habitantes que su propio c\u00f3digo postal (campo zip ). Sobre la colecci\u00f3n posts averigua: Cuantos mensajes tienen las etiquetas restaurant o moon . Los comentarios que ha escrito el usuario Salena Olmos . Recupera los mensajes que en body contengan la palabra earth , y devuelve el t\u00edtulo, 3 comentarios y 5 etiquetas. ( RA5075.1 / CE5.1d / 2p) Escribe los comandos necesarios para realizar las siguientes operaciones sobre la colecci\u00f3n zips : Crea una entrada con los siguientes datos: { ci t y : 'ELX' , zip : ' 03206 ' , loc : { x : 38.265500 , y : -0.698459 }, pop : 230224 , s tate : 'Espa\u00f1a' } Crea una entrada con los datos del c\u00f3digo postal donde vives (si es el mismo c\u00f3digo postal, crea uno diferente). Modifica la poblaci\u00f3n de tu c\u00f3digo postal a 1.000.000 . Incrementa la poblaci\u00f3n de todas los documentos de Espa\u00f1a en 666 personas. A\u00f1ade un campo prov a ambos documentos con valor Alicante . Modifica los documentos de Espa\u00f1a y a\u00f1ade un atributo tags que contenga un array vac\u00edo. Modifica todos los documentos de la provincia de Alicante y a\u00f1ade al atributo tags el valor sun . Modifica el valor de sun de tu c\u00f3digo postal y sustit\u00fayelo por house . Renombra en los documentos de la provincia de Alicante el atributo prov por provincia Elimina las coordenadas del zip 03206 . Elimina tu entrada.","title":"Actividades"},{"location":"sa/03modelado.html","text":"Antes de estudiar c\u00f3mo modelar de forma adecuada nuestros datos en un modelo de datos documental, en concreto dentro de MongoDB , es conveniente recordar que: Mediante un documento podemos mantener toda la informaci\u00f3n que se utiliza junta en un \u00fanico documento, sin necesidad de separar los datos en diferentes colecciones. Las propiedades transaccionales ACID se cumplen a nivel de documento. Si tenemos la informaci\u00f3n separada en varios documentos, tenemos que utilizar el modelo transaccional de MongoDB de forma programativa. Dependiendo del tipo de relaci\u00f3n entre dos documentos, normalizaremos los datos para minimizar la redundancia pero manteniendo en la medida de lo posible que mediante operaciones at\u00f3micas se mantenga la integridad de los datos. Para ello, bien crearemos referencias entre dos documentos o embeberemos un documento dentro de otro, pero intentando que la informaci\u00f3n m\u00e1s utilizada quepa en un \u00fanico documento. MongoDB es una base de datos documental, no relacional, donde el esquema no se debe basar en el uso de claves ajenas/ joins , ya que realmente no existen. A la hora de dise\u00f1ar un esquema, si nos encontramos que el esquema est\u00e1 en 3FN o si cuando hacemos consultas estamos teniendo que realizar varias consultas de manera programativa (primero acceder a una tabla, con ese _id ir a otra tabla, etc\u2026 o hacemos un uso extensivo del operador $lookup mediante el framework de agregaci\u00f3n.) es que no estamos siguiendo el enfoque adecuado. En resumen, dise\u00f1ar un buen modelo de datos puede suponer que nuestro c\u00f3digo sea m\u00e1s legible y mantenible, as\u00ed como un mayor rendimiento de nuestras aplicaciones. Metodolog\u00eda \u00b6 A la hora de tomar decisiones sobre nuestro modelo, nuestra primera decisi\u00f3n es si vamos a modelar para obtener una mayor simplicidad del esquema o queremos un mejor rendimiento. Si nuestro equipo de desarrollo es peque\u00f1o o estamos desarrollando una \u00fanica aplicaci\u00f3n, nos decantaremos por la simplicidad cuando las consultas suelan ser siempre las mismas, pudiendo embeber la mayor\u00eda de entidades en un \u00fanico documento. En cambio, si nuestro equipo es grande, multiples aplicaciones realizan un gran n\u00famero de lecturas/escrituras, nos centraremos en el rendimiento , donde nos centraremos en el tama\u00f1o de los datos, la cantidad de las operaciones y su calificaci\u00f3n/importancia, y embebiendo o relacionando los documentos conforme sea mejor. Simplicidad vs Rendimiento Es m\u00e1s f\u00e1cil optimizar el c\u00f3digo de una aplicaci\u00f3n para obtener mejor rendimiento que simplificar el c\u00f3digo o el esquema de una aplicaci\u00f3n compleja. As\u00ed pues, en un principio, siempre hemos de apostar por la simplicidad. Desde MongoDB recomiendan seguir la siguiente metodolog\u00eda a la hora de definir nuestro modelo de datos, la cual han separado en tres fases: Metodolog\u00eda de modelado de datos Definir la carga ( workload ): Comprender para qu\u00e9 operaciones estamos modelando. Cuantificar y calificar las operaciones de lectura y escritura. Listar las operaciones m\u00e1s importantes. Modelar las relaciones Las relaciones 1:1 normalmente se modelan con un documento embebido las relaciones 1:M y N:M mediante un array de documentos o referencias a documentos de otra colecci\u00f3n. Reconocer y aplicar patrones de dise\u00f1o sobre el esquema Realizar transformaciones sobre el esquema, que se centran en el rendimiento, mantenimiento o simplificaci\u00f3n de los requisitos. Si cruzamos nuestra decisi\u00f3n de simplicidad/rendimiento con la metodolog\u00eda tenemos: Objetivo Simplicidad Simplicidad y Rendimiento Rendimiento 1. Definir la carga Operaciones m\u00e1s frecuentes Mayor\u00eda de operaciones Tama\u00f1o de los datos Cantidad de operaciones Todas las operaciones Tama\u00f1o de los datos Cantidad de operaciones Calificaci\u00f3n de las operaciones 2. Entidades y relaciones Embeber siempre que sea posible Embeber y relacionar Embeber y relacionar 3. Patrones de transformaci\u00f3n Patr\u00f3n A Patrones A y B Patrones A, B y C A continuaci\u00f3n veremos cada una de estas fases en detalle. Definir la carga \u00b6 En esta primera fase es muy importante comprender para qu\u00e9 operaciones estamos modelando, para ello hemos medir los datos a almacenar, cuantificar y calificar las operaciones de lectura y escritura, as\u00ed como listar las operaciones m\u00e1s importantes, con m\u00e9tricas tipo operaciones por segundo, latencia requerida o atributos utilizados en las consultas. Para ello, partiremos de diferentes escenarios de uso de la aplicaci\u00f3n, los logs y estad\u00edsticas que tengamos disponibles o el conocimiento de los analistas de negocio. Dependiendo de la carga, puede provocar diferentes soluciones de modelado, ya que en unas pueden ser m\u00e1s importantes las lecturas y en otras las escrituras. Ejemplo de carga El siguiente ejemplo est\u00e1 extra\u00eddo del curso M320 de Modelado de datos de la MongoDB University . Caso de uso Una organizaci\u00f3n ha desplegado 100.000.000 de sensores meteorol\u00f3gicos. El objeto es capturar en una base de datos los datos transmitidos de todos los dispositivos para realizar predicciones y analizar tendencias. Datos principales N\u00famero de dispositivos: 100.000.000 Duraci\u00f3n: 10 a\u00f1os An\u00e1lisis: 10 a\u00f1os Supuestos: Para las predicciones, son igual de v\u00e1lidos los datos por hora que por minutos. Para an\u00e1lisis m\u00e1s profundos, es necesario mantener los datos por minutos. Operaciones: Actor CRUD Datos en operaciones Tipo de operaci\u00f3n Ratio Informaci\u00f3n extra sensor env\u00edo de datos cada minuto sensor_id , m\u00e9tricas escritura 1.666.667 por seg se almacena una copia (no hace falta redundancia), 1000 bytes de datos le\u00eddos, tiempo de vida de 10 a\u00f1os sistema identificar sensores inoperativos sensor_id , tiempos de m\u00e9tricas lectura 1 por hora latencia y tiempo de consulta de 1 hora, mediante un full scan de los datos, los datos se renuevan cada hora sistema agregar datos cada hora sensor_id , m\u00e9tricas escritura 1 por hora redundancia en la mayor\u00eda de nodos, tiempo de vida de 10 a\u00f1os analista / cient\u00edfico de datos ejecutar 10 consultas anal\u00edticas por hora m\u00e9tricas de temperatura lectura 100 por hora (10 por hora por 10 analistas) latencia y tiempo de consulta de 10 minutos, mediante un full scan de los datos, los datos se renuevan cada hora Operaci\u00f3n detallada: Actor: sensor Descripci\u00f3n: Env\u00edo de datos meteorol\u00f3gicos al servidor Tipo: escritura Datos: sensor_id , timestamp , m\u00e9tricas del sensor Frecuencia: 1.600.000 por seg === 100.000.000 por hora / 60 Tama\u00f1o de datos: 1000 bytes Tiempo de vida: 10 a\u00f1os Duraci\u00f3n de los datos: 1 nodo, sin necesidad de redundancia De este supuesto podemos deducir que la carga es mayoritariamente de escrituras (con un ratio de 99% de escrituras y 1% de lecturas), donde debemos en la medida de lo posible, reducirlas o agruparlas. Adem\u00e1s, la mayor\u00eda de lecturas requieren un full scan de los datos con baja latencia, de manera que podemos ejecutar esas consultas sobre nodos dedicados a la anal\u00edtica. La creaci\u00f3n de consultas agregadas o pre-calculadas puede acelerar estas consultas. Modelar las relaciones \u00b6 Las aplicaciones que emplean MongoDB utilizan dos t\u00e9cnicas para relacionar documentos: Crear referencias Embeber documentos Referencias manuales \u00b6 De manera similar a una base de datos relacional, se almacena el campo _id de un documento en otro documento a modo de clave ajena. De este modo, la aplicaci\u00f3n realiza una segunda consulta para obtener los datos relacionados. Estas referencias son sencillas y suficientes para la mayor\u00eda de casos de uso. Referencias manuales Por ejemplo, si nos basamos en el gr\u00e1fico anterior, podemos conseguir referenciar manualmente estos objetos del siguiente modo: var idUsuario = ObjectId (); db . usuario . insertOne ({ _id : idUsuario , nombre : \"123xyz\" }); db . contacto . insertOne ({ usuario_id : idUsuario , telefono : \"123 456 7890\" , email : \"xyz@ejemplo.com\" }); Para relacionar los dos documentos, haremos uso de la operaci\u00f3n $lookup para hacer el join , o haremos una segunda consulta para la segunda colecci\u00f3n. Un ejemplo de join mediante $lookup : db . usuario . aggregate ([ { $lookup : { from : \"contacto\" , localField : \"_id\" , foreignField : \"usuario_id\" , as : \"contacto_data\" } } ]) Y como resultado obtenemos un documento con el usuario y la informaci\u00f3n del contacto dentro de un array embebido (aunque en este ejemplo s\u00f3lo tenemos un contacto para el usuario) { _id : Objec t Id( \"634589696e96ece54fbcbca2\" ) , n ombre : ' 123 xyz' , co nta c t o_da ta : [ { _id : Objec t Id( \"634589696e96ece54fbcbca3\" ) , usuario_id : Objec t Id( \"634589696e96ece54fbcbca2\" ) , telef o n o : ' 123 456 7890 ' , email : 'xyz@ejemplo.com' } ] } El operador $lookup lo estudiaremos en profundidad en la siguiente sesi\u00f3n. DBRef \u00b6 Son referencias de un documento a otro mediante el valor del campo _id , el nombre de la colecci\u00f3n y, opcionalmente, el nombre de la base de datos. Estos objetos siguen una convenci\u00f3n para representar un documento mediante la notaci\u00f3n { \"$ref\" : <nombreColeccion>, \"$id\" : <valorCampo_id>, \"$db\" : <nombreBaseDatos> } . Al incluir estos nombres, las DBRef permite referenciar documentos localizados en diferentes colecciones. As\u00ed pues, si reescribimos el c\u00f3digo anterior mediante DBRef tendr\u00edamos que el contacto lo insertamos de la siguiente manera: db . contacto . insertOne ({ usuario_id : new DBRef ( \"usuario\" , idUsuario ), telefono : \"123-456-7890\" , email : \"xyz@example.com\" }); Y al recuperarlo, vemos que ha almacenado la referencia: { _id : Objec t Id( \"6345899d6e96ece54fbcbca4\" ) , usuario_id : DBRe f ( \"usuario\" , Objec t Id( \"634589696e96ece54fbcbca2\" )) , telef o n o : ' 123-456-7890 ' , email : 'xyz@example.com' } } De manera similar a las referencias manuales, mediante consultas adicionales se obtendr\u00e1n los documentos referenciados. Muchos drivers (incluido el de Python, mediante la clase DBRef ) contienen m\u00e9todos auxiliares que realizan las consultas con referencias DBRef autom\u00e1ticamente. Evita DBRef Desde la propia documentaci\u00f3n de MongoDB recomiendan el uso de referencias manuales y el operador $lookup , a no ser que dispongamos documentos de una colecci\u00f3n que referencian a documentos que se encuentran en varias colecciones diferentes. Datos embebidos \u00b6 En cambio, si dentro de un documento almacenamos los datos mediante sub-documentos, ya sea dentro de un atributo o un array, podremos obtener todos los datos mediante un \u00fanico acceso, sin necesidad de claves ajenas ni comprobaciones de integridad referencial. Datos embebidos Generalmente, emplearemos datos embebidos cuando tengamos: relaciones \"contiene\" entre entidades, entre relaciones de documentos \"uno a uno\" o \"uno a pocos\". relaciones \"uno a muchos\" entre entidades. En estas relaciones los documentos hijo (o \"muchos\") siempre aparecen dentro del contexto del padre o del documento \"uno\". Los datos embebidos/incrustados ofrecen mejor rendimiento al permitir obtener los datos mediante una \u00fanica operaci\u00f3n, as\u00ed como modificar datos relacionados en una sola operaci\u00f3n at\u00f3mica de escritura (sin necesidad de transacciones) Un aspecto a tener en cuenta es que un documento BSON puede contener un m\u00e1ximo de 16MB. Si quisi\u00e9ramos que un atributo contenga m\u00e1s informaci\u00f3n, tendr\u00edamos que utilizar el API de GridFS. Vamos a estudiar en detalle cada uno de los tipos de relaciones , para intentar clarificar cuando es conveniente utilizar referencias o datos embebidos. Relaciones 1:1 \u00b6 Cuando existe una relaci\u00f3n 1:1, como pueda ser entre Persona y Curriculum , o Persona y Direccion hay que embeber un documento dentro del otro, como parte de un atributo: persona.json { nombre : \"Aitor\" , edad : 45 , direccion : { calle : \"Secreta\" , ciudad : \"Elx\" } } La principal ventaja de este planteamiento es que mediante una \u00fanica consulta podemos obtener tanto los detalles del usuario como su direcci\u00f3n. Un par de aspectos que nos pueden llevar a no embeberlos son: la frecuencia de acceso. Si a uno de ellos se accede raramente, puede que convenga tenerlos separados para liberar memoria. el tama\u00f1o de los elementos. Si hay uno que es mucho m\u00e1s grande que el otro, o uno lo modificamos muchas m\u00e1s veces que el otro, para que cada vez que hagamos un cambio en un documento no tengamos que modificar el otro ser\u00e1 mejor separarlos en documentos separados. Pero siempre teniendo en cuenta la atomicidad de los datos, ya que si necesitamos modificar los dos documentos al mismo tiempo, tendremos que embeber uno dentro del otro. Relaciones 1:N \u00b6 Vamos a distinguir dos tipos: 1 a pocos (1:F) ( one to few ), como por ejemplo, dentro de un blog, la relaci\u00f3n entre Mensaje y Comentario . En este caso, la mejor soluci\u00f3n es crear un array dentro de la entidad 1 (en nuestro caso, Mensaje ). De este modo, el Mensaje contiene un array de Comentario : mensaje.json { titulo : \"La broma asesina\" , url : \"http://es.wikipedia.org/wiki/Batman:_The_Killing_Joke\" , texto : \"La dualidad de Batman y Joker\" , comentarios : [ { autor : \"Bruce Wayne\" , fecha : ISODate ( \"2022-10-11T09:31:32Z\" ), comentario : \"A m\u00ed me encant\u00f3\" }, { autor : \"Bruno D\u00edaz\" , fecha : ISODate ( \"2022-10-11T10:07:28Z\" ), comentario : \"El mejor\" } ] } Vigilar el tama\u00f1o Hay que tener siempre en mente la restricci\u00f3n de los 16 MB de BSON. Si vamos a embeber muchos documentos y estos son grandes, hay que vigilar no llegar a dicho tama\u00f1o. 1 a muchos (1:N) ( one to many ), como puede ser entre Editorial y Libro . Para este tipo de relaci\u00f3n es mejor usar referencias entre los documentos colocando la referencia en el lado del muchos: editorial.json { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } libros.json { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial_id : 1 , }, { _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial_id : 1 , } Si cada vez que recuperamos un libro queremos tener el nombre de la editorial y con una sola consulta recuperar todos los datos, en vez poner la referencia a la editorial, podemos embeber toda la informaci\u00f3n (esto se conoce como el patr\u00f3n referencia extendida ), a costa de que un futuro cambio en el nombre de la editorial conlleve modificar muchos libros: libros2.json { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial : { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } },{ _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial : { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } } Un caso particular en las relaciones uno a muchos que se traducen en documentos embebidos es cuando la informaci\u00f3n que nos interesa tiene un valor concreto en un momento determinado. Por ejemplo, dentro de un pedido, el precio de los productos debe embeberse, ya que si en un futuro se modifica el precio de un producto determinado debido a una oferta, el pedido realizado no debe modificar su precio total. Del mismo modo, al almacenar la direcci\u00f3n de una persona, tambi\u00e9n es conveniente embeberla. No queremos que la direcci\u00f3n de env\u00edo de un pedido ya enviado se modifique si un usuario cambia sus datos personales. En cambio, si necesitamos acceder por separado a los diferentes objetos de una relaci\u00f3n, puede que nos convenga separarlo en dos colecciones distintas, aunque luego tengamos que hacer un join . 1 a much\u00edsimos/tropecientos (1:S) ( one to squillions/zillions ), como puede ser entre una aplicaci\u00f3n y los mensajes del log, los cuales pueden llegar a tener un volumen de millones de mensaje por aplicaci\u00f3n. Teniendo siempre en mente la restricci\u00f3n de los 16MB de BSON, podemos modelar estas relaciones mediante un array de referencias: aplicacion.json { _id : ObjectId ( \"111111\" ), nombre : \"Gesti\u00f3n de clientes\" , } logs.json { _id : ObjectId ( \"123456\" ), app : ObjectId ( \"111111\" ), actividad : \"Alta Cliente\" , mensaje : \"El cliente XXX se ha creado correctamente\" date : ISODate ( \"2022-10-12\" ) },{ _id : ObjectId ( \"123457\" ), app : ObjectId ( \"111111\" ), actividad : \"Modificaci\u00f3n Cliente\" , mensaje : \"No se ha podido modificar el XXX por un error del sistema\" date : ISODate ( \"2022-10-12\" ) }, De esta manera, pasamos la relaci\u00f3n de 1 a muchos a realmente ser de muchos a 1, donde cada mensaje de log almacena la aplicaci\u00f3n a la que pertenecen, y ya no tenemos que mantener un array de logs dentro de cada aplicaci\u00f3n. Relaciones N:M \u00b6 M\u00e1s que relaciones muchos a muchos, suelen ser relaciones pocos a pocos, como por ejemplo, Libro y Autor , o Profesor y Estudiante . Supongamos que tenemos libros modelados de la siguiente manera: libro.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 } Y autores con la siguiente estructura: autor.json { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" } Podemos resolver esta relaci\u00f3n de tres maneras: Siguiendo un enfoque relacional, empleando un documento como la entidad que agrupa con referencias manuales a los dos documentos. libro-autor.json { _id : 1 , autor_id : 1 , libro_id : 1 } Este enfoque se desaconseja porque necesita acceder a tres colecciones para obtener toda la informaci\u00f3n. Mediante 2 documentos, cada uno con un array que contenga los identificadores del otro documento ( 2 Way Embedding ). Hay que tener cuidado porque podemos tener problemas de inconsistencia de datos si no actualizamos correctamente. libro-con-autores.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [ 1 ] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [ 1 ] } autor-con-libros.json { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" , libros : [ 1 , 2 ] } Embeber un documento dentro de otro ( One Way Embedding ). Por ejemplo: libro-con-autores-embebidos.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] } En principio este enfoque no se recomienda porque el documento puede crecer mucho y provocar anomal\u00edas de modificaciones donde la informaci\u00f3n no es consistente. Si se opta por esta soluci\u00f3n, hay que tener en cuenta que si un documento depende de otro para su creaci\u00f3n (por ejemplo, si metemos los profesores dentro de los estudiantes, no vamos a poder dar de alta a profesores sin haber dado de alta previamente a un alumno). A modo de resumen, en las relaciones N:M, hay que establecer el tama\u00f1o de N y M. Si N como m\u00e1ximo vale 3 y M 500000, entonces deber\u00edamos seguir un enfoque de embeber la N dentro de la M ( One Way Embedding ). En cambio, si N vale 3 y M vale 5, entonces podemos hacer que ambos embeban al otro documento ( Two Way Embedding ). Rendimiento e Integridad A modo de resumen, embeber documentos ofrece un mejor rendimiento que referenciar, ya que con una \u00fanica operaci\u00f3n (ya sea una lectura o una escritura) podemos acceder a varios documentos. Cuidado con los arrays Los array no pueden crecer de forma descontrolada. Si hay un par de cientos de documentos en el lado de N, no hay que embeberlos. Si hay m\u00e1s de unos pocos miles de documentos en el lado de N, no hay que usar un array de referencias. Arrays con una alta cardinalidad son una clara pista para no embeber. Jer\u00e1rquicas \u00b6 Si tenemos que modelar alguna entidad que tenga hijos y nos importa las relaciones padre-hijos (categor\u00eda-subcategor\u00eda), podemos tanto embeber un array con los hijos de un documento ( children ), como embeber un array con los padres de un documento ( ancestors ) M\u00e1s informaci\u00f3n en https://www.mongodb.com/docs/manual/applications/data-models-tree-structures/ Patrones \u00b6 Ahora que ya hemos estudiado como modelar las relaciones entre diferentes documentos, hay un conjunto de patrones , e igual de importante, anti-patrones , que nos pueden ayudar a la hora de dise\u00f1ar o migrar un sistema. Podemos agrupar los patrones en tres categor\u00edas: Representaci\u00f3n: atributo, versionado de documentos y esquema, polim\u00f3rfico Frecuencia de acceso: subconjuntos, aproximaci\u00f3n, referencia cruzada Agrupaci\u00f3n: calculado, cubo, at\u00edpico Patrones y Casos de Uso - mongodb.com Representaci\u00f3n \u00b6 Los patrones de representaci\u00f3n se centran en la representaci\u00f3n del esquema. Destacamos los patrones: Atributo Versionado de documento Versionado de esquema Polim\u00f3rfico Atributo \u00b6 El patr\u00f3n atributo / attribute se utiliza cuando tenemos un conjunto de valores separados entre varios campos que sem\u00e1nticamente est\u00e1n agrupados. Supongamos que tenemos un documento con informaci\u00f3n sobre el precio de un producto: { produc t o : \"ps5\" , precio_es : 549.99 , precio_uk : 479.99 , precio_us : 499.99 } Agrupamos los atributos en un campo precios (normalmente un array de documentos embebidos): { produc t o : \"ps5\" , precios : [ { pais : \"es\" , precio : 549.99 }, { pais : \"uk\" , precio : 479.99 }, { pais : \"us\" , precio : 499.99 }, ] } Al a\u00f1adir un nuevo elemento, en vez de a\u00f1adir un nuevo campo al documento, crearemos un nuevo documento dentro del array. Adem\u00e1s, con este patr\u00f3n podr\u00edamos a\u00f1adir m\u00e1s informaci\u00f3n, como puede ser la moneda: { produc t o : \"ps5\" , precios : [ { pais : \"es\" , precio : 549.99 , mo ne da : \"eur\" }, { pais : \"uk\" , precio : 479.99 , mo ne da : \"pound\" }, { pais : \"us\" , precio : 499.99 , mo ne da : \"dollar\" }, ] } Como ventaja, podemos destacar que esta soluci\u00f3n facilita la indexaci\u00f3n de los campos y que permite realizar la ordenaci\u00f3n de los datos por los campos embebidos. Versionado de documentos \u00b6 El patr\u00f3n versionado de documentos / document versioning se utiliza cuando necesitamos mantener diferentes versiones de un documento, por ejemplo, para mantener un hist\u00f3rico. En ese caso, podemos a\u00f1adir un atributo revision (de manera similar a como lo hace git ) a modo de un contador con las diferentes versiones de cada documento. Adem\u00e1s, tendremos dos colecciones, una con los datos hist\u00f3ricos, y otra con los \u00faltimos datos (y sobre la cual se suelen hacer las consultas). Versionado de esquema \u00b6 El patr\u00f3n versionado de esquema / schema versioning se utiliza cuando tenemos diferentes esquemas para un mismo documento, ya sea por una evoluci\u00f3n de nuestro modelo, por la integraci\u00f3n de datos externos, etc... Para ello, a\u00f1adiremos un atributo schema_version para indicar que versi\u00f3n del esquema cumplen los datos y poder identificar la estructura del documento con el que estamos trabajando. Adem\u00e1s, nos permite evitar el downtime al realizar la actualizaci\u00f3n del esquema facilitando la transici\u00f3n del esquema antiguo al nuevo. Supongamos que partimos del siguiente esquema para nuestros clientes: cliente.json _id : Objec t Id( \"12345\" ) , n ombre : \"Aitor Medrano\" , email : \"a.medrano@edu.gva.es\" , telef o n o : \"612 34 56 78\" Y conforme evoluciona nuestra aplicaci\u00f3n vemos que vamos a\u00f1adiendo m\u00e1s y m\u00e1s m\u00e9todos de contacto, por lo que decidimos crear un documento embebido para agrupar funcionalmente dichos datos. Esta acci\u00f3n puede provocar accesos err\u00f3neos al tel\u00e9fono o al email, por lo que creamos el atributo schema_version para saber cu\u00e1l es la estructura que nuestra aplicaci\u00f3n espera leer: cliente-esquema-v2.json _id : Objec t Id( \"12345\" ) , schema_versio n : 2 , n ombre : \"Aitor Medrano\" , co nta c t o : { email : \"a.medrano@edu.gva.es\" , telef o n o : \"612 34 56 78\" , t wi tter : \"@aitormedrano\" } Polim\u00f3rfico \u00b6 El patr\u00f3n polim\u00f3rfico / polymorphic se utiliza cuando tenemos un conjunto de documentos que tienen m\u00e1s similitudes que diferencias y necesitamos que est\u00e9n en una \u00fanica colecci\u00f3n. Supongamos que tenemos una base de datos sobre pel\u00edculas, y tenemos una colecci\u00f3n para los directores, otra para los actores/actrices, otra para los m\u00fasicos, etc... Claramente dichos documentos compartir\u00e1n muchos atributos, como son el nombre, la fecha de nacimiento, sexo, lugar de origen, etc... los cuales podemos colocar en la misma colecci\u00f3n a\u00f1adiendo un atributo type con el tipo del documento (por ejemplo, actor , m\u00fasico , director , etc...). Aquellos datos que son diferentes, los podemos colocar en subdocumentos para agrupar sus atributos de forma sem\u00e1ntica. Al agruparlos en una \u00fanica colecci\u00f3n, adem\u00e1s de ser m\u00e1s f\u00e1cil de implementar, permite unificar las consultas en una \u00fanica colecci\u00f3n. Frecuencia de acceso \u00b6 Son los patrones que debemos utilizar cuando los casos de uso hagan lecturas de forma intensiva, destacando los patrones: Subconjunto Aproximaci\u00f3n Referencia cruzada Subconjunto \u00b6 El patr\u00f3n subconjunto / subset se utiliza cuando tenemos documentos muy grandes, con muchos atributos y que contienen colecciones de muchos documentos que normalmente no se necesitan al recuperar un documento. Es decir, tenemos documentos de los cuales s\u00f3lo necesitamos un subconjunto de sus datos. Al aplicar este patr\u00f3n, creamos dos colecciones, una con los datos originales, y otra los atributos m\u00e1s utilizados, relacionando ambas colecciones mediante un atributo que cree una relaci\u00f3n 1:1. Si retomamos el ejemplo de la base de datos de pel\u00edcula, al entrar en una pel\u00edcula normalmente no recuperaremos toda la informaci\u00f3n de la misma ni el listado completo de actores y dem\u00e1s personal que haya trabajado en la misma. Probablemente, con una peque\u00f1a sinopsis, el director y los tres o cuatro actores/actrices principales sea suficiente, lo que conllevar\u00e1 consultas m\u00e1s r\u00e1pidas y menos tiempo de respuesta al transmitir menos informaci\u00f3n. Aproximaci\u00f3n \u00b6 El patr\u00f3n aproximaci\u00f3n / approximation se utiliza cuando tenemos una aplicaci\u00f3n que realiza much\u00edsimas escrituras y la exactitud en el resultado no es prioritario. Por ejemplo, si necesitamos almacenar la cantidad de visitas a una p\u00e1gina web, o la poblaci\u00f3n de una determinada ciudad, con un valor aproximado de los datos puede ser suficiente. En vez de realizar una operaci\u00f3n de inserci\u00f3n/modificaci\u00f3n con cada visita, la aplicaci\u00f3n cliente puede ir acumulando las operaciones y comnunicarse con la base de datos cada 100 operaciones, lo que supone ahorrarnos 99 operaciones. La aplicaci\u00f3n cliente puede utilizar un contador para ir contando las operaciones y al llegar a 100 realizar una modificaci\u00f3n o, en vez de un contador, llamar a una funci\u00f3n que devuelva un numero aleatorio entre 0 y 100, el cual devolver\u00e1 0 alrededor del 1% de las veces, de manera que al cumplirse la condici\u00f3n, realicemos la operaci\u00f3n de modificaci\u00f3n. De esta manera, conseguiremos realizar menos escrituras y mantener ciertos valores estad\u00edsticos v\u00e1lidos. En contra, no tendremos el valor exacto y la implementaci\u00f3n se debe realizar en el cliente. Referencia extendida \u00b6 El patr\u00f3n referencia extendida / extended reference se utiliza cuando vemos que realizamos muchos join para obtener toda la informaci\u00f3n a la que la aplicaci\u00f3n suele acceder. Para ello, duplicaremos los datos de los documentos referenciados. Aunque ya vimos un ejemplo de este patr\u00f3n en las relaciones 1:N, supongamos que tenemos una colecci\u00f3n de pedidos que referencia al cliente que realiza la compra: pedidos.json { _id : Objec t Id( \"12345\" ) , fe cha : ISODa te ( \"2022-10-13T17:41:33Z\" ) , clie nte _id : Objec t Id( \"112233\" ) , pedido : { produc t o : \"ps5\" , ca nt idad : 1 , precio : 499 } } Y otra colecci\u00f3n de clientes con los datos del comprador: clientes.json { _id : Objec t Id( \"12345\" ) , n ombre : \"Aitor Medrano\" , calle : \"Secreta\" , ciudad : \"Elx\" , ca te goria : \"Platino\" , co nta c t o : [ { t ipo : \"email\" , valor : \"a.medrano@edu.gva.es\" }, { t ipo : \"telefono\" , valor : \"612 345 678\" } ] } Si cada vez que recuperamos un pedido nos interesa el nombre y la direcci\u00f3n de la persona que lo realiza, vamos a necesitar siempre un join . El patr\u00f3n referencia extendida se traduce en duplicar los datos aunque haya redundancia para evitar dicho join : pedidos-extendido.json { _id : Objec t Id( \"12345\" ) , fe cha : ISODa te ( \"2022-10-13T17:41:33Z\" ) , clie nte : { _id : Objec t Id( \"112233\" ) , n ombre : \"Aitor Medrano\" , calle : \"Secreta\" , ciudad : \"Elx\" } pedido : { produc t o : \"ps5\" , ca nt idad : 1 , precio : 499 } } Agrupaci\u00f3n \u00b6 Los patrones de agrupaci\u00f3n se utilizan para escalar el esquema de manera r\u00e1pida y de forma eficiente. Se utilizan para aplicaciones que hacen muchas m\u00e1s lecturas que escrituras, destacando los patrones: Calculado Cubo At\u00edpico Calculado \u00b6 El patr\u00f3n calculado / computed se utiliza cuando queremos evitar tener que recalcular datos en cada lectura. Para ello, en una colecci\u00f3n aparte se guardan los datos calculados, de manera que cuando llega un nuevo registro a la colecci\u00f3n con los datos, se recalcula este valor y se modifica el documento oportuno en la colecci\u00f3n de datos calculados. Patr\u00f3n calculado - mongodb.com Este patr\u00f3n tiene sentido en aplicaciones donde hay muchas m\u00e1s lecturas que escrituras, ya que el c\u00e1lculo se realiza en tiempo de escritura. Cubo \u00b6 Cuando trabajamos con IoT, anal\u00edtica de datos, o series temporales es normal crear una colecci\u00f3n con un documento por cada medida que se tome. sensor.json { se ns or_id : 12345 , t imes ta mp : ISODa te ( \"2022-01-31T10:00:00.000Z\" ) , te mp : 40 }, { se ns or_id : 12345 , t imes ta mp : ISODa te ( \"2022-01-31T10:01:00.000Z\" ) , te mp : 40 }, Para evitar la creaci\u00f3n de \u00edndices que ocupen much\u00edsima memoria RAM, mediante el patr\u00f3n cubo / bucket crearemos un array de documentos con cada medida y un resumen con los datos agregados: sensor-cubo.json { se ns or_id : 12345 , fe cha_i n icio : ISODa te ( \"2022-01-31T10:00:00.000Z\" ) , fe cha_ f i n : ISODa te ( \"2022-01-31T10:59:59.000Z\" ) , medidas : [ { t imes ta mp : ISODa te ( \"2022-01-31T10:00:00.000Z\" ) , te mp : 40 }, { t imes ta mp : ISODa te ( \"2022-01-31T10:01:00.000Z\" ) , te mp : 40 } ], ca nt idad : 2 , suma : 80 } Estos documentos los podemos agrupar por horas, d\u00edas, etc... lo cuales podemos colocar en colecciones con datos trimestrales, anuales, etc... De esta manera, nos facilita el almacenamiento, an\u00e1lisis y purga de los datos dentro de los requisitos temporales de nuestras aplicaciones. At\u00edpico \u00b6 El patr\u00f3n at\u00edpico / outlier se utiliza cuando tenemos datos que son at\u00edpicos, es decir, que se salen del rango normal de comportamiento de la aplicaci\u00f3n. Supongamos que tenemos una aplicaci\u00f3n de cine donde marcamos en una lista las pel\u00edculas que queremos ver ( whistlist ), y suponemos que los usuarios colocar\u00e1n dentro de dicho array un m\u00e1ximo de 100 pel\u00edculas. \u00bfQu\u00e9 sucede si un usuario quiere a\u00f1adir 1000 pel\u00edculas? Ese usuario es un caso at\u00edpico, y en vez de cambiar todo el modelo de datos por \u00e9l, se crea una excepci\u00f3n mediante el patr\u00f3n at\u00edpico. Para ello, podemos a\u00f1adir un atributo al documento, al que podemos llamar outlier que marcaremos a true cuando necesitemos acceder a otra colecci\u00f3n para recuperar las pel\u00edculas favoritas que le faltan al usuario. Con una primera consulta, si vemos que tenemos un outlier , desde la aplicaci\u00f3n haremos el join para recuperar el resto de datos. Otros patrones Patr\u00f3n \u00c1rbol / Tree : Para la representaci\u00f3n de datos jer\u00e1rquicos y evitar joins a la hora de hacer consultas, incluyendo la referencia de hijos, padres, array de ancestors , etc... Patr\u00f3n Preasignaci\u00f3n / Pre-Allocation : El cual plantea una definici\u00f3n inicial de una estructura de datos para simplificar posteriormente su uso. Este patr\u00f3n era m\u00e1s necesario en versiones antiguas de MongoDB, cuando gestionaba la memoria de forma menos eficiente que las versiones actuales. Validaci\u00f3n de esquemas \u00b6 Aunque los esquemas son din\u00e1micos y podemos a\u00f1adir nuevos campos conforme evoluciona nuestro modelo, podemos validar los esquemas para: asegurar la existencia de un campo. asegurar que un campo est\u00e1 rellenado (no nulo). asegurar el tipo de datos de un campo. restringir entre un conjunto de valores. Desde la versi\u00f3n 3.6 de MongoDB, podemos utilizar el operador $jsonSchema para validar los documentos . Para ello, definimos un documento con, al menos, las propiedades: required : indica los campos obligatorios properties : define los nombres de los campos y sus tipos (mediante la propiedad bsonType ). Por ejemplo, vamos a basarnos en un documento que vimos en la sesi\u00f3n anterior: yo = { _id : Objec t Id( \"631704a042aae0893122f2d6\" ) , n ombre : 'Ai t or' , apellidos : 'Medra n o' , fna c : ISODa te ( \"1977-10-03T00:00:00.000Z\" ) , hobbies : [ 'programaci\u00f3 n ' , 'videojuegos' , 'balo n ces t o' ], casado : true , hijos : 2 , co nta c t o : { t wi tter : '@ai t ormedra n o' , email : 'a.medra n o@edu.gva.es' }, fe chaCreacio n : Times ta mp( { t : 1662452896 , i : 1 } ) } Podr\u00edamos definir su validador de la siguiente manera: validador = { valida t or : { $jso n Schema : { required : [ \"nombre\" , \"fnac\" , \"contacto\" ], proper t ies : { n ombre : { bso n Type : \"string\" , descrip t io n : \"debe ser un string y es un campo obligatorio\" }, apellidos : { bso n Type : \"string\" }, fna c : { bso n Type : \"date\" }, hobbies : { bso n Type : \"array\" , u n iqueI te ms : true , i te ms : { bso n Type : \"string\" , } }, casado : { bso n Type : \"bool\" }, hijos : { bso n Type : \"int\" , mi n imum : 0 , maximum : 49 , descrip t io n : \"la cantidad de hijos debe ser positiva e inferior a 50\" }, co nta c t o : { bso n Type : \"object\" , required : [ \"email\" ], proper t ies : { \"twitter\" : { bso n Type : \"string\" }, \"email\" : { bso n Type : \"string\" } } }, fe chaCreacio n : { bso n Type : \"timestamp\" } } } } } db.crea te Collec t io n ( \"personas\" , validador) De manera, que si ahora insertamos la persona, no tendremos ning\u00fan error. Vamos a crear una nueva persona que no cumpla la validaci\u00f3n: mal = { nombre : 'Usuario mal' , hijos : 333 , contacto : { twitter : '@usuario_mal' } } db . personas . insertOne ( mal ) Y recibiremos un error de validaci\u00f3n describiendo los errores encontrado: Mo n goServerError : Docume nt fa iled valida t io n Addi t io nal i nf orma t io n : { fa ili n gDocume nt Id : {}, de ta ils : { opera t orName : '$jso n Schema' , schemaRulesNo t Sa t is f ied : [ { opera t orName : 'proper t ies' , proper t iesNo t Sa t is f ied : [ { proper t yName : 'hijos' , de ta ils : [ { opera t orName : 'maximum' , speci f iedAs : { maximum : 49 }, reaso n : 'compariso n fa iled' , co ns ideredValue : 333 } ] }, { proper t yName : 'co nta c t o' , de ta ils : [ { opera t orName : 'required' , speci f iedAs : { required : [ 'email' ] }, missi n gProper t ies : [ 'email' ] } ] } ] }, { opera t orName : 'required' , speci f iedAs : { required : [ ' n ombre' , ' fna c' , 'co nta c t o' ] }, missi n gProper t ies : [ ' fna c' ] } ] } } Expresiones de validaci\u00f3n Tambi\u00e9n podemos a\u00f1adir expresiones de validaci\u00f3n entre campos, de manera que el valor de un campo depende del valor de otro: db . createCollection { \"ventas\" , { validator : { \"$and\" : [ { // Mediante expresiones de consultas \"$expr\" : { \"$lt\" : [ \"$lineaPedido.precioConDescuento\" , \"$lineaPedido.precio\" ] } }, { // Mediante el esquema JSON \"$jsonSchema\" : { \"properties\" : { \"productos\" : { \"bsonType\" : \"array\" } } } } ] } } Si queremos a\u00f1adir una validaci\u00f3n a una colecci\u00f3n ya existente, podemos hacer uso del comando collMod : db.runCommand ( \"collMod\" : <nombreColeccion>, \"validator\" : <esquemaValidador> ) M\u00e1s informaci\u00f3n sobre la validaci\u00f3n de esquemas en https://www.mongodb.com/docs/manual/core/schema-validation/ y en https://www.digitalocean.com/community/tutorials/how-to-use-schema-validation-in-mongodb Consejo final Independientemente de la carga, la manera que modelemos las relaciones y los patrones que apliquemos, lo m\u00e1s importante es que el modelado va a depender directamente de la forma que nuestras aplicaciones accedan a los datos. Por ello es tan importante definir primero los casos de uso/historias de usuario que van provocar el acceso a los datos. Referencias \u00b6 Curso M320: Data Modeling de la MongoDB University V\u00eddeo A Complete Methodology of Data Modeling for MongoDB New Cardinality Notations and Styles for Modeling NoSQL Document-store Databases MongoDB Schema Design Best Practices 6 Rules of Thumb for MongoDB Schema Design Performance Best Practices: MongoDB Data Modeling and Memory Sizing Actividades \u00b6 ( RA5075.1 / CE5.1a / 3p) Define el modelo de datos necesario para modelar al aplicaci\u00f3n de captura de datos del PIA Lara: Debes tener en cuenta la gesti\u00f3n de los usuarios, los cuales pueden ser los usuarios finales (clientes), los t\u00e9cnicos de asistencia (t\u00e9cnicos), as\u00ed como los usuarios administradores de la aplicaci\u00f3n. De los clientes, adem\u00e1s del nombre completo, sexo y la fecha de nacimiento, registraremos su enfermedad (puede que con subcategor\u00edas), as\u00ed como su patolog\u00eda/disfon\u00eda (\u00bfcon una descripci\u00f3n?), y otros datos que consideres oportunos. Un cliente va a grabar muchas muestras de audios, y adem\u00e1s del audio (el cual almacenaremos en un sistema externo y del cual en nuestra base de datos NoSQL almacenaremos su ruta/id), guardaremos la fecha de la grabaci\u00f3n, usuario, texto, \u00bfestado de \u00e1nimo del cliente?, \u00bfvelocidad de grabaci\u00f3n? ... (aqu\u00ed deber\u00e9is pensar en todos los datos que necesitemos para un mejor etiquetado de los datos). Los textos de grabaci\u00f3n ( sylabus ) pueden estar predefinidos, de manera que desde el administrador o el t\u00e9cnico se d\u00e9n de alta, o que cada cliente/t\u00e9cnico los inserte en el momento de grabar el audio. Todos los textos se agrupar\u00e1n por categor\u00eda, y cada texto/frase, puede tener un n\u00famero de orden, as\u00ed como un estado de \u00e1nimo. Los listados que luego necesitaremos para recuperar los datos son: Cliente de una determinada enfermedad/patolog\u00eda, pudiendo segmentar por rangos de edad y sexo. Audios y datos de las muestras de un determinado cliente. Audios y datos de las muestras de una determinada enfermedad. Audios y datos de las muestras de una determinada patolog\u00eda. Textos predefinidos de una categor\u00eda asignados a un cliente. Recuerda Para luego realizar un buen modelo de IA, los datos en su mayor\u00eda deber\u00edan ser num\u00e9ricos. Los datos categ\u00f3ricos sirven para matizar y/o filtrar los datos, y si son un conjunto cerrado, luego se pueden codificar como n\u00fameros, por ejemplo mediante el patr\u00f3n One Hot Encoding . Para ello, primero define la carga del modelo y a continuaci\u00f3n, crea diferentes colecciones/documentos JSON con datos de muestra as\u00ed como validadores para cada una de las colecciones que necesites. ( RA5075.1 / CE5.1a / 1p) Nuestro ayuntamiento est\u00e1 dise\u00f1ando un plan energ\u00e9tico para fomentar la instalaci\u00f3n de placas solares. Aquellos viviendas que se sit\u00faen en barrios que generan m\u00e1s energ\u00eda de la que consumen recibir\u00e1n un bono econ\u00f3mico por el exceso de energ\u00eda. Nuestra base de datos almacena, para cada vivienda, cuanta energ\u00eda produce por hora (en kW), cuanta consume, y cuanta necesita consumir de la red el\u00e9ctrica. Un dato de una muestra de energ\u00eda ser\u00eda: energia.json { \"_id\" : Objec t Id( \"6316fc1597eb703de2add36e\" ) , \"propietario_id\" : Objec t Id( \"6317048697eb703de2add36f\" ) , \"date\" : ISODa te ( \"2022-10-26T13:01:00.000Z\" ) , \"kW hora\" : { \"consumo\" : 11 , \"generado\" : 6 , \"necesidad-red\" : 5 } } Y los datos de un propietario: cliente.json { \"_id\" : Objec t Id( \"6317048697eb703de2add36f\" ) , \"nombre\" : \"Aitor Medrano\" , \"direccion\" : { \"calle\" : \"Secreta\" , \"numero\" : \"123\" , \"ciudad\" : \"Elche\" , \"barrio\" : \"Carr\u00fas\" , \"provincia\" : \"Alicante\" , \"cp\" : \"03206\" } } Refactoriza el documento/colecci\u00f3n aplicando los patrones que consideres necesarios, explicando cada uno de los patrones que hayas empleado, con el objetivo de recuperar para cada d\u00eda, la medidas acumuladas para un determinado barrio.","title":"S21.- Modelado de datos NoSQL"},{"location":"sa/03modelado.html#metodologia","text":"A la hora de tomar decisiones sobre nuestro modelo, nuestra primera decisi\u00f3n es si vamos a modelar para obtener una mayor simplicidad del esquema o queremos un mejor rendimiento. Si nuestro equipo de desarrollo es peque\u00f1o o estamos desarrollando una \u00fanica aplicaci\u00f3n, nos decantaremos por la simplicidad cuando las consultas suelan ser siempre las mismas, pudiendo embeber la mayor\u00eda de entidades en un \u00fanico documento. En cambio, si nuestro equipo es grande, multiples aplicaciones realizan un gran n\u00famero de lecturas/escrituras, nos centraremos en el rendimiento , donde nos centraremos en el tama\u00f1o de los datos, la cantidad de las operaciones y su calificaci\u00f3n/importancia, y embebiendo o relacionando los documentos conforme sea mejor. Simplicidad vs Rendimiento Es m\u00e1s f\u00e1cil optimizar el c\u00f3digo de una aplicaci\u00f3n para obtener mejor rendimiento que simplificar el c\u00f3digo o el esquema de una aplicaci\u00f3n compleja. As\u00ed pues, en un principio, siempre hemos de apostar por la simplicidad. Desde MongoDB recomiendan seguir la siguiente metodolog\u00eda a la hora de definir nuestro modelo de datos, la cual han separado en tres fases: Metodolog\u00eda de modelado de datos Definir la carga ( workload ): Comprender para qu\u00e9 operaciones estamos modelando. Cuantificar y calificar las operaciones de lectura y escritura. Listar las operaciones m\u00e1s importantes. Modelar las relaciones Las relaciones 1:1 normalmente se modelan con un documento embebido las relaciones 1:M y N:M mediante un array de documentos o referencias a documentos de otra colecci\u00f3n. Reconocer y aplicar patrones de dise\u00f1o sobre el esquema Realizar transformaciones sobre el esquema, que se centran en el rendimiento, mantenimiento o simplificaci\u00f3n de los requisitos. Si cruzamos nuestra decisi\u00f3n de simplicidad/rendimiento con la metodolog\u00eda tenemos: Objetivo Simplicidad Simplicidad y Rendimiento Rendimiento 1. Definir la carga Operaciones m\u00e1s frecuentes Mayor\u00eda de operaciones Tama\u00f1o de los datos Cantidad de operaciones Todas las operaciones Tama\u00f1o de los datos Cantidad de operaciones Calificaci\u00f3n de las operaciones 2. Entidades y relaciones Embeber siempre que sea posible Embeber y relacionar Embeber y relacionar 3. Patrones de transformaci\u00f3n Patr\u00f3n A Patrones A y B Patrones A, B y C A continuaci\u00f3n veremos cada una de estas fases en detalle.","title":"Metodolog\u00eda"},{"location":"sa/03modelado.html#definir-la-carga","text":"En esta primera fase es muy importante comprender para qu\u00e9 operaciones estamos modelando, para ello hemos medir los datos a almacenar, cuantificar y calificar las operaciones de lectura y escritura, as\u00ed como listar las operaciones m\u00e1s importantes, con m\u00e9tricas tipo operaciones por segundo, latencia requerida o atributos utilizados en las consultas. Para ello, partiremos de diferentes escenarios de uso de la aplicaci\u00f3n, los logs y estad\u00edsticas que tengamos disponibles o el conocimiento de los analistas de negocio. Dependiendo de la carga, puede provocar diferentes soluciones de modelado, ya que en unas pueden ser m\u00e1s importantes las lecturas y en otras las escrituras. Ejemplo de carga El siguiente ejemplo est\u00e1 extra\u00eddo del curso M320 de Modelado de datos de la MongoDB University . Caso de uso Una organizaci\u00f3n ha desplegado 100.000.000 de sensores meteorol\u00f3gicos. El objeto es capturar en una base de datos los datos transmitidos de todos los dispositivos para realizar predicciones y analizar tendencias. Datos principales N\u00famero de dispositivos: 100.000.000 Duraci\u00f3n: 10 a\u00f1os An\u00e1lisis: 10 a\u00f1os Supuestos: Para las predicciones, son igual de v\u00e1lidos los datos por hora que por minutos. Para an\u00e1lisis m\u00e1s profundos, es necesario mantener los datos por minutos. Operaciones: Actor CRUD Datos en operaciones Tipo de operaci\u00f3n Ratio Informaci\u00f3n extra sensor env\u00edo de datos cada minuto sensor_id , m\u00e9tricas escritura 1.666.667 por seg se almacena una copia (no hace falta redundancia), 1000 bytes de datos le\u00eddos, tiempo de vida de 10 a\u00f1os sistema identificar sensores inoperativos sensor_id , tiempos de m\u00e9tricas lectura 1 por hora latencia y tiempo de consulta de 1 hora, mediante un full scan de los datos, los datos se renuevan cada hora sistema agregar datos cada hora sensor_id , m\u00e9tricas escritura 1 por hora redundancia en la mayor\u00eda de nodos, tiempo de vida de 10 a\u00f1os analista / cient\u00edfico de datos ejecutar 10 consultas anal\u00edticas por hora m\u00e9tricas de temperatura lectura 100 por hora (10 por hora por 10 analistas) latencia y tiempo de consulta de 10 minutos, mediante un full scan de los datos, los datos se renuevan cada hora Operaci\u00f3n detallada: Actor: sensor Descripci\u00f3n: Env\u00edo de datos meteorol\u00f3gicos al servidor Tipo: escritura Datos: sensor_id , timestamp , m\u00e9tricas del sensor Frecuencia: 1.600.000 por seg === 100.000.000 por hora / 60 Tama\u00f1o de datos: 1000 bytes Tiempo de vida: 10 a\u00f1os Duraci\u00f3n de los datos: 1 nodo, sin necesidad de redundancia De este supuesto podemos deducir que la carga es mayoritariamente de escrituras (con un ratio de 99% de escrituras y 1% de lecturas), donde debemos en la medida de lo posible, reducirlas o agruparlas. Adem\u00e1s, la mayor\u00eda de lecturas requieren un full scan de los datos con baja latencia, de manera que podemos ejecutar esas consultas sobre nodos dedicados a la anal\u00edtica. La creaci\u00f3n de consultas agregadas o pre-calculadas puede acelerar estas consultas.","title":"Definir la carga"},{"location":"sa/03modelado.html#modelar-las-relaciones","text":"Las aplicaciones que emplean MongoDB utilizan dos t\u00e9cnicas para relacionar documentos: Crear referencias Embeber documentos","title":"Modelar las relaciones"},{"location":"sa/03modelado.html#referencias-manuales","text":"De manera similar a una base de datos relacional, se almacena el campo _id de un documento en otro documento a modo de clave ajena. De este modo, la aplicaci\u00f3n realiza una segunda consulta para obtener los datos relacionados. Estas referencias son sencillas y suficientes para la mayor\u00eda de casos de uso. Referencias manuales Por ejemplo, si nos basamos en el gr\u00e1fico anterior, podemos conseguir referenciar manualmente estos objetos del siguiente modo: var idUsuario = ObjectId (); db . usuario . insertOne ({ _id : idUsuario , nombre : \"123xyz\" }); db . contacto . insertOne ({ usuario_id : idUsuario , telefono : \"123 456 7890\" , email : \"xyz@ejemplo.com\" }); Para relacionar los dos documentos, haremos uso de la operaci\u00f3n $lookup para hacer el join , o haremos una segunda consulta para la segunda colecci\u00f3n. Un ejemplo de join mediante $lookup : db . usuario . aggregate ([ { $lookup : { from : \"contacto\" , localField : \"_id\" , foreignField : \"usuario_id\" , as : \"contacto_data\" } } ]) Y como resultado obtenemos un documento con el usuario y la informaci\u00f3n del contacto dentro de un array embebido (aunque en este ejemplo s\u00f3lo tenemos un contacto para el usuario) { _id : Objec t Id( \"634589696e96ece54fbcbca2\" ) , n ombre : ' 123 xyz' , co nta c t o_da ta : [ { _id : Objec t Id( \"634589696e96ece54fbcbca3\" ) , usuario_id : Objec t Id( \"634589696e96ece54fbcbca2\" ) , telef o n o : ' 123 456 7890 ' , email : 'xyz@ejemplo.com' } ] } El operador $lookup lo estudiaremos en profundidad en la siguiente sesi\u00f3n.","title":"Referencias manuales"},{"location":"sa/03modelado.html#dbref","text":"Son referencias de un documento a otro mediante el valor del campo _id , el nombre de la colecci\u00f3n y, opcionalmente, el nombre de la base de datos. Estos objetos siguen una convenci\u00f3n para representar un documento mediante la notaci\u00f3n { \"$ref\" : <nombreColeccion>, \"$id\" : <valorCampo_id>, \"$db\" : <nombreBaseDatos> } . Al incluir estos nombres, las DBRef permite referenciar documentos localizados en diferentes colecciones. As\u00ed pues, si reescribimos el c\u00f3digo anterior mediante DBRef tendr\u00edamos que el contacto lo insertamos de la siguiente manera: db . contacto . insertOne ({ usuario_id : new DBRef ( \"usuario\" , idUsuario ), telefono : \"123-456-7890\" , email : \"xyz@example.com\" }); Y al recuperarlo, vemos que ha almacenado la referencia: { _id : Objec t Id( \"6345899d6e96ece54fbcbca4\" ) , usuario_id : DBRe f ( \"usuario\" , Objec t Id( \"634589696e96ece54fbcbca2\" )) , telef o n o : ' 123-456-7890 ' , email : 'xyz@example.com' } } De manera similar a las referencias manuales, mediante consultas adicionales se obtendr\u00e1n los documentos referenciados. Muchos drivers (incluido el de Python, mediante la clase DBRef ) contienen m\u00e9todos auxiliares que realizan las consultas con referencias DBRef autom\u00e1ticamente. Evita DBRef Desde la propia documentaci\u00f3n de MongoDB recomiendan el uso de referencias manuales y el operador $lookup , a no ser que dispongamos documentos de una colecci\u00f3n que referencian a documentos que se encuentran en varias colecciones diferentes.","title":"DBRef"},{"location":"sa/03modelado.html#datos-embebidos","text":"En cambio, si dentro de un documento almacenamos los datos mediante sub-documentos, ya sea dentro de un atributo o un array, podremos obtener todos los datos mediante un \u00fanico acceso, sin necesidad de claves ajenas ni comprobaciones de integridad referencial. Datos embebidos Generalmente, emplearemos datos embebidos cuando tengamos: relaciones \"contiene\" entre entidades, entre relaciones de documentos \"uno a uno\" o \"uno a pocos\". relaciones \"uno a muchos\" entre entidades. En estas relaciones los documentos hijo (o \"muchos\") siempre aparecen dentro del contexto del padre o del documento \"uno\". Los datos embebidos/incrustados ofrecen mejor rendimiento al permitir obtener los datos mediante una \u00fanica operaci\u00f3n, as\u00ed como modificar datos relacionados en una sola operaci\u00f3n at\u00f3mica de escritura (sin necesidad de transacciones) Un aspecto a tener en cuenta es que un documento BSON puede contener un m\u00e1ximo de 16MB. Si quisi\u00e9ramos que un atributo contenga m\u00e1s informaci\u00f3n, tendr\u00edamos que utilizar el API de GridFS. Vamos a estudiar en detalle cada uno de los tipos de relaciones , para intentar clarificar cuando es conveniente utilizar referencias o datos embebidos.","title":"Datos embebidos"},{"location":"sa/03modelado.html#relaciones-11","text":"Cuando existe una relaci\u00f3n 1:1, como pueda ser entre Persona y Curriculum , o Persona y Direccion hay que embeber un documento dentro del otro, como parte de un atributo: persona.json { nombre : \"Aitor\" , edad : 45 , direccion : { calle : \"Secreta\" , ciudad : \"Elx\" } } La principal ventaja de este planteamiento es que mediante una \u00fanica consulta podemos obtener tanto los detalles del usuario como su direcci\u00f3n. Un par de aspectos que nos pueden llevar a no embeberlos son: la frecuencia de acceso. Si a uno de ellos se accede raramente, puede que convenga tenerlos separados para liberar memoria. el tama\u00f1o de los elementos. Si hay uno que es mucho m\u00e1s grande que el otro, o uno lo modificamos muchas m\u00e1s veces que el otro, para que cada vez que hagamos un cambio en un documento no tengamos que modificar el otro ser\u00e1 mejor separarlos en documentos separados. Pero siempre teniendo en cuenta la atomicidad de los datos, ya que si necesitamos modificar los dos documentos al mismo tiempo, tendremos que embeber uno dentro del otro.","title":"Relaciones 1:1"},{"location":"sa/03modelado.html#relaciones-1n","text":"Vamos a distinguir dos tipos: 1 a pocos (1:F) ( one to few ), como por ejemplo, dentro de un blog, la relaci\u00f3n entre Mensaje y Comentario . En este caso, la mejor soluci\u00f3n es crear un array dentro de la entidad 1 (en nuestro caso, Mensaje ). De este modo, el Mensaje contiene un array de Comentario : mensaje.json { titulo : \"La broma asesina\" , url : \"http://es.wikipedia.org/wiki/Batman:_The_Killing_Joke\" , texto : \"La dualidad de Batman y Joker\" , comentarios : [ { autor : \"Bruce Wayne\" , fecha : ISODate ( \"2022-10-11T09:31:32Z\" ), comentario : \"A m\u00ed me encant\u00f3\" }, { autor : \"Bruno D\u00edaz\" , fecha : ISODate ( \"2022-10-11T10:07:28Z\" ), comentario : \"El mejor\" } ] } Vigilar el tama\u00f1o Hay que tener siempre en mente la restricci\u00f3n de los 16 MB de BSON. Si vamos a embeber muchos documentos y estos son grandes, hay que vigilar no llegar a dicho tama\u00f1o. 1 a muchos (1:N) ( one to many ), como puede ser entre Editorial y Libro . Para este tipo de relaci\u00f3n es mejor usar referencias entre los documentos colocando la referencia en el lado del muchos: editorial.json { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } libros.json { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial_id : 1 , }, { _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial_id : 1 , } Si cada vez que recuperamos un libro queremos tener el nombre de la editorial y con una sola consulta recuperar todos los datos, en vez poner la referencia a la editorial, podemos embeber toda la informaci\u00f3n (esto se conoce como el patr\u00f3n referencia extendida ), a costa de que un futuro cambio en el nombre de la editorial conlleve modificar muchos libros: libros2.json { _id : 1234 , titulo : \"MongoDB: The Definitive Guide\" , autor : [ \"Kristina Chodorow\" , \"Mike Dirolf\" ], numPaginas : 216 , editorial : { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } },{ _id : 1235 , titulo : \"50 Tips and Tricks for MongoDB Developer\" , autor : \"Kristina Chodorow\" , numPaginas : 68 , editorial : { _id : 1 , nombre : \"O'Reilly\" , pais : \"EE.UU.\" } } Un caso particular en las relaciones uno a muchos que se traducen en documentos embebidos es cuando la informaci\u00f3n que nos interesa tiene un valor concreto en un momento determinado. Por ejemplo, dentro de un pedido, el precio de los productos debe embeberse, ya que si en un futuro se modifica el precio de un producto determinado debido a una oferta, el pedido realizado no debe modificar su precio total. Del mismo modo, al almacenar la direcci\u00f3n de una persona, tambi\u00e9n es conveniente embeberla. No queremos que la direcci\u00f3n de env\u00edo de un pedido ya enviado se modifique si un usuario cambia sus datos personales. En cambio, si necesitamos acceder por separado a los diferentes objetos de una relaci\u00f3n, puede que nos convenga separarlo en dos colecciones distintas, aunque luego tengamos que hacer un join . 1 a much\u00edsimos/tropecientos (1:S) ( one to squillions/zillions ), como puede ser entre una aplicaci\u00f3n y los mensajes del log, los cuales pueden llegar a tener un volumen de millones de mensaje por aplicaci\u00f3n. Teniendo siempre en mente la restricci\u00f3n de los 16MB de BSON, podemos modelar estas relaciones mediante un array de referencias: aplicacion.json { _id : ObjectId ( \"111111\" ), nombre : \"Gesti\u00f3n de clientes\" , } logs.json { _id : ObjectId ( \"123456\" ), app : ObjectId ( \"111111\" ), actividad : \"Alta Cliente\" , mensaje : \"El cliente XXX se ha creado correctamente\" date : ISODate ( \"2022-10-12\" ) },{ _id : ObjectId ( \"123457\" ), app : ObjectId ( \"111111\" ), actividad : \"Modificaci\u00f3n Cliente\" , mensaje : \"No se ha podido modificar el XXX por un error del sistema\" date : ISODate ( \"2022-10-12\" ) }, De esta manera, pasamos la relaci\u00f3n de 1 a muchos a realmente ser de muchos a 1, donde cada mensaje de log almacena la aplicaci\u00f3n a la que pertenecen, y ya no tenemos que mantener un array de logs dentro de cada aplicaci\u00f3n.","title":"Relaciones 1:N"},{"location":"sa/03modelado.html#relaciones-nm","text":"M\u00e1s que relaciones muchos a muchos, suelen ser relaciones pocos a pocos, como por ejemplo, Libro y Autor , o Profesor y Estudiante . Supongamos que tenemos libros modelados de la siguiente manera: libro.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 } Y autores con la siguiente estructura: autor.json { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" } Podemos resolver esta relaci\u00f3n de tres maneras: Siguiendo un enfoque relacional, empleando un documento como la entidad que agrupa con referencias manuales a los dos documentos. libro-autor.json { _id : 1 , autor_id : 1 , libro_id : 1 } Este enfoque se desaconseja porque necesita acceder a tres colecciones para obtener toda la informaci\u00f3n. Mediante 2 documentos, cada uno con un array que contenga los identificadores del otro documento ( 2 Way Embedding ). Hay que tener cuidado porque podemos tener problemas de inconsistencia de datos si no actualizamos correctamente. libro-con-autores.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [ 1 ] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [ 1 ] } autor-con-libros.json { _id : 1 , nombre : \"Michael Ende\" , pais : \"Alemania\" , libros : [ 1 , 2 ] } Embeber un documento dentro de otro ( One Way Embedding ). Por ejemplo: libro-con-autores-embebidos.json { _id : 1 , titulo : \"La historia interminable\" , anyo : 1979 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] },{ _id : 2 , titulo : \"Momo\" , anyo : 1973 , autores : [{ nombre : \"Michael Ende\" , pais : \"Alemania\" }] } En principio este enfoque no se recomienda porque el documento puede crecer mucho y provocar anomal\u00edas de modificaciones donde la informaci\u00f3n no es consistente. Si se opta por esta soluci\u00f3n, hay que tener en cuenta que si un documento depende de otro para su creaci\u00f3n (por ejemplo, si metemos los profesores dentro de los estudiantes, no vamos a poder dar de alta a profesores sin haber dado de alta previamente a un alumno). A modo de resumen, en las relaciones N:M, hay que establecer el tama\u00f1o de N y M. Si N como m\u00e1ximo vale 3 y M 500000, entonces deber\u00edamos seguir un enfoque de embeber la N dentro de la M ( One Way Embedding ). En cambio, si N vale 3 y M vale 5, entonces podemos hacer que ambos embeban al otro documento ( Two Way Embedding ). Rendimiento e Integridad A modo de resumen, embeber documentos ofrece un mejor rendimiento que referenciar, ya que con una \u00fanica operaci\u00f3n (ya sea una lectura o una escritura) podemos acceder a varios documentos. Cuidado con los arrays Los array no pueden crecer de forma descontrolada. Si hay un par de cientos de documentos en el lado de N, no hay que embeberlos. Si hay m\u00e1s de unos pocos miles de documentos en el lado de N, no hay que usar un array de referencias. Arrays con una alta cardinalidad son una clara pista para no embeber.","title":"Relaciones N:M"},{"location":"sa/03modelado.html#jerarquicas","text":"Si tenemos que modelar alguna entidad que tenga hijos y nos importa las relaciones padre-hijos (categor\u00eda-subcategor\u00eda), podemos tanto embeber un array con los hijos de un documento ( children ), como embeber un array con los padres de un documento ( ancestors ) M\u00e1s informaci\u00f3n en https://www.mongodb.com/docs/manual/applications/data-models-tree-structures/","title":"Jer\u00e1rquicas"},{"location":"sa/03modelado.html#patrones","text":"Ahora que ya hemos estudiado como modelar las relaciones entre diferentes documentos, hay un conjunto de patrones , e igual de importante, anti-patrones , que nos pueden ayudar a la hora de dise\u00f1ar o migrar un sistema. Podemos agrupar los patrones en tres categor\u00edas: Representaci\u00f3n: atributo, versionado de documentos y esquema, polim\u00f3rfico Frecuencia de acceso: subconjuntos, aproximaci\u00f3n, referencia cruzada Agrupaci\u00f3n: calculado, cubo, at\u00edpico Patrones y Casos de Uso - mongodb.com","title":"Patrones"},{"location":"sa/03modelado.html#representacion","text":"Los patrones de representaci\u00f3n se centran en la representaci\u00f3n del esquema. Destacamos los patrones: Atributo Versionado de documento Versionado de esquema Polim\u00f3rfico","title":"Representaci\u00f3n"},{"location":"sa/03modelado.html#frecuencia-de-acceso","text":"Son los patrones que debemos utilizar cuando los casos de uso hagan lecturas de forma intensiva, destacando los patrones: Subconjunto Aproximaci\u00f3n Referencia cruzada","title":"Frecuencia de acceso"},{"location":"sa/03modelado.html#agrupacion","text":"Los patrones de agrupaci\u00f3n se utilizan para escalar el esquema de manera r\u00e1pida y de forma eficiente. Se utilizan para aplicaciones que hacen muchas m\u00e1s lecturas que escrituras, destacando los patrones: Calculado Cubo At\u00edpico","title":"Agrupaci\u00f3n"},{"location":"sa/03modelado.html#validacion-de-esquemas","text":"Aunque los esquemas son din\u00e1micos y podemos a\u00f1adir nuevos campos conforme evoluciona nuestro modelo, podemos validar los esquemas para: asegurar la existencia de un campo. asegurar que un campo est\u00e1 rellenado (no nulo). asegurar el tipo de datos de un campo. restringir entre un conjunto de valores. Desde la versi\u00f3n 3.6 de MongoDB, podemos utilizar el operador $jsonSchema para validar los documentos . Para ello, definimos un documento con, al menos, las propiedades: required : indica los campos obligatorios properties : define los nombres de los campos y sus tipos (mediante la propiedad bsonType ). Por ejemplo, vamos a basarnos en un documento que vimos en la sesi\u00f3n anterior: yo = { _id : Objec t Id( \"631704a042aae0893122f2d6\" ) , n ombre : 'Ai t or' , apellidos : 'Medra n o' , fna c : ISODa te ( \"1977-10-03T00:00:00.000Z\" ) , hobbies : [ 'programaci\u00f3 n ' , 'videojuegos' , 'balo n ces t o' ], casado : true , hijos : 2 , co nta c t o : { t wi tter : '@ai t ormedra n o' , email : 'a.medra n o@edu.gva.es' }, fe chaCreacio n : Times ta mp( { t : 1662452896 , i : 1 } ) } Podr\u00edamos definir su validador de la siguiente manera: validador = { valida t or : { $jso n Schema : { required : [ \"nombre\" , \"fnac\" , \"contacto\" ], proper t ies : { n ombre : { bso n Type : \"string\" , descrip t io n : \"debe ser un string y es un campo obligatorio\" }, apellidos : { bso n Type : \"string\" }, fna c : { bso n Type : \"date\" }, hobbies : { bso n Type : \"array\" , u n iqueI te ms : true , i te ms : { bso n Type : \"string\" , } }, casado : { bso n Type : \"bool\" }, hijos : { bso n Type : \"int\" , mi n imum : 0 , maximum : 49 , descrip t io n : \"la cantidad de hijos debe ser positiva e inferior a 50\" }, co nta c t o : { bso n Type : \"object\" , required : [ \"email\" ], proper t ies : { \"twitter\" : { bso n Type : \"string\" }, \"email\" : { bso n Type : \"string\" } } }, fe chaCreacio n : { bso n Type : \"timestamp\" } } } } } db.crea te Collec t io n ( \"personas\" , validador) De manera, que si ahora insertamos la persona, no tendremos ning\u00fan error. Vamos a crear una nueva persona que no cumpla la validaci\u00f3n: mal = { nombre : 'Usuario mal' , hijos : 333 , contacto : { twitter : '@usuario_mal' } } db . personas . insertOne ( mal ) Y recibiremos un error de validaci\u00f3n describiendo los errores encontrado: Mo n goServerError : Docume nt fa iled valida t io n Addi t io nal i nf orma t io n : { fa ili n gDocume nt Id : {}, de ta ils : { opera t orName : '$jso n Schema' , schemaRulesNo t Sa t is f ied : [ { opera t orName : 'proper t ies' , proper t iesNo t Sa t is f ied : [ { proper t yName : 'hijos' , de ta ils : [ { opera t orName : 'maximum' , speci f iedAs : { maximum : 49 }, reaso n : 'compariso n fa iled' , co ns ideredValue : 333 } ] }, { proper t yName : 'co nta c t o' , de ta ils : [ { opera t orName : 'required' , speci f iedAs : { required : [ 'email' ] }, missi n gProper t ies : [ 'email' ] } ] } ] }, { opera t orName : 'required' , speci f iedAs : { required : [ ' n ombre' , ' fna c' , 'co nta c t o' ] }, missi n gProper t ies : [ ' fna c' ] } ] } } Expresiones de validaci\u00f3n Tambi\u00e9n podemos a\u00f1adir expresiones de validaci\u00f3n entre campos, de manera que el valor de un campo depende del valor de otro: db . createCollection { \"ventas\" , { validator : { \"$and\" : [ { // Mediante expresiones de consultas \"$expr\" : { \"$lt\" : [ \"$lineaPedido.precioConDescuento\" , \"$lineaPedido.precio\" ] } }, { // Mediante el esquema JSON \"$jsonSchema\" : { \"properties\" : { \"productos\" : { \"bsonType\" : \"array\" } } } } ] } } Si queremos a\u00f1adir una validaci\u00f3n a una colecci\u00f3n ya existente, podemos hacer uso del comando collMod : db.runCommand ( \"collMod\" : <nombreColeccion>, \"validator\" : <esquemaValidador> ) M\u00e1s informaci\u00f3n sobre la validaci\u00f3n de esquemas en https://www.mongodb.com/docs/manual/core/schema-validation/ y en https://www.digitalocean.com/community/tutorials/how-to-use-schema-validation-in-mongodb Consejo final Independientemente de la carga, la manera que modelemos las relaciones y los patrones que apliquemos, lo m\u00e1s importante es que el modelado va a depender directamente de la forma que nuestras aplicaciones accedan a los datos. Por ello es tan importante definir primero los casos de uso/historias de usuario que van provocar el acceso a los datos.","title":"Validaci\u00f3n de esquemas"},{"location":"sa/03modelado.html#referencias","text":"Curso M320: Data Modeling de la MongoDB University V\u00eddeo A Complete Methodology of Data Modeling for MongoDB New Cardinality Notations and Styles for Modeling NoSQL Document-store Databases MongoDB Schema Design Best Practices 6 Rules of Thumb for MongoDB Schema Design Performance Best Practices: MongoDB Data Modeling and Memory Sizing","title":"Referencias"},{"location":"sa/03modelado.html#actividades","text":"( RA5075.1 / CE5.1a / 3p) Define el modelo de datos necesario para modelar al aplicaci\u00f3n de captura de datos del PIA Lara: Debes tener en cuenta la gesti\u00f3n de los usuarios, los cuales pueden ser los usuarios finales (clientes), los t\u00e9cnicos de asistencia (t\u00e9cnicos), as\u00ed como los usuarios administradores de la aplicaci\u00f3n. De los clientes, adem\u00e1s del nombre completo, sexo y la fecha de nacimiento, registraremos su enfermedad (puede que con subcategor\u00edas), as\u00ed como su patolog\u00eda/disfon\u00eda (\u00bfcon una descripci\u00f3n?), y otros datos que consideres oportunos. Un cliente va a grabar muchas muestras de audios, y adem\u00e1s del audio (el cual almacenaremos en un sistema externo y del cual en nuestra base de datos NoSQL almacenaremos su ruta/id), guardaremos la fecha de la grabaci\u00f3n, usuario, texto, \u00bfestado de \u00e1nimo del cliente?, \u00bfvelocidad de grabaci\u00f3n? ... (aqu\u00ed deber\u00e9is pensar en todos los datos que necesitemos para un mejor etiquetado de los datos). Los textos de grabaci\u00f3n ( sylabus ) pueden estar predefinidos, de manera que desde el administrador o el t\u00e9cnico se d\u00e9n de alta, o que cada cliente/t\u00e9cnico los inserte en el momento de grabar el audio. Todos los textos se agrupar\u00e1n por categor\u00eda, y cada texto/frase, puede tener un n\u00famero de orden, as\u00ed como un estado de \u00e1nimo. Los listados que luego necesitaremos para recuperar los datos son: Cliente de una determinada enfermedad/patolog\u00eda, pudiendo segmentar por rangos de edad y sexo. Audios y datos de las muestras de un determinado cliente. Audios y datos de las muestras de una determinada enfermedad. Audios y datos de las muestras de una determinada patolog\u00eda. Textos predefinidos de una categor\u00eda asignados a un cliente. Recuerda Para luego realizar un buen modelo de IA, los datos en su mayor\u00eda deber\u00edan ser num\u00e9ricos. Los datos categ\u00f3ricos sirven para matizar y/o filtrar los datos, y si son un conjunto cerrado, luego se pueden codificar como n\u00fameros, por ejemplo mediante el patr\u00f3n One Hot Encoding . Para ello, primero define la carga del modelo y a continuaci\u00f3n, crea diferentes colecciones/documentos JSON con datos de muestra as\u00ed como validadores para cada una de las colecciones que necesites. ( RA5075.1 / CE5.1a / 1p) Nuestro ayuntamiento est\u00e1 dise\u00f1ando un plan energ\u00e9tico para fomentar la instalaci\u00f3n de placas solares. Aquellos viviendas que se sit\u00faen en barrios que generan m\u00e1s energ\u00eda de la que consumen recibir\u00e1n un bono econ\u00f3mico por el exceso de energ\u00eda. Nuestra base de datos almacena, para cada vivienda, cuanta energ\u00eda produce por hora (en kW), cuanta consume, y cuanta necesita consumir de la red el\u00e9ctrica. Un dato de una muestra de energ\u00eda ser\u00eda: energia.json { \"_id\" : Objec t Id( \"6316fc1597eb703de2add36e\" ) , \"propietario_id\" : Objec t Id( \"6317048697eb703de2add36f\" ) , \"date\" : ISODa te ( \"2022-10-26T13:01:00.000Z\" ) , \"kW hora\" : { \"consumo\" : 11 , \"generado\" : 6 , \"necesidad-red\" : 5 } } Y los datos de un propietario: cliente.json { \"_id\" : Objec t Id( \"6317048697eb703de2add36f\" ) , \"nombre\" : \"Aitor Medrano\" , \"direccion\" : { \"calle\" : \"Secreta\" , \"numero\" : \"123\" , \"ciudad\" : \"Elche\" , \"barrio\" : \"Carr\u00fas\" , \"provincia\" : \"Alicante\" , \"cp\" : \"03206\" } } Refactoriza el documento/colecci\u00f3n aplicando los patrones que consideres necesarios, explicando cada uno de los patrones que hayas empleado, con el objetivo de recuperar para cada d\u00eda, la medidas acumuladas para un determinado barrio.","title":"Actividades"},{"location":"sa/05agregaciones.html","text":"Para poder agrupar datos y realizar c\u00e1lculos sobre \u00e9stos, MongoDB ofrece diferentes alternativas: Mediante operaciones Map-reduce con la operaci\u00f3n mapreduce() cuyo uso est\u00e1 deprecated desde MongoBD 5.0 . Mediante el uso conjunto de $function y $accumulator que permiten definir expresiones de agregaci\u00f3n mediante JavaScript . Mediante operaciones de agrupaci\u00f3n sencilla, como pueden ser las operaciones count() o distinct() . Mediante el uso del Aggregation Framework , basado en el uso de pipelines , el cual permite realizar diversas operaciones sobre los datos. Este framework es el mecanismo m\u00e1s eficiente y usable para la realizaci\u00f3n de agregaciones, y por tanto, en el que nos vamos a centrar en esta sesi\u00f3n. Para ello, a partir de una colecci\u00f3n, mediante el m\u00e9todo aggregate le pasaremos un array con las fases a realizar: db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , numProductos : { $sum : 1 }} }, { $sort : { numProductos :- 1 }} ]) Pipeline de agregaci\u00f3n \u00b6 Las agregaciones usan un pipeline , conocido como Aggregation Pipeline , de ah\u00ed el uso de un array con [ ] donde cada elemento es una fase del pipeline , de modo que la salida de una fase es la entrada de la siguiente: db . coleccion . aggregate ([ op1 , op2 , ... opN ]) Cuidado con el tama\u00f1o El resultado del pipeline es un documento y por lo tanto est\u00e1 sujeto a la restricci\u00f3n de BSON, que limita su tama\u00f1o a 16MB. En la siguiente imagen se resumen los pasos de una agrupaci\u00f3n donde primero se eligen los elementos que vamos a agrupar mediante $match , el resultado saliente se agrupan con $group , y sobre los agrupado mediante $sum se calcula el total: Ejemplo de pipeline con $match y $group Al realizar un pipeline dividimos las consultas en fases, donde cada fase utiliza un operador para realizar una transformaci\u00f3n. Aunque no hay l\u00edmite en el n\u00famero de fases en una consulta, es importante destacar que el orden importa , y que hay optimizaciones para ayudar a que el pipeline tenga un mejor rendimiento (por ejemplo, hacer un $match al principio para reducir la cantidad de datos) Operadores del pipeline \u00b6 Antes de nada destacar que las fases se pueden repetir, por lo que una consulta puede repetir operadores . A continuaci\u00f3n vamos a estudiar todos estos operadores: Operador Descripci\u00f3n Cardinalidad $project Proyecci\u00f3n de campos, es decir, propiedades en las que estamos interesados. Tambi\u00e9n nos permite modificar un documento, o crear un subdocumento (reshape) 1:1 $match Filtrado de campos, similar a where N:1 $group Para agrupar los datos, similar a group by N:1 $sort Ordenar 1:1 $skip Saltar N:1 $limit Limitar los resultados N:1 $unwind Separa los datos que hay dentro de un array 1:N Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de productos ( productos.js ) de un tienda de electr\u00f3nica con las caracter\u00edsticas y precios de los mismos. Un ejemplo de un producto ser\u00eda: > db . productos . findOne () { \"_id\" : ObjectId ( \"5345afc1176f38ea4eda4787\" ), \"nombre\" : \"iPad 16GB Wifi\" , \"fabricante\" : \"Apple\" , \"categoria\" : \"Tablets\" , \"precio\" : 499 } Para cargar este archivo desde la consola nos podemos conectar a nuestro cluster y realizar la carga: mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/iabd < productos.js O si ya nos hemos conectado previamente: load ( \"productos.js\" ) $group \u00b6 La fase group agrupa los documentos con el prop\u00f3sito de calcular valores agregados de una colecci\u00f3n de documentos. Por ejemplo, podemos usar $group para calcular la media de p\u00e1ginas visitas de manera diaria. Cuidado La salida de $group esta desordenada La salida de $group depende de c\u00f3mo se definan los grupos. Se empieza especificando un identificador (por ejemplo, un campo _id ) para el grupo que creamos con el pipeline . Para este campo _id , podemos especificar varias expresiones, incluyendo un \u00fanico campo proveniente de un documento del pipeline , un valor calculado de una fase anterior, un documento con muchos campos y otras expresiones v\u00e1lidas, tales como constantes o campos de subdocumentos. Tambi\u00e9n podemos usar operadores de $project para el campo _id . Cuando referenciemos al valor de un campo lo haremos poniendo entre comillas un $ delante del nombre del campo. As\u00ed pues, para referenciar al fabricante de un producto lo haremos mediante $fabricante . > db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , total : { $sum : 1 } } }]) < { _id : 'Apple' , total : 4 } { _id : 'Samsung' , total : 2 } { _id : 'Sony' , total : 1 } { _id : 'Google' , total : 1 } { _id : 'Amazon' , total : 2 } Si lo que queremos es que el valor del identificador contenga un objeto, lo podemos hacer asociandolo como valor: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" }, total : { $sum : 1 } } }]) < { _id : { empresa : 'Sony' }, total : 1 } { _id : { empresa : 'Apple' }, total : 4 } { _id : { empresa : 'Google' }, total : 1 } { _id : { empresa : 'Samsung' }, total : 2 } { _id : { empresa : 'Amazon' }, total : 2 } Tambi\u00e9n podemos agrupar m\u00e1s de un atributo, de tal modo que tengamos un _id compuesto. Por ejemplo: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"tipo\" : \"$categoria\" }, total : { $sum : 1 } } }]) < { _id : { empresa : 'Apple' , tipo : 'Tablets' }, total : 3 } { _id : { empresa : 'Sony' , tipo : 'Port\u00e1tiles' }, total : 1 } { _id : { empresa : 'Apple' , tipo : 'Port\u00e1tiles' }, total : 1 } { _id : { empresa : 'Samsung' , tipo : 'Smartphones' }, total : 1 } { _id : { empresa : 'Amazon' , tipo : 'Tablets' }, total : 2 } { _id : { empresa : 'Google' , tipo : 'Tablets' }, total : 1 } { _id : { empresa : 'Samsung' , tipo : 'Tablets' }, total : 1 } Siempre _id Cada expresi\u00f3n de $group debe especificar un campo _id . Acumuladores \u00b6 Adem\u00e1s del campo _id , la expresi\u00f3n $group puede incluir campos calculados. Estos otros campos deben utilizar uno de los siguientes acumuladores . Nombre Descripci\u00f3n $addToSet Devuelve un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones) $first Devuelve el primer valor del grupo. Se suele usar despu\u00e9s de ordenar. $last Devuelve el \u00faltimo valor del grupo. Se suele usar despu\u00e9s de ordenar. $max Devuelve el mayor valor de un grupo $min Devuelve el menor valor de un grupo. $avg Devuelve el promedio de todos los valores de un grupo $push Devuelve un array con todos los valores del campo seleccionado entre cada documento del grupo (puede haber repeticiones) $sum Devuelve la suma de todos los valores del grupo A continuaci\u00f3n vamos a ver ejemplos de cada uno de estos acumuladores. $sum \u00b6 El operador $sum acumula los valores y devuelve la suma. Por ejemplo, para obtener el montante total de los productos agrupados por fabricante, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, totalPrecio : { $sum : \"$precio\" } } }]) < { _id : { empresa : 'Apple' }, totalPrecio : 2296 } { _id : { empresa : 'Samsung' }, totalPrecio : 1014.98 } { _id : { empresa : 'Sony' }, totalPrecio : 499 } { _id : { empresa : 'Google' }, totalPrecio : 199 } { _id : { empresa : 'Amazon' }, totalPrecio : 328 } $avg \u00b6 Mediante $avg podemos obtener el promedio de los valores de un campo num\u00e9rico. Por ejemplo, para obtener el precio medio de los productos agrupados por categor\u00eda, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"categoria\" : \"$categoria\" }, precioMedio : { $avg : \"$precio\" } } }]) < { _id : { categoria : 'Smartphones' }, precioMedio : 563.99 } { _id : { categoria : 'Port\u00e1tiles' }, precioMedio : 499 } { _id : { categoria : 'Tablets' }, precioMedio : 396.4271428571428 } $addToSet \u00b6 Mediante $addToSet obtendremos un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones). Por ejemplo, para obtener para cada empresa las categor\u00edas en las que tienen productos, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"fabricante\" : \"$fabricante\" }, categorias : { $addToSet : \"$categoria\" } } }]) < { _id : { fabricante : 'Apple' }, categorias : [ 'Port\u00e1tiles' , 'Tablets' ] } { _id : { fabricante : 'Amazon' }, categorias : [ 'Tablets' ] } { _id : { fabricante : 'Sony' }, categorias : [ 'Port\u00e1tiles' ] } { _id : { fabricante : 'Google' }, categorias : [ 'Tablets' ] } { _id : { fabricante : 'Samsung' }, categorias : [ 'Tablets' , 'Smartphones' ] } $push \u00b6 Mediante $push tambi\u00e9n obtendremos un array con todos los valores para los campos seleccionados entre cada documento del grupo, pero con repeticiones. Es decir, funciona de manera similar a $addToSet pero permitiendo elementos repetidos. Por ello, si reescribimos la consulta anterior pero haciendo uso de $push obtendremos categor\u00edas repetidas: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, categorias : { $push : \"$categoria\" } } }]) < { _id : { empresa : 'Sony' }, categorias : [ 'Port\u00e1tiles' ] } { _id : { empresa : 'Apple' }, categorias : [ 'Tablets' , 'Tablets' , 'Tablets' , 'Port\u00e1tiles' ] } { _id : { empresa : 'Google' }, categorias : [ 'Tablets' ] } { _id : { empresa : 'Samsung' }, categorias : [ 'Smartphones' , 'Tablets' ] } { _id : { empresa : 'Amazon' }, categorias : [ 'Tablets' , 'Tablets' ] } $max y $min \u00b6 Los operadores $max y $min permiten obtener el mayor y el menor valor, respectivamente, del campo por el que se agrupan los documentos. Por ejemplo, para obtener el precio del producto m\u00e1s caro que tiene cada empresa har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, precioMaximo : { $max : \"$precio\" }, precioMinimo : { $min : \"$precio\" }, } }]) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"precioMaximo\" : 199 , \"precioMinimo\" : 129 } { \"_id\" : { \"empresa\" : \"Sony\" }, \"precioMaximo\" : 499 , \"precioMinimo\" : 499 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"precioMaximo\" : 563.99 , \"precioMinimo\" : 450.99 } { \"_id\" : { \"empresa\" : \"Google\" }, \"precioMaximo\" : 199 , \"precioMinimo\" : 199 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"precioMaximo\" : 699 , \"precioMinimo\" : 499 } Doble $group \u00b6 Si queremos obtener el resultado de una agrupaci\u00f3n podemos aplicar el operador $group sobre otro $group . Por ejemplo, para obtener el precio medio de los precios medios de los tipos de producto por empresa har\u00edamos: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"categoria\" : \"$categoria\" }, precioMedio : { $avg : \"$precio\" } // (1)! } }, { $group : { _id : \"$_id.empresa\" , precioMedio : { $avg : \"$precioMedio\" } // (2)! } } ]) < { _id : 'Google' , precioMedio : 199 } { _id : 'Sony' , precioMedio : 499 } { _id : 'Samsung' , precioMedio : 507.49 } { _id : 'Apple' , precioMedio : 549 } { _id : 'Amazon' , precioMedio : 164 } Precio medio por empresa y categor\u00eda Precio medio por empresa en base al precio medio anterior $first y $last \u00b6 Estos operadores devuelven el valor resultante de aplicar la expresi\u00f3n al primer ( $first ) y/o \u00faltimo ( $last ) elemento de un grupo de documentos que comparten el mismo grupo por clave. Por ejemplo, para obtener para cada empresa, cual es el tipo de producto que m\u00e1s tiene y la cantidad de dicho tipo har\u00edamos: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"tipo\" : \"$categoria\" }, total : { $sum : 1 } } }, { $sort : { \"total\" :- 1 }}, { $group : { _id : \"$_id.empresa\" , producto : { $first : \"$_id.tipo\" }, // (1)! cantidad : { $first : \"$total\" } } } ]) < { _id : 'Sony' , producto : 'Port\u00e1tiles' , cantidad : 1 } { _id : 'Samsung' , producto : 'Tablets' , cantidad : 1 } { _id : 'Amazon' , producto : 'Tablets' , cantidad : 2 } { _id : 'Apple' , producto : 'Tablets' , cantidad : 3 } { _id : 'Google' , producto : 'Tablets' , cantidad : 1 } Al agrupar por empresa, elegimos la categor\u00eda de producto que tiene m\u00e1s unidades $project \u00b6 Si queremos realizar una proyecci\u00f3n sobre el conjunto de resultados y quedarnos con un subconjunto de los campos usaremos el operador $project . Como resultado obtendremos el mismo n\u00famero de documentos, y en el orden indicado en la proyecci\u00f3n. La proyecci\u00f3n dentro del framework de agregaci\u00f3n es mucho m\u00e1s potente que dentro de las consultas normales. Se emplea para: renombrar campos. introducir campos calculados en el documento resultante mediante $add , $substract , $multiply , $divide o $mod transformar campos a may\u00fasculas $toUpper o min\u00fasculas $toLower , concatenar campos mediante $concat u obtener subcadenas con $substr . transformar campos en base a valores obtenidos a partir de una condici\u00f3n mediante expresiones l\u00f3gicas con los operadores de comparaci\u00f3n vistos en las consultas. > db . productos . aggregate ([ { $project : { _id : 0 , // (1)! \"empresa\" : { \"$toUpper\" : \"$fabricante\" }, // (2)! \"detalles\" : { // (3)! \"categoria\" : \"$categoria\" , \"precio\" : { \"$multiply\" : [ \"$precio\" , 1.1 ] } // (4)! }, \"elemento\" : \"$nombre\" // (5)! } } ]) < { empresa : 'APPLE' , detalles : { categoria : 'Tablets' , precio : 548.9000000000001 }, elemento : 'iPad 16GB Wifi' } { empresa : 'APPLE' , detalles : { categoria : 'Tablets' , precio : 658.9000000000001 }, elemento : 'iPad 32GB Wifi' } ... Ocultamos el campo _id Transforma un campo y lo pasa a may\u00fasculas Crea un documento anidado Incrementa el precio el 10% Renombra el campo $match \u00b6 El operador $match se utiliza principalmente para filtrar los documentos que pasar\u00e1n a la siguiente etapa del pipeline o a la salida final. Por ejemplo, para seleccionar s\u00f3lo las tablets har\u00edamos: db . productos . aggregate ([{ $match : { categoria : \"Tablets\" }}]) Aparte de igualar un valor a un campo, podemos emplear los operadores usuales de consulta, como $gt , $lt , $in , etc\u2026\u200b Se recomienda poner el operador $match al principio del pipeline para limitar los documentos a procesar en siguientes fases. Si usamos este operador como primera fase podremos hacer uso de los indices de la colecci\u00f3n de una manera eficiente. As\u00ed pues, para obtener la cantidad de Tablets de menos de 500 euros har\u00edamos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" , precio : { $lt : 500 }}}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, cantidad : { $sum : 1 }} } ]) < { _id : { empresa : 'Samsung' }, cantidad : 1 } { _id : { empresa : 'Amazon' }, cantidad : 2 } { _id : { empresa : 'Google' }, cantidad : 1 } { _id : { empresa : 'Apple' }, cantidad : 1 } $sort \u00b6 El operador $sort ordena los documentos recibidos por el campo, y el orden indicado por la expresi\u00f3n indicada al pipeline . Por ejemplo, para ordenar los productos por precio descendentemente har\u00edamos: db . productos . aggregate ({ $sort : { precio :- 1 }}) El operador $sort ordena los datos en memoria, por lo que hay que tener cuidado con el tama\u00f1o de los datos. Por ello, se emplea en las \u00faltimas fases del pipeline , cuando el conjunto de resultados es el menor posible. Si retomamos el ejemplo anterior, y ordenamos los datos por el precio total tenemos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" }}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, totalPrecio : { $sum : \"$precio\" }} }, { $sort : { totalPrecio :- 1 }} // (1)! ]) < { _id : { empresa : 'Apple' }, totalPrecio : 1797 } { _id : { empresa : 'Samsung' }, totalPrecio : 450.99 } { _id : { empresa : 'Amazon' }, totalPrecio : 328 } { _id : { empresa : 'Google' }, totalPrecio : 199 } Al ordenar los datos, referenciamos al campo que hemos creado en la fase de $group Un operador muy relacionado es $sortByCount . Este operador es similar a realizar las siguientes operaciones: { $group : { _id : <expresio n > , ca nt idad : { $sum : 1 } } }, { $sor t : { ca nt idad : -1 } } As\u00ed pues, podemos reescribir la consulta que hemos hecho en el operador $group : db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , total : { $sum : 1 } } }, { $sort : { \"total\" : - 1 }} ]) Y hacerla con: Consulta Resultado db . productos . aggregate ([{ $sortByCount : \"$fabricante\" }]) { _id : 'Apple' , cou nt : 4 } { _id : 'Samsu n g' , cou nt : 2 } { _id : 'Amazo n ' , cou nt : 2 } { _id : 'So n y' , cou nt : 1 } { _id : 'Google' , cou nt : 1 } $skip y $limit \u00b6 El operador $limit \u00fanicamente limita el n\u00famero de documentos que pasan a trav\u00e9s del pipeline. El operador recibe un n\u00famero como par\u00e1metro: db . productos . aggregate ([{ $limit : 3 }]) Este operador no modifica los documentos, s\u00f3lo restringe quien pasa a la siguiente fase. De manera similar, con el operador $skip , saltamos un n\u00famero determinado de documentos: db . productos . aggregate ([{ $skip : 3 }]) El orden en el que empleemos estos operadores importa, y mucho, ya que no es lo mismo saltar y luego limitar, donde la cantidad de elementos la fija $limit : > db . productos . aggregate ([{ $skip : 2 }, { $limit : 3 }]) < { _id : ObjectId ( \"635194b32e6059646a8e7fee\" ), nombre : 'iPad 64GB Wifi' , categoria : 'Tablets' , fabricante : 'Apple' , precio : 699 } { _id : ObjectId ( \"635194b32e6059646a8e7fef\" ), nombre : 'Galaxy S3' , categoria : 'Smartphones' , fabricante : 'Samsung' , precio : 563.99 } { _id : ObjectId ( \"635194b32e6059646a8e7ff0\" ), nombre : 'Galaxy Tab 10' , categoria : 'Tablets' , fabricante : 'Samsung' , precio : 450.99 } En cambio, si primero limitamos y luego saltamos, la cantidad de elementos se obtiene de la diferencia entre el l\u00edmite y el salto: > db . productos . aggregate ([{ $limit : 3 }, { $skip : 2 }]) > { _id : ObjectId ( \"635194b32e6059646a8e7fee\" ), nombre : 'iPad 64GB Wifi' , categoria : 'Tablets' , fabricante : 'Apple' , precio : 699 } $sample Si tenemos un dataset muy grande, y queremos probar las consultas con un n\u00famero reducido de documentos, podemos emplear el operador $sample y reducir la cantidad de documentos de manera aleatoria: db . productos . aggregate ([ { $sample : { size : 3 } } ]) $unwind \u00b6 El operador $unwind es muy interesante y se utiliza s\u00f3lo con operadores array. Al usarlo con un campo array de tama\u00f1o N en un documento, lo transforma en N documentos con el campo tomando el valor individual de cada uno de los elementos del array. Si retomamos el ejemplo de la sesi\u00f3n anterior donde actualiz\u00e1bamos una colecci\u00f3n de enlaces, ten\u00edamos un enlace con la siguiente informaci\u00f3n: > db . enlaces . findOne () < { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : [ 'mapas' , 'videos' , 'blog' , 'calendario' , 'email' , 'mapas' ] } Podemos observar como el campo tags contiene 6 valores dentro del array (con un valor repetido). A continuaci\u00f3n vamos a desenrollar el array: > db . enlaces . aggregate ([ { $match : { titulo : \"www.google.es\" }}, { $unwind : \"$tags\" } ]) < { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'mapas' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'videos' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'blog' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'calendario' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'email' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'mapas' } As\u00ed pues hemos obtenido 6 documentos con el mismo _id y titulo , es decir, un documento por elemento del array. De este modo, podemos realizar consultas que sumen/cuenten los elementos del array. Por ejemplo, si queremos obtener las 3 etiquetas que m\u00e1s aparecen en todos los enlaces har\u00edamos: > db . enlaces . aggregate ([ { \"$unwind\" : \"$tags\" }, { \"$group\" : { \"_id\" : \"$tags\" , \"total\" : { $sum : 1 } } }, { \"$sort\" : { \"total\" :- 1 }}, { \"$limit\" : 3 } ]) < { _id : 'mapas' , total : 2 } { _id : 'blog' , total : 1 } { _id : 'calendario' , total : 1 } Doble $unwind \u00b6 Si trabajamos con documentos que tienen varios arrays, podemos necesitar desenrollar los dos arrays. Al hacer un doble unwind se crea un producto cartesiano entre los elementos de los 2 arrays. Supongamos que tenemos los datos del siguiente inventario de ropa: > db . inventario . drop (); > db . inventario . insertOne ({ 'nombre' : \"Camiseta\" , 'tallas' : [ \"S\" , \"M\" , \"L\" ], 'colores' : [ 'azul' , 'blanco' , 'naranja' , 'rojo' ]}) > db . inventario . insertOne ({ 'nombre' : \"Jersey\" , 'tallas' : [ \"S\" , \"M\" , \"L\" , \"XL\" ], 'colores' : [ 'azul' , 'negro' , 'naranja' , 'rojo' ]}) > db . inventario . insertOne ({ 'nombre' : \"Pantalones\" , 'tallas' : [ \"32x32\" , \"32x30\" , \"36x32\" ], 'colores' : [ 'azul' , 'blanco' , 'naranja' , 'negro' ]}) Para obtener un listado de cantidad de pares talla/color har\u00edamos: > db . inventario . aggregate ([ { $unwind : \"$tallas\" }, { $unwind : \"$colores\" }, { $group : { '_id' : { 'talla' : '$tallas' , 'color' : '$colores' }, 'total' : { '$sum' : 1 } } } ]) { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"rojo\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"L\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"M\" , \"color\" : \"negro\" }, \"total\" : 1 } ... $lookup \u00b6 Si necesitamos unir los datos de dos colecciones, emplearemos el operador $lookup , el cual realiza un left outer join a una colecci\u00f3n de la misma base de datos para filtrar los documentos de la colecci\u00f3n joineada . El resultado es un nuevo campo array para cada documento de entrada, el cual contiene los documentos que cumplen el criterio del join . El operador $lookup utiliza cuatro par\u00e1metros: from : colecci\u00f3n con la que se realiza el join . localField : campo de la colecci\u00f3n origen (ser\u00eda la clave ajena). foreignField : campo en la colecci\u00f3n destino que permite la uni\u00f3n (ser\u00eda la clave primaria de la otra colecci\u00f3n). as : nombre del array que contendr\u00e1 los documentos enlazados. Preparando los datos Vamos a utilizar la colecci\u00f3n zips empleada en anteriores sesiones la cual tiene una estructura similar a: { _id : Objec t Id( \"5c8eccc1caa187d17ca6ed18\" ) , ci t y : 'ACMAR' , zip : ' 35004 ' , loc : { y : 33.584132 , x : 86.51557 }, pop : 6055 , s tate : 'AL' } A continuaci\u00f3n vamos a crear una nueva colecci\u00f3n llamada state con el nombre de los estados ( states.js ), la cual cargaremos en la base de datos sample_training : db . states . insertMany ([ { \"name\" : \"Alabama\" , \"abbreviation\" : \"AL\" }, { \"name\" : \"Alaska\" , \"abbreviation\" : \"AK\" }, ... ]) Vamos a estudiar como funciona el operador $lookup mediante un ejemplo. Primero vamos a recuperar los tres estados m\u00e1s poblados. Para ello, podr\u00edamos hacer la siguiente consulta agregada: Consulta Resultado db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 } ]) { _id : 'CA' , t o tal Poblacio n : 29760021 } { _id : 'NY' , t o tal Poblacio n : 17990455 } { _id : 'TX' , t o tal Poblacio n : 16986510 } Si ahora queremos recuperar el nombre de esos tres estados, a\u00f1adimos una nueva fase: db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 }, { $lookup : { from : \"states\" , localField : \"_id\" , foreignField : \"abbreviation\" , as : \"estados\" }}, ]) Y ahora obtenemos para cada documento, un array con los documentos que coinciden (en este caso es una relaci\u00f3n 1:1, y por eso cada array s\u00f3lo contiene un elemento): { _id : 'CA' , t o tal Poblacio n : 29760021 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0cd5\" ) , na me : 'Cali f or n ia' , abbrevia t io n : 'CA' } ] } { _id : 'NY' , t o tal Poblacio n : 17990455 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0cf4\" ) , na me : 'New York' , abbrevia t io n : 'NY' } ] } { _id : 'TX' , t o tal Poblacio n : 16986510 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0d02\" ) , na me : 'Texas' , abbrevia t io n : 'TX' } ] } Como la relaci\u00f3n siempre va a provocar la creaci\u00f3n de un array, mediante $unwind , lo podemos deshacer: Consulta Resultado db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 }, { $lookup : { from : \"states\" , localField : \"_id\" , foreignField : \"abbreviation\" , as : \"estados\" }}, { $unwind : \"$estados\" } ]) { _id : 'CA' , t o tal Poblacio n : 29760021 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0cd5\" ) , na me : 'Cali f or n ia' , abbrevia t io n : 'CA' } } { _id : 'NY' , t o tal Poblacio n : 17990455 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0cf4\" ) , na me : 'New York' , abbrevia t io n : 'NY' } } { _id : 'TX' , t o tal Poblacio n : 16986510 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0d02\" ) , na me : 'Texas' , abbrevia t io n : 'TX' } } As\u00ed pues, para finalmente obtener el nombre de cada estado, mediante $project recuperamos el campo name : db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sor t :{ \"totalPoblacion\" : -1 }} , { $limi t : 3 }, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }} ] ) Obteniendo el resultado deseado: { _id : 'CA' , es ta do : 'Cali f or n ia' , poblacio n : 29760021 } { _id : 'NY' , es ta do : 'New York' , poblacio n : 17990455 } { _id : 'TX' , es ta do : 'Texas' , poblacio n : 16986510 } Persistiendo los resultados \u00b6 Una vez hemos realizado nuestras consultas mediante el framework de agregaci\u00f3n, es muy posible que queramos almacenar el resultado en una nueva colecci\u00f3n para poder volver a consultar el resultado sin necesidad de ejecutar todas las fases. Para ello, podemos emplear los operadores: $out recoge los documentos de una agregaci\u00f3n y los persiste en una colecci\u00f3n, sobrescribiendo los datos existentes. $merge similar a $out , pero permite a\u00f1adir el resultado a la misma colecci\u00f3n y adem\u00e1s soporta trabajar con colecciones particionadas. Por ejemplo, vamos a basarnos en las consultas con join , para crear una nueva colecci\u00f3n con la poblaci\u00f3n total de todos dos estados, y la vamos a almacenar en una nueva colecci\u00f3n denominada states_population : db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }}, { $ou t : \"states_population\" } ] ) Tras su ejecuci\u00f3n, podemos recuperar los datos: > db . states_population . findOne () < { _id : 'WY' , estado : 'Wyoming' , poblacion : 453588 } De SQL al Pipeline de agregaciones \u00b6 Ya hemos visto que el pipeline ofrece operadores para realizar la misma funcionalidad de agrupaci\u00f3n que ofrece SQL. Si relacionamos los comandos SQL con el pipeline de agregaciones tenemos las siguientes equivalencias: SQL Pipeline de Agregaciones WHERE $match GROUP BY $group HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum / $sortByCount join $lookup Podemos encontrar ejemplos de consultas SQL transformadas al pipeline en https://www.mongodb.com/docs/manual/reference/sql-aggregation-comparison/ Limitaciones \u00b6 Hay que tener en cuenta las siguiente limitaciones: En versiones anteriores a la 2.6, el pipeline devolv\u00eda en cada fase un objeto BSON, y por tanto, el resultado estaba limitado a 16MB Actualmente, s\u00f3lo cada documento que forme parte del resultado final debe ocupar menos de 16MB. Las fases tienen un l\u00edmite de 100MB en memoria. Si una fase excede dicho l\u00edmite, se producir\u00e1 un error. En este caso, hay que habilitar el uso de disco mediante allowDiskUse en las opciones de la agregaci\u00f3n. M\u00e1s informaci\u00f3n en https://www.mongodb.com/docs/manual/core/aggregation-pipeline-limits/ Agregaciones con Compass \u00b6 MongoDB Compass nos ofrece la herramienta Aggregation Pipeline Builder para crear, borrar y reorganizar f\u00e1cilmente fases en un pipeline, as\u00ed como evaluar los documento resultantes en tiempo real. Vamos a practicar con la colecci\u00f3n de zips : Agregaciones en MongoDB Compass Y vamos a reproducir la consulta con $lookup que acabamos de realizar, pero en nuestro caso, iremos paso a paso: db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }} ] ) Para ello, pulsamos sobre el bot\u00f3n de Add Stage , seleccionamos el operador $group y escribimos la expresi\u00f3n de agrupaci\u00f3n. Al hacerlo, en el panel anexo aparecer\u00e1n 10 documentos de muestra con el resultado de ejecutar dicha fase: $group en MongoDB Compass Paso a paso, iremos a\u00f1adiendo el resto de fases hasta tenerlas todas, pudiendo ver los datos que va generando cada fase: Resultado en MongoDB Compass Una vez tenemos nuestra agregaci\u00f3n, podemos obtener una versi\u00f3n del pipeline en Python, Java, C# o Node.js, mediante la opci\u00f3n Export-to-Language . Exportando la agregaci\u00f3n a PyMongo Referencias \u00b6 Documentaci\u00f3n oficial del Aggregation Framework . Curso M121: The MongoDB Aggregation Framework de la Mongo University. Libro Practical MongoDB Aggregations Actividades \u00b6 Para las siguientes actividades, vamos a utilizar la base de datos sample_mflix , y en concreto, las colecciones movies . Un documento de ejemplo ser\u00eda similar a: movies.json { _id : Objec t Id( \"573a1390f29313caabcd548c\" ) , plo t : 'The Civil War divides ...' , ge nres : [ 'Drama' , 'His t ory' , 'Roma n ce' ], ru nt ime : 165 , ra te d : 'NOT RATED' , cas t : [ 'Lillia n Gish' , 'Mae Marsh' , 'He nr y B. Wal t hall' , 'Miriam Cooper' ], pos ter : 'h tt ps : //m.media-amazon.com/images/M/MV5BYTM4ZDhiYTQtYzExNC00YjVlLTg2YWYtYTk3NTAzMzcwNTExXkEyXkFqcGdeQXVyNjU0OTQ0OTY@._V1_SY1000_SX677_AL_.jpg', t i tle : 'The Bir t h o f a Na t io n ' , full plo t : 'Two bro t hers...' , cou ntr ies : [ 'USA' ], released : 1915-03-03 T 00 : 00 : 00.000 Z , direc t ors : [ 'D.W. Gri ff i t h' ], wri ters : [ 'Thomas Dixo n Jr. (adap te d fr om ...\")', 'Thomas Dixon Jr. (play)', 'Thomas Dixon Jr. (novel)', 'D.W. Griffith', 'Frank E. Woods' ], awards: { wins: 2, nominations: 0, text: '2 wins.' }, lastupdated: '2015-09-11 00:32:27.763000000', year: 1915, imdb: { rating: 6.8, votes: 15715, id: 4972 }, type: 'movie', tomatoes: { viewer: { rating: 3.2, numReviews: 4358, meter: 57 }, dvd: 2004-06-29T00:00:00.000Z, critic: { rating: 8, numReviews: 38, meter: 100 }, lastUpdated: 2015-09-10T18:30:23.000Z, consensus: 'Racial depictions aside...', rotten: 0, production: 'Gravitas', fresh: 38 }, num_mflix_comments: 0 } ( RA5075.1 / CE5.1d / 3p) Haciendo uso del framework de agregaci\u00f3n y el shell de MongoDB, resuelve las siguientes consultas: (0.5) Encuentra todas las pel\u00edculas que entre sus g\u00e9neros ( genres ) se encuentre el Drama . S\u00f3lo queremos recuperar el t\u00edtulo y la calificaci\u00f3n ( rating ) de IMDB. (0.5) Recupera los t\u00edtulos de las tres pel\u00edculas rom\u00e1nticas ( Romance ) con mayor calificaci\u00f3n en IMDB que se lanzaron ( released ) antes del 2001. (0.5) Averigua la cantidad de pel\u00edculas que hay de cada categor\u00eda de calificaci\u00f3n ( rated ). (0.5) Teniendo en cuenta las pel\u00edculas anteriores al a\u00f1o 2001, para cada g\u00e9nero, recupera la media y la m\u00e1xima calificaci\u00f3n en IMDB as\u00ed como el tiempo ajustado (con trailers , los cuales duran 12 minutos) de la pel\u00edcula m\u00e1s larga, ordenando los g\u00e9neros por popularidad. El resultado ser\u00e1 similar a: { _id : 'Film - Noir' , n o ta _media : 7.62 , mejor_ n o ta : 8.3 , t iempo_ajus ta do : 123 } { _id : 'Docume ntar y' , n o ta _media : 7.555313351498638 , mejor_ n o ta : 9.4 , t iempo_ajus ta do : 1152 } { _id : 'Shor t ' , n o ta _media : 7.386 , mejor_ n o ta : 8.6 , t iempo_ajus ta do : 56 } ... (0.5) Sobre la consulta anterior, adem\u00e1s de la informaci\u00f3n ya recuperada, queremos buscar qu\u00e9 pel\u00edcula recomendar de cada categor\u00eda siempre y cuando duren un m\u00e1ximo de 218 minutos y tengan al menos una calificaci\u00f3n de 7. (pista: necesitas utilizar $first ) El resultado ser\u00e1 similar a: { _id : 'Docume ntar y' , t i tul o_recome n dado : 'Cosmos' , n o ta _recome n dado : 9.3 , t iempo_recome n dado : 60 , popularidad : 7.69695945945946 , mejor_ n o ta : 9.3 , t iempo_ajus ta do_maslargo : 212 } { _id : 'Sci - Fi' , t i tul o_recome n dado : 'Blade Ru nner ' , n o ta _recome n dado : 8.2 , t iempo_recome n dado : 117 , popularidad : 7.3999999999999995 , mejor_ n o ta : 8.2 , t iempo_ajus ta do_maslargo : 209 } ... (0.5) Recupera las 5 pel\u00edculas m\u00e1s comentadas (los comentarios se almacenan en la colecci\u00f3n comments ), devolviendo el t\u00edtulo, su g\u00e9nero y la cantidad de comentarios. Adem\u00e1s, queremos almacenar el resultado en la colecci\u00f3n movies_most_commented . > db . most_commented_movies . findOne () < { _id : ObjectId ( \"573a13bff29313caabd5e91e\" ), sumComments : 161 , movie : { imdb : { rating : 6.4 }, title : 'The Taking of Pelham 1 2 3' } } ( RA5075.1 / CE5.1d / 1p) Haciendo uso de MongoDBCompass , mejora la siguiente consulta que obtiene los tres documentales m\u00e1s premiados, siempre y cuando hayan ganado alg\u00fan premio: var pipeline = [ { $sort : { \"awards.wins\" : - 1 }}, // Ordenamos por premios ganados { $match : { \"awards.wins\" : { $gte : 1 }}}, { $limit : 20 }, // Obtenemos las 20 pel\u00edculas que han ganado m\u00e1s de un premio { $match : { genres : { $in : [ \"Documentary\" ]}, // Nos quedamos con los documentales }}, { $project : { title : 1 , genres : 1 , awards : 1 }}, { $limit : 3 }, ]; db . movies . aggregate ( pipeline ) Adjunta una captura inicial y otra final, y la exportaci\u00f3n de la agregaci\u00f3n a Python.","title":"S25.- Agregaciones"},{"location":"sa/05agregaciones.html#pipeline-de-agregacion","text":"Las agregaciones usan un pipeline , conocido como Aggregation Pipeline , de ah\u00ed el uso de un array con [ ] donde cada elemento es una fase del pipeline , de modo que la salida de una fase es la entrada de la siguiente: db . coleccion . aggregate ([ op1 , op2 , ... opN ]) Cuidado con el tama\u00f1o El resultado del pipeline es un documento y por lo tanto est\u00e1 sujeto a la restricci\u00f3n de BSON, que limita su tama\u00f1o a 16MB. En la siguiente imagen se resumen los pasos de una agrupaci\u00f3n donde primero se eligen los elementos que vamos a agrupar mediante $match , el resultado saliente se agrupan con $group , y sobre los agrupado mediante $sum se calcula el total: Ejemplo de pipeline con $match y $group Al realizar un pipeline dividimos las consultas en fases, donde cada fase utiliza un operador para realizar una transformaci\u00f3n. Aunque no hay l\u00edmite en el n\u00famero de fases en una consulta, es importante destacar que el orden importa , y que hay optimizaciones para ayudar a que el pipeline tenga un mejor rendimiento (por ejemplo, hacer un $match al principio para reducir la cantidad de datos)","title":"Pipeline de agregaci\u00f3n"},{"location":"sa/05agregaciones.html#operadores-del-pipeline","text":"Antes de nada destacar que las fases se pueden repetir, por lo que una consulta puede repetir operadores . A continuaci\u00f3n vamos a estudiar todos estos operadores: Operador Descripci\u00f3n Cardinalidad $project Proyecci\u00f3n de campos, es decir, propiedades en las que estamos interesados. Tambi\u00e9n nos permite modificar un documento, o crear un subdocumento (reshape) 1:1 $match Filtrado de campos, similar a where N:1 $group Para agrupar los datos, similar a group by N:1 $sort Ordenar 1:1 $skip Saltar N:1 $limit Limitar los resultados N:1 $unwind Separa los datos que hay dentro de un array 1:N Preparando los ejemplos Para los siguientes ejemplos, vamos a utilizar una colecci\u00f3n de productos ( productos.js ) de un tienda de electr\u00f3nica con las caracter\u00edsticas y precios de los mismos. Un ejemplo de un producto ser\u00eda: > db . productos . findOne () { \"_id\" : ObjectId ( \"5345afc1176f38ea4eda4787\" ), \"nombre\" : \"iPad 16GB Wifi\" , \"fabricante\" : \"Apple\" , \"categoria\" : \"Tablets\" , \"precio\" : 499 } Para cargar este archivo desde la consola nos podemos conectar a nuestro cluster y realizar la carga: mongosh mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/iabd < productos.js O si ya nos hemos conectado previamente: load ( \"productos.js\" )","title":"Operadores del pipeline"},{"location":"sa/05agregaciones.html#group","text":"La fase group agrupa los documentos con el prop\u00f3sito de calcular valores agregados de una colecci\u00f3n de documentos. Por ejemplo, podemos usar $group para calcular la media de p\u00e1ginas visitas de manera diaria. Cuidado La salida de $group esta desordenada La salida de $group depende de c\u00f3mo se definan los grupos. Se empieza especificando un identificador (por ejemplo, un campo _id ) para el grupo que creamos con el pipeline . Para este campo _id , podemos especificar varias expresiones, incluyendo un \u00fanico campo proveniente de un documento del pipeline , un valor calculado de una fase anterior, un documento con muchos campos y otras expresiones v\u00e1lidas, tales como constantes o campos de subdocumentos. Tambi\u00e9n podemos usar operadores de $project para el campo _id . Cuando referenciemos al valor de un campo lo haremos poniendo entre comillas un $ delante del nombre del campo. As\u00ed pues, para referenciar al fabricante de un producto lo haremos mediante $fabricante . > db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , total : { $sum : 1 } } }]) < { _id : 'Apple' , total : 4 } { _id : 'Samsung' , total : 2 } { _id : 'Sony' , total : 1 } { _id : 'Google' , total : 1 } { _id : 'Amazon' , total : 2 } Si lo que queremos es que el valor del identificador contenga un objeto, lo podemos hacer asociandolo como valor: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" }, total : { $sum : 1 } } }]) < { _id : { empresa : 'Sony' }, total : 1 } { _id : { empresa : 'Apple' }, total : 4 } { _id : { empresa : 'Google' }, total : 1 } { _id : { empresa : 'Samsung' }, total : 2 } { _id : { empresa : 'Amazon' }, total : 2 } Tambi\u00e9n podemos agrupar m\u00e1s de un atributo, de tal modo que tengamos un _id compuesto. Por ejemplo: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"tipo\" : \"$categoria\" }, total : { $sum : 1 } } }]) < { _id : { empresa : 'Apple' , tipo : 'Tablets' }, total : 3 } { _id : { empresa : 'Sony' , tipo : 'Port\u00e1tiles' }, total : 1 } { _id : { empresa : 'Apple' , tipo : 'Port\u00e1tiles' }, total : 1 } { _id : { empresa : 'Samsung' , tipo : 'Smartphones' }, total : 1 } { _id : { empresa : 'Amazon' , tipo : 'Tablets' }, total : 2 } { _id : { empresa : 'Google' , tipo : 'Tablets' }, total : 1 } { _id : { empresa : 'Samsung' , tipo : 'Tablets' }, total : 1 } Siempre _id Cada expresi\u00f3n de $group debe especificar un campo _id .","title":"$group"},{"location":"sa/05agregaciones.html#acumuladores","text":"Adem\u00e1s del campo _id , la expresi\u00f3n $group puede incluir campos calculados. Estos otros campos deben utilizar uno de los siguientes acumuladores . Nombre Descripci\u00f3n $addToSet Devuelve un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones) $first Devuelve el primer valor del grupo. Se suele usar despu\u00e9s de ordenar. $last Devuelve el \u00faltimo valor del grupo. Se suele usar despu\u00e9s de ordenar. $max Devuelve el mayor valor de un grupo $min Devuelve el menor valor de un grupo. $avg Devuelve el promedio de todos los valores de un grupo $push Devuelve un array con todos los valores del campo seleccionado entre cada documento del grupo (puede haber repeticiones) $sum Devuelve la suma de todos los valores del grupo A continuaci\u00f3n vamos a ver ejemplos de cada uno de estos acumuladores.","title":"Acumuladores"},{"location":"sa/05agregaciones.html#sum","text":"El operador $sum acumula los valores y devuelve la suma. Por ejemplo, para obtener el montante total de los productos agrupados por fabricante, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, totalPrecio : { $sum : \"$precio\" } } }]) < { _id : { empresa : 'Apple' }, totalPrecio : 2296 } { _id : { empresa : 'Samsung' }, totalPrecio : 1014.98 } { _id : { empresa : 'Sony' }, totalPrecio : 499 } { _id : { empresa : 'Google' }, totalPrecio : 199 } { _id : { empresa : 'Amazon' }, totalPrecio : 328 }","title":"$sum"},{"location":"sa/05agregaciones.html#avg","text":"Mediante $avg podemos obtener el promedio de los valores de un campo num\u00e9rico. Por ejemplo, para obtener el precio medio de los productos agrupados por categor\u00eda, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"categoria\" : \"$categoria\" }, precioMedio : { $avg : \"$precio\" } } }]) < { _id : { categoria : 'Smartphones' }, precioMedio : 563.99 } { _id : { categoria : 'Port\u00e1tiles' }, precioMedio : 499 } { _id : { categoria : 'Tablets' }, precioMedio : 396.4271428571428 }","title":"$avg"},{"location":"sa/05agregaciones.html#addtoset","text":"Mediante $addToSet obtendremos un array con todos los valores \u00fanicos para los campos seleccionados entre cada documento del grupo (sin repeticiones). Por ejemplo, para obtener para cada empresa las categor\u00edas en las que tienen productos, har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"fabricante\" : \"$fabricante\" }, categorias : { $addToSet : \"$categoria\" } } }]) < { _id : { fabricante : 'Apple' }, categorias : [ 'Port\u00e1tiles' , 'Tablets' ] } { _id : { fabricante : 'Amazon' }, categorias : [ 'Tablets' ] } { _id : { fabricante : 'Sony' }, categorias : [ 'Port\u00e1tiles' ] } { _id : { fabricante : 'Google' }, categorias : [ 'Tablets' ] } { _id : { fabricante : 'Samsung' }, categorias : [ 'Tablets' , 'Smartphones' ] }","title":"$addToSet"},{"location":"sa/05agregaciones.html#push","text":"Mediante $push tambi\u00e9n obtendremos un array con todos los valores para los campos seleccionados entre cada documento del grupo, pero con repeticiones. Es decir, funciona de manera similar a $addToSet pero permitiendo elementos repetidos. Por ello, si reescribimos la consulta anterior pero haciendo uso de $push obtendremos categor\u00edas repetidas: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, categorias : { $push : \"$categoria\" } } }]) < { _id : { empresa : 'Sony' }, categorias : [ 'Port\u00e1tiles' ] } { _id : { empresa : 'Apple' }, categorias : [ 'Tablets' , 'Tablets' , 'Tablets' , 'Port\u00e1tiles' ] } { _id : { empresa : 'Google' }, categorias : [ 'Tablets' ] } { _id : { empresa : 'Samsung' }, categorias : [ 'Smartphones' , 'Tablets' ] } { _id : { empresa : 'Amazon' }, categorias : [ 'Tablets' , 'Tablets' ] }","title":"$push"},{"location":"sa/05agregaciones.html#max-y-min","text":"Los operadores $max y $min permiten obtener el mayor y el menor valor, respectivamente, del campo por el que se agrupan los documentos. Por ejemplo, para obtener el precio del producto m\u00e1s caro que tiene cada empresa har\u00edamos: > db . productos . aggregate ([{ $group : { _id : { \"empresa\" : \"$fabricante\" }, precioMaximo : { $max : \"$precio\" }, precioMinimo : { $min : \"$precio\" }, } }]) { \"_id\" : { \"empresa\" : \"Amazon\" }, \"precioMaximo\" : 199 , \"precioMinimo\" : 129 } { \"_id\" : { \"empresa\" : \"Sony\" }, \"precioMaximo\" : 499 , \"precioMinimo\" : 499 } { \"_id\" : { \"empresa\" : \"Samsung\" }, \"precioMaximo\" : 563.99 , \"precioMinimo\" : 450.99 } { \"_id\" : { \"empresa\" : \"Google\" }, \"precioMaximo\" : 199 , \"precioMinimo\" : 199 } { \"_id\" : { \"empresa\" : \"Apple\" }, \"precioMaximo\" : 699 , \"precioMinimo\" : 499 }","title":"$max y $min"},{"location":"sa/05agregaciones.html#doble-group","text":"Si queremos obtener el resultado de una agrupaci\u00f3n podemos aplicar el operador $group sobre otro $group . Por ejemplo, para obtener el precio medio de los precios medios de los tipos de producto por empresa har\u00edamos: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"categoria\" : \"$categoria\" }, precioMedio : { $avg : \"$precio\" } // (1)! } }, { $group : { _id : \"$_id.empresa\" , precioMedio : { $avg : \"$precioMedio\" } // (2)! } } ]) < { _id : 'Google' , precioMedio : 199 } { _id : 'Sony' , precioMedio : 499 } { _id : 'Samsung' , precioMedio : 507.49 } { _id : 'Apple' , precioMedio : 549 } { _id : 'Amazon' , precioMedio : 164 } Precio medio por empresa y categor\u00eda Precio medio por empresa en base al precio medio anterior","title":"Doble $group"},{"location":"sa/05agregaciones.html#first-y-last","text":"Estos operadores devuelven el valor resultante de aplicar la expresi\u00f3n al primer ( $first ) y/o \u00faltimo ( $last ) elemento de un grupo de documentos que comparten el mismo grupo por clave. Por ejemplo, para obtener para cada empresa, cual es el tipo de producto que m\u00e1s tiene y la cantidad de dicho tipo har\u00edamos: > db . productos . aggregate ([ { $group : { _id : { \"empresa\" : \"$fabricante\" , \"tipo\" : \"$categoria\" }, total : { $sum : 1 } } }, { $sort : { \"total\" :- 1 }}, { $group : { _id : \"$_id.empresa\" , producto : { $first : \"$_id.tipo\" }, // (1)! cantidad : { $first : \"$total\" } } } ]) < { _id : 'Sony' , producto : 'Port\u00e1tiles' , cantidad : 1 } { _id : 'Samsung' , producto : 'Tablets' , cantidad : 1 } { _id : 'Amazon' , producto : 'Tablets' , cantidad : 2 } { _id : 'Apple' , producto : 'Tablets' , cantidad : 3 } { _id : 'Google' , producto : 'Tablets' , cantidad : 1 } Al agrupar por empresa, elegimos la categor\u00eda de producto que tiene m\u00e1s unidades","title":"$first y $last"},{"location":"sa/05agregaciones.html#project","text":"Si queremos realizar una proyecci\u00f3n sobre el conjunto de resultados y quedarnos con un subconjunto de los campos usaremos el operador $project . Como resultado obtendremos el mismo n\u00famero de documentos, y en el orden indicado en la proyecci\u00f3n. La proyecci\u00f3n dentro del framework de agregaci\u00f3n es mucho m\u00e1s potente que dentro de las consultas normales. Se emplea para: renombrar campos. introducir campos calculados en el documento resultante mediante $add , $substract , $multiply , $divide o $mod transformar campos a may\u00fasculas $toUpper o min\u00fasculas $toLower , concatenar campos mediante $concat u obtener subcadenas con $substr . transformar campos en base a valores obtenidos a partir de una condici\u00f3n mediante expresiones l\u00f3gicas con los operadores de comparaci\u00f3n vistos en las consultas. > db . productos . aggregate ([ { $project : { _id : 0 , // (1)! \"empresa\" : { \"$toUpper\" : \"$fabricante\" }, // (2)! \"detalles\" : { // (3)! \"categoria\" : \"$categoria\" , \"precio\" : { \"$multiply\" : [ \"$precio\" , 1.1 ] } // (4)! }, \"elemento\" : \"$nombre\" // (5)! } } ]) < { empresa : 'APPLE' , detalles : { categoria : 'Tablets' , precio : 548.9000000000001 }, elemento : 'iPad 16GB Wifi' } { empresa : 'APPLE' , detalles : { categoria : 'Tablets' , precio : 658.9000000000001 }, elemento : 'iPad 32GB Wifi' } ... Ocultamos el campo _id Transforma un campo y lo pasa a may\u00fasculas Crea un documento anidado Incrementa el precio el 10% Renombra el campo","title":"$project"},{"location":"sa/05agregaciones.html#match","text":"El operador $match se utiliza principalmente para filtrar los documentos que pasar\u00e1n a la siguiente etapa del pipeline o a la salida final. Por ejemplo, para seleccionar s\u00f3lo las tablets har\u00edamos: db . productos . aggregate ([{ $match : { categoria : \"Tablets\" }}]) Aparte de igualar un valor a un campo, podemos emplear los operadores usuales de consulta, como $gt , $lt , $in , etc\u2026\u200b Se recomienda poner el operador $match al principio del pipeline para limitar los documentos a procesar en siguientes fases. Si usamos este operador como primera fase podremos hacer uso de los indices de la colecci\u00f3n de una manera eficiente. As\u00ed pues, para obtener la cantidad de Tablets de menos de 500 euros har\u00edamos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" , precio : { $lt : 500 }}}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, cantidad : { $sum : 1 }} } ]) < { _id : { empresa : 'Samsung' }, cantidad : 1 } { _id : { empresa : 'Amazon' }, cantidad : 2 } { _id : { empresa : 'Google' }, cantidad : 1 } { _id : { empresa : 'Apple' }, cantidad : 1 }","title":"$match"},{"location":"sa/05agregaciones.html#sort","text":"El operador $sort ordena los documentos recibidos por el campo, y el orden indicado por la expresi\u00f3n indicada al pipeline . Por ejemplo, para ordenar los productos por precio descendentemente har\u00edamos: db . productos . aggregate ({ $sort : { precio :- 1 }}) El operador $sort ordena los datos en memoria, por lo que hay que tener cuidado con el tama\u00f1o de los datos. Por ello, se emplea en las \u00faltimas fases del pipeline , cuando el conjunto de resultados es el menor posible. Si retomamos el ejemplo anterior, y ordenamos los datos por el precio total tenemos: > db . productos . aggregate ([ { $match : { categoria : \"Tablets\" }}, { $group : { _id : { \"empresa\" : \"$fabricante\" }, totalPrecio : { $sum : \"$precio\" }} }, { $sort : { totalPrecio :- 1 }} // (1)! ]) < { _id : { empresa : 'Apple' }, totalPrecio : 1797 } { _id : { empresa : 'Samsung' }, totalPrecio : 450.99 } { _id : { empresa : 'Amazon' }, totalPrecio : 328 } { _id : { empresa : 'Google' }, totalPrecio : 199 } Al ordenar los datos, referenciamos al campo que hemos creado en la fase de $group Un operador muy relacionado es $sortByCount . Este operador es similar a realizar las siguientes operaciones: { $group : { _id : <expresio n > , ca nt idad : { $sum : 1 } } }, { $sor t : { ca nt idad : -1 } } As\u00ed pues, podemos reescribir la consulta que hemos hecho en el operador $group : db . productos . aggregate ([ { $group : { _id : \"$fabricante\" , total : { $sum : 1 } } }, { $sort : { \"total\" : - 1 }} ]) Y hacerla con: Consulta Resultado db . productos . aggregate ([{ $sortByCount : \"$fabricante\" }]) { _id : 'Apple' , cou nt : 4 } { _id : 'Samsu n g' , cou nt : 2 } { _id : 'Amazo n ' , cou nt : 2 } { _id : 'So n y' , cou nt : 1 } { _id : 'Google' , cou nt : 1 }","title":"$sort"},{"location":"sa/05agregaciones.html#skip-y-limit","text":"El operador $limit \u00fanicamente limita el n\u00famero de documentos que pasan a trav\u00e9s del pipeline. El operador recibe un n\u00famero como par\u00e1metro: db . productos . aggregate ([{ $limit : 3 }]) Este operador no modifica los documentos, s\u00f3lo restringe quien pasa a la siguiente fase. De manera similar, con el operador $skip , saltamos un n\u00famero determinado de documentos: db . productos . aggregate ([{ $skip : 3 }]) El orden en el que empleemos estos operadores importa, y mucho, ya que no es lo mismo saltar y luego limitar, donde la cantidad de elementos la fija $limit : > db . productos . aggregate ([{ $skip : 2 }, { $limit : 3 }]) < { _id : ObjectId ( \"635194b32e6059646a8e7fee\" ), nombre : 'iPad 64GB Wifi' , categoria : 'Tablets' , fabricante : 'Apple' , precio : 699 } { _id : ObjectId ( \"635194b32e6059646a8e7fef\" ), nombre : 'Galaxy S3' , categoria : 'Smartphones' , fabricante : 'Samsung' , precio : 563.99 } { _id : ObjectId ( \"635194b32e6059646a8e7ff0\" ), nombre : 'Galaxy Tab 10' , categoria : 'Tablets' , fabricante : 'Samsung' , precio : 450.99 } En cambio, si primero limitamos y luego saltamos, la cantidad de elementos se obtiene de la diferencia entre el l\u00edmite y el salto: > db . productos . aggregate ([{ $limit : 3 }, { $skip : 2 }]) > { _id : ObjectId ( \"635194b32e6059646a8e7fee\" ), nombre : 'iPad 64GB Wifi' , categoria : 'Tablets' , fabricante : 'Apple' , precio : 699 } $sample Si tenemos un dataset muy grande, y queremos probar las consultas con un n\u00famero reducido de documentos, podemos emplear el operador $sample y reducir la cantidad de documentos de manera aleatoria: db . productos . aggregate ([ { $sample : { size : 3 } } ])","title":"$skip y $limit"},{"location":"sa/05agregaciones.html#unwind","text":"El operador $unwind es muy interesante y se utiliza s\u00f3lo con operadores array. Al usarlo con un campo array de tama\u00f1o N en un documento, lo transforma en N documentos con el campo tomando el valor individual de cada uno de los elementos del array. Si retomamos el ejemplo de la sesi\u00f3n anterior donde actualiz\u00e1bamos una colecci\u00f3n de enlaces, ten\u00edamos un enlace con la siguiente informaci\u00f3n: > db . enlaces . findOne () < { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : [ 'mapas' , 'videos' , 'blog' , 'calendario' , 'email' , 'mapas' ] } Podemos observar como el campo tags contiene 6 valores dentro del array (con un valor repetido). A continuaci\u00f3n vamos a desenrollar el array: > db . enlaces . aggregate ([ { $match : { titulo : \"www.google.es\" }}, { $unwind : \"$tags\" } ]) < { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'mapas' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'videos' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'blog' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'calendario' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'email' } { _id : ObjectId ( \"635533668420cd585aac88f3\" ), titulo : 'www.google.es' , tags : 'mapas' } As\u00ed pues hemos obtenido 6 documentos con el mismo _id y titulo , es decir, un documento por elemento del array. De este modo, podemos realizar consultas que sumen/cuenten los elementos del array. Por ejemplo, si queremos obtener las 3 etiquetas que m\u00e1s aparecen en todos los enlaces har\u00edamos: > db . enlaces . aggregate ([ { \"$unwind\" : \"$tags\" }, { \"$group\" : { \"_id\" : \"$tags\" , \"total\" : { $sum : 1 } } }, { \"$sort\" : { \"total\" :- 1 }}, { \"$limit\" : 3 } ]) < { _id : 'mapas' , total : 2 } { _id : 'blog' , total : 1 } { _id : 'calendario' , total : 1 }","title":"$unwind"},{"location":"sa/05agregaciones.html#doble-unwind","text":"Si trabajamos con documentos que tienen varios arrays, podemos necesitar desenrollar los dos arrays. Al hacer un doble unwind se crea un producto cartesiano entre los elementos de los 2 arrays. Supongamos que tenemos los datos del siguiente inventario de ropa: > db . inventario . drop (); > db . inventario . insertOne ({ 'nombre' : \"Camiseta\" , 'tallas' : [ \"S\" , \"M\" , \"L\" ], 'colores' : [ 'azul' , 'blanco' , 'naranja' , 'rojo' ]}) > db . inventario . insertOne ({ 'nombre' : \"Jersey\" , 'tallas' : [ \"S\" , \"M\" , \"L\" , \"XL\" ], 'colores' : [ 'azul' , 'negro' , 'naranja' , 'rojo' ]}) > db . inventario . insertOne ({ 'nombre' : \"Pantalones\" , 'tallas' : [ \"32x32\" , \"32x30\" , \"36x32\" ], 'colores' : [ 'azul' , 'blanco' , 'naranja' , 'negro' ]}) Para obtener un listado de cantidad de pares talla/color har\u00edamos: > db . inventario . aggregate ([ { $unwind : \"$tallas\" }, { $unwind : \"$colores\" }, { $group : { '_id' : { 'talla' : '$tallas' , 'color' : '$colores' }, 'total' : { '$sum' : 1 } } } ]) { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"rojo\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"XL\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"L\" , \"color\" : \"negro\" }, \"total\" : 1 } { \"_id\" : { \"talla\" : \"M\" , \"color\" : \"negro\" }, \"total\" : 1 } ...","title":"Doble $unwind"},{"location":"sa/05agregaciones.html#lookup","text":"Si necesitamos unir los datos de dos colecciones, emplearemos el operador $lookup , el cual realiza un left outer join a una colecci\u00f3n de la misma base de datos para filtrar los documentos de la colecci\u00f3n joineada . El resultado es un nuevo campo array para cada documento de entrada, el cual contiene los documentos que cumplen el criterio del join . El operador $lookup utiliza cuatro par\u00e1metros: from : colecci\u00f3n con la que se realiza el join . localField : campo de la colecci\u00f3n origen (ser\u00eda la clave ajena). foreignField : campo en la colecci\u00f3n destino que permite la uni\u00f3n (ser\u00eda la clave primaria de la otra colecci\u00f3n). as : nombre del array que contendr\u00e1 los documentos enlazados. Preparando los datos Vamos a utilizar la colecci\u00f3n zips empleada en anteriores sesiones la cual tiene una estructura similar a: { _id : Objec t Id( \"5c8eccc1caa187d17ca6ed18\" ) , ci t y : 'ACMAR' , zip : ' 35004 ' , loc : { y : 33.584132 , x : 86.51557 }, pop : 6055 , s tate : 'AL' } A continuaci\u00f3n vamos a crear una nueva colecci\u00f3n llamada state con el nombre de los estados ( states.js ), la cual cargaremos en la base de datos sample_training : db . states . insertMany ([ { \"name\" : \"Alabama\" , \"abbreviation\" : \"AL\" }, { \"name\" : \"Alaska\" , \"abbreviation\" : \"AK\" }, ... ]) Vamos a estudiar como funciona el operador $lookup mediante un ejemplo. Primero vamos a recuperar los tres estados m\u00e1s poblados. Para ello, podr\u00edamos hacer la siguiente consulta agregada: Consulta Resultado db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 } ]) { _id : 'CA' , t o tal Poblacio n : 29760021 } { _id : 'NY' , t o tal Poblacio n : 17990455 } { _id : 'TX' , t o tal Poblacio n : 16986510 } Si ahora queremos recuperar el nombre de esos tres estados, a\u00f1adimos una nueva fase: db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 }, { $lookup : { from : \"states\" , localField : \"_id\" , foreignField : \"abbreviation\" , as : \"estados\" }}, ]) Y ahora obtenemos para cada documento, un array con los documentos que coinciden (en este caso es una relaci\u00f3n 1:1, y por eso cada array s\u00f3lo contiene un elemento): { _id : 'CA' , t o tal Poblacio n : 29760021 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0cd5\" ) , na me : 'Cali f or n ia' , abbrevia t io n : 'CA' } ] } { _id : 'NY' , t o tal Poblacio n : 17990455 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0cf4\" ) , na me : 'New York' , abbrevia t io n : 'NY' } ] } { _id : 'TX' , t o tal Poblacio n : 16986510 , es ta dos : [ { _id : Objec t Id( \"63565cd82889ecee358e0d02\" ) , na me : 'Texas' , abbrevia t io n : 'TX' } ] } Como la relaci\u00f3n siempre va a provocar la creaci\u00f3n de un array, mediante $unwind , lo podemos deshacer: Consulta Resultado db . zips . aggregate ([ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sort : { \"totalPoblacion\" :- 1 }} , { $limit : 3 }, { $lookup : { from : \"states\" , localField : \"_id\" , foreignField : \"abbreviation\" , as : \"estados\" }}, { $unwind : \"$estados\" } ]) { _id : 'CA' , t o tal Poblacio n : 29760021 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0cd5\" ) , na me : 'Cali f or n ia' , abbrevia t io n : 'CA' } } { _id : 'NY' , t o tal Poblacio n : 17990455 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0cf4\" ) , na me : 'New York' , abbrevia t io n : 'NY' } } { _id : 'TX' , t o tal Poblacio n : 16986510 , es ta dos : { _id : Objec t Id( \"63565cd82889ecee358e0d02\" ) , na me : 'Texas' , abbrevia t io n : 'TX' } } As\u00ed pues, para finalmente obtener el nombre de cada estado, mediante $project recuperamos el campo name : db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $sor t :{ \"totalPoblacion\" : -1 }} , { $limi t : 3 }, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }} ] ) Obteniendo el resultado deseado: { _id : 'CA' , es ta do : 'Cali f or n ia' , poblacio n : 29760021 } { _id : 'NY' , es ta do : 'New York' , poblacio n : 17990455 } { _id : 'TX' , es ta do : 'Texas' , poblacio n : 16986510 }","title":"$lookup"},{"location":"sa/05agregaciones.html#persistiendo-los-resultados","text":"Una vez hemos realizado nuestras consultas mediante el framework de agregaci\u00f3n, es muy posible que queramos almacenar el resultado en una nueva colecci\u00f3n para poder volver a consultar el resultado sin necesidad de ejecutar todas las fases. Para ello, podemos emplear los operadores: $out recoge los documentos de una agregaci\u00f3n y los persiste en una colecci\u00f3n, sobrescribiendo los datos existentes. $merge similar a $out , pero permite a\u00f1adir el resultado a la misma colecci\u00f3n y adem\u00e1s soporta trabajar con colecciones particionadas. Por ejemplo, vamos a basarnos en las consultas con join , para crear una nueva colecci\u00f3n con la poblaci\u00f3n total de todos dos estados, y la vamos a almacenar en una nueva colecci\u00f3n denominada states_population : db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }}, { $ou t : \"states_population\" } ] ) Tras su ejecuci\u00f3n, podemos recuperar los datos: > db . states_population . findOne () < { _id : 'WY' , estado : 'Wyoming' , poblacion : 453588 }","title":"Persistiendo los resultados"},{"location":"sa/05agregaciones.html#de-sql-al-pipeline-de-agregaciones","text":"Ya hemos visto que el pipeline ofrece operadores para realizar la misma funcionalidad de agrupaci\u00f3n que ofrece SQL. Si relacionamos los comandos SQL con el pipeline de agregaciones tenemos las siguientes equivalencias: SQL Pipeline de Agregaciones WHERE $match GROUP BY $group HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum / $sortByCount join $lookup Podemos encontrar ejemplos de consultas SQL transformadas al pipeline en https://www.mongodb.com/docs/manual/reference/sql-aggregation-comparison/","title":"De SQL al Pipeline de agregaciones"},{"location":"sa/05agregaciones.html#limitaciones","text":"Hay que tener en cuenta las siguiente limitaciones: En versiones anteriores a la 2.6, el pipeline devolv\u00eda en cada fase un objeto BSON, y por tanto, el resultado estaba limitado a 16MB Actualmente, s\u00f3lo cada documento que forme parte del resultado final debe ocupar menos de 16MB. Las fases tienen un l\u00edmite de 100MB en memoria. Si una fase excede dicho l\u00edmite, se producir\u00e1 un error. En este caso, hay que habilitar el uso de disco mediante allowDiskUse en las opciones de la agregaci\u00f3n. M\u00e1s informaci\u00f3n en https://www.mongodb.com/docs/manual/core/aggregation-pipeline-limits/","title":"Limitaciones"},{"location":"sa/05agregaciones.html#agregaciones-con-compass","text":"MongoDB Compass nos ofrece la herramienta Aggregation Pipeline Builder para crear, borrar y reorganizar f\u00e1cilmente fases en un pipeline, as\u00ed como evaluar los documento resultantes en tiempo real. Vamos a practicar con la colecci\u00f3n de zips : Agregaciones en MongoDB Compass Y vamos a reproducir la consulta con $lookup que acabamos de realizar, pero en nuestro caso, iremos paso a paso: db.zips.aggrega te ( [ { $group : { _id : \"$state\" , \"totalPoblacion\" : { $sum : \"$pop\" } }}, { $lookup : { fr om : \"states\" , localField : \"_id\" , f oreig n Field : \"abbreviation\" , as : \"estados\" }}, { $u n wi n d : \"$estados\" }, { $projec t : { \"estado\" : \"$estados.name\" , \"poblacion\" : \"$totalPoblacion\" }} ] ) Para ello, pulsamos sobre el bot\u00f3n de Add Stage , seleccionamos el operador $group y escribimos la expresi\u00f3n de agrupaci\u00f3n. Al hacerlo, en el panel anexo aparecer\u00e1n 10 documentos de muestra con el resultado de ejecutar dicha fase: $group en MongoDB Compass Paso a paso, iremos a\u00f1adiendo el resto de fases hasta tenerlas todas, pudiendo ver los datos que va generando cada fase: Resultado en MongoDB Compass Una vez tenemos nuestra agregaci\u00f3n, podemos obtener una versi\u00f3n del pipeline en Python, Java, C# o Node.js, mediante la opci\u00f3n Export-to-Language . Exportando la agregaci\u00f3n a PyMongo","title":"Agregaciones con Compass"},{"location":"sa/05agregaciones.html#referencias","text":"Documentaci\u00f3n oficial del Aggregation Framework . Curso M121: The MongoDB Aggregation Framework de la Mongo University. Libro Practical MongoDB Aggregations","title":"Referencias"},{"location":"sa/05agregaciones.html#actividades","text":"Para las siguientes actividades, vamos a utilizar la base de datos sample_mflix , y en concreto, las colecciones movies . Un documento de ejemplo ser\u00eda similar a: movies.json { _id : Objec t Id( \"573a1390f29313caabcd548c\" ) , plo t : 'The Civil War divides ...' , ge nres : [ 'Drama' , 'His t ory' , 'Roma n ce' ], ru nt ime : 165 , ra te d : 'NOT RATED' , cas t : [ 'Lillia n Gish' , 'Mae Marsh' , 'He nr y B. Wal t hall' , 'Miriam Cooper' ], pos ter : 'h tt ps : //m.media-amazon.com/images/M/MV5BYTM4ZDhiYTQtYzExNC00YjVlLTg2YWYtYTk3NTAzMzcwNTExXkEyXkFqcGdeQXVyNjU0OTQ0OTY@._V1_SY1000_SX677_AL_.jpg', t i tle : 'The Bir t h o f a Na t io n ' , full plo t : 'Two bro t hers...' , cou ntr ies : [ 'USA' ], released : 1915-03-03 T 00 : 00 : 00.000 Z , direc t ors : [ 'D.W. Gri ff i t h' ], wri ters : [ 'Thomas Dixo n Jr. (adap te d fr om ...\")', 'Thomas Dixon Jr. (play)', 'Thomas Dixon Jr. (novel)', 'D.W. Griffith', 'Frank E. Woods' ], awards: { wins: 2, nominations: 0, text: '2 wins.' }, lastupdated: '2015-09-11 00:32:27.763000000', year: 1915, imdb: { rating: 6.8, votes: 15715, id: 4972 }, type: 'movie', tomatoes: { viewer: { rating: 3.2, numReviews: 4358, meter: 57 }, dvd: 2004-06-29T00:00:00.000Z, critic: { rating: 8, numReviews: 38, meter: 100 }, lastUpdated: 2015-09-10T18:30:23.000Z, consensus: 'Racial depictions aside...', rotten: 0, production: 'Gravitas', fresh: 38 }, num_mflix_comments: 0 } ( RA5075.1 / CE5.1d / 3p) Haciendo uso del framework de agregaci\u00f3n y el shell de MongoDB, resuelve las siguientes consultas: (0.5) Encuentra todas las pel\u00edculas que entre sus g\u00e9neros ( genres ) se encuentre el Drama . S\u00f3lo queremos recuperar el t\u00edtulo y la calificaci\u00f3n ( rating ) de IMDB. (0.5) Recupera los t\u00edtulos de las tres pel\u00edculas rom\u00e1nticas ( Romance ) con mayor calificaci\u00f3n en IMDB que se lanzaron ( released ) antes del 2001. (0.5) Averigua la cantidad de pel\u00edculas que hay de cada categor\u00eda de calificaci\u00f3n ( rated ). (0.5) Teniendo en cuenta las pel\u00edculas anteriores al a\u00f1o 2001, para cada g\u00e9nero, recupera la media y la m\u00e1xima calificaci\u00f3n en IMDB as\u00ed como el tiempo ajustado (con trailers , los cuales duran 12 minutos) de la pel\u00edcula m\u00e1s larga, ordenando los g\u00e9neros por popularidad. El resultado ser\u00e1 similar a: { _id : 'Film - Noir' , n o ta _media : 7.62 , mejor_ n o ta : 8.3 , t iempo_ajus ta do : 123 } { _id : 'Docume ntar y' , n o ta _media : 7.555313351498638 , mejor_ n o ta : 9.4 , t iempo_ajus ta do : 1152 } { _id : 'Shor t ' , n o ta _media : 7.386 , mejor_ n o ta : 8.6 , t iempo_ajus ta do : 56 } ... (0.5) Sobre la consulta anterior, adem\u00e1s de la informaci\u00f3n ya recuperada, queremos buscar qu\u00e9 pel\u00edcula recomendar de cada categor\u00eda siempre y cuando duren un m\u00e1ximo de 218 minutos y tengan al menos una calificaci\u00f3n de 7. (pista: necesitas utilizar $first ) El resultado ser\u00e1 similar a: { _id : 'Docume ntar y' , t i tul o_recome n dado : 'Cosmos' , n o ta _recome n dado : 9.3 , t iempo_recome n dado : 60 , popularidad : 7.69695945945946 , mejor_ n o ta : 9.3 , t iempo_ajus ta do_maslargo : 212 } { _id : 'Sci - Fi' , t i tul o_recome n dado : 'Blade Ru nner ' , n o ta _recome n dado : 8.2 , t iempo_recome n dado : 117 , popularidad : 7.3999999999999995 , mejor_ n o ta : 8.2 , t iempo_ajus ta do_maslargo : 209 } ... (0.5) Recupera las 5 pel\u00edculas m\u00e1s comentadas (los comentarios se almacenan en la colecci\u00f3n comments ), devolviendo el t\u00edtulo, su g\u00e9nero y la cantidad de comentarios. Adem\u00e1s, queremos almacenar el resultado en la colecci\u00f3n movies_most_commented . > db . most_commented_movies . findOne () < { _id : ObjectId ( \"573a13bff29313caabd5e91e\" ), sumComments : 161 , movie : { imdb : { rating : 6.4 }, title : 'The Taking of Pelham 1 2 3' } } ( RA5075.1 / CE5.1d / 1p) Haciendo uso de MongoDBCompass , mejora la siguiente consulta que obtiene los tres documentales m\u00e1s premiados, siempre y cuando hayan ganado alg\u00fan premio: var pipeline = [ { $sort : { \"awards.wins\" : - 1 }}, // Ordenamos por premios ganados { $match : { \"awards.wins\" : { $gte : 1 }}}, { $limit : 20 }, // Obtenemos las 20 pel\u00edculas que han ganado m\u00e1s de un premio { $match : { genres : { $in : [ \"Documentary\" ]}, // Nos quedamos con los documentales }}, { $project : { title : 1 , genres : 1 , awards : 1 }}, { $limit : 3 }, ]; db . movies . aggregate ( pipeline ) Adjunta una captura inicial y otra final, y la exportaci\u00f3n de la agregaci\u00f3n a Python.","title":"Actividades"},{"location":"sa/06replicacion.html","text":"Replicaci\u00f3n \u00b6 Un aspecto muy importante de MongoDB es que soporta la replicaci\u00f3n de los datos de forma nativa mediante el uso de conjuntos de r\u00e9plicas. Conjunto de r\u00e9plicas \u00b6 En MongoDB se replican los datos mediante un conjunto de r\u00e9plicas ( Replica Set ), el cual es un grupo de servidores (nodos mongod ) donde uno de ellos ejerce la funci\u00f3n de primario y por tanto recibe las peticiones de los clientes, y el resto de servidores hace de secundarios, manteniendo copias de los datos del primario. Conjunto de R\u00e9plicas en MongoDB Si el nodo primario se cae, los secundarios eligen un nuevo primario entre ellos mismos, en un proceso que se conoce como votaci\u00f3n. La aplicaci\u00f3n se conectar\u00e1 al nuevo primario de manera transparente. Cuando el antiguo nodo primario vuelva en s\u00ed, ser\u00e1 un nuevo nodo secundario. Al usar replicaci\u00f3n, si un servidor se cae, siempre vamos a poder obtener los datos a partir de otros servidores del conjunto. Si los datos de un servidor se da\u00f1an o son inaccesibles, podemos crear una nueva copia desde uno de los miembros del conjunto. Elementos de un conjunto de r\u00e9plicas \u00b6 Los tipos de nodos que podemos encontrar en un conjunto de r\u00e9plica son: Regular : Es el tipo de nodo m\u00e1s com\u00fan. Primario : Acepta todas las operaciones de escritura de los clientes. Cada conjunto de r\u00e9plicas tendr\u00e1 s\u00f3lo un primario, y como s\u00f3lo un miembro acepta operaciones de escritura, ofrece consistencia estricta para todas las lecturas realizadas desde \u00e9l. Secundario : Los secundarios replican el oplog primario y aplican las operaciones a sus conjuntos de datos. De este modo, los nodos secundarios son un espejo del primario. Si el primario deja de estar disponible, el conjunto de r\u00e9plica elegir\u00e1 a un secundario para que sea el nuevo primario, mediante un proceso de votaci\u00f3n. Por defecto, los clientes realizan las lecturas desde el nodo primario. Sin embargo, los clientes pueden indicar que quieren realizar lecturas desde los nodos secundarios. Consistencia eventual Es posible que al realizar lecturas de un nodo secundario la informaci\u00f3n que se obtenga no refleje el estado del nodo primario. \u00c1rbitro : se emplea s\u00f3lo para votar. No contiene copia de los datos y no se puede convertir en primario. Los conjuntos de r\u00e9plica pueden tener \u00e1rbitros para a\u00f1adir votos en las elecciones de un nuevo primario. Siempre tienen un voto, y permiten que los conjuntos de r\u00e9plica tengan un n\u00famero impar de nodos, sin la necesidad de tener un miembro que replique los datos. Adem\u00e1s, no requieren hardware dedicado. A tener en cuenta No ejecutar un \u00e1rbitro en sistemas que tambi\u00e9n ejecutan los miembros primarios y secundarios del conjunto de r\u00e9plicas. S\u00f3lo a\u00f1adir un \u00e1rbitro a un conjunto con un n\u00famero par de miembros. Si se a\u00f1ade un \u00e1rbitro a un conjunto con un n\u00famero impar de miembros, el conjunto puede sufrir un empate. Retrasado ( delayed ): nodo que se emplea para la recuperaci\u00f3n del sistema ante un fallo. Para ello, hay que asignar la propiedad priority:0 . Este nodo nunca ser\u00e1 un nodo primario. Oculto : empleado para anal\u00edticas del sistema. oplog \u00b6 Para soportar la replicaci\u00f3n, el nodo primario almacena todos los cambios en su oplog . De manera simplificada, el oplog es un diario de todos los cambios que la instancia principal realiza en las bases de datos con el prop\u00f3sito de replicar dichos cambios en un nodo secundario para asegurar que las dos bases de datos sean id\u00e9nticas. El servidor principal mantiene el oplog , y el secundario consulta al principal por nuevas entradas que aplicar a sus propias copias de las bases de datos replicadas. Este proceso se realiza de manera as\u00edncrona, de manera que todos los miembros del conjunto de r\u00e9plicas contienen una copia del oplog . El oplog crea un timestamp para cada entrada. Esto permite que un secundario controle la cantidad de informaci\u00f3n que se ha modificado desde una lectura anterior, y qu\u00e9 entradas necesita transferir para ponerse al d\u00eda. Si paramos un secundario y lo reiniciamos m\u00e1s adelante, utilizar\u00e1 el oplog para obtener todos los cambios que ha perdido mientras estaba offline. El oplog se almacena en una colecci\u00f3n limitada ( capped ) y ordenada de un tama\u00f1o determinado. La opci\u00f3n oplogSize define en MB el tama\u00f1o del archivo. Para un sistema de 64 bits con comportamiento de lectura/escritura normales, el oplogSize deber\u00eda ser de al menos un 5% del espacio de disco disponible. Si el sistema tiene m\u00e1s escrituras que lecturas, puede que necesitemos incrementar este tama\u00f1o para asegurar que cualquier nodo secundario pueda estar offline una cantidad de tiempo razonable sin perder informaci\u00f3n. Creando un conjunto de r\u00e9plicas \u00b6 Replicaci\u00f3n y particionado en MongoAtlas El cluster gratuito de MongoAtlas ya ofrece replicaci\u00f3n de los datos, pero no nos permite administrarlo, al tratarse de un cl\u00faster compartido. Para poder probar tanto la replicaci\u00f3n como el particionado, necesitamos tener control sobre los servidores. Por ello, en esta sesi\u00f3n vamos a utilizar Docker para montar la infraestructura. Para ello, vamos a partir del archivo de docker-compose-replicaset.yml donde creamos tres contenedores (siendo mongo1 el nodo principal y mongo2 y mongo3 los secundarios) dentro de una misma red y que pertenecen al conjunto de r\u00e9plicas iabdrs : docker-compose-replicaset.yml services : mongo1 : container_name : mongo1 image : mongo volumes : - ./rs-init.sh:/scripts/rs-init.sh - ./init.js:/scripts/init.js networks : - mongo-network ports : - 27017:27017 depends_on : - mongo2 - mongo3 links : - mongo2 - mongo3 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] mongo2 : container_name : mongo2 image : mongo networks : - mongo-network ports : - 27018:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] mongo3 : container_name : mongo3 image : mongo networks : - mongo-network ports : - 27019:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] networks : mongo-network : driver : bridge Despliegue Normalmente, cada instancia mongod se coloca en un servidor f\u00edsico y todos en el puerto est\u00e1ndar (27017). En nuestro caso, vamos a crear un conjunto de tres r\u00e9plicas, y en vez de hacerlo en tres m\u00e1quinas distintas, como los tres contenedores residen en la misma m\u00e1quina, lo haremos en tres puertos diferentes (del 27017 al 27019) El nodo principal necesita un script para inicializarse, el cual cargamos en el volumen como rs-init.sh . En dicho fichero definimos un documento con la configuraci\u00f3n del cl\u00faster, donde el _id tiene que ser igual al usado al crear la r\u00e9plica, y el array de members contiene las r\u00e9plicas creadas donde los puertos han de coincidir. Dicho documento se pasar\u00e1 como par\u00e1metro a la operaci\u00f3n rs.initiate : rs-init.sh #!/bin/bash DELAY = 25 mongosh <<EOF var config = { \"_id\": \"iabdrs\", \"version\": 1, \"members\": [ { \"_id\": 1, \"host\": \"mongo1:27017\", \"priority\": 2 }, { \"_id\": 2, \"host\": \"mongo2:27017\", \"priority\": 1 }, { \"_id\": 3, \"host\": \"mongo3:27017\", \"priority\": 1 } ] }; rs.initiate(config, { force: true }); EOF echo \"****** Esperando ${ DELAY } segundos a que se apliquen la configuraci\u00f3n del conjunto de r\u00e9plicas ******\" sleep $DELAY mongosh < /scripts/init.js Finalmente, mediante el archivo init.js comprobamos el estado de la r\u00e9plica y creamos el usuario administrador: init.js rs . status (); db . createUser ({ user : 'admin' , pwd : 'admin' , roles : [ { role : 'root' , db : 'admin' } ]}); As\u00ed pues, una vez tenemos los tres archivos en la misma carpeta, ya podemos lanzar Docker Compose para crear los contenedores: docker-compose --file docker-compose-replicaset.yml --project-name iabd-mongodb-replica up -d Ya s\u00f3lo nos queda ejecutar el script de inicializaci\u00f3n sobre el nodo principal: docker exec mongo1 sh /scripts/rs-init.sh Al ejecutarse el script, se inicializa el conjunto de r\u00e9plicas y se obtiene su estado (mediante rs.status() ) el cual se muestra por consola y podemos observar como ha creado los tres nodos, diferenciando el nodo principal de los secundarios: iabdrs [ direc t : primary ] test > { se t : 'iabdrs' , da te : ISODa te ( \"2022-10-26T09:54:42.137Z\" ) , myS tate : 1 , ter m : Lo n g( \"1\" ) , sy n cSourceHos t : '' , sy n cSourceId : -1 , hear t bea t I nter valMillis : Lo n g( \"2000\" ) , majori t yVo te Cou nt : 2 , wri te Majori t yCou nt : 2 , vo t i n gMembersCou nt : 3 , wri ta bleVo t i n gMembersCou nt : 3 , op t imes : { las t Commi tte dOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, las t Commi tte dWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , readCo n cer n Majori t yOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, appliedOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, durableOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) }, las t S ta bleRecoveryTimes ta mp : Times ta mp( { t : 1666778030 , i : 1 } ) , elec t io n Ca n dida te Me tr ics : { las t Elec t io n Reaso n : 'elec t io n Timeou t ' , las t Elec t io n Da te : ISODa te ( \"2022-10-26T09:53:10.197Z\" ) , elec t io n Term : Lo n g( \"1\" ) , las t Commi tte dOpTimeA t Elec t io n : { ts : Times ta mp( { t : 1666777979 , i : 1 } ) , t : Lo n g( \"-1\" ) }, las t See n OpTimeA t Elec t io n : { ts : Times ta mp( { t : 1666777979 , i : 1 } ) , t : Lo n g( \"-1\" ) }, nu mVo tes Needed : 2 , priori t yA t Elec t io n : 2 , elec t io n Timeou t Millis : Lo n g( \"10000\" ) , nu mCa t chUpOps : Lo n g( \"0\" ) , ne wTermS tart Da te : ISODa te ( \"2022-10-26T09:53:10.363Z\" ) , wMajori t yWri te Availabili t yDa te : ISODa te ( \"2022-10-26T09:53:11.139Z\" ) }, members : [ { _id : 1 , na me : 'mo n go 1 : 27017 ' , heal t h : 1 , s tate : 1 , s tate S tr : 'PRIMARY' , up t ime : 129 , op t ime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:40.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , sy n cSourceHos t : '' , sy n cSourceId : -1 , i nf oMessage : 'Could n o t f i n d member t o sy n c fr om' , elec t io n Time : Times ta mp( { t : 1666777990 , i : 1 } ) , elec t io n Da te : ISODa te ( \"2022-10-26T09:53:10.000Z\" ) , co nf igVersio n : 1 , co nf igTerm : 1 , sel f : true , las t Hear t bea t Message : '' }, { _id : 2 , na me : 'mo n go 2 : 27017 ' , heal t h : 1 , s tate : 2 , s tate S tr : 'SECONDARY' , up t ime : 102 , op t ime : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDurable : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , op t imeDurableDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t Hear t bea t : ISODa te ( \"2022-10-26T09:54:40.322Z\" ) , las t Hear t bea t Recv : ISODa te ( \"2022-10-26T09:54:41.327Z\" ) , pi n gMs : Lo n g( \"0\" ) , las t Hear t bea t Message : '' , sy n cSourceHos t : 'mo n go 1 : 27017 ' , sy n cSourceId : 1 , i nf oMessage : '' , co nf igVersio n : 1 , co nf igTerm : 1 }, { _id : 3 , na me : 'mo n go 3 : 27017 ' , heal t h : 1 , s tate : 2 , s tate S tr : 'SECONDARY' , up t ime : 102 , op t ime : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDurable : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , op t imeDurableDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t Hear t bea t : ISODa te ( \"2022-10-26T09:54:40.322Z\" ) , las t Hear t bea t Recv : ISODa te ( \"2022-10-26T09:54:41.326Z\" ) , pi n gMs : Lo n g( \"0\" ) , las t Hear t bea t Message : '' , sy n cSourceHos t : 'mo n go 1 : 27017 ' , sy n cSourceId : 1 , i nf oMessage : '' , co nf igVersio n : 1 , co nf igTerm : 1 } ], ok : 1 , '$clus ter Time' : { clus ter Time : Times ta mp( { t : 1666778080 , i : 1 } ) , sig nature : { hash : Bi nar y(Bu ffer . fr om( \"0000000000000000000000000000000000000000\" , \"hex\" ) , 0 ) , keyId : Lo n g( \"0\" ) } }, opera t io n Time : Times ta mp( { t : 1666778080 , i : 1 } ) } Una vez que ya hemos arrancando todo, podemos conectarnos a nuestro conjunto de r\u00e9plica mediante mongosh (si no le pasamos ning\u00fan par\u00e1metro, se conecta autom\u00e1ticamente a localhost y al puerto 27017). Dentro del shell, los comandos que trabajan con r\u00e9plicas comienzan por el prefijo rs . Por ejemplo, mediante rs.help() obtendremos la ayuda de los m\u00e9todos disponibles: > mongosh iabdrs [ direct: primary ] test> rs.help () Replica Set Class: initiate Initiates the replica set. config Returns a document that contains the current replica set configuration. conf Calls replSetConfig reconfig Reconfigures an existing replica set, overwriting the existing replica set configuration. reconfigForPSASet Reconfigures an existing replica set, overwriting the existing replica set configuration, if the reconfiguration is a transition from a Primary-Arbiter to a Primary-Secondary-Arbiter set. status Calls replSetGetStatus isMaster Calls isMaster hello Calls hello printSecondaryReplicationInfo Calls db.printSecondaryReplicationInfo printSlaveReplicationInfo DEPRECATED. Use rs.printSecondaryReplicationInfo printReplicationInfo Calls db.printReplicationInfo add Adds replica set member to replica set. addArb Calls rs.add with arbiterOnly = true remove Removes a replica set member. freeze Prevents the current member from seeking election as primary for a period of time. Uses the replSetFreeze command stepDown Causes the current primary to become a secondary which forces an election. If no stepDownSecs is provided, uses 60 seconds. Uses the replSetStepDown command syncFrom Sets the member that this replica set member will sync from, overriding the default sync target selection logic. secondaryOk This method is deprecated. Use db.getMongo () .setReadPref () instead For more information on usage: https://docs.mongodb.com/manual/reference/method/js-replication/ La pr\u00f3xima vez que lancemos las r\u00e9plicas ya no deberemos configurarlas. As\u00ed pues, el proceso de enlazar e iniciar las r\u00e9plicas s\u00f3lo se realiza una vez. Trabajando con las r\u00e9plicas \u00b6 Una vez que hemos visto que las tres r\u00e9plicas est\u00e1n funcionando, vamos a comprobar c\u00f3mo podemos trabajar con ellas. Ya hemos comprobado c\u00f3mo al conectarnos al cl\u00faster, nos aparece como s\u00edmbolo del shell el nombre del conjunto de la r\u00e9plica seguido de dos puntos y primary si nos hemos conectado al nodo principal, o secondary en caso contrario. iabdrs [ direct: primary ] test> Para saber si nos hemos conectado al nodo correcto, mediante db.hello() obtendremos el tipo del nodo (propiedad isWritablePrimary ) e informaci\u00f3n sobre el resto de nodos (antes se utilizaba el m\u00e9todo isMaster el cual se ha marcado como deprecated ): iabdrs [ direct : primary ] test > db . hello () { topologyVersion : { processId : ObjectId ( \"635a9dbbc752fabab79400d6\" ), counter : Long ( \"6\" ) }, hosts : [ 'mongo1:27017' , 'mongo2:27017' , 'mongo3:27017' ], setName : 'iabdrs' , setVersion : 1 , isWritablePrimary : true , secondary : false , primary : 'mongo1:27017' , me : 'mongo1:27017' , electionId : ObjectId ( \"7fffffff0000000000000001\" ), lastWrite : { opTime : { ts : Timestamp ({ t : 1666883479 , i : 1 }), t : Long ( \"1\" ) }, lastWriteDate : ISODate ( \"2022-10-27T15:11:19.000Z\" ), majorityOpTime : { ts : Timestamp ({ t : 1666883479 , i : 1 }), t : Long ( \"1\" ) }, majorityWriteDate : ISODate ( \"2022-10-27T15:11:19.000Z\" ) }, maxBsonObjectSize : 16777216 , maxMessageSizeBytes : 48000000 , maxWriteBatchSize : 100000 , localTime : ISODate ( \"2022-10-27T15:11:20.366Z\" ), logicalSessionTimeoutMinutes : 30 , connectionId : 48 , minWireVersion : 0 , maxWireVersion : 17 , readOnly : false , ok : 1 , '$clusterTime' : { clusterTime : Timestamp ({ t : 1666883479 , i : 1 }), signature : { hash : Binary ( Buffer . from ( \"0000000000000000000000000000000000000000\" , \"hex\" ), 0 ), keyId : Long ( \"0\" ) } }, operationTime : Timestamp ({ t : 1666883479 , i : 1 }) } Ahora que sabemos que estamos en el nodo principal, vamos a a\u00f1adir datos. Para ello, vamos a insertar 1.000 documentos en la colecci\u00f3n pruebas : for ( i = 0 ; i < 1000 ; i ++ ) { db . pruebas . insertOne ({ num : i }) } Estos 1.000 documentos se han insertado en el nodo principal, y se han replicado a los secundarios. Para comprobar la replicaci\u00f3n, abrimos un nuevo terminal y nos conectamos a un nodo secundario: $ mongosh --port 27018 iabdrs [ direct: secondary ] test> Si desde el nodo secundario intentamos consultar el total de documentos de la colecci\u00f3n obtendremos un error: iabdrs [ direct: secondary ] test> db.pruebas.countDocuments () MongoServerError: not primary and secondaryOk = false - consider using db.getMongo () .setReadPref () or readPreference in the connection string El error indica que no somos un nodo primario y por lo tanto no podemos leer de \u00e9l. Para permitir lecturas en los nodos secundarios, mediante db.getMongo().setReadPref('secondary') le decimos a mongosh que sabemos que nos hemos conectado a un secundario y admitimos la posibilidad de obtener datos obsoletos. iabdrs [ direct: secondary ] test> db.getMongo () .setReadPref ( 'secondary' ) iabdrs [ direct: secondary ] test> db.pruebas.countDocuments () 1000 Pero que podamos leer no significa que podamos escribir. Si intentamos escribir en un nodo secundario obtendremos un error: iabdrs [ direct: secondary ] test> db.pruebas.insertOne ({ num : 1001 }) MongoServerError: not primary Preferencias de lectura \u00b6 En el ejemplo anterior hemos visto c\u00f3mo hemos cambiado las preferencias de lectura para permitir hacerlo desde los nodos secundarios. As\u00ed pues, las preferencias de lectura definen el modo en el que MongoDB enruta las lecturas realizadas a los miembros del conjunto de r\u00e9plicas. Preferencias de lectura Por defecto, las aplicaciones dirigen las operaciones de consulta al miembro principal (con el modo de lectura primary , el cual es el modo por defecto). Pero los clientes puede indicar otras preferencias: primaryPreferred : si est\u00e1 disponible, las lecturas se realizan en el nodo primario, pero si no estuviera en pie, habilita las lecturas de los secundarios. secondary : todas las operaciones de lectura se realizan en nodos secundarios. secondaryPreferred : primero prueba con los secundarios, y si no hay ninguno disponible, la lectura la realiza del primario. nearest : las lecturas se realizan del nodo m\u00e1s cercano en base a la latencia, independientemente que el nodo sea primario o secundario. Para indicar la preferencia de lectura, ya hemos visto que lo haremos con la funci\u00f3n Mongo.setReadPref() : iabdrs [ direct: secondary ] test> db.getMongo () .setReadPref ( 'nearest' ) Consistencia en la escritura \u00b6 Por defecto, tanto las lecturas como las escrituras se realizan de manera predeterminada en el nodo principal. Las aplicaciones pueden decidir que las escrituras vayan al nodo primario pero las lecturas al secundario. Esto puede provocar que haya lecturas caducadas, con datos obsoletos. Este hecho, que en sistemas relacionales es inadmisible, en sistemas NoSQL ofrece como beneficio que podamos escalar el sistema. La replicaci\u00f3n es un proceso as\u00edncrono. En el per\u00edodo de tiempo en el que el sistema de votaci\u00f3n sucede, no se completa ninguna escritura. MongoDB garantiza la consistencia en la escritura (Write Concern) , lo que implica que sea un sistema consistente. Para ello, ofrece un mecanismo que garantiza que una escritura ha sido exitosa. Dependiendo del nivel de configuraci\u00f3n de la consistencia, las inserciones, modificaciones y borrados pueden tardar m\u00e1s o menos. Si reducimos el nivel de consistencia, el rendimiento ser\u00e1 mejor, a costa de poder obtener datos obsoletos u perder datos que no se han terminado de serializar en disco. Con un nivel de consistencia m\u00e1s alto, los clientes esperan tras enviar una operaci\u00f3n de escritura a que MongoDB les confirme la operaci\u00f3n. Los valores que podemos configurar se realizan mediante las siguientes opciones: w : indica el n\u00famero de servidores que se han de replicar para que la inserci\u00f3n devuelva un ACK. j : indica si las escrituras se tienen que trasladar a un diario de bit\u00e1cora ( journal ) wtimeout : indica el l\u00edmite de tiempo a esperar como m\u00e1ximo, para prevenir que una escritura se bloquee indefinidamente. Niveles de consistencia de escritura \u00b6 Con estas opciones, podemos configurar diferentes niveles de consistencia son: Sin confirmaci\u00f3n: w:0 , tambi\u00e9n conocido como fire-and-forget , ya que no se espera ning\u00fan tipo de confirmaci\u00f3n. Con confirmaci\u00f3n: w:1 , el cual es el modo por defecto, y s\u00f3lo espera confirmaci\u00f3n del nodo principal. Con diario: w:1 , j:true . Cada inserci\u00f3n primero se escribe en el diario y posteriormente en el directorio de datos. Con confirmaci\u00f3n de la mayor\u00eda: w: \"majority\" , es decir, confirman la mitad + 1 de los nodos de la r\u00e9plica. Hasta que no han confirmado todos los nodos secundarios necesarios, el principal no env\u00eda el ACK a la aplicaci\u00f3n cliente. Consistencia en la escritura con confirmaci\u00f3n de la mayor\u00eda Estas opciones se indican como par\u00e1metro final en las operaciones de inserci\u00f3n y modificaci\u00f3n de datos. Por ejemplo: db . pruebas . insertOne ( { num : 1002 }, { writeConcern : { w : \"majority\" , wtimeout : 5000 }} ) En resumen, a mayor cantidad de nodos, mayor es la tolerancia a fallos pero cada operaci\u00f3n necesita m\u00e1s tiempo y recursos para realizar la persistencia de los datos. Consistencia en la lectura \u00b6 De igual manera que podemos decidir en cuantos nodos se deben propagar las escrituras, podemos indicar cuantos nodos debemos leer para dar un dato como v\u00e1lido. Para ello, MongoDB da soporte a diferentes niveles de consistencia en la lectura (Read Concern) : local : devuelve el dato m\u00e1s reciente en el cluster. Cualquier dato que haya sido escrito en el nodo primario puede ser elegido para devolverse. Sin embargo, no se garantiza que este dato sea replicado a los miembros del conjunto en caso de fallo. Este es el nivel por defecto en las operaciones de lectura contra el nodo primario. available : equivalente de local cuando las operaciones de lectura se efect\u00faan contra un nodo secundario. majority : \u00fanicamente devuelve datos que hayan sido confirmados en una mayor\u00eda de nodos dentro del conjunto. linearizable : devuelve datos que hayan sido confirmados por una mayor\u00eda de nodos, pero permite al desarrollador establecer su propia funcionalidad. snapshot : s\u00f3lo disponible para transacciones multi-documento, realiza una \"foto\" de los datos al inicio de la transacci\u00f3n. Imaginemos un escenario en el cual recibimos la confirmaci\u00f3n de escritura desde el nodo primario. Inmediatamente efectuamos una operaci\u00f3n de lectura, el nodo devuelve el dato, pero \u00e9ste nodo falla antes de replicar la operaci\u00f3n de escritura a los nodos secundarios. Sobre esta operaci\u00f3n se efectuar\u00e1 un proceso de rollback en el momento que el nodo primario vuelva a estar disponible, por lo que ese dato realmente no existir\u00e1 en el conjunto replicado. Es decir, la aplicaci\u00f3n actualmente tiene un dato que no existe en el conjunto de r\u00e9plicas. Mediante la consistencia en la lectura, podemos obtener unas m\u00ednimas garant\u00edas de que el dato que estamos leyendo es correcto y durable. Cuando se utiliza, \u00fanicamente devolver\u00e1 datos cuya grabaci\u00f3n haya sido confirmada por el n\u00famero de nodos especificados en sus opciones. Se puede escoger entre devolver el dato m\u00e1s reciente que exista en el cl\u00faster, o el dato recibido por una mayor\u00eda de miembros en el cluster. El hecho de que un documento no se considere correcto, no quiere decir necesariamente que se haya perdido, sino que en el momento de su lectura, no ha cumplido las condiciones necesarias de durabilidad para ser devuelto. Puede ser que la lectura se est\u00e9 produciendo antes de que el dato haya sido propagado al n\u00famero m\u00ednimo de nodos necesario, y por eso no se obtenga, pero en lecturas sucesivas s\u00ed pueda aparecer. Tolerancia a fallos \u00b6 Cuando un nodo primario no se comunica con otros miembros del conjunto durante m\u00e1s de 10 segundos, el conjunto de r\u00e9plicas intentar\u00e1, de entre los secundarios, que otro miembro se convierta en el nuevo primario. Para ello se realiza un proceso de votaci\u00f3n, de modo que el nodo que obtenga el mayor n\u00famero de votos se erigir\u00e1 en primario. Este proceso de votaci\u00f3n se realiza bastante r\u00e1pido (menos de 3 segundos), durante el cual no existe ning\u00fan nodo primario y por tanto la r\u00e9plica no acepta escrituras y todos los miembros se convierten en nodos de s\u00f3lo-lectura. Elecci\u00f3n de un nuevo primario Proceso de votaci\u00f3n \u00b6 Cuando un nodo secundario no puede contactar con su nodo primario, contactar\u00e1 con el resto de miembros y les indicar\u00e1 que quiere ser elegido como primario. Es decir, cada nodo que no encuentre un primario se nominar\u00e1 como posible primario, de modo que un nodo no nomina a otro a ser primario, \u00fanicamente vota sobre una nominaci\u00f3n ya existente. Antes de dar su voto, el resto de nodos comprobar\u00e1n: si ellos tienen conectividad con el primario si el nodo que solicita ser primario tienen una r\u00e9plica actualizada de los datos. Todas las operaciones replicadas est\u00e1n ordenadas por el timestamp ascendentemente, de modo que los candidatos deben tener operaciones posteriores o iguales a cualquier miembro con el que tengan conectividad. si existe alg\u00fan nodo con una prioridad mayor que deber\u00eda ser elegido. Si alg\u00fan miembro que quiere ser primario recibe una mayor\u00eda de \"s\u00eds\" se convertir\u00e1 en el nuevo primario, siempre y cuando no haya un servidor que vete la votaci\u00f3n. Si un miembro la veta es porque conoce alguna raz\u00f3n por la que el nodo que quiere ser primario no deber\u00eda serlo, es decir, ha conseguido contactar con el antiguo primario. Una vez un candidato recibe una mayor\u00eda de \"s\u00eds\", su estado pasar\u00e1 a ser primario. Configuraci\u00f3n recomendada \u00b6 Cantidad de elementos En la votaci\u00f3n, se necesita una mayor\u00eda de nodos para elegir un primario, ya que una escritura se considera segura cuando ha alcanzado a la mayor\u00eda de los nodos. Esta mayor\u00eda se define como m\u00e1s de la mitad de todos los nodos del conjunto. Hay que destacar que la mayor\u00eda no se basa en los elementos que queden en pie o est\u00e9n disponibles, sino en el conjunto definido en la configuraci\u00f3n del conjunto. Se recomiendan dos configuraciones: Mediante una mayor\u00eda del conjunto en un centro de datos. Este planteamiento es bueno si tenemos un data center donde queremos que siempre se aloje el nodo primario de la r\u00e9plica. Siempre que el centro de datos funcione normalmente, habr\u00e1 un nodo primario. Sin embargo, si el centro primario pierde la conectividad, el centro de datos secundario no podr\u00e1 elegir un nuevo primario. Mediante el mismo n\u00famero de servidores en cada centro de datos, m\u00e1s un servidor que rompe la igualdad en una tercera localizaci\u00f3n. Este dise\u00f1o es conveniente cuando ambos centros de datos tienen el mismo grado de confiabilidad y robustez. Comprobando la tolerancia \u00b6 Para comprobar la tolerancia a fallos, desde el nodo primario vamos a detenerlo: Si nos hemos conectados desde dentro del contenedor a localhost con la opci\u00f3n --host \"127.0.0.1 , vamos a poder detenerlo mediante el comando shutdown : iabdrs [ direct: primary ] test> db.adminCommand ({ \"shutdown\" : 1 }) Otra posibilidad en vez de detenerlo es degradarlo a nodo secundario mediante rs.stepDown() y forzar un proceso de votaci\u00f3n entre el resto de nodos: iabdrs [ direct: primary ] test> rs.stepDown () { ok: 1 , '$clusterTime' : { clusterTime: Timestamp ({ t: 1666885736 , i: 1 }) , signature: { hash: Binary ( Buffer.from ( \"0000000000000000000000000000000000000000\" , \"hex\" ) , 0 ) , keyId: Long ( \"0\" ) } } , operationTime: Timestamp ({ t: 1666885736 , i: 1 }) } iabdrs [ direct: secondary ] test> Si pasamos al shell del antiguo nodo secundario, y le preguntamos si es el principal, veremos que ahora indica que la propiedad isWritablePrimary es true y que ahora el primary es mongo2:27017 : iabdrs [ direc t : primary ] test > rs.hello() { t opologyVersio n : { processId : Objec t Id( \"635ab0cb3f36955c651fa584\" ) , cou nter : Lo n g( \"7\" ) }, hos ts : [ 'mo n go 1 : 27017 ' , 'mo n go 2 : 27017 ' , 'mo n go 3 : 27017 ' ], se t Name : 'iabdrs' , se t Versio n : 1 , isWri ta blePrimary : true , seco n dary : false , primary : 'mo n go 2 : 27017 ' , me : 'mo n go 2 : 27017 ' , ... Recuperaci\u00f3n del sistema \u00b6 Si en un conjunto de r\u00e9plicas se cae el primario y hay escrituras que se han pasado al oplog y tenemos que otros nodos no las han replicado, cuando el nodo primario vuelva en s\u00ed como secundario y se sincronice con el nuevo primario, se dar\u00e1 cuenta que hay operaciones de escritura pendientes y las pasar\u00e1 a rollback , para que si se desean se apliquen manualmente. Para evitar este escenario, se necesita emplear consistencia en la escritura, de manera que hasta que la escritura no se haya replicado en la mayor\u00eda de los nodos no se considere como una escritura exitosa. Particionado \u00b6 Ya vimos en la primera sesi\u00f3n que dentro del entorno de las bases de datos, particionar consiste en dividir los datos entre m\u00faltiples m\u00e1quinas. Al poner un subconjunto de los datos en cada m\u00e1quina, vamos a poder almacenar m\u00e1s informaci\u00f3n y soportar m\u00e1s carga sin necesidad de m\u00e1quinas m\u00e1s potentes, sino una mayor cantidad de m\u00e1quinas m\u00e1s modestas (y mucho m\u00e1s baratas). El Sharding es una t\u00e9cnica que fragmenta los datos de la base de datos horizontalmente agrup\u00e1ndolos de alg\u00fan modo que tenga sentido y que permita un direccionamiento m\u00e1s r\u00e1pido. Sharding Por lo tanto, estos shards (fragmentos) pueden estar localizados en diferentes bases de datos y localizaciones f\u00edsicas. El Sharding no tiene por qu\u00e9 estar basado \u00fanicamente en una colecci\u00f3n y un campo, puede ser a nivel de todas las colecciones. Por ejemplo podr\u00edamos decir \" todos los datos de usuarios cuyo perfil est\u00e9 en los Estados Unidos los redirigimos a la base de datos del servidor en Estados Unidos, y todos los de Asia van a la base de datos de Asia \". MongoDB implementa el sharding de forma nativa y autom\u00e1tica (de ah\u00ed el t\u00e9rmino de auto-sharding ), siguiendo un enfoque basado en rangos. Para ello, divide una colecci\u00f3n entre diferentes servidores, utilizando mongos como router de las peticiones entre los sharded clusters . Esto favorece que el desarrollador ignore que la aplicaci\u00f3n no se comunica con un \u00fanico servidor, balanceando de manera autom\u00e1tica los datos y permitiendo incrementar o reducir la capacidad del sistema a conveniencia. Paciencia... Antes de plantearse hacer auto-sharding sobre nuestros datos, es conveniente dominar c\u00f3mo se trabaja con MongoDB y el uso de conjuntos de r\u00e9plica. Sharded Cluster \u00b6 El particionado de MongoDB permite crear un cluster de muchas m\u00e1quinas, dividiendo a nivel de colecci\u00f3n y poniendo un subconjunto de los datos de la colecci\u00f3n en cada uno de los fragmentos. Los componentes de un sharded cluster son: Shards (Fragmentos): Cada una de las m\u00e1quinas del cluster, que almacena un subconjunto de los datos de la colecci\u00f3n. Cada shard es una instancia de mongod o un conjunto de r\u00e9plicas. En un entorno de producci\u00f3n, todos los shards son conjuntos de r\u00e9plica. Servidores de Configuraci\u00f3n : Cada servidor de configuraci\u00f3n es una instancia de mongod que almacena metadatos sobre el cluster. Los metadatos mapean los trozos con los shards, definiendo qu\u00e9 rangos de datos definen un trozo ( chunk ) de la colecci\u00f3n, y qu\u00e9 trozos se encuentran en un determinado shard . En entornos de producci\u00f3n se aconseja tener 3 servidores de configuraci\u00f3n (uno primario y dos secundarios) ya que si s\u00f3lo tuvi\u00e9semos uno, al producirse una ca\u00edda el cluster quedar\u00eda inaccesible. Enrutadores : Cada router es una instancia mongos que enruta las lecturas y escrituras de las aplicaciones a los shards . Las aplicaciones no acceden directamente a los shards , sino al router. Estos enrutadores funcionan de manera similar a una tabla de contenidos, indic\u00e1ndonos d\u00f3nde se encuentran los datos. Una vez recopilados los datos de los diferentes shards , se fusionan y se encarga de devolverlos a la aplicaci\u00f3n. En entornos de producci\u00f3n es com\u00fan tener varios routers para balancear la carga de los clientes. Componentes de un Sharded cluster Autoevaluaci\u00f3n Supongamos que queremos ejecutar m\u00faltiples routers mongos para soportar la redundancia. \u00bfQu\u00e9 elemento asegurar\u00e1 la tolerancia a fallos y cambiar\u00e1 de un mongos a otro dentro de tu aplicaci\u00f3n? 1 mongod mongos Driver Los servidores de configuraci\u00f3n de sharding Shard key \u00b6 Para que MongoDB sepa c\u00f3mo dividir una colecci\u00f3n entre rangos no solapados hay que elegir una shard key , normalmente el identificador del documento, por ejemplo, student_id . Este identificador (o su hash ) es la clave del chunk (por lo hace la misma funci\u00f3n que una clave primaria). La shard key puede ser un campo sencillo o compuesto el cual debe estar indexado, y que va a determinar la distribuci\u00f3n de los documentos entre los fragmentos del cl\u00faster. Fragmentaci\u00f3n por la shard-key Para las b\u00fasquedas, borrados y actualizaciones, al emplear la shard key , mongos sabe a que shard enviar la petici\u00f3n. En cambio, si la operaci\u00f3n no la indica, se har\u00e1 un broadcast a todas los shards para averiguar donde se encuentra. Entre los aspectos a tener en cuenta a la hora de elegir una shard key cabe destacar que debe: Tener una alta cardinalidad, para asegurar que los documentos puedan dividirse en los distintos fragmentos. Por ejemplo, si elegimos un shard key que solo tiene 3 valores posibles y tenemos 5 fragmentos, MongoDB no sabr\u00e1 como separar los documentos en los 5 fragmentos. Cuantos m\u00e1s valores posibles pueda tener la clave de fragmentaci\u00f3n, m\u00e1s eficiente ser\u00e1 la divisi\u00f3n de los trozos entre los fragmentos disponibles. Tener un alto nivel de aleatoriedad. Si utilizamos una clave que siga un patr\u00f3n incremental como una fecha o un ID, conllevar\u00e1 que al insertar documentos, el mismo fragmento estar\u00e1 siendo utilizando constantemente durante el rango de valores definido para \u00e9l. Esto provoca que los datos est\u00e9n separados de una manera \u00f3ptima, pero pondr\u00e1 siempre bajo estr\u00e9s a un fragmento en per\u00edodos de tiempo mientras que los otros posiblemente queden con muy poca actividad (comportamiento conocido como hotspotting ). Una soluci\u00f3n a las claves que siguen patrones incrementales es aplicar una funci\u00f3n hash y crear una clave hasheada que si tiene un alto nivel de aleatoriedad. Considerar los patrones de las consultas, ya que si elegimos una buena clave, al realizar consultas por la clave, todos los datos se encontrar\u00e1n en el mismo fragmento. Particionado con Docker \u00b6 Para este caso, vamos a crear dos conjuntos de r\u00e9plicas de dos nodos cada una, y a su vez, particionaremos los datos en dos shards . Adem\u00e1s, vamos a a\u00f1adir un \u00fanico router y un servidor de configuraci\u00f3n (aunque lo ideal ser\u00eda crear un conjunto de r\u00e9plicas de servidores de configuraci\u00f3n). As\u00ed pues, crearemos: un contenedor para el router ( router1 ) el cual ejecuta el servicio mongos y que va a conectar con el servidor de configuraci\u00f3n. un contenedor para el servidor de configuraci\u00f3n ( configsvr1 ) indic\u00e1ndole mediante el par\u00e1metro --configsvr su prop\u00f3sito, as\u00ed como la r\u00e9plica a la que pertenece (todo servidor de configuraci\u00f3n debe pertenecer a una r\u00e9plica, aunque en nuestro caso s\u00f3lo hemos creado uno) dos nodos ( mongo-shard1a y mongo-shard1b ) para el primer shard (par\u00e1metro --shardsvr ) que pertenecen al conjunto de r\u00e9plicas iabdshard1 . dos nodos m\u00e1s ( mongo-shard2a y mongo-shard2b ) para el segundo shard (par\u00e1metro --shardsvr ) que pertenecen al conjunto de r\u00e9plicas iabdshard2 . Para ello, hemos definido el archivo docker-compose-replicaset-sharded.yml con la definici\u00f3n de los contenedores: docker-compose-replicaset-sharded.yml services : router1 : container_name : router1 image : mongo volumes : - ./router-init.js:/scripts/router-init.js networks : - mongo-network-sharded ports : - 27117:27017 entrypoint : [ \"/usr/bin/mongos\" , \"--port\" , \"27017\" , \"--configdb\" , \"rs-config-server/configsvr1:27017\" , \"--bind_ip_all\" ] configsvr1 : container_name : configsvr1 image : mongo volumes : - ./configserver-init.js:/scripts/configserver-init.js networks : - mongo-network-sharded ports : - 27118:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--configsvr\" , \"--replSet\" , \"rs-config-server\" , \"--bind_ip_all\" ] links : - mongo-shard1a - mongo-shard2a mongo-shard1a : container_name : mongo-shard1a image : mongo volumes : - ./shard1-init.js:/scripts/shard1-init.js networks : - mongo-network-sharded ports : - 27119:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard1\" ] mongo-shard1b : container_name : mongo-shard1b image : mongo networks : - mongo-network-sharded ports : - 27120:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard1\" ] mongo-shard2a : container_name : mongo-shard2a image : mongo volumes : - ./shard2-init.js:/scripts/shard2-init.js networks : - mongo-network-sharded ports : - 27121:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard2\" ] mongo-shard2b : container_name : mongo-shard2b image : mongo networks : - mongo-network-sharded ports : - 27122:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard2\" ] networks : mongo-network-sharded : driver : bridge Y lanzamos los contenedores mediante docker-compose : docker-compose --file docker-compose-replicaset-sharded.yml --project-name iabd-mongodb-replica-sharded up -d Inicializar los servidores de configuraci\u00f3n \u00b6 El primer paso es inicializar los servidores de configuraci\u00f3n mediante el script configserver-init.js : configserver-init.js rs . initiate ({ _id : \"rs-config-server\" , configsvr : true , version : 1 , members : [ { _id : 0 , host : 'configsvr1:27017' } ] }) Y lo lanzamos: docker exec configsvr1 sh -c \"mongosh < /scripts/configserver-init.js\" Configurar el conjunto de r\u00e9plicas \u00b6 Para cada uno de los conjuntos de r\u00e9plicas ( iabdshard1 y iabdshard2 ) que van a contener los datos, hemos de configurar sus nodos, haciendo uso de los scripts shard1-init.js y shard2-init.js : shard1-init.js rs . initiate ( { _id : \"iabdshard1\" , version : 1 , members : [ { _id : 0 , host : \"mongo-shard1a:27017\" }, { _id : 1 , host : \"mongo-shard1b:27017\" }, ] } ) shard2-init.js rs . initiate ( { _id : \"iabdshard2\" , version : 1 , members : [ { _id : 0 , host : \"mongo-shard2a:27017\" }, { _id : 1 , host : \"mongo-shard2b:27017\" }, ] } ) E inicializamos los conjunto de r\u00e9plicas: docker exec mongo-shard1a sh -c \"mongosh < /scripts/shard1-init.js\" docker exec mongo-shard2a sh -c \"mongosh < /scripts/shard2-init.js\" Configurar el router \u00b6 Finalmente, configuramos el router a partir del script router-init.js donde estamos indic\u00e1ndole al router qu\u00e9 nodos forman parte del particionado: router-init.js sh . addShard ( \"iabdshard1/mongo-shard1a:27017\" ) sh . addShard ( \"iabdshard1/mongo-shard1b:27017\" ) sh . addShard ( \"iabdshard2/mongo-shard2a:27017\" ) sh . addShard ( \"iabdshard2/mongo-shard2b:27017\" ) Y lo ejecutamos: docker exec router1 sh -c \"mongosh < /scripts/router-init.js\" sh De manera similar que hemos visto que con el conjunto de r\u00e9plicas se emplean el prefijo rs , para interactuar con los componentes implicados en el sharding se emplea sh , pero \u00fanicamente desde el nodo que hace de router . Por ejemplo, mediante sh.help() obtendremos la ayuda de los m\u00e9todos disponibles. Conexi\u00f3n al router \u00b6 El router es un nodo de MongoDB que se va a encargar de aceptar las peticiones de los clientes y enrutarlas al shard adecuado. Para ello, podemos: Conectarnos al contenedor y abrimos un terminal y veremos como en el prompt aparece mongos : docker exec -it router1 bash mongosh [ direct: mongos ] test> O abrir directamente un sesi\u00f3n al puerto 27117, que es donde hab\u00edamos colocado nuestro router: mongosh 127 .0.0.1:27117 [ direct: mongos ] test> Si comprobamos el estado del shard veremos que tenemos dos shards , cada uno replicado en dos nodos, con sus identificadores y hosts : [ direct : mongos ] test > sh . status () shardingVersion { _id : 1 , minCompatibleVersion : 5 , currentVersion : 6 , clusterId : ObjectId ( \"6363af7bfab6e20f2a7c1409\" ) } --- shards [ { _id : 'iabdshard1' , host : 'iabdshard1/mongo-shard1a:27017,mongo-shard1b:27017' , state : 1 , topologyTime : Timestamp ({ t : 1667477424 , i : 6 }) }, { _id : 'iabdshard2' , host : 'iabdshard2/mongo-shard2a:27017,mongo-shard2b:27017' , state : 1 , topologyTime : Timestamp ({ t : 1667477424 , i : 12 }) } ] ... En un entorno de producci\u00f3n, en vez de tener dos shards en dos nodos, habr\u00e1 un conjunto de r\u00e9plicas para asegurar la alta disponibilidad. Adem\u00e1s, tendremos tres servidores de configuraci\u00f3n para asegurar la disponibilidad de \u00e9stos. Del mismo modo, habr\u00e1 tantos procesos mongos creados como conexiones de clientes: Sharding en un entorno de Producci\u00f3n Habilitando el Sharding \u00b6 Una vez hemos creado la estructura necesaria para soportar el particionado vamos a insertar un conjunto de datos para posteriormente particionarlos. Para ello, vamos a insertar diez mil usuarios en una colecci\u00f3n: [ direct : mongos ] test > use iabd switched to db iabd [ direct : mongos ] iabd > for ( var i = 0 ; i < 10000 ; i ++ ) { db . usuarios . insertOne ({ \"login\" : \"usuario\" + i , \"nombre\" : \"nombre\" + i * 2 , \"fcreacion\" : new Date ()}, { writeConcern : { w : \"0\" }}); } Una vez creados, comprobamos que se han insertado: [ direct : mongos ] iabd > db . usuarios . countDocuments () 10000 [ direct : mongos ] iabd > db . usuarios . findOne () { _id : ObjectId ( \"6363edb79f30da8391b700c4\" ), login : 'usuario0' , nombre : 'nombre0' , fcreacion : ISODate ( \"2022-11-03T16:35:03.184Z\" ) } Como podemos observar, interactuar con mongos es igual a hacerlo con mongosh . Ahora mismo no sabemos en cu\u00e1l de los dos shards se han almacenado los datos. Adem\u00e1s, estos datos no est\u00e1n particionados, es decir residen en s\u00f3lo uno de los shards. Para habilitar el sharding a nivel de base de datos y que los datos se repartan entre los fragmentos disponibles, ejecutaremos el comando sh.enableSharding(<nombreDB>) : [ direct : mongos ] iabd > sh . enableSharding ( \"iabd\" ) { ok : 1 , '$clusterTime' : { clusterTime : Timestamp ({ t : 1667493383 , i : 1 }), signature : { hash : Binary ( Buffer . from ( \"0000000000000000000000000000000000000000\" , \"hex\" ), 0 ), keyId : Long ( \"0\" ) } }, operationTime : Timestamp ({ t : 1667480498 , i : 1 }) } Si volvemos a comprobar el estado del shard , tenemos que se ha creado la nueva base de datos y nos indica cual es su fragmento primario. [ direct : mongos ] iabd > sh . status () ... --- databases [ { database : { _id : 'config' , primary : 'config' , partitioned : true }, collections : { 'config.system.sessions' : { shardKey : { _id : 1 }, unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 512 }, { shard : 'iabdshard2' , nChunks : 512 } ], chunks : [ 'too many chunks to print, use verbose if you want to force print' ], tags : [] } } }, { database : { _id : 'iabd' , primary : 'iabdshard1' , partitioned : false , version : { uuid : new UUID ( \"84bce0ad-38a4-4b72-a377-350272542657\" ), timestamp : Timestamp ({ t : 1667493302 , i : 1 }), lastMod : 1 } }, collections : {} } ] Antes de habilitar el sharding para una determinada colecci\u00f3n, tenemos que crear un \u00edndice sobre la shard key (si la colecci\u00f3n estuviera vac\u00eda, no necesitamos crear el \u00edndice, ya que al indicar la shard key , MongoDB autom\u00e1ticamente crear\u00e1 el \u00edndice por nosotros): db . usuarios . createIndex ({ \"login\" : 1 }) Una vez habilitado el shard ya podemos fragmentar la colecci\u00f3n: sh . shardCollection ( \"iabd.usuarios\" , { \"login\" : 1 }, false ) El m\u00e9todo shardCollection particiona una colecci\u00f3n a partir de una shard key , ya sea mediante claves hashes o utilizando rangos. En nuestro caso, al indicarle {\"login\": \"hashed\"} particionar\u00e1 los datos reparti\u00e9ndolos de manera equitativa entre los fragmentos. Para ello, recibe tres par\u00e1metros: nombre de la colecci\u00f3n, con nomenclatura de nombreBD.nombreColecci\u00f3n nombre del campo para fragmentar la colecci\u00f3n, es decir, el shard key . Uno de los requisitos es que esta clave tenga una alta cardinalidad. Si quisi\u00e9ramos indicar que queremos utilizar una clave hasheada , lo har\u00edamos indicando como valor hashed , por ejemplo, {\"login\": \"hashed\"} . Si queremos utilizar rangos, lo indicamos con valor 1 , por ejemplo, {\"login\": 1} . booleano que indica si el valor utilizado como shard key es \u00fanico. Para ello, el \u00edndice que se crea sobre el campo debe ser del tipo unique . Este comando divide la colecci\u00f3n en fragmentos ( chunks ), la cual es la unidad que utiliza MongoDB para mover los datos. Una vez que se ha ejecutado, MongoDB comenzar\u00e1 a balancear la colecci\u00f3n entre los shards del cluster. Este proceso no es instant\u00e1neo. Si la colecci\u00f3n contiene un gran conjunto de datos puede llevar horas completar el balanceo. Si ahora volvemos a comprobar el estado del shard obtendremos: [ direct : mongos ] iabd > sh . status () ... databases [ { database : { _id : 'config' , primary : 'config' , partitioned : true }, ... }, { database : { _id : 'iabd' , primary : 'iabdshard1' , partitioned : false , version : { uuid : new UUID ( \"9e694228-600c-4a50-b11c-174f846a3c64\" ), timestamp : Timestamp ({ t : 1667489404 , i : 1 }), lastMod : 1 } }, collections : { 'iabd.usuarios' : { shardKey : { login : 'hashed' }, // (1)! unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 1 } ], chunks : [ // (2)! { min : { login : MinKey () }, // (3)! max : { login : () }, 'on shard' : 'iabdshard1' , 'last modified' : Timestamp ({ t : 1 , i : 0 }) } ], tags : [] } } } ] Muestra que la shard key se basa en rangos la propiedad chunks muestra la cantidad de trozos que alberga cada partici\u00f3n. As\u00ed, pues en este momento tenemos un \u00fanico chunk Para cada uno de los fragmentos se muestra el rango de valores que alberga cada chunk, as\u00ed como en que shard se ubica. Las funciones MinKey() y MinKey() son similares a menos infinito y m\u00e1s infinito, es decir, no hay ning\u00fan valor por debajo ni por encima de ellos. Es decir, indican los topes de la colecci\u00f3n. Trabajando con particiones \u00b6 En este momento, el shard est\u00e1 creado pero todos los nodos residen en un \u00fanico fragmento dentro de una partici\u00f3n. Para obtener esta informaci\u00f3n, podemos ver el estado del sharding o consultar la distribuci\u00f3n de una colecci\u00f3n mediante el m\u00e9todo getShardDistribution : [ direct : mongos ] iabd > db . usuarios . getShardDistribution () Shard iabdshard1 at iabdshard1 / mongo - shard1a : 27017 , mongo - shard1b : 27017 { data : '852KiB' , docs : 10000 , chunks : 1 , 'estimated data per chunk' : '852KiB' , 'estimated docs per chunk' : 10000 } --- Totals { data : '852KiB' , docs : 10000 , chunks : 1 , 'Shard iabdshard1' : [ '100 % data' , '100 % docs in cluster' , '87B avg obj size on shard' ] } Vamos a volver a insertar 10.000 usuarios m\u00e1s a ver qu\u00e9 sucede: [ direct : mongos ] iabd > for ( var i = 10000 ; i < 20000 ; i ++ ) { db . usuarios . insertOne ({ \"login\" : \"usuario\" + i , \"nombre\" : \"nombre\" + i * 2 , \"fcreacion\" : new Date ()}, { writeConcern : { w : \"0\" }}); } [ direct : mongos ] iabd > db . usuarios . countDocuments () 20000 Forzando el split Si al insertar m\u00e1s datos no se reparten los datos de forma autom\u00e1tica, podemos forzarlo mediante las operaciones sh.splitAt , donde le indicamos el valor donde el fragmento en dos, y sh.splitFind que realiza la divisi\u00f3n por la mediana, de manera que ambos fragmentos deber\u00edan ser semejantes. Por ejemplo: sh . splitFind ( \"iabd.usuarios\" , { \"login\" : 1 }) Si ahora comprobamos el estado del shard , los datos se deber\u00edan haber repartido entre los shards disponibles: mongos > sh . status () { ... collections : { 'iabd.usuarios' : { shardKey : { login : 1 }, unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 1 }, { shard : 'iabdshard2' , nChunks : 1 } ], chunks : [ { min : { login : MinKey () }, max : { login : 'usuario18999' }, 'on shard' : 'iabdshard2' , 'last modified' : Timestamp ({ t : 2 , i : 0 }) }, { min : { login : 'usuario18999' }, max : { login : MaxKey () }, 'on shard' : 'iabdshard1' , 'last modified' : Timestamp ({ t : 2 , i : 1 }) } ], tags : [] } } } ] Si volvemos a comprobar la distribuci\u00f3n, ahora vemos como ha repartido los documentos: [ direct : mongos ] iabd > db . usuarios . getShardDistribution () Shard iabdshard1 at iabdshard1 / mongo - shard1a : 27017 , mongo - shard1b : 27017 { data : '1.68MiB' , docs : 20000 , chunks : 1 , 'estimated data per chunk' : '1.68MiB' , 'estimated docs per chunk' : 20000 } --- Shard iabdshard2 at iabdshard2 / mongo - shard2a : 27017 , mongo - shard2b : 27017 { data : '866KiB' , docs : 10000 , chunks : 1 , 'estimated data per chunk' : '866KiB' , 'estimated docs per chunk' : 10000 } --- Totals { data : '2.52MiB' , docs : 30000 , chunks : 2 , 'Shard iabdshard1' : [ '66.51 % data' , '66.66 % docs in cluster' , '88B avg obj size on shard' ], 'Shard iabdshard2' : [ '33.48 % data' , '33.33 % docs in cluster' , '88B avg obj size on shard' ] } Referencias \u00b6 Replicaci\u00f3n en MongoDB Particionado en MongoDB How to deploy a MongoDB replica set using docker-compose Demo MongoDB (6.0.1) Sharded Cluster with Docker Compose Ejemplos de c\u00f3digo sobre replicaci\u00f3n y particionado . Actividades \u00b6 ( RA5075.2 / CE5.2b , CE5.2c / 2p) Se pide crear una conjunto de 4 r\u00e9plicas de nombre iabdrs4 en la cual insertaremos los datos de 1000 ciudades , los cuales deber\u00e1s importar a una base de datos . Una vez creado, se pide: Obtener el estado del conjunto de r\u00e9plicas Consultar una ciudad en un nodo secundario. Habilitar las lecturas en los nodos secundarios. Volver a consultar la ciudad en el nodo secundario. Insertar una ciudad en el nodo secundario. Degradar el nodo primario Averiguar cual es el nuevo nodo primario. Adjunta un documento con los scripts de creaci\u00f3n, comandos empleados y las salidas generadas. ( RA5075.2 / CE5.2b / 2p) Se pide crear un cl\u00faster de MongoDB con tres nodos y tres particiones y volver a importar las 1000 ciudades . Una vez creado, se pide: Particionar los datos por el nombre de la ciudad. Una vez cargado los datos, obtener el estado del sharding . Si los datos no est\u00e1n particionados, forzar el split de los mismos. Tras ello, vaciar la colecci\u00f3n y volver a importar los datos. Una vez importados, obtener de nuevo el estado del sharding. Adjunta un documento con los scripts de creaci\u00f3n, los comandos empleados y las salidas generadas. El driver se encarga de manera transparente de conectar al router adecuado, y cambiar un router por otro si al que estamos conectado se cae \u21a9","title":"S28.- Replicaci\u00f3n y Particionado"},{"location":"sa/06replicacion.html#replicacion","text":"Un aspecto muy importante de MongoDB es que soporta la replicaci\u00f3n de los datos de forma nativa mediante el uso de conjuntos de r\u00e9plicas.","title":"Replicaci\u00f3n"},{"location":"sa/06replicacion.html#conjunto-de-replicas","text":"En MongoDB se replican los datos mediante un conjunto de r\u00e9plicas ( Replica Set ), el cual es un grupo de servidores (nodos mongod ) donde uno de ellos ejerce la funci\u00f3n de primario y por tanto recibe las peticiones de los clientes, y el resto de servidores hace de secundarios, manteniendo copias de los datos del primario. Conjunto de R\u00e9plicas en MongoDB Si el nodo primario se cae, los secundarios eligen un nuevo primario entre ellos mismos, en un proceso que se conoce como votaci\u00f3n. La aplicaci\u00f3n se conectar\u00e1 al nuevo primario de manera transparente. Cuando el antiguo nodo primario vuelva en s\u00ed, ser\u00e1 un nuevo nodo secundario. Al usar replicaci\u00f3n, si un servidor se cae, siempre vamos a poder obtener los datos a partir de otros servidores del conjunto. Si los datos de un servidor se da\u00f1an o son inaccesibles, podemos crear una nueva copia desde uno de los miembros del conjunto.","title":"Conjunto de r\u00e9plicas"},{"location":"sa/06replicacion.html#creando-un-conjunto-de-replicas","text":"Replicaci\u00f3n y particionado en MongoAtlas El cluster gratuito de MongoAtlas ya ofrece replicaci\u00f3n de los datos, pero no nos permite administrarlo, al tratarse de un cl\u00faster compartido. Para poder probar tanto la replicaci\u00f3n como el particionado, necesitamos tener control sobre los servidores. Por ello, en esta sesi\u00f3n vamos a utilizar Docker para montar la infraestructura. Para ello, vamos a partir del archivo de docker-compose-replicaset.yml donde creamos tres contenedores (siendo mongo1 el nodo principal y mongo2 y mongo3 los secundarios) dentro de una misma red y que pertenecen al conjunto de r\u00e9plicas iabdrs : docker-compose-replicaset.yml services : mongo1 : container_name : mongo1 image : mongo volumes : - ./rs-init.sh:/scripts/rs-init.sh - ./init.js:/scripts/init.js networks : - mongo-network ports : - 27017:27017 depends_on : - mongo2 - mongo3 links : - mongo2 - mongo3 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] mongo2 : container_name : mongo2 image : mongo networks : - mongo-network ports : - 27018:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] mongo3 : container_name : mongo3 image : mongo networks : - mongo-network ports : - 27019:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdrs\" ] networks : mongo-network : driver : bridge Despliegue Normalmente, cada instancia mongod se coloca en un servidor f\u00edsico y todos en el puerto est\u00e1ndar (27017). En nuestro caso, vamos a crear un conjunto de tres r\u00e9plicas, y en vez de hacerlo en tres m\u00e1quinas distintas, como los tres contenedores residen en la misma m\u00e1quina, lo haremos en tres puertos diferentes (del 27017 al 27019) El nodo principal necesita un script para inicializarse, el cual cargamos en el volumen como rs-init.sh . En dicho fichero definimos un documento con la configuraci\u00f3n del cl\u00faster, donde el _id tiene que ser igual al usado al crear la r\u00e9plica, y el array de members contiene las r\u00e9plicas creadas donde los puertos han de coincidir. Dicho documento se pasar\u00e1 como par\u00e1metro a la operaci\u00f3n rs.initiate : rs-init.sh #!/bin/bash DELAY = 25 mongosh <<EOF var config = { \"_id\": \"iabdrs\", \"version\": 1, \"members\": [ { \"_id\": 1, \"host\": \"mongo1:27017\", \"priority\": 2 }, { \"_id\": 2, \"host\": \"mongo2:27017\", \"priority\": 1 }, { \"_id\": 3, \"host\": \"mongo3:27017\", \"priority\": 1 } ] }; rs.initiate(config, { force: true }); EOF echo \"****** Esperando ${ DELAY } segundos a que se apliquen la configuraci\u00f3n del conjunto de r\u00e9plicas ******\" sleep $DELAY mongosh < /scripts/init.js Finalmente, mediante el archivo init.js comprobamos el estado de la r\u00e9plica y creamos el usuario administrador: init.js rs . status (); db . createUser ({ user : 'admin' , pwd : 'admin' , roles : [ { role : 'root' , db : 'admin' } ]}); As\u00ed pues, una vez tenemos los tres archivos en la misma carpeta, ya podemos lanzar Docker Compose para crear los contenedores: docker-compose --file docker-compose-replicaset.yml --project-name iabd-mongodb-replica up -d Ya s\u00f3lo nos queda ejecutar el script de inicializaci\u00f3n sobre el nodo principal: docker exec mongo1 sh /scripts/rs-init.sh Al ejecutarse el script, se inicializa el conjunto de r\u00e9plicas y se obtiene su estado (mediante rs.status() ) el cual se muestra por consola y podemos observar como ha creado los tres nodos, diferenciando el nodo principal de los secundarios: iabdrs [ direc t : primary ] test > { se t : 'iabdrs' , da te : ISODa te ( \"2022-10-26T09:54:42.137Z\" ) , myS tate : 1 , ter m : Lo n g( \"1\" ) , sy n cSourceHos t : '' , sy n cSourceId : -1 , hear t bea t I nter valMillis : Lo n g( \"2000\" ) , majori t yVo te Cou nt : 2 , wri te Majori t yCou nt : 2 , vo t i n gMembersCou nt : 3 , wri ta bleVo t i n gMembersCou nt : 3 , op t imes : { las t Commi tte dOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, las t Commi tte dWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , readCo n cer n Majori t yOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, appliedOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, durableOpTime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) }, las t S ta bleRecoveryTimes ta mp : Times ta mp( { t : 1666778030 , i : 1 } ) , elec t io n Ca n dida te Me tr ics : { las t Elec t io n Reaso n : 'elec t io n Timeou t ' , las t Elec t io n Da te : ISODa te ( \"2022-10-26T09:53:10.197Z\" ) , elec t io n Term : Lo n g( \"1\" ) , las t Commi tte dOpTimeA t Elec t io n : { ts : Times ta mp( { t : 1666777979 , i : 1 } ) , t : Lo n g( \"-1\" ) }, las t See n OpTimeA t Elec t io n : { ts : Times ta mp( { t : 1666777979 , i : 1 } ) , t : Lo n g( \"-1\" ) }, nu mVo tes Needed : 2 , priori t yA t Elec t io n : 2 , elec t io n Timeou t Millis : Lo n g( \"10000\" ) , nu mCa t chUpOps : Lo n g( \"0\" ) , ne wTermS tart Da te : ISODa te ( \"2022-10-26T09:53:10.363Z\" ) , wMajori t yWri te Availabili t yDa te : ISODa te ( \"2022-10-26T09:53:11.139Z\" ) }, members : [ { _id : 1 , na me : 'mo n go 1 : 27017 ' , heal t h : 1 , s tate : 1 , s tate S tr : 'PRIMARY' , up t ime : 129 , op t ime : { ts : Times ta mp( { t : 1666778080 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:40.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , sy n cSourceHos t : '' , sy n cSourceId : -1 , i nf oMessage : 'Could n o t f i n d member t o sy n c fr om' , elec t io n Time : Times ta mp( { t : 1666777990 , i : 1 } ) , elec t io n Da te : ISODa te ( \"2022-10-26T09:53:10.000Z\" ) , co nf igVersio n : 1 , co nf igTerm : 1 , sel f : true , las t Hear t bea t Message : '' }, { _id : 2 , na me : 'mo n go 2 : 27017 ' , heal t h : 1 , s tate : 2 , s tate S tr : 'SECONDARY' , up t ime : 102 , op t ime : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDurable : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , op t imeDurableDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t Hear t bea t : ISODa te ( \"2022-10-26T09:54:40.322Z\" ) , las t Hear t bea t Recv : ISODa te ( \"2022-10-26T09:54:41.327Z\" ) , pi n gMs : Lo n g( \"0\" ) , las t Hear t bea t Message : '' , sy n cSourceHos t : 'mo n go 1 : 27017 ' , sy n cSourceId : 1 , i nf oMessage : '' , co nf igVersio n : 1 , co nf igTerm : 1 }, { _id : 3 , na me : 'mo n go 3 : 27017 ' , heal t h : 1 , s tate : 2 , s tate S tr : 'SECONDARY' , up t ime : 102 , op t ime : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDurable : { ts : Times ta mp( { t : 1666778070 , i : 1 } ) , t : Lo n g( \"1\" ) }, op t imeDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , op t imeDurableDa te : ISODa te ( \"2022-10-26T09:54:30.000Z\" ) , las t AppliedWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t DurableWallTime : ISODa te ( \"2022-10-26T09:54:40.457Z\" ) , las t Hear t bea t : ISODa te ( \"2022-10-26T09:54:40.322Z\" ) , las t Hear t bea t Recv : ISODa te ( \"2022-10-26T09:54:41.326Z\" ) , pi n gMs : Lo n g( \"0\" ) , las t Hear t bea t Message : '' , sy n cSourceHos t : 'mo n go 1 : 27017 ' , sy n cSourceId : 1 , i nf oMessage : '' , co nf igVersio n : 1 , co nf igTerm : 1 } ], ok : 1 , '$clus ter Time' : { clus ter Time : Times ta mp( { t : 1666778080 , i : 1 } ) , sig nature : { hash : Bi nar y(Bu ffer . fr om( \"0000000000000000000000000000000000000000\" , \"hex\" ) , 0 ) , keyId : Lo n g( \"0\" ) } }, opera t io n Time : Times ta mp( { t : 1666778080 , i : 1 } ) } Una vez que ya hemos arrancando todo, podemos conectarnos a nuestro conjunto de r\u00e9plica mediante mongosh (si no le pasamos ning\u00fan par\u00e1metro, se conecta autom\u00e1ticamente a localhost y al puerto 27017). Dentro del shell, los comandos que trabajan con r\u00e9plicas comienzan por el prefijo rs . Por ejemplo, mediante rs.help() obtendremos la ayuda de los m\u00e9todos disponibles: > mongosh iabdrs [ direct: primary ] test> rs.help () Replica Set Class: initiate Initiates the replica set. config Returns a document that contains the current replica set configuration. conf Calls replSetConfig reconfig Reconfigures an existing replica set, overwriting the existing replica set configuration. reconfigForPSASet Reconfigures an existing replica set, overwriting the existing replica set configuration, if the reconfiguration is a transition from a Primary-Arbiter to a Primary-Secondary-Arbiter set. status Calls replSetGetStatus isMaster Calls isMaster hello Calls hello printSecondaryReplicationInfo Calls db.printSecondaryReplicationInfo printSlaveReplicationInfo DEPRECATED. Use rs.printSecondaryReplicationInfo printReplicationInfo Calls db.printReplicationInfo add Adds replica set member to replica set. addArb Calls rs.add with arbiterOnly = true remove Removes a replica set member. freeze Prevents the current member from seeking election as primary for a period of time. Uses the replSetFreeze command stepDown Causes the current primary to become a secondary which forces an election. If no stepDownSecs is provided, uses 60 seconds. Uses the replSetStepDown command syncFrom Sets the member that this replica set member will sync from, overriding the default sync target selection logic. secondaryOk This method is deprecated. Use db.getMongo () .setReadPref () instead For more information on usage: https://docs.mongodb.com/manual/reference/method/js-replication/ La pr\u00f3xima vez que lancemos las r\u00e9plicas ya no deberemos configurarlas. As\u00ed pues, el proceso de enlazar e iniciar las r\u00e9plicas s\u00f3lo se realiza una vez.","title":"Creando un conjunto de r\u00e9plicas"},{"location":"sa/06replicacion.html#trabajando-con-las-replicas","text":"Una vez que hemos visto que las tres r\u00e9plicas est\u00e1n funcionando, vamos a comprobar c\u00f3mo podemos trabajar con ellas. Ya hemos comprobado c\u00f3mo al conectarnos al cl\u00faster, nos aparece como s\u00edmbolo del shell el nombre del conjunto de la r\u00e9plica seguido de dos puntos y primary si nos hemos conectado al nodo principal, o secondary en caso contrario. iabdrs [ direct: primary ] test> Para saber si nos hemos conectado al nodo correcto, mediante db.hello() obtendremos el tipo del nodo (propiedad isWritablePrimary ) e informaci\u00f3n sobre el resto de nodos (antes se utilizaba el m\u00e9todo isMaster el cual se ha marcado como deprecated ): iabdrs [ direct : primary ] test > db . hello () { topologyVersion : { processId : ObjectId ( \"635a9dbbc752fabab79400d6\" ), counter : Long ( \"6\" ) }, hosts : [ 'mongo1:27017' , 'mongo2:27017' , 'mongo3:27017' ], setName : 'iabdrs' , setVersion : 1 , isWritablePrimary : true , secondary : false , primary : 'mongo1:27017' , me : 'mongo1:27017' , electionId : ObjectId ( \"7fffffff0000000000000001\" ), lastWrite : { opTime : { ts : Timestamp ({ t : 1666883479 , i : 1 }), t : Long ( \"1\" ) }, lastWriteDate : ISODate ( \"2022-10-27T15:11:19.000Z\" ), majorityOpTime : { ts : Timestamp ({ t : 1666883479 , i : 1 }), t : Long ( \"1\" ) }, majorityWriteDate : ISODate ( \"2022-10-27T15:11:19.000Z\" ) }, maxBsonObjectSize : 16777216 , maxMessageSizeBytes : 48000000 , maxWriteBatchSize : 100000 , localTime : ISODate ( \"2022-10-27T15:11:20.366Z\" ), logicalSessionTimeoutMinutes : 30 , connectionId : 48 , minWireVersion : 0 , maxWireVersion : 17 , readOnly : false , ok : 1 , '$clusterTime' : { clusterTime : Timestamp ({ t : 1666883479 , i : 1 }), signature : { hash : Binary ( Buffer . from ( \"0000000000000000000000000000000000000000\" , \"hex\" ), 0 ), keyId : Long ( \"0\" ) } }, operationTime : Timestamp ({ t : 1666883479 , i : 1 }) } Ahora que sabemos que estamos en el nodo principal, vamos a a\u00f1adir datos. Para ello, vamos a insertar 1.000 documentos en la colecci\u00f3n pruebas : for ( i = 0 ; i < 1000 ; i ++ ) { db . pruebas . insertOne ({ num : i }) } Estos 1.000 documentos se han insertado en el nodo principal, y se han replicado a los secundarios. Para comprobar la replicaci\u00f3n, abrimos un nuevo terminal y nos conectamos a un nodo secundario: $ mongosh --port 27018 iabdrs [ direct: secondary ] test> Si desde el nodo secundario intentamos consultar el total de documentos de la colecci\u00f3n obtendremos un error: iabdrs [ direct: secondary ] test> db.pruebas.countDocuments () MongoServerError: not primary and secondaryOk = false - consider using db.getMongo () .setReadPref () or readPreference in the connection string El error indica que no somos un nodo primario y por lo tanto no podemos leer de \u00e9l. Para permitir lecturas en los nodos secundarios, mediante db.getMongo().setReadPref('secondary') le decimos a mongosh que sabemos que nos hemos conectado a un secundario y admitimos la posibilidad de obtener datos obsoletos. iabdrs [ direct: secondary ] test> db.getMongo () .setReadPref ( 'secondary' ) iabdrs [ direct: secondary ] test> db.pruebas.countDocuments () 1000 Pero que podamos leer no significa que podamos escribir. Si intentamos escribir en un nodo secundario obtendremos un error: iabdrs [ direct: secondary ] test> db.pruebas.insertOne ({ num : 1001 }) MongoServerError: not primary","title":"Trabajando con las r\u00e9plicas"},{"location":"sa/06replicacion.html#preferencias-de-lectura","text":"En el ejemplo anterior hemos visto c\u00f3mo hemos cambiado las preferencias de lectura para permitir hacerlo desde los nodos secundarios. As\u00ed pues, las preferencias de lectura definen el modo en el que MongoDB enruta las lecturas realizadas a los miembros del conjunto de r\u00e9plicas. Preferencias de lectura Por defecto, las aplicaciones dirigen las operaciones de consulta al miembro principal (con el modo de lectura primary , el cual es el modo por defecto). Pero los clientes puede indicar otras preferencias: primaryPreferred : si est\u00e1 disponible, las lecturas se realizan en el nodo primario, pero si no estuviera en pie, habilita las lecturas de los secundarios. secondary : todas las operaciones de lectura se realizan en nodos secundarios. secondaryPreferred : primero prueba con los secundarios, y si no hay ninguno disponible, la lectura la realiza del primario. nearest : las lecturas se realizan del nodo m\u00e1s cercano en base a la latencia, independientemente que el nodo sea primario o secundario. Para indicar la preferencia de lectura, ya hemos visto que lo haremos con la funci\u00f3n Mongo.setReadPref() : iabdrs [ direct: secondary ] test> db.getMongo () .setReadPref ( 'nearest' )","title":"Preferencias de lectura"},{"location":"sa/06replicacion.html#consistencia-en-la-escritura","text":"Por defecto, tanto las lecturas como las escrituras se realizan de manera predeterminada en el nodo principal. Las aplicaciones pueden decidir que las escrituras vayan al nodo primario pero las lecturas al secundario. Esto puede provocar que haya lecturas caducadas, con datos obsoletos. Este hecho, que en sistemas relacionales es inadmisible, en sistemas NoSQL ofrece como beneficio que podamos escalar el sistema. La replicaci\u00f3n es un proceso as\u00edncrono. En el per\u00edodo de tiempo en el que el sistema de votaci\u00f3n sucede, no se completa ninguna escritura. MongoDB garantiza la consistencia en la escritura (Write Concern) , lo que implica que sea un sistema consistente. Para ello, ofrece un mecanismo que garantiza que una escritura ha sido exitosa. Dependiendo del nivel de configuraci\u00f3n de la consistencia, las inserciones, modificaciones y borrados pueden tardar m\u00e1s o menos. Si reducimos el nivel de consistencia, el rendimiento ser\u00e1 mejor, a costa de poder obtener datos obsoletos u perder datos que no se han terminado de serializar en disco. Con un nivel de consistencia m\u00e1s alto, los clientes esperan tras enviar una operaci\u00f3n de escritura a que MongoDB les confirme la operaci\u00f3n. Los valores que podemos configurar se realizan mediante las siguientes opciones: w : indica el n\u00famero de servidores que se han de replicar para que la inserci\u00f3n devuelva un ACK. j : indica si las escrituras se tienen que trasladar a un diario de bit\u00e1cora ( journal ) wtimeout : indica el l\u00edmite de tiempo a esperar como m\u00e1ximo, para prevenir que una escritura se bloquee indefinidamente.","title":"Consistencia en la escritura"},{"location":"sa/06replicacion.html#consistencia-en-la-lectura","text":"De igual manera que podemos decidir en cuantos nodos se deben propagar las escrituras, podemos indicar cuantos nodos debemos leer para dar un dato como v\u00e1lido. Para ello, MongoDB da soporte a diferentes niveles de consistencia en la lectura (Read Concern) : local : devuelve el dato m\u00e1s reciente en el cluster. Cualquier dato que haya sido escrito en el nodo primario puede ser elegido para devolverse. Sin embargo, no se garantiza que este dato sea replicado a los miembros del conjunto en caso de fallo. Este es el nivel por defecto en las operaciones de lectura contra el nodo primario. available : equivalente de local cuando las operaciones de lectura se efect\u00faan contra un nodo secundario. majority : \u00fanicamente devuelve datos que hayan sido confirmados en una mayor\u00eda de nodos dentro del conjunto. linearizable : devuelve datos que hayan sido confirmados por una mayor\u00eda de nodos, pero permite al desarrollador establecer su propia funcionalidad. snapshot : s\u00f3lo disponible para transacciones multi-documento, realiza una \"foto\" de los datos al inicio de la transacci\u00f3n. Imaginemos un escenario en el cual recibimos la confirmaci\u00f3n de escritura desde el nodo primario. Inmediatamente efectuamos una operaci\u00f3n de lectura, el nodo devuelve el dato, pero \u00e9ste nodo falla antes de replicar la operaci\u00f3n de escritura a los nodos secundarios. Sobre esta operaci\u00f3n se efectuar\u00e1 un proceso de rollback en el momento que el nodo primario vuelva a estar disponible, por lo que ese dato realmente no existir\u00e1 en el conjunto replicado. Es decir, la aplicaci\u00f3n actualmente tiene un dato que no existe en el conjunto de r\u00e9plicas. Mediante la consistencia en la lectura, podemos obtener unas m\u00ednimas garant\u00edas de que el dato que estamos leyendo es correcto y durable. Cuando se utiliza, \u00fanicamente devolver\u00e1 datos cuya grabaci\u00f3n haya sido confirmada por el n\u00famero de nodos especificados en sus opciones. Se puede escoger entre devolver el dato m\u00e1s reciente que exista en el cl\u00faster, o el dato recibido por una mayor\u00eda de miembros en el cluster. El hecho de que un documento no se considere correcto, no quiere decir necesariamente que se haya perdido, sino que en el momento de su lectura, no ha cumplido las condiciones necesarias de durabilidad para ser devuelto. Puede ser que la lectura se est\u00e9 produciendo antes de que el dato haya sido propagado al n\u00famero m\u00ednimo de nodos necesario, y por eso no se obtenga, pero en lecturas sucesivas s\u00ed pueda aparecer.","title":"Consistencia en la lectura"},{"location":"sa/06replicacion.html#tolerancia-a-fallos","text":"Cuando un nodo primario no se comunica con otros miembros del conjunto durante m\u00e1s de 10 segundos, el conjunto de r\u00e9plicas intentar\u00e1, de entre los secundarios, que otro miembro se convierta en el nuevo primario. Para ello se realiza un proceso de votaci\u00f3n, de modo que el nodo que obtenga el mayor n\u00famero de votos se erigir\u00e1 en primario. Este proceso de votaci\u00f3n se realiza bastante r\u00e1pido (menos de 3 segundos), durante el cual no existe ning\u00fan nodo primario y por tanto la r\u00e9plica no acepta escrituras y todos los miembros se convierten en nodos de s\u00f3lo-lectura. Elecci\u00f3n de un nuevo primario","title":"Tolerancia a fallos"},{"location":"sa/06replicacion.html#particionado","text":"Ya vimos en la primera sesi\u00f3n que dentro del entorno de las bases de datos, particionar consiste en dividir los datos entre m\u00faltiples m\u00e1quinas. Al poner un subconjunto de los datos en cada m\u00e1quina, vamos a poder almacenar m\u00e1s informaci\u00f3n y soportar m\u00e1s carga sin necesidad de m\u00e1quinas m\u00e1s potentes, sino una mayor cantidad de m\u00e1quinas m\u00e1s modestas (y mucho m\u00e1s baratas). El Sharding es una t\u00e9cnica que fragmenta los datos de la base de datos horizontalmente agrup\u00e1ndolos de alg\u00fan modo que tenga sentido y que permita un direccionamiento m\u00e1s r\u00e1pido. Sharding Por lo tanto, estos shards (fragmentos) pueden estar localizados en diferentes bases de datos y localizaciones f\u00edsicas. El Sharding no tiene por qu\u00e9 estar basado \u00fanicamente en una colecci\u00f3n y un campo, puede ser a nivel de todas las colecciones. Por ejemplo podr\u00edamos decir \" todos los datos de usuarios cuyo perfil est\u00e9 en los Estados Unidos los redirigimos a la base de datos del servidor en Estados Unidos, y todos los de Asia van a la base de datos de Asia \". MongoDB implementa el sharding de forma nativa y autom\u00e1tica (de ah\u00ed el t\u00e9rmino de auto-sharding ), siguiendo un enfoque basado en rangos. Para ello, divide una colecci\u00f3n entre diferentes servidores, utilizando mongos como router de las peticiones entre los sharded clusters . Esto favorece que el desarrollador ignore que la aplicaci\u00f3n no se comunica con un \u00fanico servidor, balanceando de manera autom\u00e1tica los datos y permitiendo incrementar o reducir la capacidad del sistema a conveniencia. Paciencia... Antes de plantearse hacer auto-sharding sobre nuestros datos, es conveniente dominar c\u00f3mo se trabaja con MongoDB y el uso de conjuntos de r\u00e9plica.","title":"Particionado"},{"location":"sa/06replicacion.html#sharded-cluster","text":"El particionado de MongoDB permite crear un cluster de muchas m\u00e1quinas, dividiendo a nivel de colecci\u00f3n y poniendo un subconjunto de los datos de la colecci\u00f3n en cada uno de los fragmentos. Los componentes de un sharded cluster son: Shards (Fragmentos): Cada una de las m\u00e1quinas del cluster, que almacena un subconjunto de los datos de la colecci\u00f3n. Cada shard es una instancia de mongod o un conjunto de r\u00e9plicas. En un entorno de producci\u00f3n, todos los shards son conjuntos de r\u00e9plica. Servidores de Configuraci\u00f3n : Cada servidor de configuraci\u00f3n es una instancia de mongod que almacena metadatos sobre el cluster. Los metadatos mapean los trozos con los shards, definiendo qu\u00e9 rangos de datos definen un trozo ( chunk ) de la colecci\u00f3n, y qu\u00e9 trozos se encuentran en un determinado shard . En entornos de producci\u00f3n se aconseja tener 3 servidores de configuraci\u00f3n (uno primario y dos secundarios) ya que si s\u00f3lo tuvi\u00e9semos uno, al producirse una ca\u00edda el cluster quedar\u00eda inaccesible. Enrutadores : Cada router es una instancia mongos que enruta las lecturas y escrituras de las aplicaciones a los shards . Las aplicaciones no acceden directamente a los shards , sino al router. Estos enrutadores funcionan de manera similar a una tabla de contenidos, indic\u00e1ndonos d\u00f3nde se encuentran los datos. Una vez recopilados los datos de los diferentes shards , se fusionan y se encarga de devolverlos a la aplicaci\u00f3n. En entornos de producci\u00f3n es com\u00fan tener varios routers para balancear la carga de los clientes. Componentes de un Sharded cluster Autoevaluaci\u00f3n Supongamos que queremos ejecutar m\u00faltiples routers mongos para soportar la redundancia. \u00bfQu\u00e9 elemento asegurar\u00e1 la tolerancia a fallos y cambiar\u00e1 de un mongos a otro dentro de tu aplicaci\u00f3n? 1 mongod mongos Driver Los servidores de configuraci\u00f3n de sharding","title":"Sharded Cluster"},{"location":"sa/06replicacion.html#shard-key","text":"Para que MongoDB sepa c\u00f3mo dividir una colecci\u00f3n entre rangos no solapados hay que elegir una shard key , normalmente el identificador del documento, por ejemplo, student_id . Este identificador (o su hash ) es la clave del chunk (por lo hace la misma funci\u00f3n que una clave primaria). La shard key puede ser un campo sencillo o compuesto el cual debe estar indexado, y que va a determinar la distribuci\u00f3n de los documentos entre los fragmentos del cl\u00faster. Fragmentaci\u00f3n por la shard-key Para las b\u00fasquedas, borrados y actualizaciones, al emplear la shard key , mongos sabe a que shard enviar la petici\u00f3n. En cambio, si la operaci\u00f3n no la indica, se har\u00e1 un broadcast a todas los shards para averiguar donde se encuentra. Entre los aspectos a tener en cuenta a la hora de elegir una shard key cabe destacar que debe: Tener una alta cardinalidad, para asegurar que los documentos puedan dividirse en los distintos fragmentos. Por ejemplo, si elegimos un shard key que solo tiene 3 valores posibles y tenemos 5 fragmentos, MongoDB no sabr\u00e1 como separar los documentos en los 5 fragmentos. Cuantos m\u00e1s valores posibles pueda tener la clave de fragmentaci\u00f3n, m\u00e1s eficiente ser\u00e1 la divisi\u00f3n de los trozos entre los fragmentos disponibles. Tener un alto nivel de aleatoriedad. Si utilizamos una clave que siga un patr\u00f3n incremental como una fecha o un ID, conllevar\u00e1 que al insertar documentos, el mismo fragmento estar\u00e1 siendo utilizando constantemente durante el rango de valores definido para \u00e9l. Esto provoca que los datos est\u00e9n separados de una manera \u00f3ptima, pero pondr\u00e1 siempre bajo estr\u00e9s a un fragmento en per\u00edodos de tiempo mientras que los otros posiblemente queden con muy poca actividad (comportamiento conocido como hotspotting ). Una soluci\u00f3n a las claves que siguen patrones incrementales es aplicar una funci\u00f3n hash y crear una clave hasheada que si tiene un alto nivel de aleatoriedad. Considerar los patrones de las consultas, ya que si elegimos una buena clave, al realizar consultas por la clave, todos los datos se encontrar\u00e1n en el mismo fragmento.","title":"Shard key"},{"location":"sa/06replicacion.html#particionado-con-docker","text":"Para este caso, vamos a crear dos conjuntos de r\u00e9plicas de dos nodos cada una, y a su vez, particionaremos los datos en dos shards . Adem\u00e1s, vamos a a\u00f1adir un \u00fanico router y un servidor de configuraci\u00f3n (aunque lo ideal ser\u00eda crear un conjunto de r\u00e9plicas de servidores de configuraci\u00f3n). As\u00ed pues, crearemos: un contenedor para el router ( router1 ) el cual ejecuta el servicio mongos y que va a conectar con el servidor de configuraci\u00f3n. un contenedor para el servidor de configuraci\u00f3n ( configsvr1 ) indic\u00e1ndole mediante el par\u00e1metro --configsvr su prop\u00f3sito, as\u00ed como la r\u00e9plica a la que pertenece (todo servidor de configuraci\u00f3n debe pertenecer a una r\u00e9plica, aunque en nuestro caso s\u00f3lo hemos creado uno) dos nodos ( mongo-shard1a y mongo-shard1b ) para el primer shard (par\u00e1metro --shardsvr ) que pertenecen al conjunto de r\u00e9plicas iabdshard1 . dos nodos m\u00e1s ( mongo-shard2a y mongo-shard2b ) para el segundo shard (par\u00e1metro --shardsvr ) que pertenecen al conjunto de r\u00e9plicas iabdshard2 . Para ello, hemos definido el archivo docker-compose-replicaset-sharded.yml con la definici\u00f3n de los contenedores: docker-compose-replicaset-sharded.yml services : router1 : container_name : router1 image : mongo volumes : - ./router-init.js:/scripts/router-init.js networks : - mongo-network-sharded ports : - 27117:27017 entrypoint : [ \"/usr/bin/mongos\" , \"--port\" , \"27017\" , \"--configdb\" , \"rs-config-server/configsvr1:27017\" , \"--bind_ip_all\" ] configsvr1 : container_name : configsvr1 image : mongo volumes : - ./configserver-init.js:/scripts/configserver-init.js networks : - mongo-network-sharded ports : - 27118:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--configsvr\" , \"--replSet\" , \"rs-config-server\" , \"--bind_ip_all\" ] links : - mongo-shard1a - mongo-shard2a mongo-shard1a : container_name : mongo-shard1a image : mongo volumes : - ./shard1-init.js:/scripts/shard1-init.js networks : - mongo-network-sharded ports : - 27119:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard1\" ] mongo-shard1b : container_name : mongo-shard1b image : mongo networks : - mongo-network-sharded ports : - 27120:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard1\" ] mongo-shard2a : container_name : mongo-shard2a image : mongo volumes : - ./shard2-init.js:/scripts/shard2-init.js networks : - mongo-network-sharded ports : - 27121:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard2\" ] mongo-shard2b : container_name : mongo-shard2b image : mongo networks : - mongo-network-sharded ports : - 27122:27017 entrypoint : [ \"/usr/bin/mongod\" , \"--port\" , \"27017\" , \"--shardsvr\" , \"--bind_ip_all\" , \"--replSet\" , \"iabdshard2\" ] networks : mongo-network-sharded : driver : bridge Y lanzamos los contenedores mediante docker-compose : docker-compose --file docker-compose-replicaset-sharded.yml --project-name iabd-mongodb-replica-sharded up -d","title":"Particionado con Docker"},{"location":"sa/06replicacion.html#conexion-al-router","text":"El router es un nodo de MongoDB que se va a encargar de aceptar las peticiones de los clientes y enrutarlas al shard adecuado. Para ello, podemos: Conectarnos al contenedor y abrimos un terminal y veremos como en el prompt aparece mongos : docker exec -it router1 bash mongosh [ direct: mongos ] test> O abrir directamente un sesi\u00f3n al puerto 27117, que es donde hab\u00edamos colocado nuestro router: mongosh 127 .0.0.1:27117 [ direct: mongos ] test> Si comprobamos el estado del shard veremos que tenemos dos shards , cada uno replicado en dos nodos, con sus identificadores y hosts : [ direct : mongos ] test > sh . status () shardingVersion { _id : 1 , minCompatibleVersion : 5 , currentVersion : 6 , clusterId : ObjectId ( \"6363af7bfab6e20f2a7c1409\" ) } --- shards [ { _id : 'iabdshard1' , host : 'iabdshard1/mongo-shard1a:27017,mongo-shard1b:27017' , state : 1 , topologyTime : Timestamp ({ t : 1667477424 , i : 6 }) }, { _id : 'iabdshard2' , host : 'iabdshard2/mongo-shard2a:27017,mongo-shard2b:27017' , state : 1 , topologyTime : Timestamp ({ t : 1667477424 , i : 12 }) } ] ... En un entorno de producci\u00f3n, en vez de tener dos shards en dos nodos, habr\u00e1 un conjunto de r\u00e9plicas para asegurar la alta disponibilidad. Adem\u00e1s, tendremos tres servidores de configuraci\u00f3n para asegurar la disponibilidad de \u00e9stos. Del mismo modo, habr\u00e1 tantos procesos mongos creados como conexiones de clientes: Sharding en un entorno de Producci\u00f3n","title":"Conexi\u00f3n al router"},{"location":"sa/06replicacion.html#habilitando-el-sharding","text":"Una vez hemos creado la estructura necesaria para soportar el particionado vamos a insertar un conjunto de datos para posteriormente particionarlos. Para ello, vamos a insertar diez mil usuarios en una colecci\u00f3n: [ direct : mongos ] test > use iabd switched to db iabd [ direct : mongos ] iabd > for ( var i = 0 ; i < 10000 ; i ++ ) { db . usuarios . insertOne ({ \"login\" : \"usuario\" + i , \"nombre\" : \"nombre\" + i * 2 , \"fcreacion\" : new Date ()}, { writeConcern : { w : \"0\" }}); } Una vez creados, comprobamos que se han insertado: [ direct : mongos ] iabd > db . usuarios . countDocuments () 10000 [ direct : mongos ] iabd > db . usuarios . findOne () { _id : ObjectId ( \"6363edb79f30da8391b700c4\" ), login : 'usuario0' , nombre : 'nombre0' , fcreacion : ISODate ( \"2022-11-03T16:35:03.184Z\" ) } Como podemos observar, interactuar con mongos es igual a hacerlo con mongosh . Ahora mismo no sabemos en cu\u00e1l de los dos shards se han almacenado los datos. Adem\u00e1s, estos datos no est\u00e1n particionados, es decir residen en s\u00f3lo uno de los shards. Para habilitar el sharding a nivel de base de datos y que los datos se repartan entre los fragmentos disponibles, ejecutaremos el comando sh.enableSharding(<nombreDB>) : [ direct : mongos ] iabd > sh . enableSharding ( \"iabd\" ) { ok : 1 , '$clusterTime' : { clusterTime : Timestamp ({ t : 1667493383 , i : 1 }), signature : { hash : Binary ( Buffer . from ( \"0000000000000000000000000000000000000000\" , \"hex\" ), 0 ), keyId : Long ( \"0\" ) } }, operationTime : Timestamp ({ t : 1667480498 , i : 1 }) } Si volvemos a comprobar el estado del shard , tenemos que se ha creado la nueva base de datos y nos indica cual es su fragmento primario. [ direct : mongos ] iabd > sh . status () ... --- databases [ { database : { _id : 'config' , primary : 'config' , partitioned : true }, collections : { 'config.system.sessions' : { shardKey : { _id : 1 }, unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 512 }, { shard : 'iabdshard2' , nChunks : 512 } ], chunks : [ 'too many chunks to print, use verbose if you want to force print' ], tags : [] } } }, { database : { _id : 'iabd' , primary : 'iabdshard1' , partitioned : false , version : { uuid : new UUID ( \"84bce0ad-38a4-4b72-a377-350272542657\" ), timestamp : Timestamp ({ t : 1667493302 , i : 1 }), lastMod : 1 } }, collections : {} } ] Antes de habilitar el sharding para una determinada colecci\u00f3n, tenemos que crear un \u00edndice sobre la shard key (si la colecci\u00f3n estuviera vac\u00eda, no necesitamos crear el \u00edndice, ya que al indicar la shard key , MongoDB autom\u00e1ticamente crear\u00e1 el \u00edndice por nosotros): db . usuarios . createIndex ({ \"login\" : 1 }) Una vez habilitado el shard ya podemos fragmentar la colecci\u00f3n: sh . shardCollection ( \"iabd.usuarios\" , { \"login\" : 1 }, false ) El m\u00e9todo shardCollection particiona una colecci\u00f3n a partir de una shard key , ya sea mediante claves hashes o utilizando rangos. En nuestro caso, al indicarle {\"login\": \"hashed\"} particionar\u00e1 los datos reparti\u00e9ndolos de manera equitativa entre los fragmentos. Para ello, recibe tres par\u00e1metros: nombre de la colecci\u00f3n, con nomenclatura de nombreBD.nombreColecci\u00f3n nombre del campo para fragmentar la colecci\u00f3n, es decir, el shard key . Uno de los requisitos es que esta clave tenga una alta cardinalidad. Si quisi\u00e9ramos indicar que queremos utilizar una clave hasheada , lo har\u00edamos indicando como valor hashed , por ejemplo, {\"login\": \"hashed\"} . Si queremos utilizar rangos, lo indicamos con valor 1 , por ejemplo, {\"login\": 1} . booleano que indica si el valor utilizado como shard key es \u00fanico. Para ello, el \u00edndice que se crea sobre el campo debe ser del tipo unique . Este comando divide la colecci\u00f3n en fragmentos ( chunks ), la cual es la unidad que utiliza MongoDB para mover los datos. Una vez que se ha ejecutado, MongoDB comenzar\u00e1 a balancear la colecci\u00f3n entre los shards del cluster. Este proceso no es instant\u00e1neo. Si la colecci\u00f3n contiene un gran conjunto de datos puede llevar horas completar el balanceo. Si ahora volvemos a comprobar el estado del shard obtendremos: [ direct : mongos ] iabd > sh . status () ... databases [ { database : { _id : 'config' , primary : 'config' , partitioned : true }, ... }, { database : { _id : 'iabd' , primary : 'iabdshard1' , partitioned : false , version : { uuid : new UUID ( \"9e694228-600c-4a50-b11c-174f846a3c64\" ), timestamp : Timestamp ({ t : 1667489404 , i : 1 }), lastMod : 1 } }, collections : { 'iabd.usuarios' : { shardKey : { login : 'hashed' }, // (1)! unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 1 } ], chunks : [ // (2)! { min : { login : MinKey () }, // (3)! max : { login : () }, 'on shard' : 'iabdshard1' , 'last modified' : Timestamp ({ t : 1 , i : 0 }) } ], tags : [] } } } ] Muestra que la shard key se basa en rangos la propiedad chunks muestra la cantidad de trozos que alberga cada partici\u00f3n. As\u00ed, pues en este momento tenemos un \u00fanico chunk Para cada uno de los fragmentos se muestra el rango de valores que alberga cada chunk, as\u00ed como en que shard se ubica. Las funciones MinKey() y MinKey() son similares a menos infinito y m\u00e1s infinito, es decir, no hay ning\u00fan valor por debajo ni por encima de ellos. Es decir, indican los topes de la colecci\u00f3n.","title":"Habilitando el Sharding"},{"location":"sa/06replicacion.html#trabajando-con-particiones","text":"En este momento, el shard est\u00e1 creado pero todos los nodos residen en un \u00fanico fragmento dentro de una partici\u00f3n. Para obtener esta informaci\u00f3n, podemos ver el estado del sharding o consultar la distribuci\u00f3n de una colecci\u00f3n mediante el m\u00e9todo getShardDistribution : [ direct : mongos ] iabd > db . usuarios . getShardDistribution () Shard iabdshard1 at iabdshard1 / mongo - shard1a : 27017 , mongo - shard1b : 27017 { data : '852KiB' , docs : 10000 , chunks : 1 , 'estimated data per chunk' : '852KiB' , 'estimated docs per chunk' : 10000 } --- Totals { data : '852KiB' , docs : 10000 , chunks : 1 , 'Shard iabdshard1' : [ '100 % data' , '100 % docs in cluster' , '87B avg obj size on shard' ] } Vamos a volver a insertar 10.000 usuarios m\u00e1s a ver qu\u00e9 sucede: [ direct : mongos ] iabd > for ( var i = 10000 ; i < 20000 ; i ++ ) { db . usuarios . insertOne ({ \"login\" : \"usuario\" + i , \"nombre\" : \"nombre\" + i * 2 , \"fcreacion\" : new Date ()}, { writeConcern : { w : \"0\" }}); } [ direct : mongos ] iabd > db . usuarios . countDocuments () 20000 Forzando el split Si al insertar m\u00e1s datos no se reparten los datos de forma autom\u00e1tica, podemos forzarlo mediante las operaciones sh.splitAt , donde le indicamos el valor donde el fragmento en dos, y sh.splitFind que realiza la divisi\u00f3n por la mediana, de manera que ambos fragmentos deber\u00edan ser semejantes. Por ejemplo: sh . splitFind ( \"iabd.usuarios\" , { \"login\" : 1 }) Si ahora comprobamos el estado del shard , los datos se deber\u00edan haber repartido entre los shards disponibles: mongos > sh . status () { ... collections : { 'iabd.usuarios' : { shardKey : { login : 1 }, unique : false , balancing : true , chunkMetadata : [ { shard : 'iabdshard1' , nChunks : 1 }, { shard : 'iabdshard2' , nChunks : 1 } ], chunks : [ { min : { login : MinKey () }, max : { login : 'usuario18999' }, 'on shard' : 'iabdshard2' , 'last modified' : Timestamp ({ t : 2 , i : 0 }) }, { min : { login : 'usuario18999' }, max : { login : MaxKey () }, 'on shard' : 'iabdshard1' , 'last modified' : Timestamp ({ t : 2 , i : 1 }) } ], tags : [] } } } ] Si volvemos a comprobar la distribuci\u00f3n, ahora vemos como ha repartido los documentos: [ direct : mongos ] iabd > db . usuarios . getShardDistribution () Shard iabdshard1 at iabdshard1 / mongo - shard1a : 27017 , mongo - shard1b : 27017 { data : '1.68MiB' , docs : 20000 , chunks : 1 , 'estimated data per chunk' : '1.68MiB' , 'estimated docs per chunk' : 20000 } --- Shard iabdshard2 at iabdshard2 / mongo - shard2a : 27017 , mongo - shard2b : 27017 { data : '866KiB' , docs : 10000 , chunks : 1 , 'estimated data per chunk' : '866KiB' , 'estimated docs per chunk' : 10000 } --- Totals { data : '2.52MiB' , docs : 30000 , chunks : 2 , 'Shard iabdshard1' : [ '66.51 % data' , '66.66 % docs in cluster' , '88B avg obj size on shard' ], 'Shard iabdshard2' : [ '33.48 % data' , '33.33 % docs in cluster' , '88B avg obj size on shard' ] }","title":"Trabajando con particiones"},{"location":"sa/06replicacion.html#referencias","text":"Replicaci\u00f3n en MongoDB Particionado en MongoDB How to deploy a MongoDB replica set using docker-compose Demo MongoDB (6.0.1) Sharded Cluster with Docker Compose Ejemplos de c\u00f3digo sobre replicaci\u00f3n y particionado .","title":"Referencias"},{"location":"sa/06replicacion.html#actividades","text":"( RA5075.2 / CE5.2b , CE5.2c / 2p) Se pide crear una conjunto de 4 r\u00e9plicas de nombre iabdrs4 en la cual insertaremos los datos de 1000 ciudades , los cuales deber\u00e1s importar a una base de datos . Una vez creado, se pide: Obtener el estado del conjunto de r\u00e9plicas Consultar una ciudad en un nodo secundario. Habilitar las lecturas en los nodos secundarios. Volver a consultar la ciudad en el nodo secundario. Insertar una ciudad en el nodo secundario. Degradar el nodo primario Averiguar cual es el nuevo nodo primario. Adjunta un documento con los scripts de creaci\u00f3n, comandos empleados y las salidas generadas. ( RA5075.2 / CE5.2b / 2p) Se pide crear un cl\u00faster de MongoDB con tres nodos y tres particiones y volver a importar las 1000 ciudades . Una vez creado, se pide: Particionar los datos por el nombre de la ciudad. Una vez cargado los datos, obtener el estado del sharding . Si los datos no est\u00e1n particionados, forzar el split de los mismos. Tras ello, vaciar la colecci\u00f3n y volver a importar los datos. Una vez importados, obtener de nuevo el estado del sharding. Adjunta un documento con los scripts de creaci\u00f3n, los comandos empleados y las salidas generadas. El driver se encarga de manera transparente de conectar al router adecuado, y cambiar un router por otro si al que estamos conectado se cae \u21a9","title":"Actividades"},{"location":"sa/07pymongo.html","text":"Para acceder a MongoDB desde Python nos vamos a centrar en la librer\u00eda PyMongo . Para instalar la librer\u00eda mediante pip usaremos el comando (recuerda hacerlo dentro de un entorno virtual): pip install pymongo Se recomienda consultar la documentaci\u00f3n o el API para cualquier duda o aclaraci\u00f3n. Versi\u00f3n En el momento de escribir los apuntes, estamos utilizando la versi\u00f3n 4.3.2 de PyMongo. Hola PyMongo \u00b6 Un ejemplo b\u00e1sico podr\u00eda ser similar a: from pymongo import MongoClient cliente = MongoClient ( 'mongodb://localhost:27017' ) iabd_db = cliente . iabd # Recuperamos las colecciones print ( iabd_db . list_collection_names ()) people_coll = iabd_db . people # Recuperamos una persona persona = people_coll . find_one () print ( persona ) MongoClient \u00b6 A partir de la URI de conexi\u00f3n a MongoDB, hemos de instanciar la clase MongoClient , ya sea pas\u00e1ndole una URI con la cadena de conexi\u00f3n, o mediante par\u00e1metros: uri = \"mongodb+srv://usuario:contrasenya@host\" cliente = MongoClient ( uri ) Par\u00e1metros adicionales A la hora de crear el cliente, tambi\u00e9n podemos indicarle diferentes opciones de configuraci\u00f3n: cliente200Retry = MongoClient ( uri , connectTimeoutMS = 200 , retryWrites = True ) Realmente, al crear la conexi\u00f3n se inicializa un pool de conexiones, de manera que se crean 100 conexiones, y en vez de crear y destruir una conexi\u00f3n por cada petici\u00f3n, se reutilizan, de manera que cada petici\u00f3n asigna y libera una conexi\u00f3n conforme necesidad. Por defecto, el tama\u00f1o del pool es de 100 conexiones. Pool de conexiones Podemos obtener informaci\u00f3n de la conexi\u00f3n mediante la propiedad state . Por ejemplo, en nuestro caso, nos hemos conectado a MongoAtlas y de la salida del estado podemos ver los diferentes hosts que forman parte del conjunto de r\u00e9plicas: cliente = MongoClient ( 'mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/' ) print ( cliente . state ) # Database(MongoClient(host=['ac-opunia9-shard-00-01.dfaz5er.mongodb.net:27017', 'ac-opunia9-shard-00-02.dfaz5er.mongodb.net:27017', 'ac-opunia9-shard-00-00.dfaz5er.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='atlas-4wikkb-shard-0', tls=True), 'state') Conexi\u00f3n a un r\u00e9plica Si nos queremos conectar a un conjunto de r\u00e9plicas e indicar los nodos a los que nos podemos conectar, podemos separar los hosts con comas e indicar finalmente con un par\u00e1metro el nombre del conjunto de r\u00e9plicas: client = MongoClient ( 'mongodb://usuario1:contra1@host1:puerto1,host2:puerto2/?replicaSet=nombreReplica' ) A partir del cliente, podemos obtener informaci\u00f3n diversa. Por ejemplo, podemos recuperar un listado de las bases de datos mediante list_database_names() : print ( cliente . list_database_names ()) # ['sample_airbnb', 'sample_analytics', 'sample_geospatial', 'sample_guides', 'sample_mflix', 'sample_restaurants', 'sample_supplies', 'sample_training', 'sample_weatherdata', 'admin', 'local'] Para conectarnos a una base de datos en concreto, \u00fanicamente accederemos a ella como una propiedad del cliente: bd = cliente . sample_mflix bd = cliente [ \"sample_mflix\" ] # tb podemos acceder como si fuera un diccionario Bases de datos y colecciones Lazy Es conveniente tener en cuenta que tantos las colecciones como las bases de datos se crean y carga de forma perezosa, esto es, hasta que no realizamos una operaci\u00f3n sobre ellas, no se accede realmente a ellas. As\u00ed pues, para crear realmente una colecci\u00f3n, hemos de insertar un documento en ella. Una vez tenemos la base de datos, el siguiente paso es obtener una colecci\u00f3n: coleccion = bd . movies coleccion = bd [ \"movies\" ] Si queremos obtener el nombre de todas las colecciones usaremos el m\u00e9todo list_collection_names() : print ( bd . list_collection_names ()) [ 'users' , 'movies' , 'sessions' , 'theaters' , 'comments' ] Tolerancia a fallos! Al crear una conexi\u00f3n, conviene especificar el tama\u00f1o del pool y el timeout de las conexiones: db = MongoClient ( URI , maxPoolSize = 50 , wtimeout = 2500 )[ nombreBD ] Se recomienda siempre especificar un wtimeout , ya que cuando se realiza una escritura con mayor\u00eda de escrituras, si fallase alg\u00fan nodo no se quedar\u00eda colgado esperando. Si no encontrase el servidor, lanzar\u00e1 un ServerSelectionTimeoutError el cual deber\u00edamos capturar. Primeras consultas \u00b6 Finalmente, sobre una colecci\u00f3n ya podemos realizar consultas y otras operaciones: movies = bd . movies movies . count_documents ({}) # 23530 movies . find_one () # {'_id': ObjectId('573a1390f29313caabcd4135'), 'plot': 'Three men hammer on an anvil and pass a bottle of beer around.', 'genres': ['Short'], 'runtime': 1, 'cast': ['Charles Kayser', 'John Ott'], 'num_mflix_comments': 1, 'title': 'Blacksmith Scene', 'fullplot': 'A stationary ...', 'countries': ['USA'], 'released': datetime.datetime(1893, 5, 9, 0, 0), 'directors': ['William K.L. Dickson'], 'rated': 'UNRATED', 'awards': {'wins': 1, 'nominations': 0, 'text': '1 win.'}, 'lastupdated': '2015-08-26 00:03:50.133000000', 'year': 1893, 'imdb': {'rating': 6.2, 'votes': 1189, 'id': 5}, 'type': 'movie', 'tomatoes': {'viewer': {'rating': 3.0, 'numReviews': 184, 'meter': 32}, 'lastUpdated': datetime.datetime(2015, 6, 28, 18, 34, 9)}} Por ejemplo, podemos filtrar las pel\u00edculas cuya actriz sea Salma Hayek . Al realizar la consulta, find devuelve un cursor , el cual podemos recorrer: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) for movie in cursor : print ( movie ) { '_id' : Objec t Id(' 573 a 1399 f 29313 caabceea 6 d') , 'awards' : { ' n omi nat io ns ' : 1 , ' te x t ' : ' 1 n omi nat io n .' , 'wi ns ' : 0 }, 'cas t ' : [ 'David Arque tte ' , 'Joh n Hawkes' , 'Salma Hayek' , 'Jaso n Wiles' ], 'cou ntr ies' : [ 'USA' ], ... 'wri ters ' : [ 'Rober t Rodriguez' , 'Tommy Nix' ], 'year' : 1994 } { '_id' : Objec t Id(' 573 a 139 a f 29313 caabce f 0 b 6 ') , 'awards' : { ' n omi nat io ns ' : 14 , ' te x t ' : ' 27 wi ns & 14 n omi nat io ns .' , 'wi ns ' : 27 }, 'cas t ' : [ 'Er nest o G\u00f3mez Cruz' , 'Mar\u00eda Rojo' , 'Salma Hayek' , 'Bru n o Bichir' ], 'cou ntr ies' : [ 'Mexico' ], ... } Si queremos contar los documentos que cumplen un criterio, en vez de find , usaremos count_documents pas\u00e1ndole el criterio de filtrado: cantidad = movies . count_documents ( { \"cast\" : \"Salma Hayek\" } ) Para seleccionar los campos que queremos recuperar, necesitamos pasar un segundo par\u00e1metro con la proyecci\u00f3n: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"cast\" : 1 } ) for movie in cursor : print ( movie ) { '_id' : ObjectId ( '573a1399f29313caabceea6d' ), 'cast' : [ 'David Arquette' , 'John Hawkes' , 'Salma Hayek' , 'Jason Wiles' ], 'title' : 'Roadracers' } { '_id' : ObjectId ( '573a139af29313caabcef0b6' ), 'cast' : [ 'Ernesto G\u00f3mez Cruz' , 'Mar\u00eda Rojo' , 'Salma Hayek' , 'Bruno Bichir' ], 'title' : 'Midaq Alley' } ... Como ya vimos al hacer consultas en sesiones anterior, si no queremos el campo _id , tenemos que indicarlo: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"cast\" : 1 , \"_id\" : 0 } ) for zip in cursor : print ( zip ) { 'cast' : [ 'David Arquette' , 'John Hawkes' , 'Salma Hayek' , 'Jason Wiles' ], 'title' : 'Roadracers' } { 'cast' : [ 'Ernesto G\u00e8mez Cruz' , 'Mar\u00e8a Rojo' , 'Salma Hayek' , 'Bruno Bichir' ], 'title' : 'Midaq Alley' } { 'title' : 'Desperado' , 'cast' : [ 'Antonio Banderas' , 'Salma Hayek' , 'Joaquim de Almeida' , 'Cheech Marin' ]} ... BSON Cuando necesitemos pasar los documentos BSON a JSON, utilizaremos la funci\u00f3n dumps que transforma el documento a JSON: cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) from bson.json_util import dumps print ( dumps ( cursor , indent = 2 )) \"\"\" [ { \"_id\": { \"$oid\": \"573a1399f29313caabceea6d\" }, \"plot\": \"Cynical look at a 50's rebellious Rocker who has to confront his future, thugs with knives, and the crooked town sheriff.\", \"genres\": [ \"Action\", \"Drama\" ], \"runtime\": 95, \"rated\": \"R\", \"cast\": [ \"David Arquette\", \"John Hawkes\", \"Salma Hayek\", \"Jason Wiles\" ], ... \"\"\" Agregaciones \u00b6 Para realizar consultas mediante el framework de agregaci\u00f3n, usaremos el m\u00e9todo aggregate , el cual recibe un array con el pipeline: Por ejemplo, vamos a recuperar el t\u00edtulo y el casting de las pel\u00edculas dirigidas por Sam Raimi : Consulta Resultados match_stage = { \"$match\" : { \"directors\" : \"Sam Raimi\" } } project_stage = { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } } pipeline = [ match_stage , project_stage ] sam_raimi_aggregation = movies . aggregate ( pipeline ) for movie in sam_raimi_aggregation : print ( movie ) { 'cas t ' : [ 'Bruce Campbell' , 'Elle n Sa n dweiss' , 'Richard DeMa n i n cor' , 'Be ts y Baker' ], ' t i tle ' : 'The Evil Dead' } { 'cas t ' : [ 'Bruce Campbell' , 'Sarah Berry' , 'Da n Hicks' , 'Kassie Wesley DePaiva' ], ' t i tle ' : 'Evil Dead II' } { 'cas t ' : [ 'Liam Neeso n ' , 'Fra n ces McDorma n d' , 'Coli n Friels' , 'Larry Drake' ], ' t i tle ' : 'Darkma n ' } Otro ejemplo, donde recuperamos los directores y la valoraci\u00f3n media de sus pel\u00edculas, ordenadas de mejor a peor: Consulta Resultado unwind_stage = { \"$unwind\" : \"$directors\" } group_stage = { \"$group\" : { \"_id\" : { \"director\" : \"$directors\" }, \"average_rating\" : { \"$avg\" : \"$imdb.rating\" } } } sort_stage = { \"$sort\" : { \"average_rating\" : - 1 } } # Creamos un pipeline con las tres fases pipeline = [ unwind_stage , group_stage , sort_stage ] director_ratings = movies . aggregate ( pipeline ) for director in director_ratings : print ( director ) { '_id' : { 'direc t or' : 'Sara Hirsh Bordo' }, 'average_ra t i n g' : 9.4 } { '_id' : { 'direc t or' : 'Kevi n Derek' }, 'average_ra t i n g' : 9.3 } { '_id' : { 'direc t or' : 'Michael Be ns o n ' }, 'average_ra t i n g' : 9.0 } { '_id' : { 'direc t or' : 'Sloboda n Sija n ' }, 'average_ra t i n g' : 8.95 } { '_id' : { 'direc t or' : \"Bozidar 'Bota' Nikolic\" }, 'average_ra t i n g' : 8.9 } ... O realizamos un join entre las pel\u00edculas y sus comentarios para recuperar una pel\u00edcula por su identificador: Consulta Resultado from bson.objectid import ObjectId peliculaId = \"573a13aef29313caabd2c349\" pipeline = [ { \"$match\" : { \"_id\" : ObjectId ( peliculaId ) } }, { \"$lookup\" : { \"from\" : \"comments\" , \"localField\" : \"_id\" , \"foreignField\" : \"movie_id\" , \"as\" : 'comentarios' } } ] peliculaConComentarios = movies . aggregate ( pipeline ) . next () print ( peliculaConComentarios ) { '_id' : Objec t Id(' 573 a 13 ae f 29313 caabd 2 c 349 ') , 'awards' : { ' n omi nat io ns ' : 48 , ' te x t ' : 'Nomi nate d f or 1 Oscar. A n o t her 21 wi ns & 48 n omi nat io ns .' , 'wi ns ' : 22 }, 'cas t ' : [ 'Chris t ia n Bale' , 'Michael Cai ne ' , 'Liam Neeso n ' , 'Ka t ie Holmes' ], 'come ntar ios' : [{ '_id' : Objec t Id(' 5 a 9427658 b 0 beebeb 696 f 672 ') , 'da te ' : da tet ime.da tet ime( 1972 , 3 , 2 , 15 , 58 , 41 ) , 'email' : 'do nal d_sump ter @gameo ft hro n .es' , 'movie_id' : Objec t Id(' 573 a 13 ae f 29313 caabd 2 c 349 ') , ' na me' : 'Maes ter Luwi n ' , ' te x t ' : 'A t que ...' }], 'cou ntr ies' : [ 'USA' , 'UK' ], 'direc t ors' : [ 'Chris t opher Nola n ' ], ' full plo t ' : 'Whe n ...\", 'genres': ['Action', 'Adventure'], 'imdb': {'id': 372784, 'rating': 8.3, 'votes': 860733}, 'languages': ['English', 'Urdu', 'Mandarin'], 'lastupdated': datetime.datetime(2015, 8, 31, 0, 1, 54, 590000), 'metacritic': 70, 'num_mflix_comments': 1, 'plot': 'After training ....', 'poster': 'https://m.media-amazon.com/images/M/MV5BZmUwNGU2ZmItMmRiNC00MjhlLTg5YWUtODMyNzkxODYzMmZlXkEyXkFqcGdeQXVyNTIzOTk5ODM@._V1_SY1000_SX677_AL_.jpg', 'rated': 'PG-13', 'released': datetime.datetime(2005, 6, 15, 0, 0), 'runtime': 140, 'title': 'Batman Begins', 'type': 'movie', 'writers': ['Bob Kane (characters)', 'David S. Goyer (story)', 'Christopher Nolan (screenplay)', 'David S. Goyer (screenplay)'], 'year': 2005} Trabajando con cursores \u00b6 A continuaci\u00f3n vamos a realizar algunas operaciones sobre los cursores con PyMongo y a comparar a c\u00f3mo podemos realizar la misma operaci\u00f3n mediante el motor de agregaciones. Limitando \u00b6 Sobre el cursor podemos restringir la cantidad de resultados devueltos mediante el m\u00e9todo .limit() equivalente a la agregaci\u00f3n $limit : PyMongo Agregaci\u00f3n Salida limited_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } ) . limit ( 2 ) for movie in limited_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } }, { \"$limit\" : 2 } ] limited_aggregation = movies . aggregate ( pipeline ) for movie in limited_aggregation : print ( movie ) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" }, { \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] } ] Ordenando \u00b6 Para ordenar usaremos el m\u00e9todo .sort() que adem\u00e1s de los campos de ordenaci\u00f3n, indicaremos si el criterio ser\u00e1 ascendente ( ASCENDING ) o descendente ( DESCENDING ), de forma similar a como lo hacemos con la operaci\u00f3n de agregaci\u00f3n $sort : PyMongo Agregaci\u00f3n Salida from pymongo import DESCENDING , ASCENDING sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) for movie in sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) for movie in sorted_aggregation : print ( movie ) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" , \"year\" : 1981 }, { \"year\" : 1987 , \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] }, { \"year\" : 1990 , ... } ] En el caso de tener una clave compuesta de ordenaci\u00f3n, le pasaremos como par\u00e1metro una lista de tuplas clave/criterio: PyMongo Agregaci\u00f3n Salida sorted_cursor = movies . find ( { \"cast\" : \"Tom Hanks\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ([( \"year\" , ASCENDING ), ( \"title\" , ASCENDING )]) for movie in sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"cast\" : \"Tom Hanks\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING , \"title\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) for movie in sorted_aggregation : print ( movie ) [ { \"cast\" : [ \"Tom Hanks\" , \"Daryl Hannah\" , \"Eugene Levy\" , \"John Candy\" ], \"title\" : \"Splash\" , \"year\" : 1984 }, { \"cast\" : [ \"Tom Hanks\" , \"Jackie Gleason\" , \"Eva Marie Saint\" , \"Hector Elizondo\" ], \"title\" : \"Nothing in Common\" , \"year\" : 1986 }, { \"cast\" : [ \"Tom Hanks\" , \"Elizabeth Perkins\" , \"Robert Loggia\" , \"John Heard\" ], \"title\" : \"Big\" , \"year\" : 1988 }, { \"cast\" : [ \"Sally Field\" , \"Tom Hanks\" , \"John Goodman\" , \"Mark Rydell\" ], \"title\" : \"Punchline\" , \"year\" : 1988 }, ... ] Saltando \u00b6 Cuando paginamos los resultados, para saltar los documentos, haremos uso del m\u00e9todo .skip() , el cual es similar a la operaci\u00f3n $skip . Por ejemplo, la siguiente consulta devuelve 13 documentos, de manera que al saltarnos 12, s\u00f3lo nos devolver\u00e1 uno: PyMongo Agregaci\u00f3n Salida skipped_sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"year\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) . skip ( 12 ) for movie in skipped_sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } }, { \"$skip\" : 12 } ] sorted_skipped_aggregation = movies . aggregate ( pipeline ) for movie in sorted_skipped_aggregation : print ( movie ) [ { \"cast\" : [ \"James Franco\" , \"Mila Kunis\" , \"Rachel Weisz\" , \"Michelle Williams\" ], \"title\" : \"Oz the Great and Powerful\" , \"year\" : 2013 } ] CRUD \u00b6 Para estas operaciones, vamos a trabajar con la colecci\u00f3n iabd.people que utilizamos en la primera sesi\u00f3n de MongoDB . Inserci\u00f3n \u00b6 from pymongo import MongoClient cliente = MongoClient ( \"mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/?retryWrites=true&w=majority\" , server_api = ServerApi ( '1' )) bd = cliente . iabd people = bd . people yo = { \"nombre\" : \"Aitor Medrano\" , \"edad\" : 45 , \"profesion\" : \"Profesor\" } resultado = people . insert_one ( yo ) print ( resultado . acknowledged ) # True print ( resultado . inserted_id ) # 6366b5c85afaf1b75dc90a20 Tras realizar una operaci\u00f3n de inserci\u00f3n con insert_one , obtenemos un objeto InsertOneResult del cual cabe destacar dos campos: acknowledged : un campo booleano que nos indica si la operaci\u00f3n ha sido exitosa, o False cuando indicamos un WriteConcern(w=0) , es decir, es escritura fire-and-forget . inserted_id : valor del ObjectId . Si queremos insertar m\u00e1s de una documento a la vez, usaremos insert_many , el cual devuelve un objeto InsertManyResult . Join \u00b6 Ya hemos estudiado que al insertar un documento que est\u00e1 relacionado con otro, necesitamos que los campos contengan el mismo valor (normalmente, el ObjectId ) Por ejemplo, si queremos a\u00f1adir un comentario a una pel\u00edcula, hemos de unir el identificador de la pel\u00edcula en cada comentario haciendo uso de objeto ObjectId : from bson.objectid import ObjectId comments = db . comments comment_doc = { \"name\" : usuario . nombre , \"email\" : usuario . email , \"movie_id\" : ObjectId ( movie_id ), \"text\" : comentario , \"date\" : fecha } comments . insert_one ( comment_doc )) Gesti\u00f3n de errores \u00b6 A la hora de insertar un documento se puede dar el caso de lanzar un DuplicateKeyError , ya sea por el atributo identificador o por un \u00edndice de tipo \u00fanico. En dicho caso, podemos capturar la excepci\u00f3n y ajustar el comportamiento o informar al usuario, dependiendo de la gravedad del error. Por ejemplo, si insertamos una persona con un _id que ya existe: doc = { \"_id\" : 1 , \"nombre\" : \"Aitor Medrano\" } try : result = people . insert_one ( doc ) print ( res . inserted_id ) except errors . DuplicateKeyError : usuario_id = doc [ \"_id\" ] print ( f \"El usuario # { usuario_id } ya existe en el sistema.\" ) Borrado \u00b6 El borrado de documento es similar a la creaci\u00f3n, pudiendo utilizar delete_one para borrar el primer documento encontrado, o delete_many para borrar todos los documentos encontrados: resultado = people . delete_many ({ \"edad\" : 45 }) print ( resultado . deleted_count ) En ambas operaciones, obtenemos un objeto DeleteResult del cual destacamos la propiedad deleted_count para averiguar cuantos documentos han sido eliminados. Borrado de colecciones Para vaciar una colecci\u00f3n, podemos borrar todos los documentos: people . delete_many ({}) Aunque es mejor borrar la colecci\u00f3n entera: people . drop () Modificaci\u00f3n \u00b6 De forma similar a como lo hemos realizado mediante mongosh , para modificar documentos emplearemos los m\u00e9todos update_one y update_many , los cuales devuelve un objeto UpdateResult : resultado = people . update_one ({ \"nombre\" : \"Aitor Medrano\" }, { \"$set\" :{ \"nombre\" : \"Marina Medrano\" , \"salario\" : 123456 }}) print ( resultado . matched_count ) # (1)! print ( resultado . modified_count ) # (2)! matched_count nos devuelve la cantidad de documentos encontrados modified_count nos devuelve la cantidad de documentos modificados Tal como vimos, en las operaciones update , le podemos pasar un tercer par\u00e1metro (opcional) con upsert=False (valor por defecto) op True , cuando queramos que se inserte un nuevo documento si no encuentra ninguno. En este caso, modified_count y matched_count ser\u00e1n 0. Otro argumento opcional que conviene conocer es bypass_document_validation=False (valor por defecto), el cual, si lo ponemos a True ignorar\u00e1 las validaciones (si hay alguna) para los documentos que modifiquemos. Operador Recuerda utilizar un operador de modificaci\u00f3n en el segundo par\u00e1metro. Si no, el documento del segundo par\u00e1metro sustituir\u00e1 por completo al documento encontrado. Operaciones consistentes \u00b6 Preferencias de lectura \u00b6 Para realizar lecturas consistentes podemos configurar la preferencias de lectura, tal como vimos en la sesi\u00f3n anterior . Para ello, PyMongo ofrece un conjunto de clases para indicar las preferencias de lectura , las cuales podemos configurar a nivel de cliente: Niveles de lectura Los diferentes niveles vienen definidos en el objeto ReadPreference el cual ofrece los valores estudiados en la sesi\u00f3n anterior: PRIMARY , PRIMARY_PREFERRED , SECONDARY , SECONDARY_PREFERRED y NEAREST . client = MongoClient ( 'localhost:27017' , replicaSet = 'iabd' , readPreference = 'secondaryPreferred' ) Si en vez de indicar la preferencia a nivel de cliente, lo queremos realizar a nivel de base de datos o de colecci\u00f3n, hemos de emplear los m\u00e9todos get_database y get_collection pas\u00e1ndole un segundo par\u00e1metro con la opci\u00f3n deseada: from pymongo import ReadPreference print ( client . read_preference ) # SecondaryPreferred(tag_sets=None) db = client . get_database ( 'iabd' , read_preference = ReadPreference . SECONDARY ) print ( db . read_preference ) # Secondary(tag_sets=None) coll = db . get_collection ( 'people' , read_preference = ReadPreference . PRIMARY ) print ( coll . read_preference ) # Primary() Finalmente, si en cualquier momento queremos cambiar las preferencias a nivel de colecci\u00f3n, podemos emplear el m\u00e9todo with_options : coll2 = coll . with_options ( read_preference = ReadPreference . NEAREST ) print ( coll . read_preference ) # Primary() print ( coll . read_preference ) # Nearest(tag_sets=None) Escrituras consistentes \u00b6 Tal como vimos en la sesi\u00f3n anterior, para indicar la consistencia en la escritura, haremos uso de la propiedad write_concern . Para ello, de la misma forma que hemos indicado la preferencia de lectura, hacemos uso del m\u00e9todo with_options : coll . with_options ( write_concern = WriteConcern ( w = \"majority\" )) . insert_one ({ \"nombre\" : nombre , \"email\" : email , \"password\" : hashed_pw }) Si tenemos una aplicaci\u00f3n cr\u00edtica, no nos podemos permitir perder los datos. Por ello, necesitamos que las escritura se propaguen a la mayor\u00eda de los nodos con la opci\u00f3n w: majority para asegurarnos que las escrituras se propagan a la mayor\u00eda de nodos del conjunto de nodos. Si sucede un problema en los nodos secundarios, puede ser que el primario no reciba los ACK necesarios. Si llegan m\u00e1s escrituras que lecturas, puede llegar el momento en que se produzca un atasco. Para evitar esto, para cada escritura que realicemos con la mayor\u00eda de los nodos, siempre hay que indicar un timeout . La longitud del timeout vendr\u00e1 determinada por la red y el hardware que dispongamos, pero siempre hemos de indicarlo. coll . with_options ( write_concern = WriteConcern ( w = \"majority\" , wtimeout = \"2500\" ) . insert_one ({ \"nombre\" : nombre , \"email\" : email , \"password\" : hashed_pw }) Bulk writes En ocasiones necesitamos ejecutar una bateria de operaciones a granel ( bulk ) las cuales se ejecutan como un proceso batch . Para ello se emplea el m\u00e9todo bulk_write : result = db . test . bulk_write ( array_operaciones ) Transacciones \u00b6 Ya hemos comentado en numerosas ocasiones toda operaci\u00f3n en un \u00fanico documento es at\u00f3mica, de ah\u00ed embeber documentos y arrays para modelar las relaciones de datos en un s\u00f3lo documento cubre la mayor\u00eda de los casos de usos transaccionales. Cuando necesitamos atomicidad de lecturas y escrituras entre varios documentos (en una o m\u00e1s colecciones), MongoDB soporta transacciones multidocumento. Mediante las transacciones distribuidas, las transacciones puede operar entre varias operaciones, colecciones, bases de datos, documentos y particiones ( shards ). Tipos de API \u00b6 MongoDB ofrece dos API para utilizar transacciones. La primera, conocida como Core API , disponible desde la versi\u00f3n 4.0 de MongoDB , tiene una sintaxis similar a las bases de datos relacionales (por ejemplo, con operaciones como start_transaction y commit_transaction ), y la segunda se conoce como Callback API , desde la versi\u00f3n 4.2, el cual es el enfoque actualmente recomendado. El Core API no ofrece l\u00f3gica de reintentos para la mayor\u00eda de errores y necesita que el desarrollador programe la l\u00f3gica de las operaciones, la funci\u00f3n transaccional que realiza commit y la l\u00f3gica de errores y reintentos necesaria. En cambio, el Callback API ofrece una \u00fanica funci\u00f3n que engloba un alto grado de funcionalidad comparadas con el Core API , incluyendo el inicio de una transacci\u00f3n asociada a una sesi\u00f3n l\u00f3gica, ejecutar la funci\u00f3n definida como callback y realizando el commit de la transacci\u00f3n (o abort\u00e1ndola en caso de error). Esta funci\u00f3n tambi\u00e9n incluye la l\u00f3gica de reintentos para manejar los errores al hacer commit . En ambas APIs, el desarrollador se responsabiliza de iniciar la sesi\u00f3n l\u00f3gica que se utilizar\u00e1 en la transacci\u00f3n. Ambas APIs requieren que las operaciones transaccionales se asocien a \u00e9sta sesi\u00f3n l\u00f3gica (pas\u00e1ndola como par\u00e1metro a cada operaci\u00f3n). Cada sesi\u00f3n l\u00f3gica en MongoDB registra el tiempo y la secuencia de las operaciones en el contexto completo del despliegue de MongoDB . Hola Mundo Callback API \u00b6 Vamos a simular que tenemos una aplicaci\u00f3n de gesti\u00f3n de un almac\u00e9n, en la cual tenemos que realizar las siguientes operaciones dentro de una transacci\u00f3n: pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 100 }, session = miSesion ) inventario . update_one ({ \"producto\" : \"ps5\" , \"cantidad\" : { \"$gte\" : 100 }}, { \"$inc\" : { \"cantidad\" : - 100 }}, session = miSesion ) Veamos mediante un ejemplo el flujo del c\u00f3digo transaccional haciendo uso del Callback API . almacenTransaccional.py from pymongo import MongoClient , ReadPreference from pymongo.write_concern import WriteConcern from pymongo.read_concern import ReadConcern uriString = 'mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/?retryWrites=true&w=majority' cliente = MongoClient ( uriString ) miWCMajority = WriteConcern ( 'majority' , wtimeout = 1000 ) # Paso 0: Creamos dos colecciones e insertamos un documento en cada una bd = client . get_database ( \"iabd\" , write_concern = miWCMajority ) bd . pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 0 }) bd . inventario . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 1000 }) # Paso 1: Definir el callback que indica la secuencia de las operaciones a realizar dentro de la transacci\u00f3n def callback ( miSesion ): pedidos = miSesion . cliente . iabd . pedidos inventario = miSesion . cliente . iabd . inventario # Importante: Debemos pasarle la sesi\u00f3n a cada operaci\u00f3n pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 100 }, session = miSesion ) inventario . update_one ({ \"producto\" : \"ps5\" , \"cantidad\" : { \"$gte\" : 100 }}, { \"$inc\" : { \"cantidad\" : - 100 }}, session = miSesion ) # Paso 2: Iniciar una sesi\u00f3n de cliente. with cliente . start_session () as session : # Paso 3: Empleamos with_transaction para iniciar la transacci\u00f3n, ejecutar el callback y realizar el commit (o abortar en caso de error). session . with_transaction ( callback , read_concern = ReadConcern ( \"local\" ), write_concern = miWCMajority , read_preference = ReadPreference . PRIMARY , ) PyMODM PyMODM es una librer\u00eda ODM que abstrae el acceso a MongoDB mediante un mapeo objeto-documento. De forma similar a los frameworks ORM en los entornos relacionales, como son Hibernate ( Java ), Doctrine / Eloquent ( PHP ) o el ORM de Django , abstrae y simplifica el acceso a la base de datos. Un ejemplo de definici\u00f3n de un objeto y su persistencia ser\u00eda: class User ( MongoModel ): # Utiliza el 'email'como campo '_id' email = fields . EmailField ( primary_key = True ) nombre = fields . CharField () apellido = fields . CharField () usuario = User ( 'a.medrano@edu.gva.es' , 'Aitor' , 'Medrano' ) . save () Hemos decidido no profundizar en su conocimiento ya que el equipo de MongoDB ha pausado su mantenimiento desde hace ya dos a\u00f1os. Caso de uso - PIA: Login \u00b6 El siguiente proyecto se basa inicialmente en el art\u00edculo How To Add Authentication to Your App with Flask-Login . A partir de \u00e9l, vamos a crear una aplicaci\u00f3n en Flask que ataca a una base de datos de MongoDB para almacenar la informaci\u00f3n del proyecto PIA Lara. Esta primera versi\u00f3n de la aplicaci\u00f3n \u00fanicamente se centra en la gesti\u00f3n de los usuarios, distinguiendo entre los diferentes roles: Administrador : superusuario, puede crear, editar y eliminar todo tipo de usuarios. T\u00e9cnico : usuario que supervisar\u00e1 a los clientes, el cual, m\u00e1s adelante, puede llegar a crear textos predefinidos para los clientes. Cliente : usuario final de la aplicaci\u00f3n que, m\u00e1s adelante, grabar\u00e1 los audios. Simplicidad El presente caso de uso se ha organizado para intentar simplificar al m\u00e1ximo el c\u00f3digo y preparar un esqueleto que facilite el crecimiento de la aplicaci\u00f3n. A\u00fan as\u00ed, una soluci\u00f3n basada en Django o el uso de herramientas de mapeo entre los datos y los objetos del dominio ser\u00edan un punto de partida para siguientes fases del proyecto. Estructura \u00b6 Una vez descargado el proyecto y tras descomprimirlo, o clonado desde https://github.com/aitor-medrano/piafplogin , veremos que tiene la siguiente estructura: PIAFPLOGIN/ \u251c\u2500\u2500 migrations/ \u2502 \u2514\u2500\u2500 user_migration.py \u251c\u2500\u2500 project/ \u2502 \u251c\u2500\u2500 static/ \u2502 \u2502 \u251c\u2500\u2500 pialara.js \u2502 \u2502 \u2514\u2500\u2500 pialara.png \u2502 \u251c\u2500\u2500 templates/ \u2502 \u2502 \u251c\u2500\u2500 base.html \u2502 \u2502 \u251c\u2500\u2500 index.html \u2502 \u2502 \u251c\u2500\u2500 login.html \u2502 \u2502 \u2514\u2500\u2500 profile.html \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 db.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 models.py \u251c\u2500\u2500 .ini \u2514\u2500\u2500 requirements.txt El primer paso es crear un entorno virtual: python3 -m venv app-env Y activarlo: source app-env/bin/activate A continuaci\u00f3n instalaremos las dependencias mediante: pip3 install -r requirements.txt Configuraci\u00f3n \u00b6 Para configurar el proyecto, partimos del fichero .ini que reside en la ra\u00edz del mismo y contiene los datos de configuraci\u00f3n a MongoDB y la clave secreta que utiliza Flask para encriptar la sesi\u00f3n: .ini [PROD] SECRET_KEY = eac5e91171438960ddec0c9c469a4c3dd42e96aea462afc5ab830f78527ad80e PIALARA_DB_URI = mongodb+srv://usuario:contrase\u00f1a@cluster0.xyz.mongodb.net PIALARA_DB_NAME = pialara [LOCAL] SECRET_KEY = eac5e91171438960ddec0c9c469a4c3dd42e96aea462afc5ab830f78527ad80e PIALARA_DB_URI = localhost PIALARA_DB_NAME = pialara Secret Key Para generar una clave secreta, tal como indica la documentaci\u00f3n de Flask , podemos ejecutar el siguiente comando: python3 -c 'import secrets; print(secrets.token_hex())' Una vez ya hemos configurado la conexi\u00f3n y antes de arrancar la aplicaci\u00f3n, vamos a cargar unos datos b\u00e1sicos con usuarios. Para ello, en la carpeta migrations tenemos el archivo users_migration.py , el cual lee la configuraci\u00f3n del archivo anterior, y crea tres usuarios: users_migration.py from pymongo import MongoClient from werkzeug.security import generate_password_hash import os import configparser config = configparser . ConfigParser () config . read ( os . path . abspath ( os . path . join ( \".ini\" ))) DB_URI = config [ 'PROD' ][ 'PIALARA_DB_URI' ] DB_NAME = config [ 'PROD' ][ 'PIALARA_DB_NAME' ] # DB_URI = config['LOCAL']['PIALARA_DB_URI'] # DB_NAME = config['LOCAL']['PIALARA_DB_NAME'] db = MongoClient ( DB_URI )[ DB_NAME ] usuarios = [ { \"nombre\" : \"Admin\" , \"email\" : \"admin@admin.com\" , \"password\" : generate_password_hash ( \"admin\" , method = 'sha256' ), \"rol\" : \"Administrador\" }, { \"nombre\" : \"Alumno\" , \"email\" : \"alumno@alumno.com\" , \"password\" : generate_password_hash ( \"alumno\" , method = 'sha256' ), \"rol\" : \"T\u00e9cnico\" }, { \"nombre\" : \"Severo Ochoa\" , \"email\" : \"s8a@s8a.com\" , \"password\" : generate_password_hash ( \"s8a\" , method = 'sha256' ), \"rol\" : \"Cliente\" , \"parent\" : \"alumno@alumno.com\" }, ] try : db . users . insert_many ( usuarios ) except Exception as e : print ( e ) Destacar que al definir los documentos con los usuarios, encriptamos la contrase\u00f1a mediante la funci\u00f3n generate_password_hash para no almacenarla en la base de datos en texto plano. As\u00ed pues, ejecutamos la migraci\u00f3n: python3 migrations/users_migration.py Finalmente podemos arrancar la aplicaci\u00f3n: flask --app project --debug run Y acceder a la aplicaci\u00f3n a trav\u00e9s de http://127.0.0.1:5000/ : Versi\u00f3n escritorio Versi\u00f3n m\u00f3vil PIA Lara - Index PIA Lara - Index en m\u00f3vil Una vez que un usuario ha entrado a la aplicaci\u00f3n, por ejemplo, si es un administrador, dispondr\u00e1 de las opciones que hemos comentado anteriormente: PIA Lara - Administrador Blueprints \u00b6 El archivo .ini que hemos configurado previamente define unos valores que la aplicaci\u00f3n va a cargar desde el archivo __init__.py , el cual act\u00faa como factor\u00eda de la aplicaci\u00f3n y le indica a Flask los blueprints a utilizar: __init__.py from flask import Flask from flask_login import LoginManager from . import db import os import configparser config = configparser . ConfigParser () config . read ( os . path . abspath ( os . path . join ( \".ini\" ))) def create_app (): app = Flask ( __name__ ) # cargamos la configuraci\u00f3n app . config [ 'PIALARA_DB_URI' ] = config [ 'LOCAL' ][ 'PIALARA_DB_URI' ] app . config [ 'PIALARA_DB_NAME' ] = config [ 'LOCAL' ][ 'PIALARA_DB_NAME' ] app . config [ 'SECRET_KEY' ] = config [ 'LOCAL' ][ 'SECRET_KEY' ] # configuramos flask-login con la ruta del login login_manager = LoginManager () login_manager . login_view = 'auth.login' login_manager . init_app ( app ) # funci\u00f3n que utiliza flask-login para recuperar el usuario @login_manager . user_loader def load_user ( user_id ): return db . get_user_by_id ( user_id ) # blueprint para las rutas de autenticaci\u00f3n from .auth import auth as auth_blueprint app . register_blueprint ( auth_blueprint ) # blueprint para la aplicaci\u00f3n from .main import main as main_blueprint app . register_blueprint ( main_blueprint ) return app Un blueprint permite organizar un grupo de vistas y c\u00f3digo en m\u00f3dulos. En vez de registrar las vistas y el resto de c\u00f3digo en la aplicaci\u00f3n, se registran en el blueprint, y \u00e9ste es que se registra en la aplicaci\u00f3n en la funci\u00f3n create_app . En nuestro caso, vamos a empezar con dos blueprints , uno para las funciones de autenticaci\u00f3n, y otra para las funciones de gesti\u00f3n de usuarios. El c\u00f3digo de cada blueprint ir\u00e1 en un m\u00f3dulo separado. Como la gesti\u00f3n de usuarios necesita primero la autenticaci\u00f3n, vamos a ver c\u00f3mo funciona. Login \u00b6 Para la gesti\u00f3n de la autenticaci\u00f3n, nos hemos apoyado en la librer\u00eda Flask-login que facilita la gesti\u00f3n la sesi\u00f3n del usuario. En archivo auth.py contiene la l\u00f3gica del login y el logout : auth.py from flask import Blueprint , render_template , redirect , url_for , request , flash from flask_login import login_user , login_required , logout_user , current_user from werkzeug.security import check_password_hash from . import db auth = Blueprint ( 'auth' , __name__ ) @auth . route ( '/login' ) def login (): return render_template ( 'login.html' ) @auth . route ( '/login' , methods = [ 'POST' ]) def login_post (): email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) remember = True if request . form . get ( 'remember' ) else False user = db . get_user ( email ) # comprobamos si el usuario existe # cogemos la contrase\u00f1a, la hasheamos y la comparamos con la contrase\u00f1a hasheada if not user or not check_password_hash ( user . password , password ): flash ( 'Por favor, comprueba tus datos y vu\u00e9lvelo a intentar.' ) # si el usuario no existe, o est\u00e1 mal la contrase\u00f1a, recargamos la p\u00e1gina return redirect ( url_for ( 'auth.login' )) # marcamos al usuario como autenticado en flask_login login_user ( user , remember = remember ) return redirect ( url_for ( 'main.profile' , nombre = current_user . nombre )) @auth . route ( '/logout' ) @login_required def logout (): logout_user () flash ( 'Sesi\u00f3n cerrada con \u00e9xito' ) return redirect ( url_for ( 'auth.login' )) Los usuarios van a entrar al sistema mediante su email y una contrase\u00f1a. As\u00ed pues, una vez hayamos recuperado un usuario por dicho email, creamos el hash de la contrase\u00f1a recibida, y vemos si comprueba con la recuperada de la base de datos. El m\u00e9todo login_user de la l\u00ednea 27 pertenece a la librer\u00eda Flask-Login y se utiliza para indicar que el usuario ha sido autenticado, de manera que lo almacena en la sesi\u00f3n. La variable user es una clase propia que hemos definido nosotros con los atributos b\u00e1sicos de un usuario,el cual se encuentra en el archivo models.py : models.py from flask_login import UserMixin class User ( UserMixin ): def __init__ ( self , id , email , nombre , password , rol , parent = \"\" ): self . id = id self . email = email self . nombre = nombre self . password = password self . rol = rol self . parent = parent def __str__ ( self ): return f \" { self . email } ( { self . nombre } / { self . password } )\" Como se puede observar, la clase define los atributos b\u00e1sicos de un usuario. El atributo parent lo vamos a emplear para que los clientes almacenen el email del t\u00e9cnico que tienen asignado. Plantillas \u00b6 Las diferentes plantillas heredan de una plantilla base.html , la cual emplea el framework Bulma para la apariencia de la web. Su funcionamiento es muy similar a Bootstrap. Por ejemplo, vamos a revisar un fragmento de la plantilla base para ver c\u00f3mo gestionamos la visualizaci\u00f3n del men\u00fa dependiendo del rol del usuario: base.html ... < section class = \"hero is-white is-fullheight\" > < nav class = \"navbar is-transparent\" > < div class = \"navbar-brand\" > < a class = \"navbar-item\" href = \"https://piafplara.es\" > < img src = \"{{ url_for('static', filename='pialara.png') }}\" alt = \"PIA Lara: un proyecto que habla por ti\" width = \"112\" height = \"28\" > </ a > < div class = \"navbar-burger burger\" data-target = \"navbarPIALara\" > < span ></ span > < span ></ span > < span ></ span > </ div > </ div > < div id = \"navbarPIALara\" class = \"navbar-menu\" > < div class = \"navbar-start\" > < a href = \"{{ url_for('main.index') }}\" class = \"navbar-item\" > Inicio </ a > {% if not current_user.is_authenticated %} < a href = \"{{ url_for('auth.login') }}\" class = \"navbar-item\" > Login </ a > {% endif %} {% if current_user.is_authenticated %} < a href = \"{{ url_for('main.profile') }}\" class = \"navbar-item\" > Perfil </ a > {% if current_user.rol == \"Administrador\" %} < div class = \"navbar-item has-dropdown is-hoverable\" > < a class = \"navbar-link\" href = \"#\" > Usuarios </ a > < div class = \"navbar-dropdown is-hidden-mobile is-boxed\" > < a class = \"navbar-item\" href = \"{{ url_for('main.user_create') }}\" > Alta </ a > < a class = \"navbar-item\" href = \"{{ url_for('main.user_list') }}\" > Listado </ a > </ div > </ div > {% endif %} ... En la l\u00ednea 6 utilizamos la funci\u00f3n url_for con el par\u00e1metro static para indicarle que cargue la imagen con el logo del proyecto desde la carpeta static . Al utilizar la librer\u00eda Flask Login , tendremos siempre disponible el usuario logueado en la variable current_user . Adem\u00e1s de las propiedades que hayamos definido en la clase, disponemos de la funci\u00f3n is_authenticated para comprobar si est\u00e1 autenticado (l\u00ednea 20). De igual forma, podemos comprobar el rol y condicionar el contenido dependiendo de si es Administrador , T\u00e9cnico o Cliente (l\u00ednea 24). Acceso a los datos \u00b6 Todo el acceso a los datos los hemos encapsulado en el archivo db.py : db.py from pymongo import MongoClient from bson.objectid import ObjectId from pymongo import ASCENDING from flask import current_app , g from werkzeug.local import LocalProxy from project.models import User def get_db (): \"\"\" M\u00e9todo de configuraci\u00f3n para obtener una instancia de db \"\"\" db = getattr ( g , \"_database\" , None ) PIALARA_DB_URI = current_app . config [ \"PIALARA_DB_URI\" ] PIALARA_DB_DB_NAME = current_app . config [ \"PIALARA_DB_NAME\" ] if db is None : db = g . _database = MongoClient ( PIALARA_DB_URI , maxPoolSize = 50 , timeoutMS = 2500 )[ PIALARA_DB_DB_NAME ] return db # Utilizamos LocalProxy para leer la variable global usando s\u00f3lo db db = LocalProxy ( get_db ) La funci\u00f3n get_db utiliza el objeto g , el cual en Flask, es un objeto especial que es \u00fanico para cada petici\u00f3n. Se utiliza para almacenar datos que ser\u00e1n accesibles desde m\u00faltiples funciones durante el request . As\u00ed pues, almacenamos la conexi\u00f3n, mejor dicho, el pool de conexiones a MongoDB en vez de crear un nuevo pool cada vez que queramos obtener acceso a la base de datos. A continuaci\u00f3n, creamos un LocalProxy para leer la variable global usando s\u00f3lo la referencia db , de manera que internamente cada referencia a db realmente est\u00e1 llamando a get_db() . A continuaci\u00f3n, mostramos un par de m\u00e9todos del mismo archivo que muestran c\u00f3mo obtenemos datos desde MongoDB haciendo uso de PyMongo : db.py def get_all_users (): \"\"\" Devuelve una lista con todos los usuarios del sistema \"\"\" try : return list ( db . users . find ({}) . sort ( \"nombre\" , ASCENDING )) except Exception as e : return e def get_user_by_id ( id ): \"\"\" Devuelve un objeto User a partir de su id \"\"\" try : usuario = db . users . find_one ({ \"_id\" : ObjectId ( id )}) usuario_obj = User ( id = usuario [ \"_id\" ], email = usuario . get ( \"email\" ), nombre = usuario . get ( \"nombre\" ), password = usuario . get ( \"password\" ), rol = usuario . get ( \"rol\" ), parent = usuario . get ( \"parent\" )) return usuario_obj except Exception as e : return e ... Cuando recuperamos un usuario por su id , lo convertimos en un objeto User para que Flask Login falicita la gesti\u00f3n de la autenticaci\u00f3n. En cambio, en el listado de todos los usuarios, vamos a acceder al cursor de usuarios que ofrece MongoDB . Referencias \u00b6 Tutorial oficial de PyMongo Introduction to Multi-Document ACID Transactions in Python How To Use Transactions in MongoDB Actividades \u00b6 ( RA5074.3 / CE4.3d / 4p) A partir del caso de uso de PIA Login, se pide: (0.25) Configurar la URI de Mongo Atlas para atacar vuestra propia base de datos. (0.25) Modificar la migraci\u00f3n para introducir m\u00e1s usuarios (al menos uno m\u00e1s de cada rol) (0.75) Cuando un usuario pulsa sobre su nombre, actualmente aparece un formulario para editar sus datos, pero no puede cambiar la contrase\u00f1a. Modifica (o crea) el/los formulario/s adecuado/s para que cada usuario pueda cambiar su propia contrase\u00f1a. (0.75) Desde el rol Administrador , al crear un usuario, si es un cliente, debe mostrar un desplegable con todos los t\u00e9cnicos disponibles. (1) Tanto el T\u00e9cnico como el Cliente , al dar de alta o editar un cliente, almacenar\u00e1n datos necesarios para el proyecto, como son el sexo, la fecha de nacimiento y la patolog\u00eda. (1) Cuando un T\u00e9cnico visualiza el listado de sus clientes, debe recuperar \u00fanicamente el nombre, el sexo, la edad y su patolog\u00eda.","title":"S30.- MongoDB y Python"},{"location":"sa/07pymongo.html#hola-pymongo","text":"Un ejemplo b\u00e1sico podr\u00eda ser similar a: from pymongo import MongoClient cliente = MongoClient ( 'mongodb://localhost:27017' ) iabd_db = cliente . iabd # Recuperamos las colecciones print ( iabd_db . list_collection_names ()) people_coll = iabd_db . people # Recuperamos una persona persona = people_coll . find_one () print ( persona )","title":"Hola PyMongo"},{"location":"sa/07pymongo.html#mongoclient","text":"A partir de la URI de conexi\u00f3n a MongoDB, hemos de instanciar la clase MongoClient , ya sea pas\u00e1ndole una URI con la cadena de conexi\u00f3n, o mediante par\u00e1metros: uri = \"mongodb+srv://usuario:contrasenya@host\" cliente = MongoClient ( uri ) Par\u00e1metros adicionales A la hora de crear el cliente, tambi\u00e9n podemos indicarle diferentes opciones de configuraci\u00f3n: cliente200Retry = MongoClient ( uri , connectTimeoutMS = 200 , retryWrites = True ) Realmente, al crear la conexi\u00f3n se inicializa un pool de conexiones, de manera que se crean 100 conexiones, y en vez de crear y destruir una conexi\u00f3n por cada petici\u00f3n, se reutilizan, de manera que cada petici\u00f3n asigna y libera una conexi\u00f3n conforme necesidad. Por defecto, el tama\u00f1o del pool es de 100 conexiones. Pool de conexiones Podemos obtener informaci\u00f3n de la conexi\u00f3n mediante la propiedad state . Por ejemplo, en nuestro caso, nos hemos conectado a MongoAtlas y de la salida del estado podemos ver los diferentes hosts que forman parte del conjunto de r\u00e9plicas: cliente = MongoClient ( 'mongodb+srv://iabd:iabdiabd@cluster0.dfaz5er.mongodb.net/' ) print ( cliente . state ) # Database(MongoClient(host=['ac-opunia9-shard-00-01.dfaz5er.mongodb.net:27017', 'ac-opunia9-shard-00-02.dfaz5er.mongodb.net:27017', 'ac-opunia9-shard-00-00.dfaz5er.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', replicaset='atlas-4wikkb-shard-0', tls=True), 'state') Conexi\u00f3n a un r\u00e9plica Si nos queremos conectar a un conjunto de r\u00e9plicas e indicar los nodos a los que nos podemos conectar, podemos separar los hosts con comas e indicar finalmente con un par\u00e1metro el nombre del conjunto de r\u00e9plicas: client = MongoClient ( 'mongodb://usuario1:contra1@host1:puerto1,host2:puerto2/?replicaSet=nombreReplica' ) A partir del cliente, podemos obtener informaci\u00f3n diversa. Por ejemplo, podemos recuperar un listado de las bases de datos mediante list_database_names() : print ( cliente . list_database_names ()) # ['sample_airbnb', 'sample_analytics', 'sample_geospatial', 'sample_guides', 'sample_mflix', 'sample_restaurants', 'sample_supplies', 'sample_training', 'sample_weatherdata', 'admin', 'local'] Para conectarnos a una base de datos en concreto, \u00fanicamente accederemos a ella como una propiedad del cliente: bd = cliente . sample_mflix bd = cliente [ \"sample_mflix\" ] # tb podemos acceder como si fuera un diccionario Bases de datos y colecciones Lazy Es conveniente tener en cuenta que tantos las colecciones como las bases de datos se crean y carga de forma perezosa, esto es, hasta que no realizamos una operaci\u00f3n sobre ellas, no se accede realmente a ellas. As\u00ed pues, para crear realmente una colecci\u00f3n, hemos de insertar un documento en ella. Una vez tenemos la base de datos, el siguiente paso es obtener una colecci\u00f3n: coleccion = bd . movies coleccion = bd [ \"movies\" ] Si queremos obtener el nombre de todas las colecciones usaremos el m\u00e9todo list_collection_names() : print ( bd . list_collection_names ()) [ 'users' , 'movies' , 'sessions' , 'theaters' , 'comments' ] Tolerancia a fallos! Al crear una conexi\u00f3n, conviene especificar el tama\u00f1o del pool y el timeout de las conexiones: db = MongoClient ( URI , maxPoolSize = 50 , wtimeout = 2500 )[ nombreBD ] Se recomienda siempre especificar un wtimeout , ya que cuando se realiza una escritura con mayor\u00eda de escrituras, si fallase alg\u00fan nodo no se quedar\u00eda colgado esperando. Si no encontrase el servidor, lanzar\u00e1 un ServerSelectionTimeoutError el cual deber\u00edamos capturar.","title":"MongoClient"},{"location":"sa/07pymongo.html#primeras-consultas","text":"Finalmente, sobre una colecci\u00f3n ya podemos realizar consultas y otras operaciones: movies = bd . movies movies . count_documents ({}) # 23530 movies . find_one () # {'_id': ObjectId('573a1390f29313caabcd4135'), 'plot': 'Three men hammer on an anvil and pass a bottle of beer around.', 'genres': ['Short'], 'runtime': 1, 'cast': ['Charles Kayser', 'John Ott'], 'num_mflix_comments': 1, 'title': 'Blacksmith Scene', 'fullplot': 'A stationary ...', 'countries': ['USA'], 'released': datetime.datetime(1893, 5, 9, 0, 0), 'directors': ['William K.L. Dickson'], 'rated': 'UNRATED', 'awards': {'wins': 1, 'nominations': 0, 'text': '1 win.'}, 'lastupdated': '2015-08-26 00:03:50.133000000', 'year': 1893, 'imdb': {'rating': 6.2, 'votes': 1189, 'id': 5}, 'type': 'movie', 'tomatoes': {'viewer': {'rating': 3.0, 'numReviews': 184, 'meter': 32}, 'lastUpdated': datetime.datetime(2015, 6, 28, 18, 34, 9)}} Por ejemplo, podemos filtrar las pel\u00edculas cuya actriz sea Salma Hayek . Al realizar la consulta, find devuelve un cursor , el cual podemos recorrer: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) for movie in cursor : print ( movie ) { '_id' : Objec t Id(' 573 a 1399 f 29313 caabceea 6 d') , 'awards' : { ' n omi nat io ns ' : 1 , ' te x t ' : ' 1 n omi nat io n .' , 'wi ns ' : 0 }, 'cas t ' : [ 'David Arque tte ' , 'Joh n Hawkes' , 'Salma Hayek' , 'Jaso n Wiles' ], 'cou ntr ies' : [ 'USA' ], ... 'wri ters ' : [ 'Rober t Rodriguez' , 'Tommy Nix' ], 'year' : 1994 } { '_id' : Objec t Id(' 573 a 139 a f 29313 caabce f 0 b 6 ') , 'awards' : { ' n omi nat io ns ' : 14 , ' te x t ' : ' 27 wi ns & 14 n omi nat io ns .' , 'wi ns ' : 27 }, 'cas t ' : [ 'Er nest o G\u00f3mez Cruz' , 'Mar\u00eda Rojo' , 'Salma Hayek' , 'Bru n o Bichir' ], 'cou ntr ies' : [ 'Mexico' ], ... } Si queremos contar los documentos que cumplen un criterio, en vez de find , usaremos count_documents pas\u00e1ndole el criterio de filtrado: cantidad = movies . count_documents ( { \"cast\" : \"Salma Hayek\" } ) Para seleccionar los campos que queremos recuperar, necesitamos pasar un segundo par\u00e1metro con la proyecci\u00f3n: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"cast\" : 1 } ) for movie in cursor : print ( movie ) { '_id' : ObjectId ( '573a1399f29313caabceea6d' ), 'cast' : [ 'David Arquette' , 'John Hawkes' , 'Salma Hayek' , 'Jason Wiles' ], 'title' : 'Roadracers' } { '_id' : ObjectId ( '573a139af29313caabcef0b6' ), 'cast' : [ 'Ernesto G\u00f3mez Cruz' , 'Mar\u00eda Rojo' , 'Salma Hayek' , 'Bruno Bichir' ], 'title' : 'Midaq Alley' } ... Como ya vimos al hacer consultas en sesiones anterior, si no queremos el campo _id , tenemos que indicarlo: Consulta Resultado cursor = movies . find ( { \"cast\" : \"Salma Hayek\" }, { \"title\" : 1 , \"cast\" : 1 , \"_id\" : 0 } ) for zip in cursor : print ( zip ) { 'cast' : [ 'David Arquette' , 'John Hawkes' , 'Salma Hayek' , 'Jason Wiles' ], 'title' : 'Roadracers' } { 'cast' : [ 'Ernesto G\u00e8mez Cruz' , 'Mar\u00e8a Rojo' , 'Salma Hayek' , 'Bruno Bichir' ], 'title' : 'Midaq Alley' } { 'title' : 'Desperado' , 'cast' : [ 'Antonio Banderas' , 'Salma Hayek' , 'Joaquim de Almeida' , 'Cheech Marin' ]} ... BSON Cuando necesitemos pasar los documentos BSON a JSON, utilizaremos la funci\u00f3n dumps que transforma el documento a JSON: cursor = movies . find ( { \"cast\" : \"Salma Hayek\" } ) from bson.json_util import dumps print ( dumps ( cursor , indent = 2 )) \"\"\" [ { \"_id\": { \"$oid\": \"573a1399f29313caabceea6d\" }, \"plot\": \"Cynical look at a 50's rebellious Rocker who has to confront his future, thugs with knives, and the crooked town sheriff.\", \"genres\": [ \"Action\", \"Drama\" ], \"runtime\": 95, \"rated\": \"R\", \"cast\": [ \"David Arquette\", \"John Hawkes\", \"Salma Hayek\", \"Jason Wiles\" ], ... \"\"\"","title":"Primeras consultas"},{"location":"sa/07pymongo.html#agregaciones","text":"Para realizar consultas mediante el framework de agregaci\u00f3n, usaremos el m\u00e9todo aggregate , el cual recibe un array con el pipeline: Por ejemplo, vamos a recuperar el t\u00edtulo y el casting de las pel\u00edculas dirigidas por Sam Raimi : Consulta Resultados match_stage = { \"$match\" : { \"directors\" : \"Sam Raimi\" } } project_stage = { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } } pipeline = [ match_stage , project_stage ] sam_raimi_aggregation = movies . aggregate ( pipeline ) for movie in sam_raimi_aggregation : print ( movie ) { 'cas t ' : [ 'Bruce Campbell' , 'Elle n Sa n dweiss' , 'Richard DeMa n i n cor' , 'Be ts y Baker' ], ' t i tle ' : 'The Evil Dead' } { 'cas t ' : [ 'Bruce Campbell' , 'Sarah Berry' , 'Da n Hicks' , 'Kassie Wesley DePaiva' ], ' t i tle ' : 'Evil Dead II' } { 'cas t ' : [ 'Liam Neeso n ' , 'Fra n ces McDorma n d' , 'Coli n Friels' , 'Larry Drake' ], ' t i tle ' : 'Darkma n ' } Otro ejemplo, donde recuperamos los directores y la valoraci\u00f3n media de sus pel\u00edculas, ordenadas de mejor a peor: Consulta Resultado unwind_stage = { \"$unwind\" : \"$directors\" } group_stage = { \"$group\" : { \"_id\" : { \"director\" : \"$directors\" }, \"average_rating\" : { \"$avg\" : \"$imdb.rating\" } } } sort_stage = { \"$sort\" : { \"average_rating\" : - 1 } } # Creamos un pipeline con las tres fases pipeline = [ unwind_stage , group_stage , sort_stage ] director_ratings = movies . aggregate ( pipeline ) for director in director_ratings : print ( director ) { '_id' : { 'direc t or' : 'Sara Hirsh Bordo' }, 'average_ra t i n g' : 9.4 } { '_id' : { 'direc t or' : 'Kevi n Derek' }, 'average_ra t i n g' : 9.3 } { '_id' : { 'direc t or' : 'Michael Be ns o n ' }, 'average_ra t i n g' : 9.0 } { '_id' : { 'direc t or' : 'Sloboda n Sija n ' }, 'average_ra t i n g' : 8.95 } { '_id' : { 'direc t or' : \"Bozidar 'Bota' Nikolic\" }, 'average_ra t i n g' : 8.9 } ... O realizamos un join entre las pel\u00edculas y sus comentarios para recuperar una pel\u00edcula por su identificador: Consulta Resultado from bson.objectid import ObjectId peliculaId = \"573a13aef29313caabd2c349\" pipeline = [ { \"$match\" : { \"_id\" : ObjectId ( peliculaId ) } }, { \"$lookup\" : { \"from\" : \"comments\" , \"localField\" : \"_id\" , \"foreignField\" : \"movie_id\" , \"as\" : 'comentarios' } } ] peliculaConComentarios = movies . aggregate ( pipeline ) . next () print ( peliculaConComentarios ) { '_id' : Objec t Id(' 573 a 13 ae f 29313 caabd 2 c 349 ') , 'awards' : { ' n omi nat io ns ' : 48 , ' te x t ' : 'Nomi nate d f or 1 Oscar. A n o t her 21 wi ns & 48 n omi nat io ns .' , 'wi ns ' : 22 }, 'cas t ' : [ 'Chris t ia n Bale' , 'Michael Cai ne ' , 'Liam Neeso n ' , 'Ka t ie Holmes' ], 'come ntar ios' : [{ '_id' : Objec t Id(' 5 a 9427658 b 0 beebeb 696 f 672 ') , 'da te ' : da tet ime.da tet ime( 1972 , 3 , 2 , 15 , 58 , 41 ) , 'email' : 'do nal d_sump ter @gameo ft hro n .es' , 'movie_id' : Objec t Id(' 573 a 13 ae f 29313 caabd 2 c 349 ') , ' na me' : 'Maes ter Luwi n ' , ' te x t ' : 'A t que ...' }], 'cou ntr ies' : [ 'USA' , 'UK' ], 'direc t ors' : [ 'Chris t opher Nola n ' ], ' full plo t ' : 'Whe n ...\", 'genres': ['Action', 'Adventure'], 'imdb': {'id': 372784, 'rating': 8.3, 'votes': 860733}, 'languages': ['English', 'Urdu', 'Mandarin'], 'lastupdated': datetime.datetime(2015, 8, 31, 0, 1, 54, 590000), 'metacritic': 70, 'num_mflix_comments': 1, 'plot': 'After training ....', 'poster': 'https://m.media-amazon.com/images/M/MV5BZmUwNGU2ZmItMmRiNC00MjhlLTg5YWUtODMyNzkxODYzMmZlXkEyXkFqcGdeQXVyNTIzOTk5ODM@._V1_SY1000_SX677_AL_.jpg', 'rated': 'PG-13', 'released': datetime.datetime(2005, 6, 15, 0, 0), 'runtime': 140, 'title': 'Batman Begins', 'type': 'movie', 'writers': ['Bob Kane (characters)', 'David S. Goyer (story)', 'Christopher Nolan (screenplay)', 'David S. Goyer (screenplay)'], 'year': 2005}","title":"Agregaciones"},{"location":"sa/07pymongo.html#trabajando-con-cursores","text":"A continuaci\u00f3n vamos a realizar algunas operaciones sobre los cursores con PyMongo y a comparar a c\u00f3mo podemos realizar la misma operaci\u00f3n mediante el motor de agregaciones.","title":"Trabajando con cursores"},{"location":"sa/07pymongo.html#limitando","text":"Sobre el cursor podemos restringir la cantidad de resultados devueltos mediante el m\u00e9todo .limit() equivalente a la agregaci\u00f3n $limit : PyMongo Agregaci\u00f3n Salida limited_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } ) . limit ( 2 ) for movie in limited_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"title\" : 1 , \"cast\" : 1 } }, { \"$limit\" : 2 } ] limited_aggregation = movies . aggregate ( pipeline ) for movie in limited_aggregation : print ( movie ) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" }, { \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] } ]","title":"Limitando"},{"location":"sa/07pymongo.html#ordenando","text":"Para ordenar usaremos el m\u00e9todo .sort() que adem\u00e1s de los campos de ordenaci\u00f3n, indicaremos si el criterio ser\u00e1 ascendente ( ASCENDING ) o descendente ( DESCENDING ), de forma similar a como lo hacemos con la operaci\u00f3n de agregaci\u00f3n $sort : PyMongo Agregaci\u00f3n Salida from pymongo import DESCENDING , ASCENDING sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) for movie in sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) for movie in sorted_aggregation : print ( movie ) [ { \"cast\" : [ \"Bruce Campbell\" , \"Ellen Sandweiss\" , \"Richard DeManincor\" , \"Betsy Baker\" ], \"title\" : \"The Evil Dead\" , \"year\" : 1981 }, { \"year\" : 1987 , \"title\" : \"Evil Dead II\" , \"cast\" : [ \"Bruce Campbell\" , \"Sarah Berry\" , \"Dan Hicks\" , \"Kassie Wesley DePaiva\" ] }, { \"year\" : 1990 , ... } ] En el caso de tener una clave compuesta de ordenaci\u00f3n, le pasaremos como par\u00e1metro una lista de tuplas clave/criterio: PyMongo Agregaci\u00f3n Salida sorted_cursor = movies . find ( { \"cast\" : \"Tom Hanks\" }, { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } ) . sort ([( \"year\" , ASCENDING ), ( \"title\" , ASCENDING )]) for movie in sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"cast\" : \"Tom Hanks\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING , \"title\" : ASCENDING } } ] sorted_aggregation = movies . aggregate ( pipeline ) for movie in sorted_aggregation : print ( movie ) [ { \"cast\" : [ \"Tom Hanks\" , \"Daryl Hannah\" , \"Eugene Levy\" , \"John Candy\" ], \"title\" : \"Splash\" , \"year\" : 1984 }, { \"cast\" : [ \"Tom Hanks\" , \"Jackie Gleason\" , \"Eva Marie Saint\" , \"Hector Elizondo\" ], \"title\" : \"Nothing in Common\" , \"year\" : 1986 }, { \"cast\" : [ \"Tom Hanks\" , \"Elizabeth Perkins\" , \"Robert Loggia\" , \"John Heard\" ], \"title\" : \"Big\" , \"year\" : 1988 }, { \"cast\" : [ \"Sally Field\" , \"Tom Hanks\" , \"John Goodman\" , \"Mark Rydell\" ], \"title\" : \"Punchline\" , \"year\" : 1988 }, ... ]","title":"Ordenando"},{"location":"sa/07pymongo.html#saltando","text":"Cuando paginamos los resultados, para saltar los documentos, haremos uso del m\u00e9todo .skip() , el cual es similar a la operaci\u00f3n $skip . Por ejemplo, la siguiente consulta devuelve 13 documentos, de manera que al saltarnos 12, s\u00f3lo nos devolver\u00e1 uno: PyMongo Agregaci\u00f3n Salida skipped_sorted_cursor = movies . find ( { \"directors\" : \"Sam Raimi\" }, { \"_id\" : 0 , \"title\" : 1 , \"year\" : 1 , \"cast\" : 1 } ) . sort ( \"year\" , ASCENDING ) . skip ( 12 ) for movie in skipped_sorted_cursor : print ( movie ) pipeline = [ { \"$match\" : { \"directors\" : \"Sam Raimi\" } }, { \"$project\" : { \"_id\" : 0 , \"year\" : 1 , \"title\" : 1 , \"cast\" : 1 } }, { \"$sort\" : { \"year\" : ASCENDING } }, { \"$skip\" : 12 } ] sorted_skipped_aggregation = movies . aggregate ( pipeline ) for movie in sorted_skipped_aggregation : print ( movie ) [ { \"cast\" : [ \"James Franco\" , \"Mila Kunis\" , \"Rachel Weisz\" , \"Michelle Williams\" ], \"title\" : \"Oz the Great and Powerful\" , \"year\" : 2013 } ]","title":"Saltando"},{"location":"sa/07pymongo.html#crud","text":"Para estas operaciones, vamos a trabajar con la colecci\u00f3n iabd.people que utilizamos en la primera sesi\u00f3n de MongoDB .","title":"CRUD"},{"location":"sa/07pymongo.html#insercion","text":"from pymongo import MongoClient cliente = MongoClient ( \"mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/?retryWrites=true&w=majority\" , server_api = ServerApi ( '1' )) bd = cliente . iabd people = bd . people yo = { \"nombre\" : \"Aitor Medrano\" , \"edad\" : 45 , \"profesion\" : \"Profesor\" } resultado = people . insert_one ( yo ) print ( resultado . acknowledged ) # True print ( resultado . inserted_id ) # 6366b5c85afaf1b75dc90a20 Tras realizar una operaci\u00f3n de inserci\u00f3n con insert_one , obtenemos un objeto InsertOneResult del cual cabe destacar dos campos: acknowledged : un campo booleano que nos indica si la operaci\u00f3n ha sido exitosa, o False cuando indicamos un WriteConcern(w=0) , es decir, es escritura fire-and-forget . inserted_id : valor del ObjectId . Si queremos insertar m\u00e1s de una documento a la vez, usaremos insert_many , el cual devuelve un objeto InsertManyResult .","title":"Inserci\u00f3n"},{"location":"sa/07pymongo.html#borrado","text":"El borrado de documento es similar a la creaci\u00f3n, pudiendo utilizar delete_one para borrar el primer documento encontrado, o delete_many para borrar todos los documentos encontrados: resultado = people . delete_many ({ \"edad\" : 45 }) print ( resultado . deleted_count ) En ambas operaciones, obtenemos un objeto DeleteResult del cual destacamos la propiedad deleted_count para averiguar cuantos documentos han sido eliminados. Borrado de colecciones Para vaciar una colecci\u00f3n, podemos borrar todos los documentos: people . delete_many ({}) Aunque es mejor borrar la colecci\u00f3n entera: people . drop ()","title":"Borrado"},{"location":"sa/07pymongo.html#modificacion","text":"De forma similar a como lo hemos realizado mediante mongosh , para modificar documentos emplearemos los m\u00e9todos update_one y update_many , los cuales devuelve un objeto UpdateResult : resultado = people . update_one ({ \"nombre\" : \"Aitor Medrano\" }, { \"$set\" :{ \"nombre\" : \"Marina Medrano\" , \"salario\" : 123456 }}) print ( resultado . matched_count ) # (1)! print ( resultado . modified_count ) # (2)! matched_count nos devuelve la cantidad de documentos encontrados modified_count nos devuelve la cantidad de documentos modificados Tal como vimos, en las operaciones update , le podemos pasar un tercer par\u00e1metro (opcional) con upsert=False (valor por defecto) op True , cuando queramos que se inserte un nuevo documento si no encuentra ninguno. En este caso, modified_count y matched_count ser\u00e1n 0. Otro argumento opcional que conviene conocer es bypass_document_validation=False (valor por defecto), el cual, si lo ponemos a True ignorar\u00e1 las validaciones (si hay alguna) para los documentos que modifiquemos. Operador Recuerda utilizar un operador de modificaci\u00f3n en el segundo par\u00e1metro. Si no, el documento del segundo par\u00e1metro sustituir\u00e1 por completo al documento encontrado.","title":"Modificaci\u00f3n"},{"location":"sa/07pymongo.html#operaciones-consistentes","text":"","title":"Operaciones consistentes"},{"location":"sa/07pymongo.html#preferencias-de-lectura","text":"Para realizar lecturas consistentes podemos configurar la preferencias de lectura, tal como vimos en la sesi\u00f3n anterior . Para ello, PyMongo ofrece un conjunto de clases para indicar las preferencias de lectura , las cuales podemos configurar a nivel de cliente: Niveles de lectura Los diferentes niveles vienen definidos en el objeto ReadPreference el cual ofrece los valores estudiados en la sesi\u00f3n anterior: PRIMARY , PRIMARY_PREFERRED , SECONDARY , SECONDARY_PREFERRED y NEAREST . client = MongoClient ( 'localhost:27017' , replicaSet = 'iabd' , readPreference = 'secondaryPreferred' ) Si en vez de indicar la preferencia a nivel de cliente, lo queremos realizar a nivel de base de datos o de colecci\u00f3n, hemos de emplear los m\u00e9todos get_database y get_collection pas\u00e1ndole un segundo par\u00e1metro con la opci\u00f3n deseada: from pymongo import ReadPreference print ( client . read_preference ) # SecondaryPreferred(tag_sets=None) db = client . get_database ( 'iabd' , read_preference = ReadPreference . SECONDARY ) print ( db . read_preference ) # Secondary(tag_sets=None) coll = db . get_collection ( 'people' , read_preference = ReadPreference . PRIMARY ) print ( coll . read_preference ) # Primary() Finalmente, si en cualquier momento queremos cambiar las preferencias a nivel de colecci\u00f3n, podemos emplear el m\u00e9todo with_options : coll2 = coll . with_options ( read_preference = ReadPreference . NEAREST ) print ( coll . read_preference ) # Primary() print ( coll . read_preference ) # Nearest(tag_sets=None)","title":"Preferencias de lectura"},{"location":"sa/07pymongo.html#escrituras-consistentes","text":"Tal como vimos en la sesi\u00f3n anterior, para indicar la consistencia en la escritura, haremos uso de la propiedad write_concern . Para ello, de la misma forma que hemos indicado la preferencia de lectura, hacemos uso del m\u00e9todo with_options : coll . with_options ( write_concern = WriteConcern ( w = \"majority\" )) . insert_one ({ \"nombre\" : nombre , \"email\" : email , \"password\" : hashed_pw }) Si tenemos una aplicaci\u00f3n cr\u00edtica, no nos podemos permitir perder los datos. Por ello, necesitamos que las escritura se propaguen a la mayor\u00eda de los nodos con la opci\u00f3n w: majority para asegurarnos que las escrituras se propagan a la mayor\u00eda de nodos del conjunto de nodos. Si sucede un problema en los nodos secundarios, puede ser que el primario no reciba los ACK necesarios. Si llegan m\u00e1s escrituras que lecturas, puede llegar el momento en que se produzca un atasco. Para evitar esto, para cada escritura que realicemos con la mayor\u00eda de los nodos, siempre hay que indicar un timeout . La longitud del timeout vendr\u00e1 determinada por la red y el hardware que dispongamos, pero siempre hemos de indicarlo. coll . with_options ( write_concern = WriteConcern ( w = \"majority\" , wtimeout = \"2500\" ) . insert_one ({ \"nombre\" : nombre , \"email\" : email , \"password\" : hashed_pw }) Bulk writes En ocasiones necesitamos ejecutar una bateria de operaciones a granel ( bulk ) las cuales se ejecutan como un proceso batch . Para ello se emplea el m\u00e9todo bulk_write : result = db . test . bulk_write ( array_operaciones )","title":"Escrituras consistentes"},{"location":"sa/07pymongo.html#transacciones","text":"Ya hemos comentado en numerosas ocasiones toda operaci\u00f3n en un \u00fanico documento es at\u00f3mica, de ah\u00ed embeber documentos y arrays para modelar las relaciones de datos en un s\u00f3lo documento cubre la mayor\u00eda de los casos de usos transaccionales. Cuando necesitamos atomicidad de lecturas y escrituras entre varios documentos (en una o m\u00e1s colecciones), MongoDB soporta transacciones multidocumento. Mediante las transacciones distribuidas, las transacciones puede operar entre varias operaciones, colecciones, bases de datos, documentos y particiones ( shards ).","title":"Transacciones"},{"location":"sa/07pymongo.html#tipos-de-api","text":"MongoDB ofrece dos API para utilizar transacciones. La primera, conocida como Core API , disponible desde la versi\u00f3n 4.0 de MongoDB , tiene una sintaxis similar a las bases de datos relacionales (por ejemplo, con operaciones como start_transaction y commit_transaction ), y la segunda se conoce como Callback API , desde la versi\u00f3n 4.2, el cual es el enfoque actualmente recomendado. El Core API no ofrece l\u00f3gica de reintentos para la mayor\u00eda de errores y necesita que el desarrollador programe la l\u00f3gica de las operaciones, la funci\u00f3n transaccional que realiza commit y la l\u00f3gica de errores y reintentos necesaria. En cambio, el Callback API ofrece una \u00fanica funci\u00f3n que engloba un alto grado de funcionalidad comparadas con el Core API , incluyendo el inicio de una transacci\u00f3n asociada a una sesi\u00f3n l\u00f3gica, ejecutar la funci\u00f3n definida como callback y realizando el commit de la transacci\u00f3n (o abort\u00e1ndola en caso de error). Esta funci\u00f3n tambi\u00e9n incluye la l\u00f3gica de reintentos para manejar los errores al hacer commit . En ambas APIs, el desarrollador se responsabiliza de iniciar la sesi\u00f3n l\u00f3gica que se utilizar\u00e1 en la transacci\u00f3n. Ambas APIs requieren que las operaciones transaccionales se asocien a \u00e9sta sesi\u00f3n l\u00f3gica (pas\u00e1ndola como par\u00e1metro a cada operaci\u00f3n). Cada sesi\u00f3n l\u00f3gica en MongoDB registra el tiempo y la secuencia de las operaciones en el contexto completo del despliegue de MongoDB .","title":"Tipos de API"},{"location":"sa/07pymongo.html#hola-mundo-callback-api","text":"Vamos a simular que tenemos una aplicaci\u00f3n de gesti\u00f3n de un almac\u00e9n, en la cual tenemos que realizar las siguientes operaciones dentro de una transacci\u00f3n: pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 100 }, session = miSesion ) inventario . update_one ({ \"producto\" : \"ps5\" , \"cantidad\" : { \"$gte\" : 100 }}, { \"$inc\" : { \"cantidad\" : - 100 }}, session = miSesion ) Veamos mediante un ejemplo el flujo del c\u00f3digo transaccional haciendo uso del Callback API . almacenTransaccional.py from pymongo import MongoClient , ReadPreference from pymongo.write_concern import WriteConcern from pymongo.read_concern import ReadConcern uriString = 'mongodb+srv://iabd:iabdiabd@cluster0.4hm7u8y.mongodb.net/?retryWrites=true&w=majority' cliente = MongoClient ( uriString ) miWCMajority = WriteConcern ( 'majority' , wtimeout = 1000 ) # Paso 0: Creamos dos colecciones e insertamos un documento en cada una bd = client . get_database ( \"iabd\" , write_concern = miWCMajority ) bd . pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 0 }) bd . inventario . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 1000 }) # Paso 1: Definir el callback que indica la secuencia de las operaciones a realizar dentro de la transacci\u00f3n def callback ( miSesion ): pedidos = miSesion . cliente . iabd . pedidos inventario = miSesion . cliente . iabd . inventario # Importante: Debemos pasarle la sesi\u00f3n a cada operaci\u00f3n pedidos . insert_one ({ \"producto\" : \"ps5\" , \"cantidad\" : 100 }, session = miSesion ) inventario . update_one ({ \"producto\" : \"ps5\" , \"cantidad\" : { \"$gte\" : 100 }}, { \"$inc\" : { \"cantidad\" : - 100 }}, session = miSesion ) # Paso 2: Iniciar una sesi\u00f3n de cliente. with cliente . start_session () as session : # Paso 3: Empleamos with_transaction para iniciar la transacci\u00f3n, ejecutar el callback y realizar el commit (o abortar en caso de error). session . with_transaction ( callback , read_concern = ReadConcern ( \"local\" ), write_concern = miWCMajority , read_preference = ReadPreference . PRIMARY , ) PyMODM PyMODM es una librer\u00eda ODM que abstrae el acceso a MongoDB mediante un mapeo objeto-documento. De forma similar a los frameworks ORM en los entornos relacionales, como son Hibernate ( Java ), Doctrine / Eloquent ( PHP ) o el ORM de Django , abstrae y simplifica el acceso a la base de datos. Un ejemplo de definici\u00f3n de un objeto y su persistencia ser\u00eda: class User ( MongoModel ): # Utiliza el 'email'como campo '_id' email = fields . EmailField ( primary_key = True ) nombre = fields . CharField () apellido = fields . CharField () usuario = User ( 'a.medrano@edu.gva.es' , 'Aitor' , 'Medrano' ) . save () Hemos decidido no profundizar en su conocimiento ya que el equipo de MongoDB ha pausado su mantenimiento desde hace ya dos a\u00f1os.","title":"Hola Mundo Callback API"},{"location":"sa/07pymongo.html#caso-de-uso-pia-login","text":"El siguiente proyecto se basa inicialmente en el art\u00edculo How To Add Authentication to Your App with Flask-Login . A partir de \u00e9l, vamos a crear una aplicaci\u00f3n en Flask que ataca a una base de datos de MongoDB para almacenar la informaci\u00f3n del proyecto PIA Lara. Esta primera versi\u00f3n de la aplicaci\u00f3n \u00fanicamente se centra en la gesti\u00f3n de los usuarios, distinguiendo entre los diferentes roles: Administrador : superusuario, puede crear, editar y eliminar todo tipo de usuarios. T\u00e9cnico : usuario que supervisar\u00e1 a los clientes, el cual, m\u00e1s adelante, puede llegar a crear textos predefinidos para los clientes. Cliente : usuario final de la aplicaci\u00f3n que, m\u00e1s adelante, grabar\u00e1 los audios. Simplicidad El presente caso de uso se ha organizado para intentar simplificar al m\u00e1ximo el c\u00f3digo y preparar un esqueleto que facilite el crecimiento de la aplicaci\u00f3n. A\u00fan as\u00ed, una soluci\u00f3n basada en Django o el uso de herramientas de mapeo entre los datos y los objetos del dominio ser\u00edan un punto de partida para siguientes fases del proyecto.","title":"Caso de uso - PIA: Login"},{"location":"sa/07pymongo.html#estructura","text":"Una vez descargado el proyecto y tras descomprimirlo, o clonado desde https://github.com/aitor-medrano/piafplogin , veremos que tiene la siguiente estructura: PIAFPLOGIN/ \u251c\u2500\u2500 migrations/ \u2502 \u2514\u2500\u2500 user_migration.py \u251c\u2500\u2500 project/ \u2502 \u251c\u2500\u2500 static/ \u2502 \u2502 \u251c\u2500\u2500 pialara.js \u2502 \u2502 \u2514\u2500\u2500 pialara.png \u2502 \u251c\u2500\u2500 templates/ \u2502 \u2502 \u251c\u2500\u2500 base.html \u2502 \u2502 \u251c\u2500\u2500 index.html \u2502 \u2502 \u251c\u2500\u2500 login.html \u2502 \u2502 \u2514\u2500\u2500 profile.html \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 db.py \u2502 \u251c\u2500\u2500 main.py \u2502 \u2514\u2500\u2500 models.py \u251c\u2500\u2500 .ini \u2514\u2500\u2500 requirements.txt El primer paso es crear un entorno virtual: python3 -m venv app-env Y activarlo: source app-env/bin/activate A continuaci\u00f3n instalaremos las dependencias mediante: pip3 install -r requirements.txt","title":"Estructura"},{"location":"sa/07pymongo.html#configuracion","text":"Para configurar el proyecto, partimos del fichero .ini que reside en la ra\u00edz del mismo y contiene los datos de configuraci\u00f3n a MongoDB y la clave secreta que utiliza Flask para encriptar la sesi\u00f3n: .ini [PROD] SECRET_KEY = eac5e91171438960ddec0c9c469a4c3dd42e96aea462afc5ab830f78527ad80e PIALARA_DB_URI = mongodb+srv://usuario:contrase\u00f1a@cluster0.xyz.mongodb.net PIALARA_DB_NAME = pialara [LOCAL] SECRET_KEY = eac5e91171438960ddec0c9c469a4c3dd42e96aea462afc5ab830f78527ad80e PIALARA_DB_URI = localhost PIALARA_DB_NAME = pialara Secret Key Para generar una clave secreta, tal como indica la documentaci\u00f3n de Flask , podemos ejecutar el siguiente comando: python3 -c 'import secrets; print(secrets.token_hex())' Una vez ya hemos configurado la conexi\u00f3n y antes de arrancar la aplicaci\u00f3n, vamos a cargar unos datos b\u00e1sicos con usuarios. Para ello, en la carpeta migrations tenemos el archivo users_migration.py , el cual lee la configuraci\u00f3n del archivo anterior, y crea tres usuarios: users_migration.py from pymongo import MongoClient from werkzeug.security import generate_password_hash import os import configparser config = configparser . ConfigParser () config . read ( os . path . abspath ( os . path . join ( \".ini\" ))) DB_URI = config [ 'PROD' ][ 'PIALARA_DB_URI' ] DB_NAME = config [ 'PROD' ][ 'PIALARA_DB_NAME' ] # DB_URI = config['LOCAL']['PIALARA_DB_URI'] # DB_NAME = config['LOCAL']['PIALARA_DB_NAME'] db = MongoClient ( DB_URI )[ DB_NAME ] usuarios = [ { \"nombre\" : \"Admin\" , \"email\" : \"admin@admin.com\" , \"password\" : generate_password_hash ( \"admin\" , method = 'sha256' ), \"rol\" : \"Administrador\" }, { \"nombre\" : \"Alumno\" , \"email\" : \"alumno@alumno.com\" , \"password\" : generate_password_hash ( \"alumno\" , method = 'sha256' ), \"rol\" : \"T\u00e9cnico\" }, { \"nombre\" : \"Severo Ochoa\" , \"email\" : \"s8a@s8a.com\" , \"password\" : generate_password_hash ( \"s8a\" , method = 'sha256' ), \"rol\" : \"Cliente\" , \"parent\" : \"alumno@alumno.com\" }, ] try : db . users . insert_many ( usuarios ) except Exception as e : print ( e ) Destacar que al definir los documentos con los usuarios, encriptamos la contrase\u00f1a mediante la funci\u00f3n generate_password_hash para no almacenarla en la base de datos en texto plano. As\u00ed pues, ejecutamos la migraci\u00f3n: python3 migrations/users_migration.py Finalmente podemos arrancar la aplicaci\u00f3n: flask --app project --debug run Y acceder a la aplicaci\u00f3n a trav\u00e9s de http://127.0.0.1:5000/ : Versi\u00f3n escritorio Versi\u00f3n m\u00f3vil PIA Lara - Index PIA Lara - Index en m\u00f3vil Una vez que un usuario ha entrado a la aplicaci\u00f3n, por ejemplo, si es un administrador, dispondr\u00e1 de las opciones que hemos comentado anteriormente: PIA Lara - Administrador","title":"Configuraci\u00f3n"},{"location":"sa/07pymongo.html#blueprints","text":"El archivo .ini que hemos configurado previamente define unos valores que la aplicaci\u00f3n va a cargar desde el archivo __init__.py , el cual act\u00faa como factor\u00eda de la aplicaci\u00f3n y le indica a Flask los blueprints a utilizar: __init__.py from flask import Flask from flask_login import LoginManager from . import db import os import configparser config = configparser . ConfigParser () config . read ( os . path . abspath ( os . path . join ( \".ini\" ))) def create_app (): app = Flask ( __name__ ) # cargamos la configuraci\u00f3n app . config [ 'PIALARA_DB_URI' ] = config [ 'LOCAL' ][ 'PIALARA_DB_URI' ] app . config [ 'PIALARA_DB_NAME' ] = config [ 'LOCAL' ][ 'PIALARA_DB_NAME' ] app . config [ 'SECRET_KEY' ] = config [ 'LOCAL' ][ 'SECRET_KEY' ] # configuramos flask-login con la ruta del login login_manager = LoginManager () login_manager . login_view = 'auth.login' login_manager . init_app ( app ) # funci\u00f3n que utiliza flask-login para recuperar el usuario @login_manager . user_loader def load_user ( user_id ): return db . get_user_by_id ( user_id ) # blueprint para las rutas de autenticaci\u00f3n from .auth import auth as auth_blueprint app . register_blueprint ( auth_blueprint ) # blueprint para la aplicaci\u00f3n from .main import main as main_blueprint app . register_blueprint ( main_blueprint ) return app Un blueprint permite organizar un grupo de vistas y c\u00f3digo en m\u00f3dulos. En vez de registrar las vistas y el resto de c\u00f3digo en la aplicaci\u00f3n, se registran en el blueprint, y \u00e9ste es que se registra en la aplicaci\u00f3n en la funci\u00f3n create_app . En nuestro caso, vamos a empezar con dos blueprints , uno para las funciones de autenticaci\u00f3n, y otra para las funciones de gesti\u00f3n de usuarios. El c\u00f3digo de cada blueprint ir\u00e1 en un m\u00f3dulo separado. Como la gesti\u00f3n de usuarios necesita primero la autenticaci\u00f3n, vamos a ver c\u00f3mo funciona.","title":"Blueprints"},{"location":"sa/07pymongo.html#login","text":"Para la gesti\u00f3n de la autenticaci\u00f3n, nos hemos apoyado en la librer\u00eda Flask-login que facilita la gesti\u00f3n la sesi\u00f3n del usuario. En archivo auth.py contiene la l\u00f3gica del login y el logout : auth.py from flask import Blueprint , render_template , redirect , url_for , request , flash from flask_login import login_user , login_required , logout_user , current_user from werkzeug.security import check_password_hash from . import db auth = Blueprint ( 'auth' , __name__ ) @auth . route ( '/login' ) def login (): return render_template ( 'login.html' ) @auth . route ( '/login' , methods = [ 'POST' ]) def login_post (): email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) remember = True if request . form . get ( 'remember' ) else False user = db . get_user ( email ) # comprobamos si el usuario existe # cogemos la contrase\u00f1a, la hasheamos y la comparamos con la contrase\u00f1a hasheada if not user or not check_password_hash ( user . password , password ): flash ( 'Por favor, comprueba tus datos y vu\u00e9lvelo a intentar.' ) # si el usuario no existe, o est\u00e1 mal la contrase\u00f1a, recargamos la p\u00e1gina return redirect ( url_for ( 'auth.login' )) # marcamos al usuario como autenticado en flask_login login_user ( user , remember = remember ) return redirect ( url_for ( 'main.profile' , nombre = current_user . nombre )) @auth . route ( '/logout' ) @login_required def logout (): logout_user () flash ( 'Sesi\u00f3n cerrada con \u00e9xito' ) return redirect ( url_for ( 'auth.login' )) Los usuarios van a entrar al sistema mediante su email y una contrase\u00f1a. As\u00ed pues, una vez hayamos recuperado un usuario por dicho email, creamos el hash de la contrase\u00f1a recibida, y vemos si comprueba con la recuperada de la base de datos. El m\u00e9todo login_user de la l\u00ednea 27 pertenece a la librer\u00eda Flask-Login y se utiliza para indicar que el usuario ha sido autenticado, de manera que lo almacena en la sesi\u00f3n. La variable user es una clase propia que hemos definido nosotros con los atributos b\u00e1sicos de un usuario,el cual se encuentra en el archivo models.py : models.py from flask_login import UserMixin class User ( UserMixin ): def __init__ ( self , id , email , nombre , password , rol , parent = \"\" ): self . id = id self . email = email self . nombre = nombre self . password = password self . rol = rol self . parent = parent def __str__ ( self ): return f \" { self . email } ( { self . nombre } / { self . password } )\" Como se puede observar, la clase define los atributos b\u00e1sicos de un usuario. El atributo parent lo vamos a emplear para que los clientes almacenen el email del t\u00e9cnico que tienen asignado.","title":"Login"},{"location":"sa/07pymongo.html#plantillas","text":"Las diferentes plantillas heredan de una plantilla base.html , la cual emplea el framework Bulma para la apariencia de la web. Su funcionamiento es muy similar a Bootstrap. Por ejemplo, vamos a revisar un fragmento de la plantilla base para ver c\u00f3mo gestionamos la visualizaci\u00f3n del men\u00fa dependiendo del rol del usuario: base.html ... < section class = \"hero is-white is-fullheight\" > < nav class = \"navbar is-transparent\" > < div class = \"navbar-brand\" > < a class = \"navbar-item\" href = \"https://piafplara.es\" > < img src = \"{{ url_for('static', filename='pialara.png') }}\" alt = \"PIA Lara: un proyecto que habla por ti\" width = \"112\" height = \"28\" > </ a > < div class = \"navbar-burger burger\" data-target = \"navbarPIALara\" > < span ></ span > < span ></ span > < span ></ span > </ div > </ div > < div id = \"navbarPIALara\" class = \"navbar-menu\" > < div class = \"navbar-start\" > < a href = \"{{ url_for('main.index') }}\" class = \"navbar-item\" > Inicio </ a > {% if not current_user.is_authenticated %} < a href = \"{{ url_for('auth.login') }}\" class = \"navbar-item\" > Login </ a > {% endif %} {% if current_user.is_authenticated %} < a href = \"{{ url_for('main.profile') }}\" class = \"navbar-item\" > Perfil </ a > {% if current_user.rol == \"Administrador\" %} < div class = \"navbar-item has-dropdown is-hoverable\" > < a class = \"navbar-link\" href = \"#\" > Usuarios </ a > < div class = \"navbar-dropdown is-hidden-mobile is-boxed\" > < a class = \"navbar-item\" href = \"{{ url_for('main.user_create') }}\" > Alta </ a > < a class = \"navbar-item\" href = \"{{ url_for('main.user_list') }}\" > Listado </ a > </ div > </ div > {% endif %} ... En la l\u00ednea 6 utilizamos la funci\u00f3n url_for con el par\u00e1metro static para indicarle que cargue la imagen con el logo del proyecto desde la carpeta static . Al utilizar la librer\u00eda Flask Login , tendremos siempre disponible el usuario logueado en la variable current_user . Adem\u00e1s de las propiedades que hayamos definido en la clase, disponemos de la funci\u00f3n is_authenticated para comprobar si est\u00e1 autenticado (l\u00ednea 20). De igual forma, podemos comprobar el rol y condicionar el contenido dependiendo de si es Administrador , T\u00e9cnico o Cliente (l\u00ednea 24).","title":"Plantillas"},{"location":"sa/07pymongo.html#acceso-a-los-datos","text":"Todo el acceso a los datos los hemos encapsulado en el archivo db.py : db.py from pymongo import MongoClient from bson.objectid import ObjectId from pymongo import ASCENDING from flask import current_app , g from werkzeug.local import LocalProxy from project.models import User def get_db (): \"\"\" M\u00e9todo de configuraci\u00f3n para obtener una instancia de db \"\"\" db = getattr ( g , \"_database\" , None ) PIALARA_DB_URI = current_app . config [ \"PIALARA_DB_URI\" ] PIALARA_DB_DB_NAME = current_app . config [ \"PIALARA_DB_NAME\" ] if db is None : db = g . _database = MongoClient ( PIALARA_DB_URI , maxPoolSize = 50 , timeoutMS = 2500 )[ PIALARA_DB_DB_NAME ] return db # Utilizamos LocalProxy para leer la variable global usando s\u00f3lo db db = LocalProxy ( get_db ) La funci\u00f3n get_db utiliza el objeto g , el cual en Flask, es un objeto especial que es \u00fanico para cada petici\u00f3n. Se utiliza para almacenar datos que ser\u00e1n accesibles desde m\u00faltiples funciones durante el request . As\u00ed pues, almacenamos la conexi\u00f3n, mejor dicho, el pool de conexiones a MongoDB en vez de crear un nuevo pool cada vez que queramos obtener acceso a la base de datos. A continuaci\u00f3n, creamos un LocalProxy para leer la variable global usando s\u00f3lo la referencia db , de manera que internamente cada referencia a db realmente est\u00e1 llamando a get_db() . A continuaci\u00f3n, mostramos un par de m\u00e9todos del mismo archivo que muestran c\u00f3mo obtenemos datos desde MongoDB haciendo uso de PyMongo : db.py def get_all_users (): \"\"\" Devuelve una lista con todos los usuarios del sistema \"\"\" try : return list ( db . users . find ({}) . sort ( \"nombre\" , ASCENDING )) except Exception as e : return e def get_user_by_id ( id ): \"\"\" Devuelve un objeto User a partir de su id \"\"\" try : usuario = db . users . find_one ({ \"_id\" : ObjectId ( id )}) usuario_obj = User ( id = usuario [ \"_id\" ], email = usuario . get ( \"email\" ), nombre = usuario . get ( \"nombre\" ), password = usuario . get ( \"password\" ), rol = usuario . get ( \"rol\" ), parent = usuario . get ( \"parent\" )) return usuario_obj except Exception as e : return e ... Cuando recuperamos un usuario por su id , lo convertimos en un objeto User para que Flask Login falicita la gesti\u00f3n de la autenticaci\u00f3n. En cambio, en el listado de todos los usuarios, vamos a acceder al cursor de usuarios que ofrece MongoDB .","title":"Acceso a los datos"},{"location":"sa/07pymongo.html#referencias","text":"Tutorial oficial de PyMongo Introduction to Multi-Document ACID Transactions in Python How To Use Transactions in MongoDB","title":"Referencias"},{"location":"sa/07pymongo.html#actividades","text":"( RA5074.3 / CE4.3d / 4p) A partir del caso de uso de PIA Login, se pide: (0.25) Configurar la URI de Mongo Atlas para atacar vuestra propia base de datos. (0.25) Modificar la migraci\u00f3n para introducir m\u00e1s usuarios (al menos uno m\u00e1s de cada rol) (0.75) Cuando un usuario pulsa sobre su nombre, actualmente aparece un formulario para editar sus datos, pero no puede cambiar la contrase\u00f1a. Modifica (o crea) el/los formulario/s adecuado/s para que cada usuario pueda cambiar su propia contrase\u00f1a. (0.75) Desde el rol Administrador , al crear un usuario, si es un cliente, debe mostrar un desplegable con todos los t\u00e9cnicos disponibles. (1) Tanto el T\u00e9cnico como el Cliente , al dar de alta o editar un cliente, almacenar\u00e1n datos necesarios para el proyecto, como son el sexo, la fecha de nacimiento y la patolog\u00eda. (1) Cuando un T\u00e9cnico visualiza el listado de sus clientes, debe recuperar \u00fanicamente el nombre, el sexo, la edad y su patolog\u00eda.","title":"Actividades"},{"location":"sa/08indices.html","text":"\u00cdndices \u00b6","title":"\u00cdndices"},{"location":"sa/08indices.html#indices","text":"","title":"\u00cdndices"},{"location":"sa/09pymodm.html","text":"PyMODM \u00b6 Mastering MongodB - https://learning.oreilly.com/library/view/mastering-mongodb-6-x/9781803243863/B18155_02.xhtml#_idParaDest-53 PyMODM ODM Similar to Ruby\u2019s Mongoid, PyMODM is an ODM for Python that follows Django\u2019s built-in ORM closely. Installing pymodm can be done via pip, as shown in the following code: pip install pymodm Then, we need to edit settings.py and replace the ENGINE database with a dummy database, as shown in the following code: DATABASES = { 'default': { 'ENGINE': 'django.db.backends.dummy' } } Then we add our connection string anywhere in settings.py, as shown in the following code: from pymodm import connect connect(\"mongodb://localhost:27017/myDatabase\", alias=\"MyApplication\") Here, we have to use a connection string that has the following structure: mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]] Options have to be pairs of name=value with an & between each pair. Some interesting pairs are shown in the following table: Table 2.3 \u2013 PyMODM configuration options Table 2.3 \u2013 PyMODM configuration options Model classes need to inherit from MongoModel. The following code shows what a sample class will look like: from pymodm import MongoModel, fields class User(MongoModel): email = fields.EmailField(primary_key=True) first_name = fields.CharField() last_name = fields.CharField() This has a User class with first_name, last_name, and email fields, where email is the primary field. Inheritance with PyMODM models Handling one-to-one and one-to-many relationships in MongoDB can be done using references or embedding. The following example shows both ways, which are references for the model user and embedding for the comment model: from pymodm import EmbeddedMongoModel, MongoModel, fields class Comment(EmbeddedMongoModel): author = fields.ReferenceField(User) content = fields.CharField() class Post(MongoModel): title = fields.CharField() author = fields.ReferenceField(User) revised_on = fields.DateTimeField() content = fields.CharField() comments = fields.EmbeddedDocumentListField(Comment) Similar to Mongoid for Ruby, we can define relationships as being embedded or referenced depending on our design decision. Consultas \u00b6 Querying documents Querying is done using QuerySet, as described previously. Some of the convenience methods that are available include the following: all() count() first() exclude( fields): To exclude some fields from the result only( fields): To include only some fields in the result (this can be chained for a union of fields) limit(limit) order_by(ordering) reverse(): If we want to reverse the order_by() order skip(number) values(): To return Python dict instances instead of model instances By using raw(), we can use the same queries that we described in the previous PyMongo section for querying and still exploit the flexibility and convenience methods provided by the ODM layer. CRUD \u00b6 PyMODM is a core ODM that provides simple and extensible functionality. It is developed and maintained by MongoDB\u2019s engineers who get fast updates and support for the latest stable version of MongoDB available. In Chapter 2, Schema Design and Data Modeling, we explored how to define different models and connect to MongoDB. CRUD, when using PyMODM, as with every ODM, is simpler than when using low-level drivers. Creating documents A new user object, as defined in Chapter 2, Schema Design and Data Modeling, can be created with a single line: user = User('alexgiamas@packt.com', 'Alex', 'Giamas').save() In this example, we used positional arguments in the same order that they were defined in the user model to assign values to the user model attributes. We can also use keyword arguments or a mix of both, as follows: user = User(email='alexgiamas@packt.com', 'Alex', last_name='Giamas').save() Bulk saving can be done by passing in an array of users to bulk_create(): users = [ user1, user2,...,userN] User.bulk_create(users) Updating documents We can modify a document by directly accessing the attributes and calling save() again: user.first_name = 'Alexandros' user.save() If we want to update one or more documents, we must use raw() to filter out the documents that will be affected and chain update() to set the new values: User.objects.raw({'first_name': {'$exists': True}}) .update({'$set': {'updated_at': datetime.datetime.now()}}) In the preceding example, we search for all User documents that have a first name and set a new field, updated_at, to the current timestamp. The result of the raw() method is QuerySet, a class used in PyMODM to handle queries and work with documents in bulk. Deleting documents Deleting an API is similar to updating it \u2013 by using QuerySet to find the affected documents and then chaining on a .delete() method to delete them: User.objects.raw({'first_name': {'$exists': True}}).delete()","title":"PyMODM"},{"location":"sa/09pymodm.html#pymodm","text":"Mastering MongodB - https://learning.oreilly.com/library/view/mastering-mongodb-6-x/9781803243863/B18155_02.xhtml#_idParaDest-53 PyMODM ODM Similar to Ruby\u2019s Mongoid, PyMODM is an ODM for Python that follows Django\u2019s built-in ORM closely. Installing pymodm can be done via pip, as shown in the following code: pip install pymodm Then, we need to edit settings.py and replace the ENGINE database with a dummy database, as shown in the following code: DATABASES = { 'default': { 'ENGINE': 'django.db.backends.dummy' } } Then we add our connection string anywhere in settings.py, as shown in the following code: from pymodm import connect connect(\"mongodb://localhost:27017/myDatabase\", alias=\"MyApplication\") Here, we have to use a connection string that has the following structure: mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]] Options have to be pairs of name=value with an & between each pair. Some interesting pairs are shown in the following table: Table 2.3 \u2013 PyMODM configuration options Table 2.3 \u2013 PyMODM configuration options Model classes need to inherit from MongoModel. The following code shows what a sample class will look like: from pymodm import MongoModel, fields class User(MongoModel): email = fields.EmailField(primary_key=True) first_name = fields.CharField() last_name = fields.CharField() This has a User class with first_name, last_name, and email fields, where email is the primary field. Inheritance with PyMODM models Handling one-to-one and one-to-many relationships in MongoDB can be done using references or embedding. The following example shows both ways, which are references for the model user and embedding for the comment model: from pymodm import EmbeddedMongoModel, MongoModel, fields class Comment(EmbeddedMongoModel): author = fields.ReferenceField(User) content = fields.CharField() class Post(MongoModel): title = fields.CharField() author = fields.ReferenceField(User) revised_on = fields.DateTimeField() content = fields.CharField() comments = fields.EmbeddedDocumentListField(Comment) Similar to Mongoid for Ruby, we can define relationships as being embedded or referenced depending on our design decision.","title":"PyMODM"},{"location":"sa/09pymodm.html#consultas","text":"Querying documents Querying is done using QuerySet, as described previously. Some of the convenience methods that are available include the following: all() count() first() exclude( fields): To exclude some fields from the result only( fields): To include only some fields in the result (this can be chained for a union of fields) limit(limit) order_by(ordering) reverse(): If we want to reverse the order_by() order skip(number) values(): To return Python dict instances instead of model instances By using raw(), we can use the same queries that we described in the previous PyMongo section for querying and still exploit the flexibility and convenience methods provided by the ODM layer.","title":"Consultas"},{"location":"sa/09pymodm.html#crud","text":"PyMODM is a core ODM that provides simple and extensible functionality. It is developed and maintained by MongoDB\u2019s engineers who get fast updates and support for the latest stable version of MongoDB available. In Chapter 2, Schema Design and Data Modeling, we explored how to define different models and connect to MongoDB. CRUD, when using PyMODM, as with every ODM, is simpler than when using low-level drivers. Creating documents A new user object, as defined in Chapter 2, Schema Design and Data Modeling, can be created with a single line: user = User('alexgiamas@packt.com', 'Alex', 'Giamas').save() In this example, we used positional arguments in the same order that they were defined in the user model to assign values to the user model attributes. We can also use keyword arguments or a mix of both, as follows: user = User(email='alexgiamas@packt.com', 'Alex', last_name='Giamas').save() Bulk saving can be done by passing in an array of users to bulk_create(): users = [ user1, user2,...,userN] User.bulk_create(users) Updating documents We can modify a document by directly accessing the attributes and calling save() again: user.first_name = 'Alexandros' user.save() If we want to update one or more documents, we must use raw() to filter out the documents that will be affected and chain update() to set the new values: User.objects.raw({'first_name': {'$exists': True}}) .update({'$set': {'updated_at': datetime.datetime.now()}}) In the preceding example, we search for all User documents that have a first name and set a new field, updated_at, to the current timestamp. The result of the raw() method is QuerySet, a class used in PyMODM to handle queries and work with documents in bulk. Deleting documents Deleting an API is similar to updating it \u2013 by using QuerySet to find the affected documents and then chaining on a .delete() method to delete them: User.objects.raw({'first_name': {'$exists': True}}).delete()","title":"CRUD"},{"location":"sa/10changeStreams.html","text":"Change Streams \u00b6 https://learning.oreilly.com/library/view/mastering-mongodb-6-x/9781803243863/B18155_05.xhtml#_idParaDest-98 Change streams The change streams functionality was introduced in version 3.6 and updated in versions 4.0 and 5.1, making it a safe and efficient way to listen for database changes. Introduction The fundamental problem that change streams solve is the need for applications to react immediately to changes in the underlying data. Modern web applications need to be reactive to data changes and refresh the page view without reloading the entire page. This is one of the problems that frontend frameworks (such as Angular, React, and Vue.js) are solving. When a user performs an action, the frontend framework will submit the request to the server asynchronously and refresh the relevant fragment of the page based on the response from the server. Thinking of a multiuser web application, there are cases where a database change may have occurred as a result of another user\u2019s action. For example, in a project management Kanban board, user A may be viewing the Kanban board, while another user, B, may be changing the status of a ticket from \u201cTo do\u201d to \u201cIn progress.\u201d User A\u2019s view needs to be updated with the change that user B has performed in real time, without refreshing the page. There are already three approaches to this problem, as follows: The most simple approach is to poll the database every X number of seconds and determine if there has been a change. Usually, this code will need to use some kind of status, timestamp, or version number to avoid fetching the same change multiple times. This is simple, yet inefficient, as it cannot scale with a great number of users. Having thousands of users polling the database at the same time will result in a high database-locking rate. To overcome the problems imposed by the first approach, database-and application-level triggers have been implemented. A database trigger relies on the underlying database executing some code in response to a database change. However, the main downside is, again, similar to the first approach in that the more triggers that we add to a database, the slower our database will become. It is also coupled to the database, instead of being a part of the application code base. Finally, we can use the database transaction or replication log to query for the latest changes and react to them. This is the most efficient and scalable approach of the three as it doesn\u2019t put a strain on the database. The database writes to this log anyway; it is usually appended only and our background task serially reads entries as they come into the log. The downside of this method is that it is the most complicated one to implement and one that can lead to nasty bugs if it\u2019s not implemented properly. Change streams provide a way to solve this problem that is developer-friendly and easy to implement and maintain. Change streams are based on the oplog, which is MongoDB\u2019s operations log and contains every operation happening server-wide across all databases on the server. This way, the developer does not have to deal with the server-wide oplog or tailable cursors, which are often not exposed or as easy to develop from the MongoDB language-specific drivers. Also, the developer does not have to decipher and understand any of the internal oplog data structures that are designed and built for MongoDB\u2019s benefit, and not for an application developer. Change streams also have other advantages around security: Users can only create change streams on collections, databases, or deployments that they have read access to. Change streams are also idempotent by design. Even in the case that the application cannot fetch the absolute latest change stream event notification ID, it can resume applying from an earlier known one and it will eventually reach the same state. Finally, change streams are resumable. Every change stream response document includes a resume token. If the application gets out of sync with the database, it can send the latest resume token back to the database and continue processing from there. This token needs to be persisted in the application, as the MongoDB driver won\u2019t keep application failures and restarts. It will only keep state and retry in case of transient network failures and MongoDB replica set elections. Setup A change stream can be opened against a collection, a database, or an entire deployment (such as a replica set or sharded cluster). A change stream will not react to changes in any system collection or any collection in the admin, config, and local databases. A change stream requires a WiredTiger storage engine and replica set protocol version 1 (pv1). pv1 is the only supported version starting from MongoDB 4.0. Change streams are compatible with deployments that use encryption-at-rest. Trabajando con \u00b6 Using change streams To use a change stream, we need to connect to our replica set. A replica set is a prerequisite to using change streams. As change streams internally use the oplog, it\u2019s not possible to work without it. Change streams will also output documents that won\u2019t be rolled back in a replica set setting, so they need to follow a majority read concern. Either way, it\u2019s a good practice to develop and test locally using a replica set, as this is the recommended deployment for production. As an example, we are going to use a signals collection within our database named streams. We will use the following sample Python code: from pymongo import MongoClient class MongoExamples : def __init__ ( self ): self . client = MongoClient ( 'localhost' , 27017 ) db = self . client . streams self . signals = db . signals # a basic watch on signals collection def change_books ( self ): with self . client . watch () as stream : for change in stream : print ( change ) def main (): MongoExamples () . change_books () if __name__ == '__main__' : main () We can open one Terminal and run it using python change_streams.py. Then, in another Terminal, we connect to our MongoDB replica set using the following code: mongo use streams db.signals.insert({value: 114.3, signal:1}) Going back to our first Terminal window, we can now observe that the output is similar to the following code block: {'_id': {'_data': '825BB7A25E0000000129295A1004A34408FB07864F8F960BF14453DFB98546645F696400645BB7A25EE10ED33145BCF7A70004'}, 'operationType': 'insert', 'clusterTime': Timestamp(1538761310, 1), 'fullDocument': {'_id': ObjectId('5bb7a25ee10ed33145bcf7a7'), 'value': 114.3, 'signal': 1.0}, 'ns': {'db': 'streams', 'coll': 'signals'}, 'documentKey': {'_id': ObjectId('5bb7a25ee10ed33145bcf7a7')}} Here, we have opened a cursor that\u2019s watching the entire streams database for changes. Every data update in our database will be logged and outputted in the console. For example, if we go back to the mongo shell, we can issue the following code: db.a_random_collection.insert({test: 'bar'}) The Python code output should be similar to the following code: {'_id': {'_data': '825BB7A3770000000229295A10044AB37F707D104634B646CC5810A40EF246645F696400645BB7A377E10ED33145BCF7A80004'}, 'operationType': 'insert', 'clusterTime': Timestamp(1538761591, 2), 'fullDocument': {'_id': ObjectId('5bb7a377e10ed33145bcf7a8'), 'test': 'bar'}, 'ns': {'db': 'streams', 'coll': 'a_random_collection'}, 'documentKey': {'_id': ObjectId('5bb7a377e10ed33145bcf7a8')}} This means that we are getting notifications for every data update across all the collections in our database. Now, we can change line 11 of our code to the following: with self.signals.watch() as stream: This will result in only watching the signals collection, as should be the most common use case. PyMongo\u2019s watch command can take several parameters, as follows: watch(pipeline=None, full_document='default', resume_after=None, max_await_time_ms=None, batch_size=None, collation=None, start_at_operation_time=None, session=None) The most important parameters are as follows: Pipeline: This is an optional parameter that we can use to define an aggregation pipeline to be executed on each document that matches watch(). Because the change stream itself uses the aggregation pipeline, we can attach events to it. The aggregation pipeline events we can use are as follows: $match $project $addFields $replaceRoot $redact $replaceWith $set $unset Full_document: This is an optional parameter that we can use by setting it to 'updateLookup' to force the change stream to return a copy of the document as it has been modified in the fullDocument field, along with the document\u2019s delta in the updateDescription field. The default None value will only return the document\u2019s delta. start_at_operation_time: This is an optional parameter that we can use to only watch for changes that occurred at, or after, the specified timestamp. session: This is an optional parameter in case our driver supports passing a ClientSession object to watch for updates. Change streams response documents have to be under 16 MB in size. This is a global limit in MongoDB for BSON documents and the change stream has to follow this rule. Specification The following document shows all of the possible fields that a change event response may or may not include, depending on the actual change that happened: { _id : { }, \"operationType\" : \" \", \"fullDocument\" : { }, \"ns\" : { \"db\" : \" \", \"coll\" : \"<collection\" }, \"documentKey\" : { \"_id\" : }, \"updateDescription\" : { \"updatedFields\" : { }, \"removedFields\" : [ \" \", ... ] } \"clusterTime\" : , \"txnNumber\" : , \"lsid\" : { \"id\" : , \"uid\" : } } The most important fields are as follows: Table 5.6 \u2013 Change streams \u2013 the most common events Table 5.6 \u2013 Change streams \u2013 the most common events Important notes When using a sharded database, change streams need to be opened against a MongoDB server. When using replica sets, a change stream can only be opened against any data-bearing instance. Each change stream will open a new connection, as of 4.0.2. If we want to have lots of change streams in parallel, we need to increase the connection pool (as per the SERVER-32946 JIRA MongoDB ticket) to avoid severe performance degradation. Production recommendations Let\u2019s look at some of the best recommendations by MongoDB and expert architects at the time of writing. Replica sets Starting from MongoDB 4.2, a change stream can still be available even if the Read Concern of the majority is not satisfied. The way to enable this behavior is by setting { majority : false }. Invalidating events, such as dropping or renaming a collection, will close the change stream. We cannot resume a change stream after an invalidating event closes it. As the change stream relies on the oplog size, we need to make sure that the oplog size is large enough to hold events until they are processed by the application. We can open a change stream operation against any data-bearing member in a replica set. Sharded clusters On top of the considerations for replica sets, there are a few more to keep in mind for sharded clusters. They are as follows: The change stream is executed against every shard in a cluster and will be as fast as the slowest shard To avoid creating change stream events for orphaned documents, we need to use the new feature of ACID-compliant transactions if we have multi-document updates under sharding We can only open a change stream operation against the mongos member in a sharded cluster. While sharding an unsharded collection (that is, migrating from a replica set to sharding), the documentKey property of the change stream notification document will include _id until the change stream catches up to the first chunk migration.","title":"Change Streams"},{"location":"sa/10changeStreams.html#change-streams","text":"https://learning.oreilly.com/library/view/mastering-mongodb-6-x/9781803243863/B18155_05.xhtml#_idParaDest-98 Change streams The change streams functionality was introduced in version 3.6 and updated in versions 4.0 and 5.1, making it a safe and efficient way to listen for database changes. Introduction The fundamental problem that change streams solve is the need for applications to react immediately to changes in the underlying data. Modern web applications need to be reactive to data changes and refresh the page view without reloading the entire page. This is one of the problems that frontend frameworks (such as Angular, React, and Vue.js) are solving. When a user performs an action, the frontend framework will submit the request to the server asynchronously and refresh the relevant fragment of the page based on the response from the server. Thinking of a multiuser web application, there are cases where a database change may have occurred as a result of another user\u2019s action. For example, in a project management Kanban board, user A may be viewing the Kanban board, while another user, B, may be changing the status of a ticket from \u201cTo do\u201d to \u201cIn progress.\u201d User A\u2019s view needs to be updated with the change that user B has performed in real time, without refreshing the page. There are already three approaches to this problem, as follows: The most simple approach is to poll the database every X number of seconds and determine if there has been a change. Usually, this code will need to use some kind of status, timestamp, or version number to avoid fetching the same change multiple times. This is simple, yet inefficient, as it cannot scale with a great number of users. Having thousands of users polling the database at the same time will result in a high database-locking rate. To overcome the problems imposed by the first approach, database-and application-level triggers have been implemented. A database trigger relies on the underlying database executing some code in response to a database change. However, the main downside is, again, similar to the first approach in that the more triggers that we add to a database, the slower our database will become. It is also coupled to the database, instead of being a part of the application code base. Finally, we can use the database transaction or replication log to query for the latest changes and react to them. This is the most efficient and scalable approach of the three as it doesn\u2019t put a strain on the database. The database writes to this log anyway; it is usually appended only and our background task serially reads entries as they come into the log. The downside of this method is that it is the most complicated one to implement and one that can lead to nasty bugs if it\u2019s not implemented properly. Change streams provide a way to solve this problem that is developer-friendly and easy to implement and maintain. Change streams are based on the oplog, which is MongoDB\u2019s operations log and contains every operation happening server-wide across all databases on the server. This way, the developer does not have to deal with the server-wide oplog or tailable cursors, which are often not exposed or as easy to develop from the MongoDB language-specific drivers. Also, the developer does not have to decipher and understand any of the internal oplog data structures that are designed and built for MongoDB\u2019s benefit, and not for an application developer. Change streams also have other advantages around security: Users can only create change streams on collections, databases, or deployments that they have read access to. Change streams are also idempotent by design. Even in the case that the application cannot fetch the absolute latest change stream event notification ID, it can resume applying from an earlier known one and it will eventually reach the same state. Finally, change streams are resumable. Every change stream response document includes a resume token. If the application gets out of sync with the database, it can send the latest resume token back to the database and continue processing from there. This token needs to be persisted in the application, as the MongoDB driver won\u2019t keep application failures and restarts. It will only keep state and retry in case of transient network failures and MongoDB replica set elections. Setup A change stream can be opened against a collection, a database, or an entire deployment (such as a replica set or sharded cluster). A change stream will not react to changes in any system collection or any collection in the admin, config, and local databases. A change stream requires a WiredTiger storage engine and replica set protocol version 1 (pv1). pv1 is the only supported version starting from MongoDB 4.0. Change streams are compatible with deployments that use encryption-at-rest.","title":"Change Streams"},{"location":"sa/10changeStreams.html#trabajando-con","text":"Using change streams To use a change stream, we need to connect to our replica set. A replica set is a prerequisite to using change streams. As change streams internally use the oplog, it\u2019s not possible to work without it. Change streams will also output documents that won\u2019t be rolled back in a replica set setting, so they need to follow a majority read concern. Either way, it\u2019s a good practice to develop and test locally using a replica set, as this is the recommended deployment for production. As an example, we are going to use a signals collection within our database named streams. We will use the following sample Python code: from pymongo import MongoClient class MongoExamples : def __init__ ( self ): self . client = MongoClient ( 'localhost' , 27017 ) db = self . client . streams self . signals = db . signals # a basic watch on signals collection def change_books ( self ): with self . client . watch () as stream : for change in stream : print ( change ) def main (): MongoExamples () . change_books () if __name__ == '__main__' : main () We can open one Terminal and run it using python change_streams.py. Then, in another Terminal, we connect to our MongoDB replica set using the following code: mongo use streams db.signals.insert({value: 114.3, signal:1}) Going back to our first Terminal window, we can now observe that the output is similar to the following code block: {'_id': {'_data': '825BB7A25E0000000129295A1004A34408FB07864F8F960BF14453DFB98546645F696400645BB7A25EE10ED33145BCF7A70004'}, 'operationType': 'insert', 'clusterTime': Timestamp(1538761310, 1), 'fullDocument': {'_id': ObjectId('5bb7a25ee10ed33145bcf7a7'), 'value': 114.3, 'signal': 1.0}, 'ns': {'db': 'streams', 'coll': 'signals'}, 'documentKey': {'_id': ObjectId('5bb7a25ee10ed33145bcf7a7')}} Here, we have opened a cursor that\u2019s watching the entire streams database for changes. Every data update in our database will be logged and outputted in the console. For example, if we go back to the mongo shell, we can issue the following code: db.a_random_collection.insert({test: 'bar'}) The Python code output should be similar to the following code: {'_id': {'_data': '825BB7A3770000000229295A10044AB37F707D104634B646CC5810A40EF246645F696400645BB7A377E10ED33145BCF7A80004'}, 'operationType': 'insert', 'clusterTime': Timestamp(1538761591, 2), 'fullDocument': {'_id': ObjectId('5bb7a377e10ed33145bcf7a8'), 'test': 'bar'}, 'ns': {'db': 'streams', 'coll': 'a_random_collection'}, 'documentKey': {'_id': ObjectId('5bb7a377e10ed33145bcf7a8')}} This means that we are getting notifications for every data update across all the collections in our database. Now, we can change line 11 of our code to the following: with self.signals.watch() as stream: This will result in only watching the signals collection, as should be the most common use case. PyMongo\u2019s watch command can take several parameters, as follows: watch(pipeline=None, full_document='default', resume_after=None, max_await_time_ms=None, batch_size=None, collation=None, start_at_operation_time=None, session=None) The most important parameters are as follows: Pipeline: This is an optional parameter that we can use to define an aggregation pipeline to be executed on each document that matches watch(). Because the change stream itself uses the aggregation pipeline, we can attach events to it. The aggregation pipeline events we can use are as follows: $match $project $addFields $replaceRoot $redact $replaceWith $set $unset Full_document: This is an optional parameter that we can use by setting it to 'updateLookup' to force the change stream to return a copy of the document as it has been modified in the fullDocument field, along with the document\u2019s delta in the updateDescription field. The default None value will only return the document\u2019s delta. start_at_operation_time: This is an optional parameter that we can use to only watch for changes that occurred at, or after, the specified timestamp. session: This is an optional parameter in case our driver supports passing a ClientSession object to watch for updates. Change streams response documents have to be under 16 MB in size. This is a global limit in MongoDB for BSON documents and the change stream has to follow this rule. Specification The following document shows all of the possible fields that a change event response may or may not include, depending on the actual change that happened: { _id : { }, \"operationType\" : \" \", \"fullDocument\" : { }, \"ns\" : { \"db\" : \" \", \"coll\" : \"<collection\" }, \"documentKey\" : { \"_id\" : }, \"updateDescription\" : { \"updatedFields\" : { }, \"removedFields\" : [ \" \", ... ] } \"clusterTime\" : , \"txnNumber\" : , \"lsid\" : { \"id\" : , \"uid\" : } } The most important fields are as follows: Table 5.6 \u2013 Change streams \u2013 the most common events Table 5.6 \u2013 Change streams \u2013 the most common events Important notes When using a sharded database, change streams need to be opened against a MongoDB server. When using replica sets, a change stream can only be opened against any data-bearing instance. Each change stream will open a new connection, as of 4.0.2. If we want to have lots of change streams in parallel, we need to increase the connection pool (as per the SERVER-32946 JIRA MongoDB ticket) to avoid severe performance degradation. Production recommendations Let\u2019s look at some of the best recommendations by MongoDB and expert architects at the time of writing. Replica sets Starting from MongoDB 4.2, a change stream can still be available even if the Read Concern of the majority is not satisfied. The way to enable this behavior is by setting { majority : false }. Invalidating events, such as dropping or renaming a collection, will close the change stream. We cannot resume a change stream after an invalidating event closes it. As the change stream relies on the oplog size, we need to make sure that the oplog size is large enough to hold events until they are processed by the application. We can open a change stream operation against any data-bearing member in a replica set. Sharded clusters On top of the considerations for replica sets, there are a few more to keep in mind for sharded clusters. They are as follows: The change stream is executed against every shard in a cluster and will be as fast as the slowest shard To avoid creating change stream events for orphaned documents, we need to use the new feature of ACID-compliant transactions if we have multi-document updates under sharding We can only open a change stream operation against the mongos member in a sharded cluster. While sharding an unsharded collection (that is, migrating from a replica set to sharding), the documentKey property of the change stream notification document will include _id until the change stream catches up to the first chunk migration.","title":"Trabajando con"},{"location":"sa/mflix.html","text":"MFlix \u00b6 https://s3.amazonaws.com/edu-downloads.10gen.com/M220P/2022/July/static/handouts/m220/mflix-python.zip Estructura del proyecto \u00b6 verything you will implement is located in the mflix/db.py file, which contains all database interfacing methods. The API will make calls to db.py to interact with MongoDB. The unit tests in tests will test these database access methods directly, without going through the API. The UI will run these methods in integration tests, and therefore requires the full application to be running. The API layer is fully implemented, as is the UI. If you need to run on a port other than 5000, you can edit the index.html file in the build directory to modify the value of window.host. Please do not modify the API layer in any way, movies.py and user.py under the mflix/api directory. Doing so will most likely result in the frontend application failing to validate some of the labs. Preparando el entorno \u00b6 Descargamos y descomprimimos el archivo Dentro de la carpeta, vamos a crear un entorno virtual con venv: virtualenv mflix_venv A continuaci\u00f3n, lo activamos: source mflix_venv/bin/activate E instalamos los requisitos: pip install -r requirements.txt Running the Application In the mflix-python directory you can find a file called dotini. Open this file and enter your Atlas SRV connection string as directed in the comment. This is the information the driver will use to connect. Make sure not to wrap your Atlas SRV connection between quotes: COPY MFLIX_DB_URI = mongodb+srv://... Rename this file to .ini with the following command: COPY mv dotini_unix .ini # on Unix ren dotini_win .ini # on Windows Note: Once you rename this file to .ini, it will no longer be visible in Finder or File Explorer. However, it will be visible from Command Prompt or Terminal, so if you need to edit it again, you can open it from there: COPY vi .ini # on Unix notepad .ini # on Windows Arrancando y Probando \u00b6 Para arrancar la aplicaci\u00f3n ejecutaremos el script run.py : python run.py Al ejecutar el script, arrancar\u00e1 la aplicaci\u00f3n y podremos acceder a ella a trav\u00e9s de http://127.0.0.1:5000/ . PANTALLAZO Si queremos ejecutar los test: Running the Unit Tests To run the unit tests for this course, you will use pytest and needs to be run from mflix-python directory. Each course lab contains a module of unit tests that you can call individually with a command like the following: COPY pytest -m LAB_UNIT_TEST_NAME Each ticket will contain the command to run that ticket's specific unit tests. For example to run the Connection Ticket test your shell command will be: COPY pytest -m connection","title":"MFlix"},{"location":"sa/mflix.html#mflix","text":"https://s3.amazonaws.com/edu-downloads.10gen.com/M220P/2022/July/static/handouts/m220/mflix-python.zip","title":"MFlix"},{"location":"sa/mflix.html#estructura-del-proyecto","text":"verything you will implement is located in the mflix/db.py file, which contains all database interfacing methods. The API will make calls to db.py to interact with MongoDB. The unit tests in tests will test these database access methods directly, without going through the API. The UI will run these methods in integration tests, and therefore requires the full application to be running. The API layer is fully implemented, as is the UI. If you need to run on a port other than 5000, you can edit the index.html file in the build directory to modify the value of window.host. Please do not modify the API layer in any way, movies.py and user.py under the mflix/api directory. Doing so will most likely result in the frontend application failing to validate some of the labs.","title":"Estructura del proyecto"},{"location":"sa/mflix.html#preparando-el-entorno","text":"Descargamos y descomprimimos el archivo Dentro de la carpeta, vamos a crear un entorno virtual con venv: virtualenv mflix_venv A continuaci\u00f3n, lo activamos: source mflix_venv/bin/activate E instalamos los requisitos: pip install -r requirements.txt Running the Application In the mflix-python directory you can find a file called dotini. Open this file and enter your Atlas SRV connection string as directed in the comment. This is the information the driver will use to connect. Make sure not to wrap your Atlas SRV connection between quotes: COPY MFLIX_DB_URI = mongodb+srv://... Rename this file to .ini with the following command: COPY mv dotini_unix .ini # on Unix ren dotini_win .ini # on Windows Note: Once you rename this file to .ini, it will no longer be visible in Finder or File Explorer. However, it will be visible from Command Prompt or Terminal, so if you need to edit it again, you can open it from there: COPY vi .ini # on Unix notepad .ini # on Windows","title":"Preparando el entorno"},{"location":"sa/mflix.html#arrancando-y-probando","text":"Para arrancar la aplicaci\u00f3n ejecutaremos el script run.py : python run.py Al ejecutar el script, arrancar\u00e1 la aplicaci\u00f3n y podremos acceder a ella a trav\u00e9s de http://127.0.0.1:5000/ . PANTALLAZO Si queremos ejecutar los test: Running the Unit Tests To run the unit tests for this course, you will use pytest and needs to be run from mflix-python directory. Each course lab contains a module of unit tests that you can call individually with a command like the following: COPY pytest -m LAB_UNIT_TEST_NAME Each ticket will contain the command to run that ticket's specific unit tests. For example to run the Connection Ticket test your shell command will be: COPY pytest -m connection","title":"Arrancando y Probando"},{"location":"sa/planning.html","text":"Planning (10h) \u00b6 01 NoSQL (1h) \u00b6 Revisar: https://aws.amazon.com/es/nosql/ Revisar: https://www.mongodb.com/es/nosql-explained 02 MongoDB (1h + 1h) \u00b6 Conceptos Uso mediante Docker ... poner enlaces para instalar Mongo desde comandos Restore/dump Consultas sencillas (1h) CRUD En MongoDB, los documentos se almacenan en formato: BSON JSON CSV SQL Para borrar todos los datos de una colecci\u00f3n llamada pruebas usaremos: db.pruebas.deleteMany({}) db.pruebas.deleteOne({}) db.pruebas.drop() db.pruebas.removeCollection() db.pruebas.unset() Sobre la colecci\u00f3n trips trabajada en el aula, para recuperar la duraci\u00f3n de los viajes de las clientes nacidos en los a\u00f1os 1977 y 1981 ejecutaremos la consulta: db.trips.find({ \"birth year\": {$in: [1977, 1981]} }, {tripduration:1}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}, {tripduration:0}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}, {tripduration:1}) 03 Modelado (1h) \u00b6 22 - Relaciones 1 a muchos Para las relaciones uno a muchos: Si hay pocos datos, lo mejor en colocar las referencias en un array. Si hay pocos datos, lo mejor es colocar los documentos embebidos dentro de un array. Si hay muchos datos, lo mejor en colocar las referencias en un array. Si hay muchos datos, lo mejor es colocar los documentos embebidos dentro de un array. Si hay muchos datos, la referencia se coloca en el muchos hacia el 1. Si hay muchos datos, el documento embebido se coloca en el muchos hacia el 1. Si hay much\u00edsimos datos, la referencia se coloca en el muchos hacia el 1. Si hay much\u00edsimos datos, lo mejor en colocar las referencias en un array. Respecto a la validaci\u00f3n de los documentos respecto a un esquema: Es obligatorio definir siempre un esquema. Si hemos a\u00f1adido una validaci\u00f3n, los nuevos documentos no pueden contener nuevos campos S\u00f3lo podemos comprobar la existencia y el tipo de los datos Siempre indicaremos el esquema a validar cuando creamos las colecciones Podemos definir expresiones de validaci\u00f3n entre campos Si tenemos un documento que provoca tener que realizar joins para recuperar unos pocos atributos, usaremos el patron: Atributo Referencia extendida Cubo At\u00edpico Framework de agregaci\u00f3n (1h) \u00b6 Sobre la colecci\u00f3n de productos trabajada en clase, trabajando con el framework de agreagaci\u00f3n, si queremos obtener la cantidad de productos que tenemos de cada tipo, realizaremos: una fase para $group y otra para $count en la fase de $group hacemos $count en la fase de $group hacemos $sum una fase para $gorup y otra para $sum db.productos.aggregate([ { $group: { _id: \"$fabricante\", total: { $sum:1 } } }]) var pipeline = [ { $match: { genres: {$in: [\"Romance\"]}, // Romance movies only. released: {$lte: new ISODate(\"2001-01-01T00:00: 00Z\") }}}, { $sort: {\"imdb.rating\": -1}}, // Sort by IMDB rating. { $limit: 3 }, // Limit to 3 results. { $project: { title: 1, genres: 1, released: 1, \"imdb.rating\": 1}} ]; var pipeline = [ {$group: { _id: \"$rated\", \"numTitles\": { $sum: 1}, }} ]; var pipeline = [ { $match: { released: {$lte: new ISODate(\"2001-01-01T00:00:00Z\") }}}, { $group: { _id: {\"$arrayElemAt\": [\"$genres\", 0]}, \"popularity\": { $avg: \"$imdb.rating\"}, \"top_movie\": { $max: \"$imdb.rating\"}, \"longest_runtime\": { $max: \"$runtime\"} }}, { $sort: { popularity: -1}}, { $project: { _id: 1, popularity: 1, top_movie: 1, adjusted_runtime: { $add: [ \"$longest_runtime\", 12 ] } } } ]; var pipeline = [ { $match: { released: {$lte: new ISODate(\"2001-01-01T00:00:00Z\") }, runtime: {$lte: 218}, \"imdb.rating\": {$gte: 7.0} } }, { $sort: {\"imdb.rating\": -1}}, { $group: { _id: {\"$arrayElemAt\": [\"$genres\", 0]}, \"titulo_recomendado\": {$first: \"$title\"}, \"nota_recomendado\": {$first: \"$imdb.rating\"}, \"tiempo_recomendado\": {$first: \"$runtime\"}, \"popularidad\": { $avg: \"$imdb.rating\"}, \"mejor_nota\": { $max: \"$imdb.rating\"}, \"tiempo_maslargo\": { $max: \"$runtime\"} }}, { $sort: { popularity: -1}}, { $project: { _id: 1, popularidad: 1, mejor_nota: 1, titulo_recomendado: 1, nota_recomendado: 1, tiempo_recomendado: 1, tiempo_ajustado_maslargo: { $add: [ \"$tiempo_maslargo\", 12 ] } } } ]; db.movies.aggregate(pipeline); var pipeline = [ { $group: { _id: \"$movie_id\", \"sumComments\": { $sum: 1} }}, { $sort: { \"sumComments\": -1}}, { $limit: 5}, { $lookup: { from: \"movies\", localField: \"_id\", foreignField: \"_id\", as: \"movie\" }}, { $unwind: \"$movie\" }, { $project: { \"movie.title\": 1, \"movie.imdb.rating\": 1, \"sumComments\": 1, }}, { $out: \"most_commented_movies\" } ]; db.comments.aggregate(pipeline); var pipeline = [ { $match: { \"awards.wins\": { $gte: 1}, genres: {$in: [\"Documentary\"]}, }}, { $sort: {\"awards.wins\": -1}}, // Sort by award wins. { $limit: 3}, { $project: { title: 1, genres: 1, awards: 1}}, ]; 04 Formatos (1h) \u00b6 csv json columnar avro parquet orc 05 Escalabilidad y Rendimiento (2h) \u00b6 MongoAtlas / Compass Cluster ReplicaSet Replicaci\u00f3n Sharding Rendimiento \u00cdndices 06 PyMongo (2h) \u00b6 Pendiente \u00b6 Capped collections: colecciones limitadas","title":"Planning (10h)"},{"location":"sa/planning.html#planning-10h","text":"","title":"Planning (10h)"},{"location":"sa/planning.html#01-nosql-1h","text":"Revisar: https://aws.amazon.com/es/nosql/ Revisar: https://www.mongodb.com/es/nosql-explained","title":"01 NoSQL (1h)"},{"location":"sa/planning.html#02-mongodb-1h-1h","text":"Conceptos Uso mediante Docker ... poner enlaces para instalar Mongo desde comandos Restore/dump Consultas sencillas (1h) CRUD En MongoDB, los documentos se almacenan en formato: BSON JSON CSV SQL Para borrar todos los datos de una colecci\u00f3n llamada pruebas usaremos: db.pruebas.deleteMany({}) db.pruebas.deleteOne({}) db.pruebas.drop() db.pruebas.removeCollection() db.pruebas.unset() Sobre la colecci\u00f3n trips trabajada en el aula, para recuperar la duraci\u00f3n de los viajes de las clientes nacidos en los a\u00f1os 1977 y 1981 ejecutaremos la consulta: db.trips.find({ \"birth year\": {$in: [1977, 1981]} }, {tripduration:1}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}, {tripduration:0}) db.trips.find({ \"birth year\": {$in: [1977, 1981]} , {usertype:\"Customer\"}}, {tripduration:1})","title":"02 MongoDB (1h + 1h)"},{"location":"sa/planning.html#03-modelado-1h","text":"22 - Relaciones 1 a muchos Para las relaciones uno a muchos: Si hay pocos datos, lo mejor en colocar las referencias en un array. Si hay pocos datos, lo mejor es colocar los documentos embebidos dentro de un array. Si hay muchos datos, lo mejor en colocar las referencias en un array. Si hay muchos datos, lo mejor es colocar los documentos embebidos dentro de un array. Si hay muchos datos, la referencia se coloca en el muchos hacia el 1. Si hay muchos datos, el documento embebido se coloca en el muchos hacia el 1. Si hay much\u00edsimos datos, la referencia se coloca en el muchos hacia el 1. Si hay much\u00edsimos datos, lo mejor en colocar las referencias en un array. Respecto a la validaci\u00f3n de los documentos respecto a un esquema: Es obligatorio definir siempre un esquema. Si hemos a\u00f1adido una validaci\u00f3n, los nuevos documentos no pueden contener nuevos campos S\u00f3lo podemos comprobar la existencia y el tipo de los datos Siempre indicaremos el esquema a validar cuando creamos las colecciones Podemos definir expresiones de validaci\u00f3n entre campos Si tenemos un documento que provoca tener que realizar joins para recuperar unos pocos atributos, usaremos el patron: Atributo Referencia extendida Cubo At\u00edpico","title":"03 Modelado (1h)"},{"location":"sa/planning.html#framework-de-agregacion-1h","text":"Sobre la colecci\u00f3n de productos trabajada en clase, trabajando con el framework de agreagaci\u00f3n, si queremos obtener la cantidad de productos que tenemos de cada tipo, realizaremos: una fase para $group y otra para $count en la fase de $group hacemos $count en la fase de $group hacemos $sum una fase para $gorup y otra para $sum db.productos.aggregate([ { $group: { _id: \"$fabricante\", total: { $sum:1 } } }]) var pipeline = [ { $match: { genres: {$in: [\"Romance\"]}, // Romance movies only. released: {$lte: new ISODate(\"2001-01-01T00:00: 00Z\") }}}, { $sort: {\"imdb.rating\": -1}}, // Sort by IMDB rating. { $limit: 3 }, // Limit to 3 results. { $project: { title: 1, genres: 1, released: 1, \"imdb.rating\": 1}} ]; var pipeline = [ {$group: { _id: \"$rated\", \"numTitles\": { $sum: 1}, }} ]; var pipeline = [ { $match: { released: {$lte: new ISODate(\"2001-01-01T00:00:00Z\") }}}, { $group: { _id: {\"$arrayElemAt\": [\"$genres\", 0]}, \"popularity\": { $avg: \"$imdb.rating\"}, \"top_movie\": { $max: \"$imdb.rating\"}, \"longest_runtime\": { $max: \"$runtime\"} }}, { $sort: { popularity: -1}}, { $project: { _id: 1, popularity: 1, top_movie: 1, adjusted_runtime: { $add: [ \"$longest_runtime\", 12 ] } } } ]; var pipeline = [ { $match: { released: {$lte: new ISODate(\"2001-01-01T00:00:00Z\") }, runtime: {$lte: 218}, \"imdb.rating\": {$gte: 7.0} } }, { $sort: {\"imdb.rating\": -1}}, { $group: { _id: {\"$arrayElemAt\": [\"$genres\", 0]}, \"titulo_recomendado\": {$first: \"$title\"}, \"nota_recomendado\": {$first: \"$imdb.rating\"}, \"tiempo_recomendado\": {$first: \"$runtime\"}, \"popularidad\": { $avg: \"$imdb.rating\"}, \"mejor_nota\": { $max: \"$imdb.rating\"}, \"tiempo_maslargo\": { $max: \"$runtime\"} }}, { $sort: { popularity: -1}}, { $project: { _id: 1, popularidad: 1, mejor_nota: 1, titulo_recomendado: 1, nota_recomendado: 1, tiempo_recomendado: 1, tiempo_ajustado_maslargo: { $add: [ \"$tiempo_maslargo\", 12 ] } } } ]; db.movies.aggregate(pipeline); var pipeline = [ { $group: { _id: \"$movie_id\", \"sumComments\": { $sum: 1} }}, { $sort: { \"sumComments\": -1}}, { $limit: 5}, { $lookup: { from: \"movies\", localField: \"_id\", foreignField: \"_id\", as: \"movie\" }}, { $unwind: \"$movie\" }, { $project: { \"movie.title\": 1, \"movie.imdb.rating\": 1, \"sumComments\": 1, }}, { $out: \"most_commented_movies\" } ]; db.comments.aggregate(pipeline); var pipeline = [ { $match: { \"awards.wins\": { $gte: 1}, genres: {$in: [\"Documentary\"]}, }}, { $sort: {\"awards.wins\": -1}}, // Sort by award wins. { $limit: 3}, { $project: { title: 1, genres: 1, awards: 1}}, ];","title":"Framework de agregaci\u00f3n (1h)"},{"location":"sa/planning.html#04-formatos-1h","text":"csv json columnar avro parquet orc","title":"04 Formatos (1h)"},{"location":"sa/planning.html#05-escalabilidad-y-rendimiento-2h","text":"MongoAtlas / Compass Cluster ReplicaSet Replicaci\u00f3n Sharding Rendimiento \u00cdndices","title":"05 Escalabilidad y Rendimiento (2h)"},{"location":"sa/planning.html#06-pymongo-2h","text":"","title":"06 PyMongo (2h)"},{"location":"sa/planning.html#pendiente","text":"Capped collections: colecciones limitadas","title":"Pendiente"}]}