{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e273bf80-bccc-4685-bdeb-b520cac021f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/29 11:49:37 WARN Utils: Your hostname, MacBook-Air-de-Aitor.local resolves to a loopback address: 127.0.0.1; using 192.168.1.203 instead (on interface en0)\n",
      "22/03/29 11:49:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/03/29 11:49:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Zip: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- Revenue: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"s8a-dataframes-sql\").getOrCreate()\n",
    "\n",
    "# Lectura de CSV con el ; como separador de columnas y con encabezado\n",
    "df = spark.read.option(\"sep\",\";\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"pdi_sales_small.csv\")\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57290ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.203:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>s8a-dataframes-sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f956a54a580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6779003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|ProductID|     Date|            Zip|Units|Revenue|Country|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "|      725|1/15/1999|41540          |    1|  115.5|Germany|\n",
      "|      787| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      788| 6/6/2002|41540          |    1|  314.9|Germany|\n",
      "|      940|1/15/1999|22587          |    1|  687.7|Germany|\n",
      "+---------+---------+---------------+-----+-------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e37c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "df = df.withColumn(\"Country\", trim(col(\"Country\"))).withColumn(\"Zip\", trim(col(\"Zip\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b327a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn(\"Date\", to_date(df.Date, \"M/d/yyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a930fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2003|\n",
      "|      2004|\n",
      "|      2001|\n",
      "|      2000|\n",
      "|      1999|\n",
      "|      2002|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df.select(year(\"Date\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9665759b-8e82-48dd-87d3-0b2d94b622ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"ventas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e73cc7-e041-457d-bfc3-11205124e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Country)|\n",
      "+--------------+\n",
      "|        120239|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(Country) from ventas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda3461-5748-4167-bcdc-5e328a90bf5b",
   "metadata": {},
   "source": [
    "## Contando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c6fb23-a359-4efa-aae7-a87b5b8544e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Country)|\n",
      "+--------------+\n",
      "|        120239|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0526ed9d-1ba2-4c8d-a19b-8bd4ab6fee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=================================================>    (185 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+\n",
      "|count(DISTINCT Country)|count(DISTINCT Zip)|\n",
      "+-----------------------+-------------------+\n",
      "|                      4|               2585|\n",
      "+-----------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"Country\"), countDistinct(\"Zip\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f37bd3f5-15bb-4326-b868-88e174b767de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/29 11:50:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 20:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------------+\n",
      "|approx_count_distinct(Country)|approx_count_distinct(Zip)|\n",
      "+------------------------------+--------------------------+\n",
      "|                             4|                      2575|\n",
      "+------------------------------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"Country\"), approx_count_distinct(\"Zip\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3813ce-14f8-42fa-8b3c-ffa9311b1e42",
   "metadata": {},
   "source": [
    "## Calculando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eeff582-9dca-444b-98a5-fc3d70b9e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|min(Units)|max(Units)|\n",
      "+----------+----------+\n",
      "|         1|        77|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Units\"), max(\"Units\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7da3c5-dfbf-4d76-a115-e9987180edd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|sum(Units)|        sum(Revenue)|\n",
      "+----------+--------------------+\n",
      "|    125728|5.0107274999986745E7|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Units\"), sum(\"Revenue\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d64819e-9863-4a4f-a695-f28678b84561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:================================================>     (178 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+\n",
      "|sum(DISTINCT Units)|sum(DISTINCT Revenue)|\n",
      "+-------------------+---------------------+\n",
      "|                308|            1189127.1|\n",
      "+-------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Units\"), sumDistinct(\"Revenue\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4071e20c-d871-4bf7-80ea-1065310b910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------+\n",
      "|     avg(Revenue)|(sum(Revenue) / count(Revenue))|\n",
      "+-----------------+-------------------------------+\n",
      "|416.7306364822291|              416.7306364822291|\n",
      "+-----------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg\n",
    "df.select(avg(\"Revenue\"), sum(\"Revenue\")/count(\"Revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e1ade",
   "metadata": {},
   "source": [
    "## Agrupando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de6d0fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|Country|        sum(Revenue)|\n",
      "+-------+--------------------+\n",
      "|Germany|1.4982119999999512E7|\n",
      "| France|1.2087942100000832E7|\n",
      "| Mexico| 1.139459870000116E7|\n",
      "| Canada|1.1642614200001905E7|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(\"Country\").sum(\"Revenue\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a25b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Country|count|\n",
      "+-------+-----+\n",
      "|Germany|30059|\n",
      "| France|30060|\n",
      "| Mexico|30060|\n",
      "| Canada|30060|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(\"Country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb555e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+\n",
      "|Country|        sum(Revenue)|count(Revenue)|\n",
      "+-------+--------------------+--------------+\n",
      "|Germany|1.4982119999999512E7|         30059|\n",
      "| France|1.2087942100000832E7|         30060|\n",
      "| Mexico| 1.139459870000116E7|         30060|\n",
      "| Canada|1.1642614200001905E7|         30060|\n",
      "+-------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").agg(sum(\"Revenue\"), count(\"Revenue\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6021769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+\n",
      "|Country|count(Zip)|      avg(Revenue)|\n",
      "+-------+----------+------------------+\n",
      "|Germany|     30059| 498.4237665923521|\n",
      "| France|     30060| 402.1271490352905|\n",
      "| Mexico|     30060| 379.0618330007039|\n",
      "| Canada|     30060|387.31251497012323|\n",
      "+-------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Country\").agg({\"Zip\":\"count\", \"Revenue\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5df1cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|Country|   collect_list(Zip)|    collect_set(Zip)|\n",
      "+-------+--------------------+--------------------+\n",
      "|Germany|[22397, 22111, 40...|[22111, 12589, 22...|\n",
      "| France|[75213 CEDEX 16, ...|[06082 CEDEX 1, 0...|\n",
      "| Mexico|[7100, 7810, 9739...|[9739, 10300, 781...|\n",
      "| Canada|[T2X, V6G, V6G, T6V]|     [V6G, T2X, T6V]|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df.where(\"Units > 5\").groupBy(\"Country\").agg(collect_list(\"Zip\"), collect_set(\"Zip\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06c6341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2003|\n",
      "|      2004|\n",
      "|      2001|\n",
      "|      2000|\n",
      "|      1999|\n",
      "|      2002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df.select(year(\"Date\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99fbdcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+\n",
      "|year(Date)|            Canada|            France|           Germany|            Mexico|\n",
      "+----------+------------------+------------------+------------------+------------------+\n",
      "|      2003| 2360085.999999947|1105230.9000000046|1407120.0000000007|         1049457.5|\n",
      "|      2004| 1539140.499999946|              null|              null|              null|\n",
      "|      2001| 2193437.799999908|              null|              null|233419.20000000004|\n",
      "|      2000|1806678.3999999042|1108846.8999999764| 4510606.799999941| 4240448.399999928|\n",
      "|      1999|1382756.6999999764| 7594921.200000435| 5928459.100000297|3419368.2000001906|\n",
      "|      2002|2360514.7999998857| 2278943.099999957| 3135934.099999964|2451905.3999999263|\n",
      "+----------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(year(\"Date\")).pivot(\"Country\").sum(\"Revenue\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4084894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------------+------------------+---------------+------------------+----------------+------------------+---------------+\n",
      "|year(Date)|      Canada_total|Canada_cantidad|      France_total|France_cantidad|     Germany_total|Germany_cantidad|      Mexico_total|Mexico_cantidad|\n",
      "+----------+------------------+---------------+------------------+---------------+------------------+----------------+------------------+---------------+\n",
      "|      2003| 2360085.999999947|           6375|1105230.9000000046|           2794|1407120.0000000007|            3099|         1049457.5|           2510|\n",
      "|      2004| 1539140.499999946|           3636|              null|           null|              null|            null|              null|           null|\n",
      "|      2001| 2193437.799999908|           5976|              null|           null|              null|            null|233419.20000000004|            583|\n",
      "|      2000|1806678.3999999042|           5049|1108846.8999999764|           2456| 4510606.799999941|            9738| 4240448.399999928|          11935|\n",
      "|      1999|1382756.6999999764|           3964| 7594921.200000435|          20432| 5928459.100000297|           12266|3419368.2000001906|           9895|\n",
      "|      2002|2360514.7999998857|           6148| 2278943.099999957|           6057| 3135934.099999964|            6643|2451905.3999999263|           6172|\n",
      "+----------+------------------+---------------+------------------+---------------+------------------+----------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(year(\"Date\")).pivot(\"Country\").agg(sum(\"Revenue\").alias(\"total\"), sum(\"Units\").alias(\"cantidad\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed8889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3cc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d7053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf4ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee6f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a84f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6b4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7d3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
