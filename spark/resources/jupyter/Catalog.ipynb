{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf36e8a-f6dc-4984-8e2a-fc586dbe949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate() # SparkSession de forma programativa\n",
    "sc = spark.sparkContext      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2b83b4-bc6d-4dfc-9b68-a2b85ee0aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/home/jovyan/work/jupyter/spark-warehouse')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791dcc34-efaf-40e9-a31e-4183228902bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentDB = spark.catalog.currentDatabase()\n",
    "currentDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3aa499-2b7b-4684-8e66-8496e13bb250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists s8a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe7300a-ce9a-4b5e-9b60-9ed733d1f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/home/jovyan/work/jupyter/spark-warehouse'),\n",
       " Database(name='s8a', description='', locationUri='file:/home/jovyan/work/jupyter/spark-warehouse/s8a.db')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f6f8a0-c469-4945-a4de-7226a104b71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use database s8a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ec4ca0-1eb2-48ad-b40b-6d1e29e7e78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s8a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c35b7b-fc06-49d7-842f-9f4b10e1c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://iabd-mysql\") \\\n",
    "    .option(\"dbtable\", \"retail_db.customers\") \\\n",
    "    .option(\"port\", \"3306\") \\\n",
    "    .option(\"user\", \"iabd\") \\\n",
    "    .option(\"password\", \"iabd\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3fe034-f7bc-435f-a76a-9a2d8b0f6d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac9b793-ab21-45bb-971a-aab81da933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.createOrReplaceTempView(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0feae606-7149-48a4-857b-03ecadabe929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='clientes', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c46146-81ab-4e56-bd89-21cf555d9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.write.mode(\"errorIfExists\").saveAsTable(\"clientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39d95e9-4015-48a9-9923-42b8d1c9bcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='clientes', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientes', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ac1c351-8489-4f3a-899d-bbaf6a122c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      customer_id|      int|   null|\n",
      "|   customer_fname|   string|   null|\n",
      "|   customer_lname|   string|   null|\n",
      "|   customer_email|   string|   null|\n",
      "|customer_password|   string|   null|\n",
      "|  customer_street|   string|   null|\n",
      "|    customer_city|   string|   null|\n",
      "|   customer_state|   string|   null|\n",
      "| customer_zipcode|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table s8a.clientes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13d15d5e-b8c6-4413-88f1-e4cdb4634667",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"clientesp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f29512-5d7a-4403-b456-a8e0376ca9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF.write.format(\"json\").mode(\"append\").saveAsTable(\"clientesj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d8aa4d-fc4a-41ca-b03e-9eb4f1377990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_fname: string (nullable = true)\n",
      " |-- customer_lname: string (nullable = true)\n",
      " |-- customer_email: string (nullable = true)\n",
      " |-- customer_password: string (nullable = true)\n",
      " |-- customer_street: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_zipcode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clientes = spark.table(\"clientes\")\n",
    "df_clientes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d41f1ef-2879-4e84-b733-96f7aaec22f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='clientes', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientesj', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientesp', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientes', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tablas = spark.catalog.listTables()\n",
    "tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b004eee-466a-4b03-8fa6-ffca16bb844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|      s8a| clientes|      false|\n",
      "|      s8a|clientesj|      false|\n",
      "|      s8a|clientesp|      false|\n",
      "|         | clientes|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e9d35ef-f2f6-49d0-89fd-2a58c5fae4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE clientesp\n",
    "SET TBLPROPERTIES (\n",
    "  'comment' = 'Datos de clientes de retail_db',\n",
    "  'active' = 'true'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7583c3b3-f148-4b5d-bcf1-768a06f1b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|      s8a| clientes|      false|\n",
      "|      s8a|clientesj|      false|\n",
      "|      s8a|clientesp|      false|\n",
      "|         | clientes|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2b942eb-e602-4ae0-b5d7-e235e1987e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='clientes', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientesj', database='s8a', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientesp', database='s8a', description='Datos de clientes de retail_db', tableType='MANAGED', isTemporary=False),\n",
       " Table(name='clientes', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "216a8df7-57d6-4b3a-8375-6ef064882205",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\ncol",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE clientesp ALTER COLUMN customer_id COMMENT \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midentificador del cliente. PK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\ncol"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE clientesp ALTER COLUMN customer_id COMMENT 'identificador del cliente. PK';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8d1501d-6ade-48ae-a9b2-813d9b91a019",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\ncol",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mALTER TABLE clientesp\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    ALTER COLUMN customer_fname\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        COMMENT \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNombre del cliente\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :\ncol"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "ALTER TABLE clientesp\n",
    "    ALTER COLUMN customer_fname\n",
    "        COMMENT 'Nombre del cliente';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b8b3822-963e-4176-bbbd-412805a6565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|      customer_id|      int|   null|\n",
      "|   customer_fname|   string|   null|\n",
      "|   customer_lname|   string|   null|\n",
      "|   customer_email|   string|   null|\n",
      "|customer_password|   string|   null|\n",
      "|  customer_street|   string|   null|\n",
      "|    customer_city|   string|   null|\n",
      "|   customer_state|   string|   null|\n",
      "| customer_zipcode|   string|   null|\n",
      "+-----------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table s8a.clientesp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c467e48-0b9e-4294-b69a-bdacab80c9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='customer_id', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_fname', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_lname', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_email', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_password', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_street', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_city', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_state', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='customer_zipcode', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"clientesp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e746290-db9a-4aaf-871a-517e456ca9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
